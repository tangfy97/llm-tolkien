{"45": "Objection: Code Is Harder to Read When Spread Out\n\u201cCode becomes harder to read when spread out over multiple units.\u201d\nWell, psychology says that is not the case. People have a working memory of about\nseven items, so someone who is reading a unit that is significantly longer than seven\nlines of code cannot process all of it. The exception is probably the original author of\na piece of source code while he or she is working on it (but not a week later).\nWrite code that is easy to read and understand for your successors\n(and for your future self).\nGuideline Encourages Improper Formatting\n\u201cYour guideline encourages improper source code formatting.\u201d\nDo not try to comply with guideline by cutting corners in the area of formatting. We\nare talking about putting multiple statements or multiple curly brackets on one line.\nIt makes the code slightly harder to read and thus decreases its maintainability. Resist\nthe temptation to do so.\nConsider what purpose the guideline really serves. We simply cannot leave unit\nlength unconstrained. That would be akin to removing speed limits in traffic because\nthey discourage being on time. It is perfectly possible to obey speed limits and arrive\non time: just leave home a bit earlier. It is equally possible to write short units. Our\nexperience is that 15 lines of properly formatted code is enough to write useful units.\nAs proof, Table 2-1 presents some data from a typical Java 2 Enterprise Edition sys\u2010\ntem, consisting of Java source files but also some XSD and XSLT. The system, cur\u2010\nrently in production at a SIG client, provides reporting functionality for its owner.\nThe Java part consists of about 28,000 lines of code (a medium-sized system). Of"}
{"46": "Out of the 3,220 units in this system, 3,071 (95.4%) are at most 15 lines of code, while\n149 units (4.6% of all units) are longer. This shows that it is very possible in practice\nto write short units\u2014at least for a vast majority of units.\nAgree on formatting conventions in your team. Keep units short\nand comply with these conventions.\nThis Unit Is Impossible to Split Up\n\u201cMy unit really cannot be split up.\u201d\nSometimes, splitting a method is indeed difficult. Take, for instance, a properly for\u2010\nmatted switch statement in C#. For each case of the switch statement, there is a line\nfor the case itself, at least one line to do anything useful, and a line for the break\nstatement. So, anything beyond four cases becomes very hard to fit into 15 lines of\ncode, and a case statement cannot be split. In Chapter 3, we present some guidelines\non how to deal specifically with switch statements.\nHowever, it is true that sometimes a source code statement simply cannot be split. A\ntypical example in enterprise software is SQL query construction. Consider the fol\u2010\nlowing example (adapted from a real-world system analyzed by the authors of this\nbook):\npublic static void PrintDepartmentEmployees(string department)\n{\nQuery q = new Query();\nforeach (Employee e in q.AddColumn(\"FamilyName\")\n.AddColumn(\"Initials\")\n.AddColumn(\"GivenName\")\n.AddColumn(\"AddressLine1\")\n.AddColumn(\"ZIPcode\")"}
{"47": "expression starting with q.AddColumn(\"FamilyName\") can be extracted into a new\nmethod. But before doing that (and seeing the newly created method grow to over 15\nlines when the query gets more complex in the future), rethink the architecture. Is it\nwise to create a SQL query piece by piece as in this snippet? Should the HTML\nmarkup really appear here? A templating solution such as ASP or Razor may be more\nsuitable for the job at hand.\nSo, if you are faced with a unit that seems impossible to refactor, do not ignore it and\nmove on to another programming task, but indeed raise the issue with your team\nmembers and team lead.\nWhen a refactoring seems possible but doesn\u2019t make sense, rethink\nthe architecture of your system.\nThere Is No Visible Advantage in Splitting Units\n\u201cPutting code in DoSomethingOne, DoSomethingTwo, DoSomethingThree has no benefit\nover putting the same code all together in one long DoSomething.\u201d\nActually, it does, provided you choose better names than DoSomethingOne, DoSome\nthingTwo, and so on. Each of the shorter units is, on its own, easier to understand\nthan the long DoSomething. More importantly, you may not even need to consider all\nthe parts, especially since each of the method names, when chosen carefully, serves as\ndocumentation indicating what the unit of code is supposed to do. Moreover, the long\nDoSomething typically will combine multiple tasks. That means that you can only\nreuse DoSomething if you need the exact same combination. Most likely, you can\nreuse each of DoSomethingOne, DoSomethingTwo, and so on much more easily.\nPut code in short units (at most 15 lines of code) that have carefully"}
{"48": "2.4 See Also\nSee Chapters 3, 4, and 5 for additional refactoring techniques. For a discussion on\nhow to test methods, see Chapter 10.\nHow SIG Rates Unit Size\nThe size (length) of units (methods and constructors in C#) is one of the eight system\nproperties of the SIG/T\u00dcViT Evaluation Criteria for Trusted Product Maintainability.\nTo rate unit size, every unit of the system is categorized in one of four risk categories\ndepending on the number of lines of code it contains. Table 2-2 lists the four risk cat\u2010\negories used in the 2015 version of the SIG/T\u00dcViT Evaluation Criteria.\nThe criteria (rows) in Table 2-2 are conjunctive: a codebase needs to comply with all\nfour of them. For example, if 6.9% of all lines of code are in methods longer than\n60 lines, the codebase can still be rated at 4 stars. However, in that case, at most\n22.3% \u2013 6.9% = 15.4% of all lines of code can be in methods that are longer than 30\nlines but not longer than 60 lines. To the contrary, if a codebase does not have any\nmethods of more than 60 lines of code, at most 22.3% of all lines of code can be in\nmethods that are longer than 30 lines but not longer than 60 lines.\nTable 2-2. Minimum thresholds for a 4-star unit size rating (2015 version of the SIG/\nT\u00dcViT Evaluation Criteria)\nLines of code in methods with \u2026 Percentage allowed for 4 stars for unit size\n\u2026 more than 60 lines of code At most 6.9%\n\u2026 more than 30 lines of code At most 22.3%\n\u2026 more than 15 lines of code At most 43.7%\n\u2026 at most 15 lines of code At least 56.3%\nSee the three quality profiles shown in Figure 2-2 as an example:"}
{"49": "Figure 2-2. Three quality profiles for unit size"}
{"50": ""}
{"51": "CHAPTER 3\nWrite Simple Units of Code\nEach problem has smaller problems inside.\n\u2014Martin Fowler\nGuideline:\n\u2022 Limit the number of branch points per unit to 4.\n\u2022 Do this by splitting complex units into simpler ones and\navoiding complex units altogether.\n\u2022 This improves maintainability because keeping the number of\nbranch points low makes units easier to modify and test.\nComplexity is an often disputed quality characteristic. Code that appears complex to\nan outsider or novice developer can appear straightforward to a developer that is inti\u2010\nmately familiar with it. To a certain extent, what is \u201ccomplex\u201d is in the eye of the"}
{"52": "later). Branch points can be counted for a complete codebase, a class, a namespace, or\na unit. The number of branch points of a unit is equal to the minimum number of\npaths needed to cover all branches created by all branch points of that unit. This is\ncalled branch coverage. However, when you consider all paths through a unit from the\nfirst line of the unit to a final statement, combinatory effects are possible. The reason\nis that it may matter whether a branch follows another in a particular order. All possi\u2010\nble combinations of branches are the execution paths of the unit\u2014that is, the maxi\u2010\nmum number of paths through the unit.\nConsider a unit containing two consecutive if statements. Figure 3-1 depicts the con\u2010\ntrol flow of the unit and shows the difference between branch coverage and execution\npath coverage."}
{"53": "In summary, the number of branch points is the number of paths that cover all\nbranches created by branch points. It is the minimum number of paths and can be\nzero (for a unit that has no branch points). The number of execution paths is a maxi\u2010\nmum, and can be very large due to combinatorial explosion. Which one to choose?\nThe answer is to take the number of branch points plus one. This is called cyclomatic\ncomplexity or McCabe complexity. Consequently, the guideline \u201climit the number of\nbranch points per unit to 4\u201d is equal to \u201climit code McCabe complexity to 5.\u201d This is\nthe minimum number of test cases that you need to cover a unit such that every path\nhas a part not covered by the other paths. The cyclomatic (McCabe) complexity of a\nunit is at least one, which is easy to understand as follows. Consider a unit with no\nbranch points. According to the definition, its cyclomatic complexity is one (number\nof branch points plus one). It also fits intuitively: a unit with no branch points has\none execution path, and needs at least one test.\nFor the sake of completeness: only for units with one exit point, the cyclomatic or\nMcCabe complexity is equal to the number of branch points plus one. It becomes\nmore complex for units with more than one exit point. Do not worry about that:\nfocus on limiting the number of branch points to four.\nThe minimum number of tests needed to cover all independent\nexecution paths of a unit is equal to the number of branch points\nplus one.\nNow consider the following example. Given a nationality, the GetFlagColors method\nreturns the correct flag colors:\npublic IList<Color> GetFlagColors(Nationality nationality)\n{\nList<Color> result;\nswitch (nationality)"}
{"54": "break;\ncase Nationality.UNCLASSIFIED:\ndefault:\nresult = new List<Color> { Color.Gray };\nbreak;\n}\nreturn result;\n}\nThe switch statement in the method body needs to handle all cases of the nationality\nenumeration type and return the correct flag colors. As there are five possible nation\u2010\nalities and the unclassified/default case, the number of isolated paths to be tested\n(control flow branches) is six.\nOn first sight, the GetFlagColors method might seem harmless. Indeed, the method\nis quite readable, and its behavior is as expected. Still, if we want to test the behavior\nof this method, we would need six unique test cases (one for each nationality plus one\nfor the default/unclassified case). Writing automated tests might seem excessive for\nthe GetFlagColors method, but suppose a developer adds the flag of Luxembourg\n(which is very similar to the Dutch flag) as a quick fix:\n...\ncase Nationality.DUTCH:\nresult = new List<Color> { Color.Red, Color.White, Color.Blue };\ncase Nationality.LUXEMBOURGER:\nresult = new List<Color> { Color.Red, Color.White, Color.LightBlue };\nbreak;\ncase Nationality.GERMAN:\n....\nBeing in a hurry, the developer copied the constructor call for the Dutch flag\nand updated the last argument to the right color. Unfortunately, the break statement\nescaped the developer\u2019s attention, and now all Dutch nationalities will see the flag\nfrom Luxembourg on their profile page!\nThis example looks like a forged scenario, but we know from our consultancy prac\u2010"}
{"55": "*/\nprivate static User getOrCreate(string id, string fullName, bool create)\n{\nstring idkey = idStrategy().keyFor(id);\nbyNameLock.readLock().doLock();\nUser u;\ntry\n{\nu = byName.get(idkey);\n}\nfinally\n{\nbyNameLock.readLock().unlock();\n}\nFileInfo configFile = getConfigFileFor(id);\nif (!configFile.Exists && !Directory.Exists(configFile.Directory.FullName))\n{\n// check for legacy users and migrate if safe to do so.\nFileInfo[] legacy = getLegacyConfigFilesFor(id);\nif (legacy != null && legacy.Length > 0)\n{\nforeach (FileInfo legacyUserDir in legacy)\n{\nXmlFile legacyXml = new XmlFile(XmlFile.XSTREAM,\nnew FileInfo(Path.Combine(\nlegacyUserDir.FullName, \"config.xml\")));\ntry\n{\nobject o = legacyXml.read();\nif (o is User)\n{\nif (idStrategy().equals(id, legacyUserDir.Name)\n&& !idStrategy()\n.filenameOf(legacyUserDir.Name)\n.Equals(legacyUserDir.Name))\n{"}
{"56": "}\nelse\n{\nLOGGER.log(Level.FINE,\n\"Unexpected object loaded from {0}: {1}\",\nnew object[] { legacyUserDir, o });\n}\n}\ncatch (IOException e)\n{\nLOGGER.log(Level.FINE,\nstring.Format(\n\"Exception trying to load user from {0}: {1}\",\nnew Object[] { legacyUserDir, e.Message }),\ne);\n}\n}\n}\n}\nif (u == null && (create || configFile.Exists))\n{\nUser tmp = new User(id, fullName);\nUser prev;\nbyNameLock.readLock().doLock();\ntry\n{\nprev = byName.putIfAbsent(idkey, u = tmp);\n}\nfinally\n{\nbyNameLock.readLock().unlock();\n}\nif (prev != null)\n{\nu = prev; // if some has already put a value in the map, use it\nif (LOGGER.isLoggable(Level.FINE)\n&& !fullName.Equals(prev.getFullName()))"}
{"57": "}\ncatch (IOException x)\n{\nLOGGER.log(Level.WARNING, null, x);\n}\n}\n}\nreturn u;\n}\n3.1 Motivation\nBased on the code examples in the previous section, keeping your units simple is\nimportant for two main reasons:\n\u2022 A simple unit is easier to understand, and thus modify, than a complex one.\n\u2022 Simple units ease testing.\nSimple Units Are Easier to Modify\nUnits with high complexity are generally hard to understand, which makes them hard\nto modify. The first code example of the first section was not overly complicated, but\nit would be when it checks for, say, 15 or more nationalities. The second code exam\u2010\nple covers many use cases for looking up or creating users. Understanding the second\ncode example in order to make a functional change is quite a challenge. The time it\ntakes to understand the code makes modification harder.\nSimple Units Are Easier to Test\nThere is a good reason you should keep your units simple: to make the process of\ntesting easier. If there are six control flow branches in a unit, you will need at least six\ntest cases to cover all of them. Consider the GetFlagColors method: six tests to cover"}
{"58": "\u2022 &&, ||\n\u2022 while\n\u2022 for , foreach\n\u2022 catch\nSo how can we limit the number of branch points? Well, this is mainly a matter of\nidentifying the proper causes of high complexity. In a lot of cases, a complex unit\nconsists of several code blocks glued together, where the complexity of the unit is the\nsum of its parts. In other cases, the complexity arises as the result of nested if-then-\nelse statements, making the code increasingly harder to understand with each level\nof nesting. Another possibility is the presence of a long chain of if-then-else state\u2010\nments or a long switch statement, of which the GetFlagColors method in the intro\u2010\nduction is an example.\nEach of these cases has its own problem, and thus, its own solution. The first case,\nwhere a unit consists of several code blocks that execute almost independently, is a\ngood candidate for refactoring using the Extract Method pattern. This way of reduc\u2010\ning complexity is similar to Chapter 2. But what to do when faced with the other\ncases of complexity?\nDealing with Conditional Chains\nA chain of if-then-else statements has to make a decision every time a conditional\nif is encountered. An easy-to-handle situation is the one in which the conditionals\nare mutually exclusive; that is, they each apply to a different situation. This is also the\ntypical use case for a switch statement, like the switch from the GetFlagColors\nmethod.\nThere are many ways to simplify this type of complexity, and selecting the best solu\u2010\ntion is a trade-off that depends on the specific situation. For the GetFlagColors"}
{"59": "Color.Red };\nFLAGS[Nationality.FRENCH] = new List<Color>{ Color.Blue, Color.White,\nColor.Red };\nFLAGS[Nationality.ITALIAN] = new List<Color>{ Color.Green, Color.White,\nColor.Red };\n}\npublic IList<Color> GetFlagColors(Nationality nationality)\n{\nIList<Color> colors = FLAGS[nationality];\nreturn colors ?? new List<Color> { Color.Gray };\n}\nA second, more advanced way to reduce the complexity of the GetFlagColors\nmethod is to apply a refactoring pattern that separates functionality for different flags\nin different flag types. You can do this by applying the Replace Conditional with Poly\u2010\nmorphism pattern: each flag will get its own type that implements a general interface.\nThe polymorphic behavior of the C# language will ensure that the right functionality\nis called during runtime.\nFor this refactoring, we start with a general IFlag interface:\npublic interface IFlag\n{\nIList<Color> Colors { get; }\n}\nand specific flag types for different nationalities, such as for the Dutch:\npublic class DutchFlag : IFlag\n{\npublic IList<Color> Colors\n{\nget\n{\nreturn new List<Color> { Color.Red, Color.White, Color.Blue };\n}\n}"}
{"60": "The GetFlagColors method now becomes even more concise and less error-prone:\nprivate static readonly Dictionary<Nationality, IFlag> FLAGS =\nnew Dictionary<Nationality, IFlag>();\nstatic FlagFactory()\n{\nFLAGS[Nationality.DUTCH] = new DutchFlag();\nFLAGS[Nationality.GERMAN] = new GermanFlag();\nFLAGS[Nationality.BELGIAN] = new BelgianFlag();\nFLAGS[Nationality.FRENCH] = new FrenchFlag();\nFLAGS[Nationality.ITALIAN] = new ItalianFlag();\n}\npublic IList<Color> GetFlagColors(Nationality nationality)\n{\nIFlag flag = FLAGS[nationality];\nflag = flag ?? new DefaultFlag();\nreturn flag.Colors;\n}\nThis refactoring offers the most flexible implementation. For example, it allows the\nflag type hierarchy to grow over time by implementing new flag types and testing\nthese types in isolation. A drawback of this refactoring is that it introduces more code\nspread out over more classes. The developer much choose between extensibility and\nconciseness.\nDealing with Nesting\nSuppose a unit has a deeply nested conditional, as in the following example. Given a\nbinary search tree root node and an integer, the CalculateDepth method determines\nwhether the integer occurs in the tree. If so, the method returns the depth of the inte\u2010\nger in the tree; otherwise, it throws a TreeException:\npublic static int CalculateDepth(BinaryTreeNode<int> t, int n)\n{"}
{"61": "{\nreturn 1 + CalculateDepth(left, n);\n}\n}\nelse\n{\nBinaryTreeNode<int> right = t.Right;\nif (right == null)\n{\nthrow new TreeException(\"Value not found in tree!\");\n}\nelse\n{\nreturn 1 + CalculateDepth(right, n);\n}\n}\n}\n}\nTo improve readability, we can get rid of the nested conditional by identifying the dis\u2010\ntinct cases and insert return statements for these. In terms of refactoring, this is\ncalled the Replace Nested Conditional with Guard Clauses pattern. The result will be\nthe following method:\npublic static int CalculateDepth(BinaryTreeNode<int> t, int n)\n{\nint depth = 0;\nif (t.Value == n)\n{\nreturn depth;\n}\nif ((n < t.Value) && (t.Left != null))\n{\nreturn 1 + CalculateDepth(t.Left, n);\n}\nif ((n > t.Value) && (t.Right != null))\n{"}
{"62": "}\nelse\n{\nreturn TraverseByValue(t, n);\n}\n}\nprivate static int TraverseByValue(BinaryTreeNode<int> t, int n)\n{\nBinaryTreeNode<int> childNode = GetChildNode(t, n);\nif (childNode == null)\n{\nthrow new TreeException(\"Value not found in tree!\");\n}\nelse\n{\nreturn 1 + CalculateDepth(childNode, n);\n}\n}\nprivate static BinaryTreeNode<int> GetChildNode(\nBinaryTreeNode<int> t, int n)\n{\nif (n < t.Value)\n{\nreturn t.Left;\n}\nelse\n{\nreturn t.Right;\n}\n}\nThis actually does decrease the complexity of the unit. Now we have achieved two\nthings: the methods are easier to understand, and they are easier to test in isolation\nsince we can now write unit tests for the distinct functionalities."}
{"63": "it is natural to think that the domain\u2019s complexity carries over to the implementation,\nand that this is an unavoidable fact of life.\nWe argue against this common interpretation. Complexity in the domain does not\nrequire the technical implementation to be complex as well. In fact, it is your respon\u2010\nsibility as a developer to simplify problems such that they lead to simple code. Even if\nthe system as a whole performs complex functionality, it does not mean that units on\nthe lowest level should be complex as well. In cases where a system needs to process\nmany conditions and exceptions (such as certain legislative requirements), one solu\u2010\ntion may be to implement a default, simple process and model the exceptions\nexplicitly.\nIt is true that the more demanding a domain is, the more effort the developer must\nexpend to build technically simple solutions. But it can be done! We have seen many\nhighly maintainable systems solving complex business problems. In fact, we believe\nthat the only way to solve complex business problems and keep them under control is\nthrough simple code.\nObjection: Splitting Up Methods Does Not Reduce Complexity\n\u201cReplacing one method with McCabe 15 by three methods with McCabe 5 each means\nthat overall McCabe is still 15 (and therefore, there are 15 control flow branches overall).\nSo nothing is gained.\u201d\nOf course, you will not decrease the overall McCabe complexity of a system by refac\u2010\ntoring a method into several new methods. But from a maintainability perspective,\nthere is an advantage to doing so: it will become easier to test and understand the\ncode that was written. So, as we already mentioned, newly written unit tests allow you\nto more easily identify the root cause of your failing tests.\nPut your code in simple units (at most four branch points) that\nhave carefully chosen names describing their function and cases."}
{"64": "How SIG Rates Unit Complexity\nThe complexity (McCabe) of units (methods and constructors in C#) is one of the\neight system properties of the SIG/T\u00dcViT Evaluation Criteria for Trusted Product\nMaintainability. To rate unit complexity, every unit of the system is categorized in one\nof four risk categories depending on its McCabe measurement. Table 3-1 lists the four\nrisk categories used in the 2015 version of the SIG/T\u00dcViT Evaluation Criteria.\nThe criteria (rows) in Table 3-1 are conjunctive: a codebase needs to comply with\nall four of them. For example, if 1.5% of all lines of code are in methods with a\nMcCabe over 25, it can still be rated at 4 stars. However, in that case, at most\n10.0% - 1.5% = 8.5% of all lines of code can be in methods that have a McCabe over\n10 but not over 25.\nTable 3-1. Minimum thresholds for a 4-star unit complexity rating (2015 version of the\nSIG/T\u00dcViT Evaluation Criteria)\nLines of code in methods with \u2026 Percentage allowed for 4 stars for unit complexity\n\u2026 a McCabe above 25 At most 1.5%\n\u2026 a McCabe above 10 At most 10.0%\n\u2026 a McCabe above 5 At most 25.2%\n\u2026 a McCabe of at most 5 At least 74.8%\nSee the three quality profiles in Figure 3-2 as an example:\n\u2022 Left: an open source system, in this case Jenkins\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for unit complexity\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic"}
{"65": "CHAPTER 4\nWrite Code Once\nNumber one in the stink parade is duplicated code.\n\u2014Kent Beck and Martin Fowler,\nBad Smells in Code\nGuideline:\n\u2022 Do not copy code.\n\u2022 Do this by writing reusable, generic code and/or calling\nexisting methods instead.\n\u2022 This improves maintainability because when code is copied,\nbugs need to be fixed at multiple places, which is inefficient\nand error-prone.\nCopying existing code looks like a quick win\u2014why write something anew when it"}
{"66": "if (amount.GreaterThan(this.transferLimit))\n{\nthrow new BusinessException(\"Limit exceeded!\");\n}\n// 2. Assuming result is 9-digit bank account number, validate 11-test:\nint sum = 0;\nfor (int i = 0; i < counterAccount.Length; i++)\n{\nsum = sum + (9 - i) * (int)Char.GetNumericValue(\ncounterAccount[i]);\n}\nif (sum % 11 == 0)\n{\n// 3. Look up counter account and make transfer object:\nCheckingAccount acct = Accounts.FindAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\nreturn result;\n}\nelse\n{\nthrow new BusinessException(\"Invalid account number!\");\n}\n}\n}\nGiven the account number of the account to transfer money to (as a string), the Make\nTransfer method creates a Transfer object. MakeTransfer first checks whether the\namount to be transferred does not exceed a certain limit. In this example, the limit is\nsimply hardcoded. MakeTransfer then checks whether the number of the account to\ntransfer the money to complies with a checksum (see the sidebar \u201cThe 11-Check for\nBank Account Numbers\u201d on page 12 for an explanation of the checksum used). If that\nis the case, the object that represents this account is retrieved, and a Transfer object\nis created and returned.\nNow assume the bank introduces a new account type, called a savings account. A sav\u2010\nings account does not have a transfer limit, but it does have a restriction: money can"}
{"67": "int sum = 0;\nfor (int i = 0; i < counterAccount.Length; i++)\n{\nsum = sum + (9 - i) * (int)Char.GetNumericValue(\ncounterAccount[i]);\n}\nif (sum % 11 == 0)\n{\n// 2. Look up counter account and make transfer object:\nCheckingAccount acct = Accounts.FindAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\n// 3. Check whether withdrawal is to registered counter account:\nif (result.CounterAccount.Equals(this.RegisteredCounterAccount))\n{\nreturn result;\n}\nelse\n{\nthrow new BusinessException(\"Counter-account not registered!\");\n}\n}\nelse\n{\nthrow new BusinessException(\"Invalid account number!!\");\n}\n}\n}\nStart of code clone.\nEnd of code clone.\nBoth classes exist in the same codebase. By copying and pasting an existing class, we\nhave introduced some duplicated code in the codebase. There are now two fragments\n(ten lines of code each) of consecutive lines of code that are exactly the same. These\nfragments are called code clones or duplicates."}
{"68": "Coding is about finding generic solutions for specific problems. Either reuse (by call\u2010\ning) an existing, generic method in your codebase, or make an existing method more\ngeneric.\nTypes of Duplication\nWe define a duplicate or code clone as an identical piece of code at least six lines long.\nThe line count excludes whitespace and comments, just like in the regular definition\nof \u201cline of code\u201d (see also Chapter 1). That means that the lines need to be exactly the\nsame to be considered a duplicate. Such clones are called Type 1 clones. It does not\nmatter where the duplicates occur. Two clones can be in the same method, in\ndifferent methods in the same class, or in different methods in different classes in the\nsame codebase. Code clones can cross method boundaries. For instance, if the follow\u2010\ning fragment appears twice in the codebase, it is considered one clone of six lines of\ncode, not two clones of three lines each:\npublic void SetGivenName(string givenName)\n{\nthis.givenName = givenName;\n}\npublic void SetFamilyName(string familyName)\n{\nthis.familyName = familyName;\n}\nThe following two methods are not considered duplicates of each other even though\nthey differ only in literals and the names of identifiers:\npublic void SetPageWidthInInches(float newWidth)\n{\nfloat cmPerInch = 2.54f;\nthis.pageWidthInCm = newWidth * cmPerInch;\n// A few more lines.\n}"}
{"69": "The guideline presented in this chapter is about Type 1 clones, for two reasons:\n\u2022 Source code maintenance benefits most from the removal of Type 1 clones.\n\u2022 Type 1 clones are easier to detect and recognize (both by humans and computers,\nas detecting Type 2 clones requires full parsing).\nThe limit of six lines of code may appear somewhat arbitrary, since other books and\ntools use a different limit. In our experience, the limit of six lines is the right balance\nbetween identifying too many and too few clones. As an example, a ToString method\ncould be three or four lines, and those lines may occur in many domain objects.\nThose clones can be ignored, as they are not what we are looking for\u2014namely, delib\u2010\nerate copies of functionality.\n4.1 Motivation\nTo understand the advantages of a codebase with little duplication, in this section we\ndiscuss the effects that duplication has on system maintainability.\nDuplicated Code Is Harder to Analyze\nIf you have a problem, you want to know how to fix it. And part of that \u201chow\u201d is\nwhere to locate the problem. When you are calling an existing method, you can easily\nfind the source. When you are copying code, the source of the problem may exist\nelsewhere as well. However, the only way to find out is by using a clone detection\ntool. A well-known tool for clone detection is CPD, which is included in a source\ncode analysis tool called PMD. Several editions of Visual Studio come with a clone\ndetection tool built-in.\nThe fundamental problem of duplication is not knowing whether\nthere is another copy of the code that you are analyzing, how many\ncopies exist, and where they are located."}
{"70": "The same problem holds for regular changes. When code is duplicated, changes may\nneed to be made in multiple places, and having many duplicates makes changing a\ncodebase unpredictable.\n4.2 How to Apply the Guideline\nTo avoid the problem of duplicated bugs, never reuse code by copying and pasting\nexisting code fragments. Instead, put it in a method if it is not already in one, so that\nyou can call it the second time that you need it. That is why, as we have covered in the\nprevious chapters, the Extract Method refactoring technique is the workhorse that\nsolves many duplication problems.\nIn the example presented at the beginning of the chapter, the code that implements\nthe checksum (which is part of the duplicate) is an obvious candidate for extraction.\nTo resolve duplication using Extract Method, the duplicate (or a part thereof) is\nextracted into a new method which is then called multiple times, once from each\nduplicate.\nIn Chapter 2, the new extracted method became a private method of the class in\nwhich the long method occurs. That does not work if duplication occurs across\nclasses, as in CheckingAccount and SavingsAccount. One option in that case is to\nmake the extracted method a method of a utility class. In the example, we already\nhave an appropriate class for that (Accounts). So the new static method, IsValid, is\nsimply a method of that class:\npublic static bool IsValid(string number)\n{\nint sum = 0;\nfor (int i = 0; i < number.Length; i++)\n{\nsum = sum + (9 - i) * (int)Char.GetNumericValue(number[i]);\n}\nreturn sum % 11 == 0;"}
{"71": "{\n// 2. Look up counter account and make transfer object:\nCheckingAccount acct = Accounts.FindAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\nreturn result;\n}\nelse\n{\nthrow new BusinessException(\"Invalid account number!\");\n}\n}\n}\nStart of short clone (three lines of code).\nEnd of short clone (three lines of code).\nAnd also in SavingsAccount:\npublic class SavingsAccount\n{\npublic CheckingAccount RegisteredCounterAccount { get; set; }\npublic Transfer MakeTransfer(string counterAccount, Money amount)\n{\n// 1. Assuming result is 9-digit bank account number, validate 11-test:\nif (Accounts.IsValid(counterAccount))\n{\n// 2. Look up counter account and make transfer object:\nCheckingAccount acct = Accounts.FindAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\nif (result.CounterAccount.Equals(this.RegisteredCounterAccount))\n{\nreturn result;\n}\nelse\n{"}
{"72": "Mission accomplished: according to the definition of duplication presented at the\nbeginning of this chapter, the clone has disappeared (because the repeated fragment is\nfewer than six lines of code). But the following issues remain:\n\u2022 Even though according to the definition, the clone has disappeared, there is still\nlogic repeated in the two classes.\n\u2022 The extracted fragment had to be put in a third class, just because in C# every\nmethod needs to be in a class (or struct). The class to which the extracted method\nwas added runs the risk of becoming a hodgepodge of unrelated methods. This\nleads to a large class smell and tight coupling. Having a large class is a smell\nbecause it signals that there are multiple unrelated functionalities within the\nclass. This tends to lead to tight coupling when methods need to know imple\u2010\nmentation details in order to interact with such a large class. (For elaboration, see\nChapter 6.)\nThe refactoring technique presented in the next section solves these problems.\nThe Extract Superclass Refactoring Technique\nIn the preceding code snippets, there are separate classes for a checking account and\na savings account. They are functionally related. However, they are not related in\nC# (they are just two classes that each derive directly from System.Object).\nBoth have common functionality (the checksum validation), which introduced a\nduplicate when we created SavingsAccount by copying and pasting (and modifying)\nCheckingAccount. One could say that a checking account is a special type of a (gen\u2010\neral) bank account, and that a savings account is also a special type of a (general)\nbank account. C# (and other object-oriented languages) has a feature to represent the\nrelationship between something general and something specific: inheritance from a\nsuperclass to a subclass.\nThe Extract Superclass refactoring technique uses this feature by extracting a frag\u2010"}
{"73": "{\n// 2. Look up counter account and make transfer object:\nCheckingAccount acct = Accounts.FindAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\nreturn result;\n}\nelse\n{\nthrow new BusinessException(\"Invalid account number!\");\n}\n}\n}\nStart of extracted clone.\nEnd of extracted clone.\nThe new superclass, Account, contains logic shared by the two types of special\naccounts. You can now turn both the CheckingAccount and SavingsAccount classes\ninto subclasses of this new superclass. For CheckingAccount, the result looks like this:\npublic class CheckingAccount : Account\n{\nprivate int transferLimit = 100;\npublic override Transfer MakeTransfer(string counterAccount, Money amount)\n{\nif (amount.GreaterThan(this.transferLimit))\n{\nthrow new BusinessException(\"Limit exceeded!\");\n}\nreturn base.MakeTransfer(counterAccount, amount);\n}\n}\nThe CheckingAccount class declares its own member, transferLimit, and overrides\nMakeTransfer. The MakeTransfer method first checks to be sure the amount to be"}
{"74": "return result;\n}\nelse\n{\nthrow new BusinessException(\"Counter-account not registered!\");\n}\n}\n}\nThe SavingsAccount class declares RegisteredCounterAccount and, just like Check\ningAccount, overrides MakeTransfer. The MakeTransfer method does not need to\ncheck a limit (because savings accounts do not have a limit). Instead, it calls Make\nTransfer directly in the superclass to create a transfer. It then checks whether the\ntransfer is actually with the registered counter account.\nAll functionality is now exactly where it belongs. The part of making a transfer that is\nthe same for all accounts is in the Account class, while the parts that are specific to\ncertain types of accounts are in their respective classes. All duplication has been\nremoved.\nAs the comments indicate, the MakeTransfer method in the Account superclass has\ntwo responsibilities. Although the duplication introduced by copying and pasting\nCheckingAccount has already been resolved, one more refactoring\u2014extracting the\n11-test to its own method\u2014makes the new Account class even more maintainable:\npublic class Account\n{\npublic Transfer MakeTransfer(string counterAccount, Money amount)\n{\nif (IsValid(counterAccount))\n{\nCheckingAccount acct = Accounts.FindAcctByNumber(counterAccount);\nreturn new Transfer(this, acct, amount);\n}\nelse\n{"}
{"75": "The Account class is now a natural place for IsValid, the extracted method.\n4.3 Common Objections to Avoiding Code Duplication\nThis section discusses common objections regarding code duplication. From our\nexperience, these are developers\u2019 arguments for allowing duplication, such as copying\nfrom other codebases, claiming there are \u201cunavoidable\u201d cases, and insisting that some\ncode will \u201cnever change.\u201d\nCopying from Another Codebase Should Be Allowed\n\u201cCopying and pasting code from another codebase is not a problem because it will not\ncreate a duplicate in the codebase of the current system.\u201d\nTechnically, that is correct: it does not create a duplicate in the codebase of the cur\u2010\nrent system. Copying code from another system may seem beneficial if the code\nsolves the exact same problem in the exact same context. However, in any of the fol\u2010\nlowing situations you will run into problems:\nThe other (original) codebase is still maintained\nYour copy will not benefit from the improvements made in the original codebase.\nTherefore, do not copy, but rather import the functionality needed (that is, add\nthe other codebase to your classpath).\nThe other codebase is no longer maintained and you are working on rebuilding this\ncodebase\nIn this case, you definitely should not copy the code. Often, rebuilds are caused\nby maintainability problems or technology renewals. In the case of maintainabil\u2010\nity issues, you would be defeating the purpose by copying code. You are introduc\u2010\ning code that is determined to be (on average) hard to maintain. In the case of\ntechnology renewals, you would be introducing limitations of the old technology\ninto the new codebase, such as an inability to use abstractions that are needed for"}
{"76": "model variations in the code in such a way that they are explicit, isolated, and\ntestable.\nThis Code Will Never Change\n\u201cThis code will never, ever change, so there is no harm in duplicating it.\u201d\nIf it is absolutely, completely certain that code will never, ever change, duplication\n(and every other aspect of maintainability) is not an issue. For a start, you have to be\nabsolutely, completely certain that the code in question also does not contain any\nbugs that need fixing. Apart from that, the reality is that systems change for many\nreasons, each of which may eventually lead to changes in parts deemed to never, ever\nchange:\n\u2022 The functional requirements of the system may change because of changing users,\nchanging behavior, or a change in the way the organization does business.\n\u2022 The organization may change in terms of ownership, responsibilities, develop\u2010\nment approach, development process, or legislative requirements.\n\u2022 Technology may change, typically in the system\u2019s environment, such as the operat\u2010\ning system, libraries, frameworks, or interfaces to other applications.\n\u2022 Code itself may change, because of bugs, refactoring efforts, or even cosmetic\nimprovements.\nThat is why we argue that most of the time the expectation that code never changes is\nunfounded. So accepting duplication is really nothing more than accepting the risk\nthat someone else will have to deal with it later if it happens.\nYour code will change. Really."}
{"77": "Unit Tests Are Covering Me\n\u201cUnit tests will sort out whether something goes wrong with a duplicate.\u201d\nThis is true only if the duplicates are in the same method, and the unit test of the\nmethod covers both. If the duplicates are in other methods, it can be true only if a\ncode analyzer alerts you if duplicates are changing. Otherwise, unit tests would not\nnecessarily signal that something is wrong if only one duplicate has changed. Hence,\nyou cannot rely only on the tests (identifying symptoms) instead of addressing the\nroot cause of the problem (using duplicate code). You should not assume that even\u2010\ntual problems will be fixed later in the development process, when you could avoid\nthem altogether right now.\nDuplication in String Literals Is Unavoidable and Harmless\n\u201cI need long string literals with a lot of duplication in them. Duplication is unavoidable\nand does not hurt because it is just in literals.\u201d\nThis is a variant of one of the objections discussed in Chapter 2 (\u201cThis unit is impos\u2010\nsible to split\u201d). We often see code that contains long SQL queries or XML or HTML\ndocuments appearing as string literals in C# code. Sometimes such literals are com\u2010\nplete clones, but more often parts of them are repeated. For instance, we have seen\nSQL queries of more than a hundred lines of code that differed only in the sorting\norder (order by asc versus order by desc). This type of duplication is not harmless\neven though technically they are not in the C# logic itself. It is also not unavoidable;\nin fact this type of duplication can be avoided in a straightforward fashion:\n\u2022 Extract to a method that uses string concatenation and parameters to deal with\nvariants.\n\u2022 Use a templating engine to generate HTML output from smaller, nonduplicated\nfragments that are kept in separate files."}
{"78": "How SIG Rates Duplication\nThe amount of duplication is one of the eight system properties of the SIG/T\u00dcViT\nEvaluation Criteria for Trusted Product Maintainability. To rate duplication, all Type\n1 (i.e., textually equal) code clones of at least six lines of code are considered, except\nclones consisting entirely of import statements. Code clones are then categorized in\ntwo risk categories: redundant clones and nonredundant clones, as follows. Take a\nfragment of 10 lines of code that appears three times in the codebase. In other words,\nthere is a group of three code clones, each 10 lines of code. Theoretically, two of these\ncan be removed: they are considered technically redundant. Consequently, 10 + 10 =\n20 lines of code are categorized as redundant. One clone is categorized as nonredun\u2010\ndant, and hence, 10 lines of code are categorized as nonredundant. To be rated at 4\nstars, at most 4.6% of the total number of lines of code in the codebase can be catego\u2010\nrized as redundant. See Table 4-1\nTable 4-1. Minimum thresholds for a 4-star duplication rating (2015 version of the SIG/\nT\u00dcViT Evaluation Criteria)\nLines of code categorized as \u2026 Percentage allowed for 4 stars\n\u2026 nonredundant At least 95.4%\n\u2026 redundant At most 4.6%\nSee the three quality profiles in Figure 4-1 as an example:\n\u2022 Left: an open source system, in this case Jenkins\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for duplication\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic"}
{"79": "CHAPTER 5\nKeep Unit Interfaces Small\nBunches of data that hang around together really ought to be made into their own\nobject.\n\u2014Martin Fowler\nGuideline:\n\u2022 Limit the number of parameters per unit to at most 4.\n\u2022 Do this by extracting parameters into objects.\n\u2022 This improves maintainability because keeping the number of\nparameters low makes units easier to understand and reuse.\nThere are many situations in the daily life of a programmer where long parameter\nlists seem unavoidable. In the rush of getting things done, you might add a few\nparameters more to that one method in order to make it work for exceptional cases."}
{"80": "/// <param name=\"g\">The graphics context to draw on.</param>\n/// <param name=\"x\">The x position to start drawing.</param>\n/// <param name=\"y\">The y position to start drawing.</param>\n/// <param name=\"w\">The width of this square (in pixels.)</param>\n/// <param name=\"h\">The height of this square (in pixels.)</param>\nprivate void Render(Square square, Graphics g, int x, int y, int w, int h)\n{\nsquare.Sprite.Draw(g, x, y, w, h);\nforeach (Unit unit in square.Occupants)\n{\nunit.Sprite.Draw(g, x, y, w, h);\n}\n}\nThis method exceeds the parameter limit of 4. Especially the last four arguments, all\nof type int, make the method harder to understand and its usage more error-prone\nthan necessary. It is not unthinkable that after a long day of writing code, even an\nexperienced developer could mix up the x,y,w and h parameters\u2014a mistake that the\ncompiler and possibly even the unit tests will not catch.\nBecause the x,y,w, and h variables are related (they define a rectangle with a 2D\nanchor point, a width and a height), and the render method does not manipulate\nthese variables independently, it makes sense to group them into an object of type\nRectangle. The next code snippets show the Rectangle class and the refactored\nrender method:\npublic class Rectangle\n{\npublic Point Position { get; set; }\npublic int Width { get; set; }\npublic int Height { get; set; }\npublic Rectangle(Point position, int width, int height)\n{"}
{"81": "{\nPoint position = r.Position;\nsquare.Sprite.Draw(g, position.X, position.Y, r.Width, r.Height);\nforeach (Unit unit in square.Occupants)\n{\nunit.Sprite.Draw(g, position.X, position.Y, r.Width, r.Height);\n}\n}\nNow the render method has only three parameters instead of six. Next to that, in the\nwhole system we now have the Rectangle class available to work with. This allows us\nto also create a smaller interface for the draw method:\nprivate void Render(Square square, Graphics g, Rectangle r)\n{\nPoint position = r.Position;\nsquare.Sprite.Draw(g, r);\nforeach (Unit unit in square.Occupants)\n{\nunit.Sprite.Draw(g, r);\n}\n}\nThe preceding refactorings are an example of the Introduce Parameter Object refac\u2010\ntoring pattern. Avoiding long parameter lists, as shown in the previous example,\nimproves the readability of your code. In the next section, we explain why small inter\u2010\nfaces contribute to the overall maintainability of a system.\n5.1 Motivation\nAs we already discussed in the introduction, there are good reasons to keep interfaces\nsmall and to introduce suitable objects for the parameters you keep passing around in\nconjunction. Methods with small interfaces keep their context simple and thus are\neasier to understand. Furthermore, they are easier to reuse and modify because they\ndo not depend on too much external input."}
{"82": "Methods with Small Interfaces Are Easier to Modify\nLarge interfaces do not only make your methods obscure, but in many cases also\nindicate multiple responsibilities (especially when you feel that you really cannot\ngroup your objects together anymore). In this sense, interface size correlates with unit\nsize and unit complexity. So it is pretty obvious that methods with large interfaces are\nhard to modify. If you have, say, a method with eight parameters and a lot is going on\nin the method body, it can be difficult to see where you can split your method into\ndistinct parts. However, once you have done so, you will have several methods with\ntheir own responsibility, and moreover, each method will have a small number of\nparameters! Now it will be much easier to modify each of these methods, because you\ncan more easily locate exactly where your modification needs to be done.\n5.2 How to Apply the Guideline\nBy the time you have read this, you should be convinced that having small interfaces\nis a good idea. How small should an interface be? In practice, an upper bound of four\nseems reasonable: a method with four parameters is still reasonably clear, but a\nmethod with five parameters is already getting difficult to read and has too many\nresponsibilities.\nSo how can you ensure small interfaces? Before we show you how you can fix meth\u2010\nods with large interfaces, keep in mind that large interfaces are not the problem, but\nrather are indicators of the actual problem\u2014a poor data model or ad hoc code modi\u2010\nfication. So, you can view interface size as a code smell, to see whether your data\nmodel needs improvement.\nLarge interfaces are usually not the main problem; rather, they are a\ncode smell that indicates a deeper maintainability problem."}
{"83": "message1 + message2 + message3);\n// Send message\nm.Send(mId, subject, mMessage);\n}\nThe buildAndSendMail method clearly has too many responsibilities; the construc\u2010\ntion of the email address does not have much to do with sending the actual email.\nFurthermore, you would not want to confuse your fellow programmer with five\nparameters that together will make up a message body! We propose the following\nrevision of the method:\npublic void DoBuildAndSendMail(MailMan m, MailAddress mAddress,\nMailBody mBody)\n{\n// Build the mail\nMail mail = new Mail(mAddress, mBody);\n// Send the mail\nm.SendMail(mail);\n}\npublic class Mail\n{\npublic MailAddress Address { get; set; }\npublic MailBody Body { get; set; }\npublic Mail(MailAddress mAddress, MailBody mBody)\n{\nthis.Address = mAddress;\nthis.Body = mBody;\n}\n}\npublic class MailBody\n{\npublic string Subject { get; set; }\npublic MailMessage Message { get; set; }"}
{"84": "$\"@{division.Substring(0, 5)}.compa.ny\";\n}\n}\nThe buildAndSendMail method is now considerably less complex. Of course, you\nnow have to construct the email address and message body before you invoke the\nmethod. But if you want to send the same message to several addresses, you only have\nto build the message once, and similarly for the case where you want to send a bunch\nof messages to one email address. In conclusion, we have now separated concerns,\nand while we did so we introduced some nice, structured classes.\nThe examples presented in this chapter all group parameters into objects. Such\nobjects are often called data transfer objects or parameter objects. In the examples,\nthese new objects actually represent meaningful concepts from the domain. A point, a\nwidth, and a height represent a rectangle, so grouping these in a class called Rectan\ngle makes sense. Likewise, a first name, a last name, and a division make an address,\nso grouping these in a class called MailAddress makes sense, too. It is not unlikely\nthat these classes will see a lot of use in the codebase because they are useful generali\u2010\nzations, not just because they may decrease the number of parameters of a method.\nWhat if we have a number of parameters that do not fit well together? We can always\nmake a parameter object out of them, but probably, it will be used only once. In such\ncases, another approach is often possible, as illustrated by the following example.\nSuppose we are creating a library that can draw charts, such as bar charts and pie\ncharts, on a System.Drawing.Graphics canvas. To draw a nice-looking chart, you\nusually need quite a bit of information, such as the size of the area to draw on, config\u2010\nuration of the category axis and value axis, the actual dataset to chart, and so forth.\nOne way to supply this information to the charting library is like this:\npublic static void DrawBarChart(Graphics g,\nCategoryItemRendererState state,\nRectangle graphArea,\nCategoryPlot plot,"}
{"85": "public static void DrawBarChart(Graphics g, CategoryDataset dataset)\n{\nCharts.DrawBarChart(g,\nCategoryItemRendererState.DEFAULT,\nnew Rectangle(new Point(0, 0), 100, 100),\nCategoryPlot.DEFAULT,\nCategoryAxis.DEFAULT,\nValueAxis.DEFAULT,\ndataset);\n}\nThis covers the case where we want to use defaults for all parameters whose data\ntypes have a default value defined. However, that is just one case. Before you know it,\nyou are defining more than a handful of alternatives like these. And the version with\nseven parameters is still there.\nAnother way to solve this is to use the Replace Method with Method Object refactoring\ntechnique presented in Chapter 2. This refactoring technique is primarily used to\nmake methods shorter, but it can also be used to reduce the number of method\nparameters.\nTo apply the Replace Method with Method Object technique to this example, we\ndefine a BarChart class like this:\npublic class BarChart\n{\nprivate CategoryItemRendererState state =\nCategoryItemRendererState.DEFAULT;\nprivate Rectangle graphArea = new Rectangle(new Point(0, 0), 100, 100);\nprivate CategoryPlot plot = CategoryPlot.DEFAULT;\nprivate CategoryAxis domainAxis = CategoryAxis.DEFAULT;\nprivate ValueAxis rangeAxis = ValueAxis.DEFAULT;\nprivate CategoryDataset dataset = CategoryDataset.DEFAULT;\npublic BarChart Draw(Graphics g)\n{\n// .."}
{"86": "}\nThe static method drawBarChart from the original version is replaced by the (non\u2010\nstatic) method draw in this class. Six of the seven parameters of drawBarChart have\nbeen turned into of BarChart class. All of these have default values. We have chosen\nto keep parameter g (of type System.Drawing.Graphics) as a parameter of draw. This\nis a sensible choice: draw always needs a Graphics object, and there is no sensible\ndefault value. But it is not necessary: we could also have made g into the seventh pri\u2010\nvate member and supplied a getter and setter for it.\nWe made another choice: all setters return this to create what is called a fluent inter\u2010\nface. The setters can then be called in a cascading style, like so:\nprivate void ShowMyBarChart()\n{\nGraphics g = this.CreateGraphics();\nBarChart b = new BarChart()\n.SetRangeAxis(myValueAxis)\n.SetDataset(myDataset)\n.Draw(g);\n}\nIn this particular call of draw, we provide values for the range axis, dataset, and g, and\nuse default values for the other members of BarChart. We could have used more\ndefault values or fewer, without having to define additional overloaded draw methods.\n5.3 Common Objections to Keeping Unit Interfaces Small\nIt may take some time to get rid of all large interfaces. Typical objections to this effort\nare discussed next.\nObjection: Parameter Objects with Large Interfaces"}
{"87": "Refactoring Large Interfaces Does Not Improve My Situation\n\u201cWhen I refactor my method, I am still passing a lot of parameters to another method.\u201d\nGetting rid of large interfaces is not always easy. It usually takes more than refactor\u2010\ning one method. Normally, you should continue splitting responsibilities in your\nmethods, so that you access the most primitive parameters only when you need to\nmanipulate them separately. For instance, the refactored version of the render\nmethod needs to access all parameters in the Rectangle object because they are input\nto the draw method. But it would be better, of course, to also refactor the draw\nmethod to access the x,y,w, and h parameters inside the method body. In this way,\nyou have just passed a Rectangle in the render method, because you do not actually\nmanipulate its class variables before you begin drawing!\nFrameworks or Libraries Prescribe Interfaces with Long Parameter\nLists\n\u201cThe interface of a framework we\u2019re using has nine parameters. How can I implement\nthis interface without creating a unit interface violation?\u201d\nSometimes frameworks/libraries define interfaces or classes with methods that have\nlong parameter lists. Implementing or overriding these methods will inevitably lead\nto long parameter lists in your own code. These types of violations are impossible to\nprevent, but their impact can be limited. To limit the impact of violations caused by\nthird-party frameworks or libraries, it is best to isolate these violations\u2014for instance,\nby using wrappers or adapters. Selecting a different framework/library is also a viable\nalternative, although this can have a large impact on other parts of the codebase.\n5.4 See Also\nMethods with multiple responsibilities are more likely when the methods are large"}
{"88": "How SIG Rates Unit Interfacing\nUnit interfacing is one of the eight system properties of the SIG/T\u00dcViT Evaluation\nCriteria for Trusted Product Maintainability. To rate unit interfacing, every unit of the\nsystem is categorized in one of four risk categories depending on its number of\nparameters. Table 5-1 lists the four risk categories used in the 2015 version of the SIG/\nT\u00dcViT Evaluation Criteria.\nTable 5-1. Minimum thresholds for a 4-star unit size rating (2015 version of the SIG/\nT\u00dcViT Evaluation Criteria)\nLines of code in methods with \u2026 Percentage allowed for 4 stars for unit interfacing\n\u2026 more than seven parameters At most 0.7%\n\u2026 five or more parameters At most 2.7%\n\u2026 three or more parameters At most 13.8%\n\u2026 at most two parameters At least 86.2%\nSee the three quality profiles shown in Figure 5-1 as an example:\n\u2022 Left: an open source system, in this case Jenkins\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for unit interfacing\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic"}
{"89": "CHAPTER 6\nSeparate Concerns in Modules\nIn a system that is both complex and tightly coupled, accidents are inevitable.\n\u2014Charles Perrow\u2019s Normal Accidents theory in one sentence\nGuideline:\n\u2022 Avoid large modules in order to achieve loose coupling\nbetween them.\n\u2022 Do this by assigning responsibilities to separate modules\nand hiding implementation details behind interfaces.\n\u2022 This improves maintainability because changes in a loosely\ncoupled codebase are much easier to oversee and execute\nthan changes in a tightly coupled codebase.\nThe guidelines presented in the previous chapters are all what we call unit guidelines:"}
{"90": "We will use a true story to illustrate what tight coupling between classes is and why it\nleads to maintenance problems. This story is about how a class called UserService in\nthe service layer of a web application started growing while under development and\nkept on growing until it violated the guideline of this chapter.\nIn the first development iteration, the UserService class started out as a class with\nonly three methods, the names and responsibilities of which are shown in this code\nsnippet:\npublic class UserService\n{\npublic User LoadUser(string userId)\n{\n// ...\n}\npublic bool DoesUserExist(string userId)\n{\n// ...\n}\npublic User ChangeUserInfo(UserInfo userInfo)\n{\n// ...\n}\n}\n// end::UserSerice[]\n}\nIn this case, the backend of the web application provides a REST interface to the\nfrontend code and other systems.\nA REST interface is an approach for providing web services in a simplified manner.\nREST is a common way to expose functionality outside of the system. The class in\nthe REST layer that implements user operations uses the UserService class like this:\npublic class UserController : System.Web.Http.ApiController"}
{"91": "}\n}\nDuring the second development iteration, the UserService class is not modified at\nall. In the third development iteration, new requirements were implemented that\nallowed a user to register to receive certain notifications. Three new methods were\nadded to the UserService class for this requirement:\npublic class UserService\n{\npublic User LoadUser(string userId)\n{\n// ...\n}\npublic bool DoesUserExist(string userId)\n{\n// ...\n}\npublic User ChangeUserInfo(UserInfo userInfo)\n{\n// ...\n}\npublic List<NotificationType> GetNotificationTypes(User user)\n{\n// ...\n}\npublic void RegisterForNotifications(User user, NotificationType type)\n{\n// ...\n}\npublic void UnregisterForNotifications(User user, NotificationType type)\n{"}
{"92": "string notificationType)\n{\nUser user = userService.LoadUser(id);\nuserService.RegisterForNotifications(user,\nNotificationType.FromString(notificationType));\nreturn Ok();\n}\n[System.Web.Http.HttpPost]\n[System.Web.Http.ActionName(\"unregister\")]\npublic System.Web.Http.IHttpActionResult Unregister(string id,\nstring notificationType)\n{\nUser user = userService.LoadUser(id);\nuserService.UnregisterForNotifications(user,\nNotificationType.FromString(notificationType));\nreturn Ok();\n}\n}\nIn the fourth development iteration, new requirements for searching users, blocking\nusers, and listing all blocked users were implemented (management requested that\nlast requirement for reporting purposes). All of these requirements caused new meth\u2010\nods to be added to the UserService class.\npublic class UserService\n{\npublic User LoadUser(string userId)\n{\n// ...\n}\npublic bool DoesUserExist(string userId)\n{\n// ...\n}"}
{"93": "public void UnregisterForNotifications(User user, NotificationType type)\n{\n// ...\n}\npublic List<User> SearchUsers(UserInfo userInfo)\n{\n// ...\n}\npublic void BlockUser(User user)\n{\n// ...\n}\npublic List<User> GetAllBlockedUsers()\n{\n// ...\n}\n}\n// end::UserSerice[]\n}\nAt the end of this development iteration, the class had grown to an impressive size. At\nthis point the UserService class had become the most used service in the service\nlayer of the system. Three frontend views (pages for Profile, Notifications, and\nSearch), connected through three REST API services, used the UserService class.\nThe number of incoming calls from other classes (the fan-in) has increased to over\n50. The size of class has increased to more than 300 lines of code.\nThese kind of classes have what is called the large class smell, briefly discussed in\nChapter 4. The code contains too much functionality and also knows implementation\ndetails about the code that surrounds it. The consequence is that the class is now\ntightly coupled. It is called from a large number of places in the code, and the class\nitself knows details on other parts of the codebase. For example, it uses different data"}
{"94": "find the UserService class increasingly more difficult to understand as it becomes\nlarge and unmanageable. Less experienced developers on the team will find the class\nintimidating and will hesitate to make changes to it.\nTwo principles are necessary to understand the significance of coupling between\nclasses.\n\u2022 Coupling is an issue on the class level of source code. Each of the methods in\nUserService complies with all guidelines presented in the preceding chapters.\nHowever, it is the combination of methods in the UserService class that makes\nUserService tightly coupled with the classes that use it.\n\u2022 Tight and loose coupling are a matter of degree. The actual maintenance conse\u2010\nquence of tight coupling is determined by the number of calls to that class and the\nsize of that class. Therefore, the more calls to a particular class that is tightly cou\u2010\npled, the smaller its size should be. Consider that even when classes are split up,\nthe number of calls may not necessarily be lower. However, the coupling is then\nlower, because less code is coupled.\n6.1 Motivation\nThe biggest advantage of keeping classes small is that it provides a direct path toward\nloose coupling between classes. Loose coupling means that your class-level design will\nbe much more flexible to facilitate future changes. By \u201cflexibility\u201d we mean that you\ncan make changes while limiting unexpected effects of those changes. Thus, loose\ncoupling allows developers to work on isolated parts of the codebase without creating\nchange ripples that affect the rest of the codebase. A third advantage, which cannot be\nunderestimated, is that the codebase as a whole will be much more open to less expe\u2010\nrienced developers.\nThe following sections discuss the advantages of having small, loosely coupled classes"}
{"95": "Small, Loosely Coupled Modules Ease Navigation Through the\nCodebase\nNot only does a good separation of concerns keep the codebase flexible to facilitate\nfuture changes, it also improves the analyzability of the codebase since classes encap\u2010\nsulate data and implement logic to perform a single task. Just as it is easier to name\nmethods that only do one thing, classes also become easier to name and understand\nwhen they have one responsibility. Making sure classes have only one responsibility is\nalso known as the single responsibility principle.\nSmall, Loosely Coupled Modules Prevent No-Go Areas for New\nDevelopers\nClasses that violate the single responsibility principle become tightly coupled and accu\u2010\nmulate a lot of code over time. As with the UserService example in the introduction\nof this chapter, these classes become intimidating to less experienced developers, and\neven experienced developers are hesitant to make changes to their implementation. A\ncodebase that has a large number of classes that lack a good separation of concerns is\nvery difficult to adapt to new requirements.\n6.2 How to Apply the Guideline\nIn general, this guideline prescribes keeping your classes small (by addressing only\none concern) and limiting the number of places where a class is called by code out\u2010\nside the class itself. Following are three development best practices that help to pre\u2010\nvent tight coupling between classes in a codebase.\nSplit Classes to Separate Concerns\nDesigning classes that collectively implement functionality of a software system is the\nmost essential step in modeling and designing object-oriented systems. In typical"}
{"96": "}\npublic void Register(User user, NotificationType type)\n{\n// ...\n}\npublic void Unregister(User user, NotificationType type)\n{\n// ...\n}\n}\npublic class UserBlockService\n{\npublic void BlockUser(User user)\n{\n// ...\n}\npublic IList<User> GetAllBlockedUsers()\n{\n// ...\n}\n}\npublic class UserService\n{\npublic User LoadUser(string userId)\n{\n// ...\n}\npublic bool DoesUserExist(string userId)\n{\n// ...\n}"}
{"97": "more likely to put new functionalities in separate classes instead of defaulting to the\nUserService class.\nHide Specialized Implementations Behind Interfaces\nWe can also achieve loose coupling by hiding specific and detailed implementations\nbehind a high-level interface. Consider the following class, which implements the\nfunctionality of a digital camera that can take snapshots with the flash on or off:\npublic class DigitalCamera\n{\npublic Image TakeSnapshot()\n{\n// ...\n}\npublic void FlashLightOn()\n{\n// ...\n}\npublic void FlashLightOff()\n{\n// ...\n}\n}\nAnd suppose this code runs inside an app on a smartphone device, like this:\npublic class SmartphoneApp\n{\nprivate static DigitalCamera camera = new DigitalCamera();\npublic static void Main(string[] args)\n{\n// ...\nImage image = camera.TakeSnapshot();"}
{"98": "public void FlashLightOn()\n{\n// ...\n}\npublic void FlaslLightOff()\n{\n// ...\n}\npublic Image TakePanoramaSnapshot()\n{\n// ...\n}\npublic Video Record()\n{\n// ...\n}\npublic void SetTimer(int seconds)\n{\n// ...\n}\npublic void ZoomIn()\n{\n// ...\n}\npublic void ZoomOut()\n{\n// ...\n}\n}\nFrom this example implementation, it is not difficult to imagine that the extended\nDigitalCamera"}
{"99": "public interface ISimpleDigitalCamera\n{\nImage TakeSnapshot();\nvoid FlashLightOn();\nvoid FlashLightOff();\n}\npublic class DigitalCamera : ISimpleDigitalCamera\n{\n// ...\n}\npublic class SmartphoneApp\n{\nprivate static ISimpleDigitalCamera camera = SDK.GetCamera();\npublic static void Main(string[] args)\n{\n// ...\nImage image = camera.TakeSnapshot();\n// ...\n}\n}\nThis change leads to lower coupling by a higher degree of encapsulation. In other\nwords, classes that use only basic digital camera functionalities now do not know\nabout all of the advanced digital camera functionalities. The SmartphoneApp class\naccesses only the SimpleDigitalCamera interface. This guarantees that Smart\nphoneApp does not use any of the methods of the more advanced camera.\nAlso, this way your system becomes more modular: it is composed such that a change\nto one class has minimal impact on other classes. This, in turn, increases modifiabil\u2010\nity: it is easier and less work to modify the system, and there is less risk that modifica\u2010\ntions introduce defects."}
{"100": "6.3 Common Objections to Separating Concerns\nThe following are typical objections to the principle explained in this chapter.\nObjection: Loose Coupling Conflicts With Reuse\n\u201cTight coupling is a side effect of code reuse, so this guideline conflicts with that best\npractice.\u201d\nOf course, code reuse can increase the number of calls to a method. However, there\nare two reasons why this should not lead to tight coupling:\n\u2022 Reuse does not necessarily lead to methods that are called from as many places as\npossible. Good software design\u2014for example, using inheritance and hiding\nimplementation behind interfaces\u2014will stimulate code reuse while keeping the\nimplementation loosely coupled, since interfaces hide implementation details.\n\u2022 Making your code more generic, to solve more problems with less code, does not\nmean it should become a tightly coupled codebase. Clearly, utility functionality is\nexpected to be called from more places than specific functionality. Utility func\u2010\ntionality should then also embody less source code. In that way, there may be\nmany incoming dependencies, but the dependencies refer to a small amount of\ncode.\nObjection: C# Interfaces Are Not Just for Loose Coupling\n\u201cIt doesn\u2019t make sense to use C# interfaces to prevent tight coupling.\u201d\nIndeed, using interfaces is a great way to improve encapsulation by hiding implemen\u2010\ntations, but it does not make sense to provide an interface for every class. As a rule of\nthumb, an interface should be implemented by at least two classes in your codebase.\nConsider splitting your class if the only reason to put an interface in front of your"}
{"101": "Objection: Not All Loose Coupling Solutions Increase Maintainability\n\u201cFrameworks that implement inversion of control (IoC) achieve loose coupling but make\nit harder to maintain the codebase.\u201d\nInversion of control is a design principle to achieve loose coupling. There are frame\u2010\nworks available that implement this for you. IoC makes a system more flexible for\nextension and decreases the amount of knowledge that pieces of code have of each\nother.\nThis objection holds when such frameworks add complexity for which the maintain\u2010\ning developers are not experienced enough. Therefore, in cases where this objection\nis true, it is not IoC that is the problem, but the framework that implements it.\nThus, the design decision to use a framework for implementing IoC should be con\u2010\nsidered with care. As with all engineering decisions, this is a trade-off that does not\npay off in all cases. Using these types of frameworks just to achieve loose coupling is a\nchoice that can almost never be justified."}
{"102": "How SIG Rates Module Coupling\nModule coupling is one of the eight system properties of the SIG/T\u00dcViT Evaluation\nCriteria for Trusted Product Maintainability. To rate module coupling, the fan-in of\nevery method is calculated. Each module (class in C#) is then categorized in one of\nfour risk categories depending on the total fan-in of all methods in the class. Table 6-1\nlists the four risk categories used in the 2015 version of the SIG/T\u00dcViT Evaluation\nCriteria. The table shows the maximum amount of code that may fall in the risk cate\u2010\ngories in order to achieve a 4-star rating. For example, a maximum of 21.8% of code\nvolume may be in classes with a fan-in in the moderate risk category, and likewise for\nthe other risk categories.\nTable 6-1. Module coupling risk categories (2015 version of the SIG/T\u00dcViT Evaluation\nCriteria)\nFan-in of modules in the category Percentage allowed for 4 stars\n51+ At most 6.6%\n21\u201350 At most 13.8%\n11\u201320 At most 21.6%\n1\u201310 No constraint\nSee the three quality profiles in Figure 6-1 as an example:\n\u2022 Left: an open source system, in this case Jenkins. Note that Jenkins does not fulfill\nthe 4-star requirement here for the highest risk category (in red).\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for module coupling.\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic."}
{"103": "CHAPTER 7\nCouple Architecture Components Loosely\nThere are two ways of constructing a software design: one way is to make it so simple\nthat there are obviously no deficiencies, and the other way is to make it so complicated\nthat there are no obvious deficiencies.\n\u2014C.A.R. Hoare\nGuideline:\n\u2022 Achieve loose coupling between top-level components.\n\u2022 Do this by minimizing the relative amount of code within\nmodules that is exposed to (i.e., can receive calls from)\nmodules in other components.\n\u2022 This improves maintainability because independent compo\u2010\nnents ease isolated maintenance."}
{"104": "However, the implementation of software architecture always remains the responsi\u2010\nbility of you as a developer.\nComponents should be loosely coupled; that is, they should be clearly separated by\nhaving few entry points for other components and a limited amount of information\nshared among components. In that case, implementation details of methods are hid\u2010\nden (or encapsulated) which makes the system more modular.\nSounds familiar? Yes, both as a general design principle and on a module level, loose\ncoupling has been discussed in Chapter 6. Component coupling applies the same rea\u2010\nsoning but at the higher level of components rather than modules. Module coupling\nfocuses on the exposure of individual modules (classes) to the rest of the codebase.\nComponent coupling focuses specifically on the exposure of modules in one compo\u2010\nnent (group of modules) to the modules in another component.\nSo a module being called from a module in the same component is\nconsidered to be an internal call if we assess at the component level,\nbut when we assess it at the module level, there is module coupling\nindeed.\nIn this chapter, we refer to the characteristic of being loosely coupled on a component\nlevel as component independence. The opposite of component independence is com\u2010\nponent dependence. In that case, the inner workings of components are exposed too\nmuch to other components that rely on them. That kind of entanglement makes it\nharder to oversee effects that code changes in one component may have on others,\nbecause it does not behave in an isolated manner. This complicates testing, when we\nmust make assumptions or simulations of what happens within another component.\n7.1 Motivation\nSystem maintenance is easier when changes within a component have effects that are"}
{"105": "Figure 7-1. Low component dependence (left) and high component dependence (right)\nThe left side of the figure shows a low level of component dependence. Most calls\nbetween modules are internal (within the component). Let us elaborate on internal\nand noninternal dependencies.\nCalls that improve maintainability:\n\u2022 Internal calls are healthy. Since the modules calling each other are part of the\nsame component, they should implement closely related functionality. Their\ninner logic is hidden from the outside.\n\u2022 Outgoing calls are also healthy. As they delegate tasks to other components, they\ncreate a dependency outward. In general, delegation of distinct concerns to other\ncomponents is a good thing. Delegation can be done from anywhere within a\ncomponent and does not need to be restricted to a limited set of modules within\nthe component."}
{"106": "Calls that have a negative impact on maintainability:\n\u2022 Incoming calls provide functionality for other components by offering an inter\u2010\nface. The code volume that is involved in this should be limited. Conversely, the\ncode within a component should be encapsulated as much as possible\u2014that is, it\nshould be shielded against direct invocations from other components. This\nimproves information hiding. Also, modifying code involved in incoming depen\u2010\ndencies potentially has a large impact on other components. By having a small\npercentage of code involved in incoming dependencies, you may dampen the\nnegative ripple effects of modifications to other components.\n\u2022 Throughput code is risky and must be avoided. Throughput code both receives\nincoming calls and delegates to other components. Throughput code accom\u2010\nplishes the opposite of information hiding: it exposes its delegates (implementa\u2010\ntion) to its clients. It is like asking a question to a help desk that does not\nformulate its own answer but instead forwards your question to another com\u2010\npany. Now you are dependent on two parties for the answer. In the case of code,\nthis indicates that responsibilities are not well divided over components. As it is\nhard to trace back the path that the request follows, it is also hard to test and\nmodify: tight coupling may cause effects to spill over to other components.\nThe right side of the figure shows a component with a high level of component\ndependence. The component has many dependencies with modules outside the com\u2010\nponent and is thus tightly coupled. It will be hard to make isolated changes, since the\neffects of changes cannot be easily overseen.\nNote that the effects of component independence are enhanced by\ncomponent balance. Component balance is achieved when the num\u2010\nber of components and their relative size are balanced. For elabora\u2010\ntion on this topic, see Chapter 8."}
{"107": "Figure 7-2. Designed versus implemented architecture\nLow Component Dependence Allows for Isolated Maintenance\nA low level of dependence means that changes can be made in an isolated manner.\nThis applies when most of a component\u2019s code volume is either internal or outgoing.\nIsolated maintenance means less work, as coding changes do not have effects outside\nthe functionality that you are modifying.\nNote that this reasoning about isolation applies to code on a\nsmaller level. For example, a system consisting of small, simple\nclasses signals a proper separation of concerns, but does not guar\u2010\nantee it. For that, you will need to investigate the actual dependen\u2010\ncies (see, for example, Chapter 6).\nLow Component Dependence Separates Maintenance Responsibilities\nIf all components are independent from each other, it is easier to distribute responsi\u2010\nbilities for maintenance among separate teams. This follows from the advantage of\nisolated modification. Isolation is in fact a prerequisite for efficient division of devel\u2010"}
{"108": "Low Component Dependence Eases Testing\nCode that has a low dependence on other components (modules with mainly internal\nand outgoing code) is easier to test. For internal calls, functionality can be traced and\ntested within the component. For outgoing calls, you do not need to mock or stub\nfunctionality that is provided by other components (given that functionality in that\nother component is finished).\nFor elaboration on (unit) testing, see also Chapter 10.\n7.2 How to Apply the Guideline\nThe goal for this chapter\u2019s guideline is to achieve loose coupling between compo\u2010\nnents. In practice, we find that you can help yourself by adhering to the following\nprinciples for implementing interfaces and requests between components.\nThe following principles help you apply the guideline of this chapter:\n\u2022 Limit the size of modules that are the component\u2019s interface.\n\u2022 Define component interfaces on a high level of abstraction. This limits the types\nof requests that cross component borders. That avoids requests that \u201cknow too\nmuch\u201d about the implementation details.\n\u2022 Avoid throughput code, because it has the most serious effect on testing func\u2010\ntionality. In other words, avoid interface modules that put through calls to other\ncomponents. If throughput code exists, analyze the concerned modules in order\nto solve calls that are put through to other components.\nAbstract Factory Design Pattern\nComponent independence reflects the high-level architecture of a software system."}
{"109": "The Abstract Factory design pattern hides (or encapsulates) the creation of specific\n\u201cproducts\u201d behind a generic \u201cproduct factory\u201d interface. In this context, products are\ntypically entities for which more than one variant exists. Examples are audio format\ndecoder/encoder algorithms or user interface widgets that have different themes\nfor \u201clook and feel.\u201d In the following example, we use the Abstract Factory design pat\u2010\nten to encapsulate the specifics of cloud hosting platforms behind a small factory\ninterface.\nSuppose our codebase contains a component, called PlatformServices, that imple\u2010\nments the management of services from a cloud hosting platform. Two specific cloud\nhosting providers are supported by the PlatformServices component: Amazon AWS\nand Microsoft Azure (more could be added in the future).\nTo start/stop servers and reserve storage space, we have to implement the following\ninterface for a cloud hosting platform:\npublic interface ICloudServerFactory\n{\nICloudServer LaunchComputeServer();\nICloudServer LaunchDatabaseServer();\nICloudStorage CreateCloudStorage(long sizeGb);\n}\nBased on this interface, we create two specific factory classes for AWS and Azure:\npublic class AWSCloudServerFactory : ICloudServerFactory\n{\npublic ICloudServer LaunchComputeServer()\n{\nreturn new AWSComputeServer();\n}\npublic ICloudServer LaunchDatabaseServer()\n{"}
{"110": "return new AzureDatabaseServer();\n}\npublic ICloudStorage CreateCloudStorage(long sizeGb) {\nreturn new AzureCloudStorage(sizeGb);\n}\n}\nNote that these factories make calls to specific AWS and Azure implementation\nclasses (which in turn do specific AWS and Azure API calls), but return generic inter\u2010\nface types for servers and storage.\nCode outside the PlatformServices component can now use the concise interface\nmodule ICloudServerFactory\u2014for example, like this:\npublic class ApplicationLauncher\n{\npublic static void Main(string[] args)\n{\nICloudServerFactory factory;\nif (args[1].Equals(\"-azure\"))\n{\nfactory = new AzureCloudServerFactory();\n}\nelse\n{\nfactory = new AWSCloudServerFactory();\n}\nICloudServer computeServer = factory.LaunchComputeServer();\nICloudServer databaseServer = factory.LaunchDatabaseServer();\nThe ICloudServerFactory interface of the PlatformServices provides a small inter\u2010\nface for other components in the codebase. This way, these other components can be\nloosely coupled to it.\n7.3 Common Objections to Loose Component Coupling"}
{"111": "Objection: Component Dependence Cannot Be Fixed Because the\nComponents Are Entangled\n\u201cWe cannot get component dependence right because of mutual dependencies between\ncomponents.\u201d\nEntangled components are a problem that you experience most clearly during main\u2010\ntenance. You should start by analyzing the modules in the throughput category, as it\nhas the most serious effect on the ease of testing and on predicting what exactly the\nfunctionality does.\nWhen you achieve clearer boundaries for component responsibilities, it improves the\nanalyzability and testability of the modules within. For example, modules with an\nextraordinary number of incoming calls may signal that they have multiple responsi\u2010\nbilities and can be split up. When they are split up, the code becomes easier to analyze\nand test. For elaboration, please refer to Chapter 6.\nObjection: No Time to Fix\n\u201cIn the maintenance team, we understand the importance of achieving low component\ndependence, but we are not granted time to fix it.\u201d\nWe understand how this is an issue. Development deadlines are real, and there may\nnot be time for refactoring, or what a manager may see as \u201ctechnical aesthetics.\u201d What\nis important is the trade-off. One should resolve issues that pose a real problem for\nmaintainability. So dependencies should be resolved if the team finds that they inhibit\ntesting, analysis, or stability. You can solidify your case by measuring what percentage\nof issues arises/maintenance effort is needed in components that are tightly coupled\nwith each other.\nFor example, throughput code follows complex paths that are hard to test for devel\u2010\nopers. There may be more elegant solutions that require less time and effort."}
{"112": "Objection: Throughput Is a Requirement\n\u201cWe have a requirement for a software architecture for a layer that puts through calls.\u201d\nIt is true that some architectures are designed to include an intermediate layer. Typi\u2010\ncally, this is a service layer that collects requests from one side (e.g., the user interface)\nand bundles them for passing on to another layer in the system. The existence of such\na layer is not necessarily a problem\u2014given that this layer implements loose coupling.\nIt should have a clear separation of incoming and outgoing requests. So the module\nthat receives requests in this layer:\n\u2022 Should not process the request itself.\n\u2022 Should not know where and how to process that request (its implementation\ndetails).\nIf both are true, the receiving module in the service layer has an incoming request\nand an outgoing request, instead of putting requests through to a specific module in\nthe receiving component.\nA large-volume service layer containing much logic is a typical code smell. In that\ncase, the layer does not merely abstract and pass on requests, but also transforms\nthem. Hence, for transformation, the layer knows about the implementation details.\nThat means that the layer does not properly encapsulate both request and implemen\u2010\ntation. If throughput code follows from software architecture requirements, you may\nraise the issue to the software or enterprise architect.\n7.4 See Also\nA related concept to component independence is that of component balance, dis\u2010\ncussed in Chapter 8. That chapter deals with achieving an overseeable number of\ncomponents that are balanced in size."}
{"113": "\u2022 Hidden code is composed of modules that have no incoming dependencies from\nmodules in other components: they call only within their own component (inter\u2010\nnal) and may have calls outside their own component (outgoing calls).\n\u2022 Interface code is composed of modules that have incoming dependencies from\nmodules in other components. They consist of code in modules with incoming\nand throughput code.\nFollowing the principle of loose coupling, a low level of dependence between modules\nis better than a high level of dependence. That signals the risk that changes within one\ncomponent propagate to other components.\nSIG measures component independence as the percentage of code that is classified as\nhidden code. To achieve a SIG/T\u00dcViT rating of 4 stars for highly-maintainable soft\u2010\nware, the percentage of code residing in modules with incoming dependencies from\nother components (incoming or throughput) should not exceed 14.2%.\nSee the three quality profiles in Figure 7-3 as an example:\n\u2022 Left: an open source system, in this case Jenkins\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for component independence\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic"}
{"114": ""}
{"115": "CHAPTER 8\nKeep Architecture Components Balanced\nBuilding encapsulation boundaries is a crucial skill in software architecture.\n\u2014George H. Fairbanks in Just Enough Architecture\nGuideline:\n\u2022 Balance the number and relative size of top-level compo\u2010\nnents in your code.\n\u2022 Do this by organizing source code in a way that the number\nof components is close to 9 (i.e., between 6 and 12) and that\nthe components are of approximately equal size.\n\u2022 This improves maintainability because balanced components\nease locating code and allow for isolated maintenance.\nA well-balanced software architecture is one with not too many and not too few com\u2010"}
{"116": "narily large, the architecture becomes monolithic; it becomes hard to navigate the\ncodebase and do isolated maintenance. In the third situation (bottom left), where the\narchitecture is scattered among many components, it becomes hard to keep a mental\nmap of the codebase and to grasp how components interact."}
{"117": "8.1 Motivation\nNow we know what component balance is, but not why it is important. The reason is\nsimple: software maintenance is easier when the software architecture is balanced.\nThis section discusses in what ways you can benefit from a good system component\nbalance: it makes it easier to find and analyze code, it better isolates effects of mainte\u2010\nnance, and it separates maintenance responsibilities.\nA Good Component Balance Eases Finding and Analyzing Code\nA clear code organization in components makes it easier to find the piece of code that\nyou want to change. Of course, proper code hygiene helps in this process as well, such\nas using a consistent naming convention (see Chapter 11). When the number of com\u2010\nponents is manageable (around nine) and their volume is consistent, they allow for a\ndrill-down each time that you need to analyze code to modify it.\nIn contrast, an unbalanced organization of components is more likely to have unclear\nfunctional boundaries. For example, a component that is very large compared to oth\u2010\ners is more likely to contain functionalities that are unrelated, and therefore that\ncomponent is harder to analyze.\nA Good Component Balance Better Isolates Maintenance Effects\nWhen a system\u2019s component balance clearly describes functional boundaries, it has a\nproper separation of concerns, which makes for isolated behavior in the system. Iso\u2010\nlated behavior within system components is relevant because it guards against unex\u2010\npected effects, such as regression.\nMore broadly, isolation of code within components has the general advantage of\nmodularity: components with clear functional and technical boundaries are easier to\nsubstitute, remove, and test than components with mixed functionalities and techni\u2010\ncal intertwinement."}
{"118": "This reasoning about isolation applies to code on a smaller level as\nwell. For example, a system that consists of small, simple classes\nsignals a proper separation of concerns, but does not guarantee it.\nFor that you will need to investigate the actual dependencies (see,\ne.g., Chapter 6).\nA Good Component Balance Separates Maintenance Responsibilities\nHaving clear functional boundaries between components makes it easier to distribute\nresponsibilities for maintenance among separate teams. The number of components\nof a system and their relative size should indicate the system\u2019s decomposition into\nfunctional groups.\nWhen a system has too many or too few components, it is considered more difficult\nto understand and harder to maintain. If the number of components is too low, it\ndoes not help you much to navigate through the functionalities of the system. On the\nother hand, too many components make it hard to get a clear overview of the entire\nsystem.\n8.2 How to Apply the Guideline\nThe two principles of component balance are:\n\u2022 The number of top-level system components should ideally be 9, and generally\nbetween 6 and 12.\n\u2022 The components\u2019 volume in terms of source code should be roughly equal.\nNote that component balance is an indicator for a clear component\nseparation, not a goal in itself. It should follow from the system"}
{"119": "Decide on the Right Conceptual Level for Grouping Functionality into\nComponents\nTo achieve a good system division that is easy to navigate for developers, you need to\nchoose the right conceptual level for grouping functionality. Usually, software systems\nare organized along high-level functional domains that describe what kind of func\u2010\ntions the system performs for the user. Alternatively, a division is made along the sep\u2010\narations of technical specialities.\nFor example, a system that bases component division on function domains might\nhave components like Data Retrieval, Invoice Administration, Reporting, Adminis\u2010\ntrator, and so on. Each component contains source code that offers end-to-end func\u2010\ntionality, ranging from the database to the frontend. A functional division has\nthe advantage of being available during design, before development starts. For devel\u2010\nopers, it has the advantage that they can analyze source code while thinking in\nhigh-level functionalities. A disadvantage can be that developers may need to be pro\u2010\nficient and comfortable in multiple technical domains to make changes to a single\ncomponent.\nAn example of a system that uses technical division might have components like\nFrontend, Backend, Interfaces, Logging, and so on. This approach has advantages for\nteams that have a division of responsibilities based on technology specialization. The\ncomponent division then reflects the division of labor among various specialists.\nChoosing the right concepts for grouping functionality within a system is part of the\nsoftware architect role. This role may be assigned to a single person, or it may be dis\u2010\ntributed over various people within the development team. When changes are needed\nto the component division, those in the architect role must be consulted.\nClarify the System\u2019s Domains and Apply Those Consistently\nOnce a choice for the type of system division into components has been made, you"}
{"120": "8.3 Common Objections to Balancing Components\nThis section discusses objections regarding component balance. Common objections\nare that component imbalance is not really a problem, or that it is a problem that can\u2010\nnot be fixed.\nObjection: Component Imbalance Works Just Fine\n\u201cOur system may seem to have bad component balance, but we\u2019re not having any prob\u2010\nlems with it.\u201d\nComponent balance, as we define it, is not binary. There are different degrees of bal\u2010\nance, and its definition allows for some deviation from the \u201cideal\u201d of nine compo\u2010\nnents of equal size. Whether a component imbalance is an actual maintenance\nburden depends on the degree of deviation, the experience of the maintenance team,\nand the cause of the imbalance.\nThe most important maintenance burden occurs when the imbalance is caused by\nlack of discipline during maintenance\u2014when developers do not put code in the com\u2010\nponent where it belongs. Since inconsistency is the enemy of predictability, that may\nlead to unexpected effects. Code that is placed in the wrong components may lead to\nunintended dependencies between components, which hurts testability and flexibility.\nObjection: Entanglement Is Impairing Component Balance\n\u201cWe cannot get component balance right because of entanglement among components.\u201d\nThis situation points to another problem: technical dependence between compo\u2010\nnents. Entanglement between components signals an improper separation of con\u2010\ncerns. This issue and guideline are further described in Chapter 7. In this case, it is\nmore important and urgent to fix component dependencies\u2014for example, by hiding\nimplementation details behind interfaces and fixing circular dependencies. After that,"}
{"121": "How SIG Rates Component Balance\nSIG defines and measures component balance as a combined calculation (i.e., multi\u2010\nplication) of the following:\n\u2022 The number of top-level system components\n\u2022 The uniformity of component size\nThe ideal number of top-level system components is nine, as SIG has identified that as\nthe median in its benchmark. The closer the actual number of components is to nine,\nthe better.\nThe score for the number of top-level system components ranges from 0 to 1. A sys\u2010\ntem with nine components gives a score of 1, linearly decreasing to 0 for a system\nwith one component. A correction is applied upward, to allow for a more lenient\ncount when the number of components is higher than 17, which would otherwise\nlead to a score of 0 with a linear model. The correction is based on the 95th percentile\nscores within the benchmark.\nUniformity of component size means the distribution of source code volume between\ncomponents. An equally sized distribution of top-level components is better than an\nunequal distribution.\nSIG uses the adjusted Gini coefficient as a measure of component size uniformity.\nThe Gini coefficient measures the inequality of distribution between things and\nranges from 0 (perfect equality) to 1 (perfect inequality).\nTo achieve a SIG/T\u00dcViT rating of 4 stars for highly-maintainable software, the num\u2010\nber of components should be close to the ideal of nine, and the adjusted Gini coeffi\u2010\ncient of the component sizes should be 0.71 maximum.\nSee the volume charts in Figure 8-2 as an example:"}
{"122": ""}
{"123": "CHAPTER 9\nKeep Your Codebase Small\nProgram complexity grows until it exceeds the capability of the programmer who must\nmaintain it.\n\u20147th Law of Computer Programming\nGuideline:\n\u2022 Keep your codebase as small as feasible.\n\u2022 Do this by avoiding codebase growth and actively reducing\nsystem size.\n\u2022 This improves maintainability because having a small prod\u2010\nuct, project, and team is a success factor.\nA codebase is a collection of source code that is stored in one repository, can be com\u2010\npiled and deployed independently, and is maintained by one team. A system has at"}
{"124": "Given two systems with the same functionality, in which one has a small codebase\nand the other has a large codebase, you surely would prefer the small system. In a\nsmall system it is easier to search through, analyze, and understand code. If you mod\u2010\nify something, it is easier to tell whether the change has effects elsewhere in the sys\u2010\ntem. This ease of maintenance leads to fewer mistakes and lower costs. That much is\nobvious.\n9.1 Motivation\nSoftware development and maintenance become increasingly hard with growing sys\u2010\ntem size. Building larger systems requires larger teams and longer-lasting projects,\nwhich bring additional overhead and risks of (project) failure. The rest of this section\ndiscusses the adverse effects of large software systems.\nA Project That Sets Out to Build a Large Codebase Is More Likely to\nFail\nThere is a strong correlation between project size and project risks. A large project\nleads to a larger team, complex design, and longer project duration. As a result, there\nis more complex communication and coordination among stakeholders and team\nmembers, less overview over the software design, and a larger number of require\u2010\nments that change during the project. This all increases the chance of reduced quality,\nproject delays, and project failure. The probabilities in the graph in Figure 9-1 are\ncumulative: for example, for all projects over 500 man-years of development effort,\nmore than 90% are indentified as \u201cpoor project quality.\u201d A subset of this is projects\nwith delays (80\u201390% of the total) and failed projects (50% of the total).\nFigure 9-1 illustrates the relationship between project size and project failure: it\nshows that as the size of a project increases, the chances of project failure (i.e., project\nis terminated or does not deliver results), of project delay, and of a project delivered\nwith poor quality are increasingly high."}
{"125": "Figure 9-1. Probability of project failures by project size1"}
{"126": "Large Codebases Are Harder to Maintain\nFigure 9-2 illustrates how codebase size affects maintainability.\nFigure 9-2. Distribution of system maintainability in SIG benchmark among different\nvolume groups\nThe graph is based on a set of codebases of over 1,500 systems in the SIG Software\nAnalysis Warehouse. Volume is measured as the amount of development effort in\nman-years to reproduce the system (see also \u201cHow SIG Rates Codebase Volume\u201d on\npage 110). Each bar shows the distribution of systems in different levels of maintaina\u2010\nbility (benchmarked in stars). As the graph shows, over 30% of systems in the small\u2010\nest volume category manage to reach 4- or 5-star maintainability, while in the largest"}
{"127": "Figure 9-3. Impact of code volume on the number of defects2\n9.2 How to Apply the Guideline\nAll other things being equal, a system that has less functionality will be smaller than a\nsystem that has more functionality. Then, the implementation of that functionality\nmay be either concise or verbose. Therefore, achieving a small codebase first requires\nkeeping the functionality of a system limited, and then requires attention to keep the\namount of code limited.\nFunctional Measures\nFunctionality-related measures are not always within your span of control, but when\u2010\never new or adapted functionality is being discussed with developers, the following\nshould be considered:"}
{"128": "Standardize functionality:\nBy standardization of functionality we mean consistency in the behavior and\ninteractions of the program. First of all, this is intended to avoid the implementa\u2010\ntion of the same core functionality in multiple, slightly different ways. Secondly,\nstandardization of functionality offers possibilities for reuse of code\u2014assuming\nthe code itself is written in a reusable way.\nTechnical Measures\nFor the technical implementation, the goal is to use less code to implement the same\nfunctionality. You can achieve this mainly through reusing code by referral (instead\nof writing or copying and pasting code again) or by avoiding coding altogether, but\nusing existing libraries or frameworks.\nDo not copy and paste code:\nReferring to existing code is always preferable to copying and pasting code in\npieces that will need to be maintained individually. If there are multiple copies of\na piece of code, maintenance needs to occur in multiple places, too. Mistakes\neasily crop up if an update in one piece of logic requires individual adjustment\n(or not) and testing of multiple, scattered copies. Note that the intention of the\nguideline presented in Chapter 4 is precisely to avoid copying and pasting.\nRefactor existing code:\nWhile refactoring has many merits for code maintainability, it can have an imme\u2010\ndiate and visible effect in reducing the codebase. Typically, refactoring involves\nrevisiting code, simplifying its structure, removing code redundancies, and\nimproving the amount of reuse. This may be as simple as removing unused/obso\u2010\nlete functionality. See, for example, the refactoring patterns in Chapter 4.\nUse third-party libraries and frameworks:\nMany applications share the same type of behavior for which a vast number of\nframeworks and libraries exist\u2014for example, UI behavior (e.g., jQuery), database"}
{"129": "Do not make changes to the source code of a third-party library. If\nyou do, essentially you have made the library code part of your\nown codebase. In particular, updates of changed libraries are cum\u2010\nbersome and can easily lead to bugs. Typically, difficulties arise\nwhen developers try to update the library to a newer version, since\nthey need to analyze what has been changed in the library code and\nhow that impacts the locally changed code.\nSplit up a large system:\nSplitting up a large system into multiple smaller systems is a way to minimize the\nissues that come with larger systems. A prerequisite is that the system can be\ndivided into parts that are independent, from a functional, technical, and lifecycle\nperspective. To the users, the systems (or plugins) must be clearly separated.\nTechnically, the code in the different systems must be loosely coupled; that is,\ntheir code is related via interfaces instead of direct dependencies. Systems are\nonly really independent if their lifecycles are decoupled (i.e., they are developed\nand released independently). Note that the split systems may well have some\nmutual or shared dependencies. There is an additional advantage. It might turn\nout that some of the new subsystems can be replaced by a third-party package,\ncompletely removing the need to have any codebase for this subsystem. An\nexample is a Linux distribution such as Ubuntu. The Linux kernel is a codebase\nthat lives at kernel.org and is maintained by a large team of volunteers headed by\nLinus Torvalds. Next to the actual Linux kernel, a distribution contains thou\u2010\nsands of other software applications, each of which has its own codebase. These\nare the types of plugins that we mean here.\nDecoupling (on a code level) is discussed in more detail in the chapters that deal with\nloose coupling, particularly Chapter 7.\n9.3 Common Objections to Keeping the Codebase Small"}
{"130": "The most visible improvements will appear once a system is big and parts of it can be\nremoved\u2014for example, when functionality is being replaced by third-party code or\nafter a system has been split into multiple parts.\nObjection: Reducing the Codebase Size Is Impeded by Productivity\nMeasures\n\u201cI cannot possibly reduce the size of my system, since my programming productivity is\nbeing measured in terms of added code volume.\u201d\nIf this is the case, we suggest escalating this issue. Measuring development productiv\u2010\nity in terms of added code volume is a bad practice. It provides a negative incentive,\nas it encourages the bad habit of copying and pasting code. Code reference is better\nbecause it improves analyzing, testing, and changing code.\nWe understand that the number of code additions can help managers monitor pro\u2010\ngress and predict timelines. However, productivity should be measured in terms of\nvalue added, not lines of code added. Experienced developers can often add function\u2010\nality with a minimum number of additional lines of code, and they will refactor the\ncode whenever they see an opportunity, often resulting in reduction of the code size.\nObjection: Reducing the Codebase Size is Impeded by the\nProgramming Language\n\u201cI work with a language that is more verbose than others, so I cannot achieve a small\ncodebase.\u201d\nIn most projects, the programming language is a given. It may very well be true that\nin some programming languages, it is impossible to get a small codebase (SQL-based\nlanguages come to mind). However, you can always strive to get a smaller codebase\nthan you currently have, in the same programming language. Every codebase benefits\nfrom decreasing its size, even those in low-level languages with little possibility for"}
{"131": "original functionality can be split up into multiple parts, then ideally you end up with\na piece of code that can be referred to independently by the new functionality, avoid\u2010\ning duplication and taming codebase growth. Write unit tests for the new units to\nverify that you understand the inner workings of the unit. Besides, it is recommended\npractice; see Chapter 10.\nObjection: Splitting the Codebase Is Impossible Because of Platform\nArchitecture\n\u201cWe cannot split the system into smaller parts because we are building for a platform\nwhere all functionality is tied to a common codebase.\u201d\nYes, platform-based software tends to grow large over time because it assimilates new\nfunctionality and rarely reduces functionality. One way to dramatically decrease the\nsize of the codebase is to decouple the system into a plug-in architecture. This leads\nto multiple codebases that are each smaller than the original one. There is a codebase\nfor the common core, and one or more codebases for the plugins. If those plugins are\ntechnically decoupled, they allow for separate release cycles. That means that small\nchanges in functionality do not need an update of the whole system. Keep in mind\nthat those small updates still need full integration/regression tests to ensure that the\nsystem as a whole still functions as expected.\nObjection: Splitting the Codebase Leads to Duplication\n\u201cSplitting the codebase forces me to duplicate code.\u201d\nThere may be cases in which decoupling a system into separate parts (such as plu\u2010\ngins/extensions) requires (interfaces to) common functionality or data structures to\nbe duplicated in those extensions.\nIn such a case, duplication is a bigger problem than having a large codebase, and the\nguideline of Chapter 4 prevails over this guideline of achieving a small codebase. It is\nthen preferable to code common functionality either as a separate extension or as"}
{"132": "Keep in mind that the goal is to have subsystems that can be\nmaintained independently, not necessarily systems that operate\nindependently.\nHow SIG Rates Codebase Volume\nThe metric for codebase volume does not have different risk categories, since it con\u2010\nsists of only one metric. To be rated at 4 stars, the codebase should be at most equiva\u2010\nlent to 20 man-years of rebuild value. If C# is the only technology in a system, this\ntranslates to at most 160,000 lines of code.\nMan-months and man-years\nThe total volume in a codebase is the volume in lines of code converted to man-\nmonths. A man-month is a standard measure of source code volume. It is the amount\nof source code that one developer with average productivity could write in one\nmonth. The advantage of \u201cman-month\u201d is that it allows for comparisons of source\ncode volume between technologies. This is relevant because programming languages\nhave different productivity measures, or \u201clevels of verbosity.\u201d Therefore, a system with\nmultiple programming languages can be converted to an aggregate measure that tells\nyou the approximate effort it would take to rebuild it: the \u201crebuild value.\u201d\nSIG\u2019s experience has shown that the man-month is an effective metric to assess the\nsize of a system and to compare systems with each other. A man-year is simply 12\nman-months. Of course, actual productivity is also dependent on skill and program\u2010\nming style. The volume metric does not tell you how many months or years of effort\nactually went into building the system."}
{"133": "CHAPTER 10\nAutomate Tests\nKeep the bar green to keep the code clean.\n\u2014The jUnit motto\nGuideline:\n\u2022 Automate tests for your codebase.\n\u2022 Do this by writing automated tests using a test framework.\n\u2022 This improves maintainability because automated testing\nmakes development predictable and less risky.\nIn Chapter 4, we have presented IsValid, a method to check whether bank account\nnumbers comply with a checksum. That method contains a small algorithm that\nimplements the checksum. It is easy to make mistakes in a method like this. That is\nwhy probably every programmer in the world at some point has written a little, one-"}
{"134": "Console.WriteLine(\"Type a bank account number on the next line.\");\nacct = Console.ReadLine();\nConsole.WriteLine($\"Bank account number '{acct}' is\" +\n(Accounts.IsValid(acct) ? \"\" : \" not\") + \" valid.\");\n} while (!String.IsNullOrEmpty(acct));\n}\n}\n}\nThis is a C# class with a Main method, so it can be run from the command line:\nC:\\> Program.exe\nType a bank account number on the next line.\n123456789\nBank account number '123456789' is valid.\nType a bank account number on the next line.\n123456788\nBank account number '123456788' is not valid.\nC:\\>\nA program like this can be called a manual unit test. It is a unit test because it is used\nto test just one unit, IsValid. It is manual because the user of this program has to\ntype in test cases manually, and manually assess whether the output of the program is\ncorrect.\nWhile better than having no unit testing at all, this approach has several problems:\n\u2022 Test cases have to be provided by hand, so the test cannot be executed automati\u2010\ncally in an easy way.\n\u2022 The developer who has written this test is focusing on logic to execute the test\n(the do \u2026 while loop, all input/output handling), not on the test itself.\n\u2022 The program does not show how IsValid is expected to behave.\n\u2022 The program is not recognizable as a test (although the rather generic name\nProgram is an indication it is meant as a one-off experiment)."}
{"135": "10.1 Motivation\nThis section describes the advantages of automating your tests as much as possible.\nAutomated Testing Makes Testing Repeatable\nJust like other programs and scripts, automated tests are executed in exactly the same\nway every time they are run. This makes testing repeatable: if a certain test executes at\ntwo different points in time yet gives different answers, it cannot be that the test exe\u2010\ncution itself was faulty. One can conclude that something has changed in the system\nthat has caused the different outcome. With manual tests, there is always the possibil\u2010\nity that tests are not performed consistently or that human errors are made.\nAutomated Testing Makes Development Efficient\nAutomated tests can be executed with much less effort than manual tests. The effort\nthey require is negligible and can be repeated as often as you see fit. They are also\nfaster than manual code review. You should also test as early in the development pro\u2010\ncess as possible, to limit the effort it takes to fix problems.\nPostponing testing to a late stage in the development pipeline risks\nlate identification of problems. That costs more effort to fix,\nbecause code needs to go back through the development pipeline\nand be merged, and tests must be rerun.\nAutomated Testing Makes Code Predictable\nTechnical tests can be automated to a high degree. Take unit tests and integration\ntests: they test the technical inner workings of code and the cohesion/integration of\nthat code. Without being sure of the inner workings of your system, you might get"}
{"136": "Thus, running automated tests provides certainty about how the code works. There\u2010\nfore, the predictability of automated tests also makes the quality of developed code\nmore predictable.\nTests Document the Code That Is Tested\nThe script or program code of a test contains assertions about the expected\nbehavior of the system under test. For example, as will be illustrated later in this\nchapter, an appropriate test of IsValid contains the following line of code:\nAssert.IsFalse(IsValid(\"\")). This documents, in C# code, that we expect IsValid\nto return false when checking the empty string. In this way, the Assert.IsFalse state\u2010\nment plays a double role: as the actual test, and as documentation of the expected\nbehavior. In other words, tests are examples of what the system does.\nWriting Tests Make You Write Better Code\nWriting tests helps you to write testable code. As a side effect, this leads to code con\u2010\nsisting of units that are shorter, are simpler, have fewer parameters, and are more\nloosely coupled (as the guidelines in the previous chapters advise). For example, a\nmethod is more difficult to test when it performs multiple functions instead of only\none. To make it easier to test, you move responsibilities to different methods, improv\u2010\ning the maintainability of the whole. That is why some development approaches\nadvocate writing a unit test before writing the code that conforms to the test. Such\napproaches are called test-driven development (TDD) approaches. You will see that\ndesigning a method becomes easier when you think about how you are going to test\nit: what are the valid arguments of the method, and what should the method return as\na result?\n10.2 How to Apply the Guideline\nHow you automate tests differs by the types of tests you want to automate. Test types"}
{"137": "Table 10-1. Types of testing\nType What it tests Why Who\nUnit test Functionality of one unit in isolation Verify that unit behaves as Developer (preferably\nexpected of the unit)\nIntegration test Functionality, performance, or other quality Verify that parts of the system Developer\ncharacteristic of at least two classes work together\nEnd-to-end test System interaction (with a user or another Verify that system behaves as Developer\nsystem) expected\nRegression test Previously erroneous behavior of a unit, class, Ensure that bugs do not re- Developer\nor system interaction appear\nAcceptance test System interaction (with a user or another Confirm the system behaves as End-user\nsystem) required representative (never\nthe developer)\nTable 10-1 shows that a regression test is a unit test, an integration test, or an end-to-\nend test that has been created when a bug was fixed. Acceptance tests are end-to-end\ntests executed by end user representatives.\nDifferent types of testing call for different automation frameworks. For unit testing,\nseveral well-known C# frameworks are available, such as NUnit. For automated\nend-to-end testing, you need a framework that can mimic user input and capture\noutput. A well-known framework that does just that for web development is Sele\u2010\nnium. For integration testing, it all depends on the environment in which you are\nworking and the quality characteristics you are testing. SoapUI is a framework for\nintegration tests that focuses on web services and messaging middleware. Apache\njMeter is a framework for testing the performance of C# applications under heavy\nworkloads.\nChoosing a test framework needs to be done at the team level. Writing integration\ntests is a specialized skill\u2014but unit testing is for each and every individual developer.\nThat is why the rest of this chapter focuses on writing unit tests using the most well-"}
{"138": "Writing unit tests also requires the smallest upfront investment:1 just download\nNUnit from http://nunit.org.\nGetting Started with NUnit Tests\nAs we noted in the introduction of this chapter, we want to test IsValid, a method of\nthe class Accounts. Accounts is called the class under test. In NUnit, tests are put in a\ndifferent class, the test class, or test fixture. This class is indicated as a test fixture by\nthe [TestFixture] attribute. By convention, the name of the test class is the name of\nthe class under test with the suffix Test added. In this case, that would mean the\nname of the test class is AccountsTest. It must be a public class, but apart from that,\nthere are no other requirements for a test class. In particular, it does not need to\nextend any other class. It is convenient, but not required, to place the test class in the\nsame namespace as the class under test. That way, the test class has access to all mem\u2010\nbers of the test class under test that have namespace (but not public) access.\nIn NUnit, a test itself is any method that has the [Test] attribute. To test IsValid,\nyou can use the following NUnit test class:\nusing NUnit.Framework;\nnamespace eu.sig.training.ch04.v1\n{\n[TestFixture]\npublic class AccountsTest\n{\n[Test]\npublic void TestIsValidNormalCases()\n{\nAssert.IsTrue(Accounts.IsValid(\"123456789\"));\nAssert.IsFalse(Accounts.IsValid(\"123456788\"));\n}\n}"}
{"139": "Unit tests can be run directly in Visual Studio. In addition, NUnit comes with test\nrunners to run tests from the command line. Tests can also be executed by Maven or\nAnt. Figure 10-1 shows the result of running the preceding test in Visual Studio. The\nred bar indicates that there are failed tests.\nFigure 10-1. All tests succeeded!\nThe test in the preceding test class only tests normal cases: two bank account num\u2010\nbers of the expected format (exactly nine characters, all digits). How about corner\ncases? One obvious special case is the empty string. The empty string is, of course,\nnot a valid bank account number, so we test it by calling Assert.IsFalse:\n[Test]\npublic void TestEmptyString()\n{\nAssert.IsFalse(Accounts.IsValid(\"\"));\n}\nAs Figure 10-2 shows, it turns out that this test fails! While the call to IsValid should\nreturn false, it actually returned something else (which, of course, must be true, as\nthere is no other option)."}
{"140": "This indeed returns true, while it should return false. This reminds us to add code\nto IsValid that checks the length of the bank account number.2\nThe NUnit runner reports this as a test failure and not as a test error. A test failure\nmeans that the test itself (the method TestEmptyString) is executed perfectly, but the\nassertion failed. A test error means that the test method itself did not execute cor\u2010\nrectly. The following code snippet illustrates this: the ShowError method raises a\ndivision-by-zero exception and never even executes Assert.IsTrue:\n[Test]\npublic void ShowError()\n{\nint tmp = 0, dummy = 1 / tmp;\n// Next line is never executed because the previous one raises an\n// exception.\n// If it were executed, you'll never see the assert message because\n// the test always succeeds.\nAssert.IsTrue(true);\n}\nNext, we present some basic principles that will help you write good unit tests. We\nstart with the most basic principles and then progress to more advanced ones that\napply when your test efforts become more mature.\nGeneral Principles for Writing Good Unit Tests\nWhen writing tests, it is important to keep in mind the following general principles:\nTest both normal and special cases\nAs in the examples given in this chapter, test two kinds of cases. Write tests that\nconfirm that a unit indeed behaves as expected on normal input (called happy\nflow or sunny-side testing). Also write tests that confirm that a unit behaves sensi\u2010\nbly on non-normal input and circumstances (called unhappy flow or rainy-side\ntesting). For instance, in NUnit it is possible to write tests to confirm that a"}
{"141": "Write tests that are isolated: their outcomes should reflect only the behavior of the sub\u2010\nject being tested\nThat is, each test should act independently of all other tests. For unit testing, this\nmeans that each test case should test only one functionality. No unit test should\ndepend on state, such as files written by other tests. That is why a unit test that,\nsay, causes the class under test to access the filesystem or a database server is not\na good unit test.\nConsequently, in unit testing you should simulate the state/input of other classes\nwhen those are needed (e.g., as arguments). Otherwise, the test is not isolated and\nwould test more than one unit. This was easy for the test of IsValid, because IsValid\ntakes a string as an argument, and it does not call other methods of our system. For\nother situations, you may need a technique like stubbing or mocking.\nIn Chapter 6, we introduced a C# interface for a simple digital camera, which is\nrepeated here for ease of reference:\npublic interface ISimpleDigitalCamera\n{\nImage TakeSnapshot();\nvoid FlashLightOn();\nvoid FlashLightOff();\n}\nSuppose this interface is used in an application that ensures people never forget to\nturn on the flash at night:\npublic const int DAYLIGHT_START = 6;\npublic Image TakePerfectPicture(int currentHour)\n{\nImage image;\nif (currentHour < PerfectPicture.DAYLIGHT_START)\n{"}
{"142": "automatic and independent. That means that the normal implementation of the digi\u2010\ntal camera interface cannot be used. On a typical device, the normal implementation\nrequires a (human) user to point the camera at something interesting and press a\nbutton. The picture taken can be any picture, so it is hard to test whether the (suppos\u2010\nedly perfect) picture taken is the one expected.\nThe solution is to use an implementation of the camera interface that has been made\nespecially for testing. This implementation is a fake object, called a test stub or simply\na stub.3 In this case, we want this fake object to behave in a preprogrammed (and\ntherefore predictable) way. We write a test stub like this:\nclass DigitalCameraStub : ISimpleDigitalCamera\n{\npublic Image TestImage;\npublic Image TakeSnapshot()\n{\nreturn this.TestImage;\n}\npublic void FlashLightOn()\n{\n}\npublic void FlashLightOff()\n{\n}\n}\nIn this stub, TakeSnapshot always returns the same image, which we can set simply\nby assigning to testImage (for reasons of simplicity, we have made testImage a pub\u2010\nlic field and do not provide a setter). This stub can now be used in a test:\n[Test]\npublic void TestDayPicture()\n{"}
{"143": "value of the call, 12, means that TakePerfectPicture assumes it is between noon\nand 1 p.m.\nNow suppose we want to test TakePerfectPicture for nighttime behavior; that is, we\nwant to ensure that if TakePerfectPicture is called with a value lower than Perfect\nPicture.DAYLIGHT_START, it indeed switches on the flash. So, we want to test whether\nTakePerfectPicture indeed calls FlashLightOn. However, FlashLightOn does not\nreturn any value, and the ISimpleDigitalCamera interface also does not provide any\nother way to know whether the flash has been switched on. So what to check?\nThe solution is to provide the fake digital camera implementation with some mecha\u2010\nnism to record whether the method we are interested in gets called. A fake object that\nrecords whether expected calls have taken place is called a mock object. So, a mock\nobject is a stub object with added test-specific behavior. The digital camera mock\nobject looks like this:\nclass DigitalCameraMock : ISimpleDigitalCamera\n{\npublic Image TestImage;\npublic int FlashOnCounter = 0;\npublic Image TakeSnapshot()\n{\nreturn this.TestImage;\n}\npublic void FlashLightOn()\n{\nthis.FlashOnCounter++;\n}\npublic void FlashLightOff()\n{\n}\n}"}
{"144": "Assert.AreEqual(1, cameraMock.FlashOnCounter);\n}\nIn these examples, we have written our own stub and mock objects. This leads to a lot\nof code. Generally, it is most efficient to use a mocking framework such as Moq.\nMocking frameworks use features of the .Net runtime to automatically create mock\nobjects from normal interfaces or classes. They also provide methods to test whether\nmethods of a mock object have been called, and with which arguments. Some mock\u2010\ning frameworks also provide ways to specify preprogrammed behavior of mock\nobjects, giving them the characteristics of both stubs and mocks.\nIndeed, using Moq as an example, you can write TestNightPicture without any need\nto write a class like DigitalCameraMock yourself:\n[Test]\npublic void TestNightPictureMoq()\n{\nImage image =\nImage.FromFile(\"../../../../test/resources/VanGoghStarryNight.jpg\");\nvar cameraMock = new Mock<ISimpleDigitalCamera>();\ncameraMock.Setup(foo => foo.TakeSnapshot()).Returns(image);\nPerfectPicture.camera = cameraMock.Object;\nAssert.AreSame(image, new PerfectPicture().TakePerfectPicture(0));\ncameraMock.Verify(foo => foo.FlashLightOn(), Times.AtMostOnce());\n}\nIn this test, Moq\u2019s Mock constructor is used to create cameraMock, the mock object\nused in this test. With Moq\u2019s Setup and Returns method, the desired behavior is\nspecified. Moq\u2019s Verify method is used to verify whether FlashLightOn has been\ncalled.\nMeasure Coverage to Determine Whether There Are Enough Tests\nHow many unit tests are needed? One way to assess whether you have written enough\nunit tests is to measure coverage of your unit tests. Coverage, or more precisely, line"}
{"145": "never test getters. Take a typical class that represents postal mail addresses. It typically\nhas two or three string fields that represent (additional) address lines. It is easy to\nmake a mistake like this one:\npublic string getAddressLine3() {\nreturn this.addressLine2;\n}\nA minimum of 80% coverage alone is not enough to ensure high-quality unit tests.\nIt is possible to get high coverage by testing just a few high-level methods (like Main,\nthe first method called by the .NET runtime) and not mock out lower-level methods.\nThat is why we advise a 1-to-1 ratio of production code versus test code.\nYou can measure coverage using a code coverage tool. Some editions of Visual Studio\nprovide a built-in code coverage tool. Figure 10-3 shows coverage of the examples of\nthis book, using Visual Studio 2015 Enterprise Edition.\nFigure 10-3. Coverage report of the examples of this book in Visual Studio 2015 Enter\u2010\nprise Edition.\n10.3 Common Objections to Automating Tests\nThis section discusses typical objections and limitations regarding automation. They\ndeal with the reasons and considerations to invest in test automation."}
{"146": "Manual acceptance testing can largely be automated with automa\u2010\nted regression tests. With those, the scope of remaining manual\ntests decreases. You may still need manual review or acceptance\ntests to verify that business logic is correct. This typically concerns\nthe process flow of a functionality.\nObjection: I Am Not Allowed to Write Unit Tests\n\u201cI am not allowed to write unit tests because they lower productivity according to my\nmanager.\u201d\nWriting unit tests during development actually improves productivity. It improves\nsystem code by shifting the focus from \u201cwhat code should do\u201d toward \u201cwhat it should\nnot do.\u201d If you never take into account how the code may fail, you cannot be sure\nwhether your code is resilient to unexpected situations.\nThe disadvantages of not having unit tests are mainly in uncertainty and rework.\nEvery time a piece of code is changed, it requires painstaking review to verify whether\nthe code does what it is supposed to do.\nObjection: Why Should We Invest in Unit Tests When the Current\nCoverage Is Low?\n\u201cThe current unit test coverage of my system is very low. Why should I invest time now\nin writing unit tests?\u201d\nWe have elaborated on the reasons why unit tests are useful and help you develop\ncode that works predictably. However, when a very large system has little to no unit\ntest code, this may be a burden. After all, it would be a significant investment to start\nwriting unit tests from scratch for an existing system because you would need to ana\u2010\nlyze all units again. Therefore, you should make a significant investment in unit tests\nonly if the added certainty is worth the effort. This especially applies to critical, cen\u2010\ntral functionality and when there is reason to believe that units are behaving in an"}
{"147": "10.4 See Also\nStandardization and consistency in applying it are important in achieving a well-\nautomated development environment. For elaboration, see Chapter 11.\nHow SIG Rates Testability\nTestability is one of the five subcharacteristics of maintainability according to ISO\n25010. SIG rates testability by aggregating the ratings of system properties unit com\u2010\nplexity (see Chapter 3), component independence (see Chapter 7), and system volume\n(see Chapter 9), using an aggregation mechanism as explained in Appendix A.\nThe rationale for this is that complex units are especially hard to test, poor compo\u2010\nnent independence increases the need for mocking and stubbing, and higher volumes\nof production code require higher volumes of test code."}
{"148": ""}
{"149": "CHAPTER 11\nWrite Clean Code\nWriting clean code is what you must do in order to call yourself a professional.\n\u2014Robert C. Martin\nGuideline:\n\u2022 Write clean code.\n\u2022 Do this by not leaving code smells behind after development\nwork.\n\u2022 This improves maintainability because clean code is main\u2010\ntainable code.\nCode smells are coding patterns that hint that a problem is present. Introducing or\nnot removing such patterns is bad practice, as they decrease the maintainability of\ncode. In this chapter we discuss guidelines for keeping the codebase clean from code"}
{"150": "11.2 How to Apply the Guideline\nTrying to be a clean coder is an ambitious goal, and there are many best practices that\nyou can follow. From our consultancy experience we have distilled seven developer\n\u201cBoy Scout rules\u201d that will help you to prevent code smells that impact maintainabil\u2010\nity most:\n1. Leave no unit-level code smells behind.\n2. Leave no bad comments behind.\n3. Leave no code in comments behind.\n4. Leave no dead code behind.\n5. Leave no long identifier names behind.\n6. Leave no magic constants behind.\n7. Leave no badly handled exceptions behind.\nThese seven rules are explained in the following sections.\nRule 1: Leave No-Unit Level Code Smells Behind\nAt this point in the book you are familiar with nine guidelines for building maintain\u2010\nable software, discussed in the previous nine chapters. Of those nine guidelines, three\ndeal with smells at the unit level: long units (Chapter 2), complex units (Chapter 3),\nand units with large interfaces (Chapter 5). For modern programming languages,\nthere is really no good reason why any of these guidelines should be violated when\nyou are writing new code.\nTo follow this rule is to refactor \u201csmelly\u201d code in time. By \u201cin time,\u201d we mean as soon\nas possible but certainly before the code is committed to the version control system.\nOf course, it is OK to have small violations when you are working on a development\nticket\u2014for example, a method of 20 lines of code or a method with 5 parameters. But"}
{"151": "Rule 2: Leave No Bad Comments Behind\nComments are sometimes considered the anti-pattern of good code. From our expe\u2010\nrience we can confirm that inline comments typically indicate a lack of elegant engi\u2010\nneering solutions. Consider the following method taken from the Jenkins codebase\n(which is in Java):\npublic HttpResponse doUploadPlugin(StaplerRequest req)\nthrows IOException, ServletException {\ntry {\nJenkins.getInstance().checkPermission(UPLOAD_PLUGINS);\nServletFileUpload upload = new ServletFileUpload(\nnew DiskFileItemFactory());\n// Parse the request\nFileItem fileItem = (FileItem)upload.parseRequest(req).get(0);\nString fileName = Util.getFileName(fileItem.getName());\nif (\"\".equals(fileName)) {\nreturn new HttpRedirect(\"advanced\");\n}\n// we allow the upload of the new jpi's and the legacy hpi's\nif (!fileName.endsWith(\".jpi\") && !fileName.endsWith(\".hpi\")) {\nthrow new Failure(\"Not a plugin: \" + fileName);\n}\n// first copy into a temporary file name\nFile t = File.createTempFile(\"uploaded\", \".jpi\");\nt.deleteOnExit();\nfileItem.write(t);\nfileItem.delete();\nfinal String baseName = identifyPluginShortName(t);\npluginUploaded = true;\n// Now create a dummy plugin that we can dynamically load"}
{"152": "Although the doUploadPlugin is not very hard to maintain (it has only 1 parameter,\n32 lines of code, and a McCabe index of 6), the inline comments indicate separate\nconcerns that could easily be addressed outside this method. For example, copying\nthe fileItem to a temporary file and creating the plugin configuration are tasks that\ndeserve their own methods (where they can be tested and potentially reused).\nComments in code may reveal many different problems:\n\u2022 Lack of understanding of the code itself\n// I don't know what is happening here, but if I remove this line\n// an infinite loop occurs\n\u2022 Issue tracking systems not properly used\n// JIRA-1234: Fixes a bug when summing negative numbers\n\u2022 Conventions or tooling are being bypassed\n// CHECKSTYLE:OFF\n// NOPMD\n\u2022 Good intentions\n// TODO: Make this method a lot faster some day\nComments are valuable in only a small number of cases. Helpful API documentation\ncan be such a case, but always be cautious to avoid dogmatic boilerplate commentary.\nIn general, the best advice we can give is to keep your code free of comments.\nRule 3: Leave No Code in Comments Behind\nAlthough there might be rare occasions where there is a good reason to use com\u2010\nments in your code, there is never an excuse for checking in code that is commented\nout. The version control system will always keep a record of old code, so it is perfectly\nsafe to delete it. Take a look at the following example, taken from the Apache Tomcat\ncodebase (which is in Java, but we present a C# translation here):"}
{"153": "// FIXME: Older spec revisions may still check this\n/*\nif ((servletNames.length != 0) && (urlPatterns.length != 0))\nthrow new IllegalArgumentException\n(sm.getString(\"standardContext.filterMap.either\"));\n*/\nfor (int i = 0; i < urlPatterns.Length; i++) {\nif (!ValidateURLPattern(urlPatterns[i])) {\nthrow new Exception(\nsm.GetString(\"standardContext.filterMap.pattern\",\nurlPatterns[i]));\n}\n}\n}\nThe FIXME note and accompanying code are understandable from the original develo\u2010\nper\u2019s perspective, but to a new developer they act as a distractor. The original devel\u2010\noper had to make a decision before leaving this commented-out code: either fix\nit at the spot, create a new ticket to fix it at some other time, or reject this corner case\naltogether.\nRule 4: Leave No Dead Code Behind\nDead code comes in different shapes. Dead code is code that is not executed at all or\nits output is \u201cdead\u201d: the code may be executed, but its output is not used elsewhere in\nthe system. Code in comments, as discussed in the previous section, is an example of\ndead code, but there are many other forms of dead code. In this section, we give three\nmore examples of dead code.\nUnreachable code in methods\npublic Transaction GetTransaction(long uid)\n{\nTransaction result = new Transaction(uid);\nif (result != null)"}
{"154": "Unused private methods\nPrivate methods can be called only from other code in the same class. If they are not,\nthey are dead code. Nonprivate methods that are not called by methods in the same\nclass may also be dead, but you cannot determine this by looking at the code of the\nclass alone.\nCode in comments\nThis is not to be confused with commented-out code. Sometimes it can be useful to\nuse short code snippets in API documentation (such as in C# XMLDOC tags), but\nremember that keeping those snippets in sync with the actual code is a task that is\nquickly overlooked. Avoid code in comments if possible.\nRule 5: Leave No Long Identifiers Behind\nGood identifiers make all the difference between code that is a pleasure to read and\ncode that is hard to wrap your head around. A famous saying by Phil Karlton is\n\u201cThere are only two hard problems in computer science: cache invalidation and nam\u2010\ning things.\u201d In this book we won\u2019t discuss the first, but we do want to say a few things\nabout long identifiers.\nIdentifiers name the items in your codebase, from units to modules to components to\neven the system itself. It is important to choose good names so that developers can\nfind their way through the codebase without great effort. The names of most of the\nidentifiers in a codebase will be dependent on the domain in which the system oper\u2010\nates. It is typical for teams to have a formal naming convention or an informal, but\nconsistent, use of domain-specific terminology.\nIt is not easy to choose the right identifiers in your code, and unfortunately there are\nno guidelines for what is and what isn\u2019t a good identifier. Sometimes it may even take\nyou a couple of iterations to find the right name for a method or class.\nAs a general rule, long identifiers must be avoided. A maximum length for an identi\u2010"}
{"155": "Rule 6: Leave No Magic Constants Behind\nMagic constants are number or literal values that are used in code without a clear def\u2010\ninition of their meaning (hence the name magic constant). Consider the following\ncode example:\nfloat CalculateFare(Customer c, long distance)\n{\nfloat travelledDistanceFare = distance * 0.10f;\nif (c.Age < 12)\n{\ntravelledDistanceFare *= 0.25f;\n}\nelse\nif (c.Age >= 65)\n{\ntravelledDistanceFare *= 0.5f;\n}\nreturn 3.00f + travelledDistanceFare;\n}\nAll the numbers in this code example could be considered magic numbers. For\ninstance, the age thresholds for children and the elderly may seem like familiar num\u2010\nbers, but remember they could be used at many other places in the codebase. The fare\nrates are constants that are likely to change over time by business demands.\nThe next snippet shows what the code looks like if we define all magic constants\nexplicitly. The code volume increased with six extra lines of code, which is a lot com\u2010\npared to the original source, but remember that these constants can be reused in\nmany other places in the code:\nprivate static readonly float BASE_RATE = 3.00f;\nprivate static readonly float FARE_PER_KM = 0.10f;\nprivate static readonly float DISCOUNT_RATE_CHILDREN = 0.25f;\nprivate static readonly float DISCOUNT_RATE_ELDERLY = 0.5f;"}
{"156": "return BASE_RATE + travelledDistanceFare;\n}\nRule 7: Leave No Badly Handled Exception Behind\nThree guidelines for good exception handling are discussed here specifically because\nin our practice we see many flaws in implementing exception handling:\n\u2022 Always catch exceptions. You are logging failures of the system to help you\nunderstand these failures and then improve the system\u2019s reaction to them. That\nmeans that exceptions must always be caught. Also, in some cases an empty\ncatch block compiles, but it is bad practice since it does not provide information\nabout the context of the exception.\n\u2022 Catch specific exceptions. To make exceptions traceable to a specific event,\nyou should catch specific exceptions. General exceptions that do not provide\ninformation specific to the state or event that triggered it fail to provide that\ntraceability. Therefore, you should not catch Exception or SystemException\ndirectly.\n\u2022 Translate specific exceptions to general messages before showing them to end\nusers. End users should not be \u201cbothered\u201d with detailed exceptions, since they\nare mostly confusing and a security bad practice (i.e., providing more informa\u2010\ntion than necessary about the inner workings of the system).\n11.3 Common Objections to Writing Clean Code\nThis section discusses typical objections regarding clean code. The most common\nobjections are reasons why commenting would be a good way to document code and\nwhether corners can be cut for doing exception handling.\nObjection: Comments Are Our Documentation"}
{"157": "Objection: Exception Handling Causes Code Additions\n\u201cImplementing exception classes forces me to add a lot of extra code without visible\nbenefits.\u201d\nException handling is an important part of defensive programming: coding to prevent\nunstable situations and unpredictable system behavior. Anticipating unstable situa\u2010\ntions means trying to foresee what can go wrong. This does indeed add to the burden\nof analysis and coding. However, this is an investment. The benefits of exception han\u2010\ndling may not be visible now, but they definitely will prove valuable in preventing and\nabsorbing unstable situations in the future.\nBy defining exceptions, you are documenting and safeguarding your assumptions.\nThey can later be adjusted when circumstances change.\nObjection: Why Only These Coding Guidelines?\n\u201cWe use a much longer list of coding conventions and quality checks in our team. This\nlist of seven seems like an arbitrary selection with many important omissions.\u201d\nHaving more guidelines and checks than the seven in this chapter is of course not a\nproblem. These seven rules are the ones we consider the most important for writing\nmaintainable code and the ones that should be adhered to by every member on the\ndevelopment team. A risk of having many guidelines and checks is that developers\ncan be overwhelmed by them and focus their efforts on the less critical issues. How\u2010\never, teams are obviously allowed to extend this list with items that they find indis\u2010\npensable for building a maintainable system."}
{"158": ""}
{"159": "CHAPTER 12\nNext Steps\nAt this point, you know a lot more about what maintainable code is, why it is impor\u2010\ntant, and how to apply the 10 guidelines in this book. But writing maintainable code\nis not something you learn from a book. You learn it by doing it! Therefore, here we\nwill discuss simple advice on practicing the 10 guidelines for achieving maintainable\nsoftware.\n12.1 Turning the Guidelines into Practice\nEnsuring that your code is easy to maintain depends on two behaviors in your daily\nroutine: discipline and setting priorities. Discipline helps you to constantly keep\nimproving your coding techniques, up to a point where any new code you write will\nalready be maintainable. As for priorities, some of the presented guidelines can seem\nto contradict each other. It takes consideration on your side about which guideline\nhas the most impact on the actual maintainability of your system. Be sure to take\nsome time to deliberate and ask your team for their opinion."}
{"160": "units into multiple smaller units slightly grows the total codebase. But the advantage\nof small units in terms of reusability will have a huge pay-off when more functionality\nis added to the system.\nThe same applies to the architecture-level guidelines (see Chapters 7 and 8): it makes\nno sense to reorganize your code structure when it makes your components highly\ndependent on each other. To put it succinctly: fix your dependencies before trying to\nbalance your components.\n12.3 Remember That Every Commit Counts\nThe hardest part of applying the guidelines in this book may be keeping the discipline\nto apply them. It is tempting to violate the guidelines when a \u201cquick fix\u201d seems more\nefficient. To keep this discipline, follow the Boy Scout rule presented in Chapter 11.\nThe Boy Scout rule is especially effective on large codebases. Unless you have the time\nto sort out your whole system and improve maintainability, you will have to do it\nstep-by-step while doing your regular work. This gradually improves maintainability\nand hones your refactoring skills. So, in the long run, you also have the skill to write\nhighly maintainable software.\n12.4 Development Process Best Practices Are Discussed in\nthe Follow-Up Book\nAs discussed in the preface, the process part of developing high-quality software is\ndiscussed in detail in the follow-up book in this series: Building Software Teams. It\nprovides 10 guidelines for managing and measuring the software development pro\u2010\ncess. It focuses on how to measure and manage best practices for software develop\u2010\nment (e.g., development tool support, automation, standardization)."}
{"161": "APPENDIX A\nHow SIG Measures Maintainability\nSIG measures system maintainability based on eight metrics. Those eight metrics are\ndiscussed in Chapters 2 through 9. Those chapters include sidebars explaining how\nSIG rates source code properties relevant to those guidelines. These ratings are\nderived from the SIG/T\u00dcViT1 Evaluation Criteria for Trusted Product Maintainabil\u2010\nity. In this appendix, we provide you with additional background.\nTogether with T\u00dcViT, SIG has determined eight properties of source code that can be\nmeasured automatically. See \u201cWhy These Ten Specific Guidelines?\u201d on page xi for\nhow these properties have been chosen.\nTo assess maintainability of a system, we measure these eight source code properties\nand summarize these measurements either in a single number (for instance, the per\u2010\ncentage of code duplication) or a couple of numbers (for instance, the percentage of\ncode in four categories of complexity, which we call a quality profile; see \u201cRating\nMaintainability\u201d).\nWe then compare these numbers against a benchmark containing several hundreds of"}
{"162": "Table A-1. SIG maintainability ratings\nRating Maintainability\n5 stars Top 5% of the systems in the benchmark\n4 stars Next 30% of the systems in the benchmark (above-average systems)\n3 stars Next 30% of the systems in the benchmark (average systems)\n2 stars Next 30% of the systems in the benchmark (below-average systems)\n1 star Bottom 5% least maintainable systems\nWe then aggregate the ratings to arrive at one overall rating. We do this in two steps.\nFirst, we determine the ratings for the subcharacteristics of maintainability as defined\nby ISO 25010 (i.e., analyzability, modifiability, etc.) by taking the weighted averages\naccording to the rows of Table A-2. Each cross in a given row indicates that the corre\u2010\nsponding system property (column) contributes to this subcharacteristic. Second, we\ntake a weighted average of the five subcharacteristics to determine an overall rating\nfor maintainability.\nTable A-2. Relation of subcharacteristics and system properties\nVolume Duplication Unit Unit Unit Module Component Component\nsize complexity interfacing coupling balance independence\nAnalyzability X X X X\nModifiability X X X\nTestability X X X\nModularity X X X\nReusability X X\nThis describes the SIG maintainability model in a nutshell, since there is more detail\nto it than what we can cover in this appendix. If you would like to learn more about\nthe details of the maintainability model, a good start for elaboration is the following\npublication:"}
{"163": "Background on the development of the model and its application is provided in the\nfollowing publications:\n\u2022 Heitlager, Ilja, Tobias Kuipers, and Joost Visser. \u201cA Practical Model for Measuring\nMaintainability.\u201d In Proceedings of the 6th International Conference on the Quality\nof Information and Communications Technology (QUATIC 2007), 30\u201339. IEEE\nComputer Society Press, 2007.\n\u2022 Baggen, Robert, Jos\u00e9 Pedro Correia, Katrin Schill, and Joost Visser. \u201cStandardized\ncode quality benchmarking for improving software maintainability.\u201d Software\nQuality Journal 20, no. 2 (2012): 287\u2013307.\n\u2022 Bijlsma, Dennis, Miguel Alexandre Ferreira, Bart Luijten, and Joost Visser.\n\u201cFaster issue resolution with higher technical quality of software.\u201d Software Qual\u2010\nity Journal 20, no. 2 (2012): 265\u2013285.\nDoes Maintainability Improve Over Time?\nA question we often get at SIG is whether maintainability improves over time across\nall systems we see. The answer is yes, but very slowly. The recalibration that we carry\nout every year consistently shows that the thresholds become stricter over time. This\nmeans that for one system to get a high maintainability rating, over time it must have\nfewer units that are overly long or complex, must have less duplication, lower cou\u2010\npling, and so on. Given the structure of our model, the reason for this must be that\nsystems in our benchmark over time have less duplication, less tight coupling, and so\non. One could argue that this means that maintainability across the systems we\nacquire for our benchmark is improving. We are not talking about big changes. In\nbroad terms, we can say this: it is about a tenth of a star per year.\nThe selection of systems within the SIG benchmark is a representative cross-cut of the\nsoftware industry, including both proprietary and open source systems, developed in\na variety of languages, functional domains, platforms, and so on. Therefore, the tenth"}
{"164": ""}
{"165": "Index\nSymbols C\n11-check, 12 C#\nconcept names in, xvi\nA namespaces vs. components, xvii\nC# interfaces, 78\nAbstract Factory design pattern, 86-88\ncalls, 83\nacceptance tests\nchains, conditional, 36-38\nautomation of, 124\nchange\ncharacteristics, 114\nand duplication, 54\nadaptive maintenance, 2\npossible reasons for, 54\nanalysis\nchecksum, 12\nand component balance, 95\nclasses, splitting, 73\nand duplication, 47\nclean coding\narchitecture components, balance of (see com\u2010\nand badly-handled exceptions, 134\nponent balance)\nand Boy Scout rule, 124\nautomation of tests (see test automation)\nand commented-out code, 130\nand comments as documentation, 134\nB\nand dead code, 131\nbackups, code duplication and, 54\nand exception class implementation, 135\nbalance of components (see component bal\u2010\nand number of coding guidelines, 135\nance)"}
{"166": "finding/analyzing, 95 of system as impediment to reducing code\u2010\nhidden vs. interface, 90 base volume, 108\nimprovement through test automation, 114 component balance, 93-99\nminimizing unit size, 11-26 advantages of, 95\nreading when spread out over multiple and component independence, 84\nunits, 23 and deceptive acceptability of imbalance, 98\nreplacing custom code with libraries/frame\u2010 and entanglements, 98\nworks, 77 applying guidelines for, 96\ntest automation and predictability of, 113 clarifying domains for, 97\nunreachable, 131 common objections to, 98\nwriting clean (see clean coding) deciding on proper conceptual level for\ncode clones grouping functionality, 97\nand SIG duplication ratings, 56 importance of, 93-99\ndefined, 46 isolation of maintenance effects, 95\ncodebase separation of maintenance responsibilities,\nBoy Scout rule with large, 138 96\ndefined, 101 SIG rating thresholds, 99\nnavigation of, 73 when finding/analyzing code, 95\nvolume (see codebase volume) component coupling, module coupling vs., 82\nworking on isolated parts of, 72 component dependence, 82\ncodebase volume component guidelines, unit guidelines vs., 137\nadvantages of minimizing, 102-104 component imbalance, 93, 98\nand defect density, 104 component independence\napplying guidelines for, 105-107 advantages of, 82-86\ncommon objections to minimizing of, and Abstract Factory design pattern, 86-88\n107-110 and entangled components, 89\nduplication as impediment to reducing, 109 and isolated maintenance, 85\nfunctionality-related measures to minimize, and separation of maintenance responsibili\u2010\n105 ties, 85\nimportance of minimizing, 101-110 and testing, 86\nmaintenance and, 104 applying guidelines for, 86-88\nplatform architecture and, 109 common objections to, 88\nproductivity measures and, 108 defined, 82\nprogramming languages and, 108 importance of, 81-91\nproject failures and, 102 SIG rating thresholds, 90"}
{"167": "reuse vs., 14 and unit tests, 55\nsystem complexity as cause of, 108 as impediment to reducing codebase vol\u2010\ncopying code (see duplication [code]) ume, 109\ncore classes, 59 avoiding, 43-56\ncorrective maintenance, 2 common objections to avoiding, 53-55\ncoupling, 71 guidelines for avoiding, 48-53\n(see also module coupling) SIG ratings, 56\nbetween classes, 72 types of, 46\ndefined, 71\nloose (see component independence) (see E\nloose coupling)\nembedded software, 6\ntight (see tight coupling)\nencapsulation, 73, 77, 78, 84, 87, 90\ncoverage\nend-to-end tests, 114\nmeasuring for test adequacy, 122\nentangled components\ntest automation in low-coverage situation,\nand component balance, 98\n124\nand component independence, 89\nCPD (clone detection tool), 47\nevolution over time, 84\ncyclomatic complexity, 31\nerrors, test, 118\n(see also McCabe complexity)\nexception handling\nand clean coding, 134\nD\nand exception class implementation, 135\ndata transfer objects, 62 execution paths, 30\ndead code, 131 Extract Method refactoring technique, 17, 48\ndeadlines, component independence and, 89 Extract Superclass refactoring technique, 50-53,\ndefect density, codebase volume and, 104 50\ndefensive programming, exception handling\nand, 135 F\ndependency injection, 86\nfailures, 118\ndiscipline, coding and, 137\n(see also exception handling)\ndocumentation\ncodebase volume and, 102\nand test automation, 114\ntest errors vs. test failures, 118\ncomments as, 134\nfluent interface, 64\ndomains\nfor loops, 13\nclarifying, 97\nformatting, unit size guidelines and, 23\ncomplex, 40\nframeworks"}
{"168": "importance of simple, 4 lower-level guidelines, 137\nlower-level vs. higher-level, 137\nmaintaining discipline to apply, 138 M\noverview, 9\nmagic constants, 133\npracticing, 137\nmaintainability\nprinciples of, 4\nand discipline during development, 5\nas enabler for other quality characteristics, 4\nH\nas industry-independent, 6\nhappy flow testing, 118 as language-independent, 6\nhidden code, 90 as nonbinary quantity, 7\nhigher-level guidelines, 137 business impact of, 3\ndefined, 2\nI guidelines (see guidelines, maintainability)\nimportance of, 3-4\nidentifiers, long, 132\nimportance of simple guidelines, 4\nif-then-else statements, 36-38\nimprovement over time, 141\nimplementations, specialized, 75-77\nmetrics for, xi\nincoming calls, 84\nmisunderstandings about, 6\nindustry-dependent software development, 6\nperformance vs., 22\nintegration tests, 114\nrating, 7-9\ninterface code, 90\nmaintenance, four types of, 2\ninterfaces, unit (see unit interfaces)\nman-months/years, 110\ninternal calls, 83\nmanual testing\nIntroduce Parameter Object refactoring pat\u2010\nand test automation, 123\ntern, 59\nlimitations of, 112\ninversion of control (IoC), 79\nunit test, 112\nisolated maintenance, 85\nMcCabe complexity\nas SIG/T\u00dcViT rating criteria, 42\nJ\ndefined, 31\nJMeter, 115\nmethod invocations, 22\nJPacman, 16\nmethod modification, 60\nmethod splitting, 41\nL\nmethods\nlarge class smell, 50, 71, 73 unreachable code in, 131\nlarge systems, splitting up of, 107 unused private, 132"}
{"169": "and utility code, 78 as impediment to reducing codebase vol\u2010\napplying guidelines for, 73-77 ume, 108\ncommon objections to separating concerns, as maintainability factor, 6\n78 project failures, codebase volume and, 102\ncomponent coupling vs., 82\nhiding specialized implementations behind Q\ninterfaces, 75-77\nquality profiles, 5, 8\nloose (see loose coupling)\nquality, maintainability as enabler for, 4\nreplacing custom code with libraries/frame\u2010\nquick fixes, 138\nworks, 77\nSIG rating thresholds, 80\nR\nsplitting classes to separate concerns, 73\nrainy-side testing, 118\ntight (see tight coupling)\nrefactoring\nto prevent no-go areas for new developers,\nand unit complexity, 41\n73\nand unit interfaces, 65\nwhen working on isolated parts of codebase,\nand unit size, 17-22\n72\ndifficulties as maintainability issue, 25\nMoq, 122\nExtract Method technique, 17, 48\nmutual dependencies, 89\nExtract Superclass technique, 50-53, 50\nfor improved maintainability, 127\nN\nIntroduce Parameter Object pattern, 59\nnamespaces, components vs., xvii\nReplace Method with Method Object tech\u2010\nnested conditionals, 38-40\nnique, 19-22, 63\nNUnit tests, 112, 116-118\nto reduce codebase, 106\nreferral, reuse by, 106\nO\nregression, 95, 109, 112-115, 124\nobservers, 15 automated testing to identify, 113\noutgoing calls, 83 bugs, 47\nregression tests\nP characteristics, 114\nfor automation of manual acceptance test\u2010\nparameter lists, 65\ning, 124\n(see also unit interfaces)\nrepeatability, test automation and, 113\nparameter objects, 62, 64\nReplace Conditional with Polymorphism pat\u2010\nperfective maintenance, 2\ntern, 37"}
{"170": "scope creep, 105 and SIG testability rating thresholds, 125\nSelenium, 115 applying guidelines for, 114\nself-taught developers, xi common objections to, 123\nseparation of concerns (see concerns, separa\u2010 general principles for writing good unit\ntion of) tests, 118-122\nSIG (Software Improvement Group), xiii in low-coverage situation, 124\nSIG/T\u00dcViT Evaluation Criteria Trusted Prod\u2010 measuring coverage of, 122\nuct Maintainability test stub, 120\ncodebase volume rating thresholds, 110 test-driven development (TDD), 114\ncomponent balance rating thresholds, 99 testability\ncomponent independence rating thresholds, and unit size, 14\n90 SIG rating thresholds, 125\nduplication rating thresholds, 56 tests/testing\nmetrics, xi, 139-140 and unit complexity, 35\nmodule coupling rating thresholds, 80 automation of (see test automation)\nstar ratings, 7-9 component independence and ease of, 86\ntestability rating thresholds, 125 dangers of postponing, 113\nunit complexity rating thresholds, 42 errors vs. failures, 118\nunit interface rating thresholds, 66 types of, 114\nunit size rating thresholds, 26 third-party libraries/frameworks (see frame\u2010\nsimple units (see unit complexity) works) (see libraries)\nsingle responsibility principle throughput code, 84\nclasses that violate, 73 throughput, component independence and, 90\ndefined, 73 tight coupling\nsmells, Boy Scout rules for preventing, 128-134 as impediment to reducing codebase vol\u2010\n(see also large class smell) ume, 109\nSoapUI, 115 as risk when removing clones, 50\nSoftware Improvement Group (see SIG) maintenance consequences of, 72\nSoftware Risk Monitoring service, xiii maintenance problems with, 68-72\nSQL queries, 24 of classes, 71\nstandardization of functionality, 106 Type 1 clones, 46\nstar rating system, 8 Type 2 clones, 46\nstring literals, 55\nstub (test stub), 120 U\nsunny-side testing, 118\nunhappy flow testing, 118"}
{"171": "in complex domains, 40 in real-world systems, 23\nminimizing, 29, 42 minimizing, 11-26\nSIG rating of, 42 perceived lack of advantage in splitting\nunit guidelines units, 25\ncomponent guidelines vs., 137 quantity of units and performance, 22\ndefined, 67 reading code when spread out over multiple\nunit interfaces units, 23\nadvantages of minimizing size of, 59 refactoring techniques for guideline applica\u2010\nand libraries/frameworks with long parame\u2010 tions, 17-22\nter lists, 65 SIG thresholds for, 26\nand method modification, 60 when extending unit with new functionality,\nand parameter objects, 64 16\napplying guidelines for, 60-64 when writing new unit, 15\nC# interfaces and loose coupling, 78 unit tests\ncommon objections to minimizing size of, and Boy Scout rule, 124\n64 and duplication, 55\nease of understanding, 59 characteristics, 114\nhiding specialized implementations behind, common objections to automating tests, 123\n75-77 failures vs. errors, 118\nminimizing size of, 57-66 general principles for writing, 118-122\nrefactoring, 65 measuring coverage to determine proper\nreuse, 59 number of, 124\nSIG rating thresholds, 66 NUnit tests, 116-118\nunit size unit-level code, clean coding and, 128\nadvantages of minimizing, 14 unreachable code, 131\nand ease of analysis, 14 unused private methods, 132\nand improper formatting, 23 utility code, module coupling and, 78\nand reuse, 14\nand testability, 14 V\napplying guidelines to, 15-22\nviolations\ncommon objections to writing short units,\nprioritizing, 5\n22-25\nwith frameworks/libraries, 65\ndifficulty of optimizing by splitting units, 24"}
{"172": "Colophon\nThe animal on the cover of Building Maintainable Software is a grey-headed wood\u2010\npecker (Picus canus). Like all woodpeckers, which consitute about half of the Pici\u2010\nformes order, grey-headed woodpeckers use strong bills to puncture the surface of\ntrees and seek small insects that inhabit the wood. Very long, bristly tongues coated\nwith an adhesive extend into deep cracks, holes, and crevices to gather food in the\nbird\u2019s bill. A membrane that closes over the woodpecker\u2019s eye protects it from the\ndebris that may result from each blow at the tree. Slit-like nostrils provide a similar\nprotection, as do feathers that cover them. Adaptations to the brain like small size\nand a position that maximizes its contact with the skull\u2014permitting optimal shock\nabsorption\u2014represent further guards against the violence of the woodpecker\u2019s drill\u2010\ning. The zygodactyl arrangement of the feet, putting two toes forward and two back,\nallow the woodpecker to maintain its position on the tree\u2019s trunk during this activity,\nas well as to traverse vertically up and down it.\nGrey-headed woodpeckers maintain a vast range across Eurasia, though individual\nmembers of the species tend to be homebodies to particular forest and woodland\nhabitats. As such, they rarely travel overseas and switch to a seed-based diet in winter.\nMating calls that begin with high-pitched whistles lead to monogamous pairs roost\u2010\ning with clutches of 5 to 10 eggs in the holes that males bore into the trunks of trees,\nwhere both parents remain to incubate eggs and nurse the hatchlings for the three to\nfour weeks in which the hatchlings progress to juveniles. At this point, the young can\nfly from the nest and gather their own food.\nIn their greenish back and tail plumage, grey-headed woodpeckers very much resem\u2010\nble the closely related green woodpecker, and males of the species will develop on\ntheir foreheads the red patch that appears on many other species of woodpecker.\nMany of the animals on O\u2019Reilly covers are endangered; all of them are important to\nthe world. To learn more about how you can help, go to animals.oreilly.com."}
{"45": "Objection: Code Is Harder to Read When Spread Out\n\u201cCode becomes harder to read when spread out over multiple units.\u201d\nWell, psychology says that is not the case. People have a working memory of about\nseven items, so someone who is reading a unit that is significantly longer than seven\nlines of code cannot process all of it. The exception is probably the original author of\na piece of source code while he or she is working on it (but not a week later).\nWrite code that is easy to read and understand for your successors\n(and for your future self).\nGuideline Encourages Improper Formatting\n\u201cYour guideline encourages improper source code formatting.\u201d\nDo not try to comply with guideline by cutting corners in the area of formatting. We\nare talking about putting multiple statements or multiple curly brackets on one line.\nIt makes the code slightly harder to read and thus decreases its maintainability. Resist\nthe temptation to do so.\nConsider what purpose the guideline really serves. We simply cannot leave unit\nlength unconstrained. That would be akin to removing speed limits in traffic because\nthey discourage being on time. It is perfectly possible to obey speed limits and arrive\non time: just leave home a bit earlier. It is equally possible to write short units. Our\nexperience is that 15 lines of properly formatted code is enough to write useful units.\nAs proof, Table 2-1 presents some data from a typical Java 2 Enterprise Edition sys\u2010\ntem, consisting of Java source files but also some XSD and XSLT. The system, cur\u2010\nrently in production at a SIG client, provides reporting functionality for its owner.\nThe Java part consists of about 28,000 lines of code (a medium-sized system). Of"}
{"46": "Out of the 3,220 units in this system, 3,071 (95.4%) are at most 15 lines of code, while\n149 units (4.6% of all units) are longer. This shows that it is very possible in practice\nto write short units\u2014at least for a vast majority of units.\nAgree on formatting conventions in your team. Keep units short\nand comply with these conventions.\nThis Unit Is Impossible to Split Up\n\u201cMy unit really cannot be split up.\u201d\nSometimes, splitting a method is indeed difficult. Take, for instance, a properly for\u2010\nmatted switch statement in C#. For each case of the switch statement, there is a line\nfor the case itself, at least one line to do anything useful, and a line for the break\nstatement. So, anything beyond four cases becomes very hard to fit into 15 lines of\ncode, and a case statement cannot be split. In Chapter 3, we present some guidelines\non how to deal specifically with switch statements.\nHowever, it is true that sometimes a source code statement simply cannot be split. A\ntypical example in enterprise software is SQL query construction. Consider the fol\u2010\nlowing example (adapted from a real-world system analyzed by the authors of this\nbook):\npublic static void PrintDepartmentEmployees(string department)\n{\nQuery q = new Query();\nforeach (Employee e in q.AddColumn(\"FamilyName\")\n.AddColumn(\"Initials\")\n.AddColumn(\"GivenName\")\n.AddColumn(\"AddressLine1\")\n.AddColumn(\"ZIPcode\")"}
{"47": "expression starting with q.AddColumn(\"FamilyName\") can be extracted into a new\nmethod. But before doing that (and seeing the newly created method grow to over 15\nlines when the query gets more complex in the future), rethink the architecture. Is it\nwise to create a SQL query piece by piece as in this snippet? Should the HTML\nmarkup really appear here? A templating solution such as ASP or Razor may be more\nsuitable for the job at hand.\nSo, if you are faced with a unit that seems impossible to refactor, do not ignore it and\nmove on to another programming task, but indeed raise the issue with your team\nmembers and team lead.\nWhen a refactoring seems possible but doesn\u2019t make sense, rethink\nthe architecture of your system.\nThere Is No Visible Advantage in Splitting Units\n\u201cPutting code in DoSomethingOne, DoSomethingTwo, DoSomethingThree has no benefit\nover putting the same code all together in one long DoSomething.\u201d\nActually, it does, provided you choose better names than DoSomethingOne, DoSome\nthingTwo, and so on. Each of the shorter units is, on its own, easier to understand\nthan the long DoSomething. More importantly, you may not even need to consider all\nthe parts, especially since each of the method names, when chosen carefully, serves as\ndocumentation indicating what the unit of code is supposed to do. Moreover, the long\nDoSomething typically will combine multiple tasks. That means that you can only\nreuse DoSomething if you need the exact same combination. Most likely, you can\nreuse each of DoSomethingOne, DoSomethingTwo, and so on much more easily.\nPut code in short units (at most 15 lines of code) that have carefully"}
{"48": "2.4 See Also\nSee Chapters 3, 4, and 5 for additional refactoring techniques. For a discussion on\nhow to test methods, see Chapter 10.\nHow SIG Rates Unit Size\nThe size (length) of units (methods and constructors in C#) is one of the eight system\nproperties of the SIG/T\u00dcViT Evaluation Criteria for Trusted Product Maintainability.\nTo rate unit size, every unit of the system is categorized in one of four risk categories\ndepending on the number of lines of code it contains. Table 2-2 lists the four risk cat\u2010\negories used in the 2015 version of the SIG/T\u00dcViT Evaluation Criteria.\nThe criteria (rows) in Table 2-2 are conjunctive: a codebase needs to comply with all\nfour of them. For example, if 6.9% of all lines of code are in methods longer than\n60 lines, the codebase can still be rated at 4 stars. However, in that case, at most\n22.3% \u2013 6.9% = 15.4% of all lines of code can be in methods that are longer than 30\nlines but not longer than 60 lines. To the contrary, if a codebase does not have any\nmethods of more than 60 lines of code, at most 22.3% of all lines of code can be in\nmethods that are longer than 30 lines but not longer than 60 lines.\nTable 2-2. Minimum thresholds for a 4-star unit size rating (2015 version of the SIG/\nT\u00dcViT Evaluation Criteria)\nLines of code in methods with \u2026 Percentage allowed for 4 stars for unit size\n\u2026 more than 60 lines of code At most 6.9%\n\u2026 more than 30 lines of code At most 22.3%\n\u2026 more than 15 lines of code At most 43.7%\n\u2026 at most 15 lines of code At least 56.3%\nSee the three quality profiles shown in Figure 2-2 as an example:"}
{"49": "Figure 2-2. Three quality profiles for unit size"}
{"50": ""}
{"51": "CHAPTER 3\nWrite Simple Units of Code\nEach problem has smaller problems inside.\n\u2014Martin Fowler\nGuideline:\n\u2022 Limit the number of branch points per unit to 4.\n\u2022 Do this by splitting complex units into simpler ones and\navoiding complex units altogether.\n\u2022 This improves maintainability because keeping the number of\nbranch points low makes units easier to modify and test.\nComplexity is an often disputed quality characteristic. Code that appears complex to\nan outsider or novice developer can appear straightforward to a developer that is inti\u2010\nmately familiar with it. To a certain extent, what is \u201ccomplex\u201d is in the eye of the"}
{"52": "later). Branch points can be counted for a complete codebase, a class, a namespace, or\na unit. The number of branch points of a unit is equal to the minimum number of\npaths needed to cover all branches created by all branch points of that unit. This is\ncalled branch coverage. However, when you consider all paths through a unit from the\nfirst line of the unit to a final statement, combinatory effects are possible. The reason\nis that it may matter whether a branch follows another in a particular order. All possi\u2010\nble combinations of branches are the execution paths of the unit\u2014that is, the maxi\u2010\nmum number of paths through the unit.\nConsider a unit containing two consecutive if statements. Figure 3-1 depicts the con\u2010\ntrol flow of the unit and shows the difference between branch coverage and execution\npath coverage."}
{"53": "In summary, the number of branch points is the number of paths that cover all\nbranches created by branch points. It is the minimum number of paths and can be\nzero (for a unit that has no branch points). The number of execution paths is a maxi\u2010\nmum, and can be very large due to combinatorial explosion. Which one to choose?\nThe answer is to take the number of branch points plus one. This is called cyclomatic\ncomplexity or McCabe complexity. Consequently, the guideline \u201climit the number of\nbranch points per unit to 4\u201d is equal to \u201climit code McCabe complexity to 5.\u201d This is\nthe minimum number of test cases that you need to cover a unit such that every path\nhas a part not covered by the other paths. The cyclomatic (McCabe) complexity of a\nunit is at least one, which is easy to understand as follows. Consider a unit with no\nbranch points. According to the definition, its cyclomatic complexity is one (number\nof branch points plus one). It also fits intuitively: a unit with no branch points has\none execution path, and needs at least one test.\nFor the sake of completeness: only for units with one exit point, the cyclomatic or\nMcCabe complexity is equal to the number of branch points plus one. It becomes\nmore complex for units with more than one exit point. Do not worry about that:\nfocus on limiting the number of branch points to four.\nThe minimum number of tests needed to cover all independent\nexecution paths of a unit is equal to the number of branch points\nplus one.\nNow consider the following example. Given a nationality, the GetFlagColors method\nreturns the correct flag colors:\npublic IList<Color> GetFlagColors(Nationality nationality)\n{\nList<Color> result;\nswitch (nationality)"}
{"54": "break;\ncase Nationality.UNCLASSIFIED:\ndefault:\nresult = new List<Color> { Color.Gray };\nbreak;\n}\nreturn result;\n}\nThe switch statement in the method body needs to handle all cases of the nationality\nenumeration type and return the correct flag colors. As there are five possible nation\u2010\nalities and the unclassified/default case, the number of isolated paths to be tested\n(control flow branches) is six.\nOn first sight, the GetFlagColors method might seem harmless. Indeed, the method\nis quite readable, and its behavior is as expected. Still, if we want to test the behavior\nof this method, we would need six unique test cases (one for each nationality plus one\nfor the default/unclassified case). Writing automated tests might seem excessive for\nthe GetFlagColors method, but suppose a developer adds the flag of Luxembourg\n(which is very similar to the Dutch flag) as a quick fix:\n...\ncase Nationality.DUTCH:\nresult = new List<Color> { Color.Red, Color.White, Color.Blue };\ncase Nationality.LUXEMBOURGER:\nresult = new List<Color> { Color.Red, Color.White, Color.LightBlue };\nbreak;\ncase Nationality.GERMAN:\n....\nBeing in a hurry, the developer copied the constructor call for the Dutch flag\nand updated the last argument to the right color. Unfortunately, the break statement\nescaped the developer\u2019s attention, and now all Dutch nationalities will see the flag\nfrom Luxembourg on their profile page!\nThis example looks like a forged scenario, but we know from our consultancy prac\u2010"}
{"55": "*/\nprivate static User getOrCreate(string id, string fullName, bool create)\n{\nstring idkey = idStrategy().keyFor(id);\nbyNameLock.readLock().doLock();\nUser u;\ntry\n{\nu = byName.get(idkey);\n}\nfinally\n{\nbyNameLock.readLock().unlock();\n}\nFileInfo configFile = getConfigFileFor(id);\nif (!configFile.Exists && !Directory.Exists(configFile.Directory.FullName))\n{\n// check for legacy users and migrate if safe to do so.\nFileInfo[] legacy = getLegacyConfigFilesFor(id);\nif (legacy != null && legacy.Length > 0)\n{\nforeach (FileInfo legacyUserDir in legacy)\n{\nXmlFile legacyXml = new XmlFile(XmlFile.XSTREAM,\nnew FileInfo(Path.Combine(\nlegacyUserDir.FullName, \"config.xml\")));\ntry\n{\nobject o = legacyXml.read();\nif (o is User)\n{\nif (idStrategy().equals(id, legacyUserDir.Name)\n&& !idStrategy()\n.filenameOf(legacyUserDir.Name)\n.Equals(legacyUserDir.Name))\n{"}
{"56": "}\nelse\n{\nLOGGER.log(Level.FINE,\n\"Unexpected object loaded from {0}: {1}\",\nnew object[] { legacyUserDir, o });\n}\n}\ncatch (IOException e)\n{\nLOGGER.log(Level.FINE,\nstring.Format(\n\"Exception trying to load user from {0}: {1}\",\nnew Object[] { legacyUserDir, e.Message }),\ne);\n}\n}\n}\n}\nif (u == null && (create || configFile.Exists))\n{\nUser tmp = new User(id, fullName);\nUser prev;\nbyNameLock.readLock().doLock();\ntry\n{\nprev = byName.putIfAbsent(idkey, u = tmp);\n}\nfinally\n{\nbyNameLock.readLock().unlock();\n}\nif (prev != null)\n{\nu = prev; // if some has already put a value in the map, use it\nif (LOGGER.isLoggable(Level.FINE)\n&& !fullName.Equals(prev.getFullName()))"}
{"57": "}\ncatch (IOException x)\n{\nLOGGER.log(Level.WARNING, null, x);\n}\n}\n}\nreturn u;\n}\n3.1 Motivation\nBased on the code examples in the previous section, keeping your units simple is\nimportant for two main reasons:\n\u2022 A simple unit is easier to understand, and thus modify, than a complex one.\n\u2022 Simple units ease testing.\nSimple Units Are Easier to Modify\nUnits with high complexity are generally hard to understand, which makes them hard\nto modify. The first code example of the first section was not overly complicated, but\nit would be when it checks for, say, 15 or more nationalities. The second code exam\u2010\nple covers many use cases for looking up or creating users. Understanding the second\ncode example in order to make a functional change is quite a challenge. The time it\ntakes to understand the code makes modification harder.\nSimple Units Are Easier to Test\nThere is a good reason you should keep your units simple: to make the process of\ntesting easier. If there are six control flow branches in a unit, you will need at least six\ntest cases to cover all of them. Consider the GetFlagColors method: six tests to cover"}
{"58": "\u2022 &&, ||\n\u2022 while\n\u2022 for , foreach\n\u2022 catch\nSo how can we limit the number of branch points? Well, this is mainly a matter of\nidentifying the proper causes of high complexity. In a lot of cases, a complex unit\nconsists of several code blocks glued together, where the complexity of the unit is the\nsum of its parts. In other cases, the complexity arises as the result of nested if-then-\nelse statements, making the code increasingly harder to understand with each level\nof nesting. Another possibility is the presence of a long chain of if-then-else state\u2010\nments or a long switch statement, of which the GetFlagColors method in the intro\u2010\nduction is an example.\nEach of these cases has its own problem, and thus, its own solution. The first case,\nwhere a unit consists of several code blocks that execute almost independently, is a\ngood candidate for refactoring using the Extract Method pattern. This way of reduc\u2010\ning complexity is similar to Chapter 2. But what to do when faced with the other\ncases of complexity?\nDealing with Conditional Chains\nA chain of if-then-else statements has to make a decision every time a conditional\nif is encountered. An easy-to-handle situation is the one in which the conditionals\nare mutually exclusive; that is, they each apply to a different situation. This is also the\ntypical use case for a switch statement, like the switch from the GetFlagColors\nmethod.\nThere are many ways to simplify this type of complexity, and selecting the best solu\u2010\ntion is a trade-off that depends on the specific situation. For the GetFlagColors"}
{"59": "Color.Red };\nFLAGS[Nationality.FRENCH] = new List<Color>{ Color.Blue, Color.White,\nColor.Red };\nFLAGS[Nationality.ITALIAN] = new List<Color>{ Color.Green, Color.White,\nColor.Red };\n}\npublic IList<Color> GetFlagColors(Nationality nationality)\n{\nIList<Color> colors = FLAGS[nationality];\nreturn colors ?? new List<Color> { Color.Gray };\n}\nA second, more advanced way to reduce the complexity of the GetFlagColors\nmethod is to apply a refactoring pattern that separates functionality for different flags\nin different flag types. You can do this by applying the Replace Conditional with Poly\u2010\nmorphism pattern: each flag will get its own type that implements a general interface.\nThe polymorphic behavior of the C# language will ensure that the right functionality\nis called during runtime.\nFor this refactoring, we start with a general IFlag interface:\npublic interface IFlag\n{\nIList<Color> Colors { get; }\n}\nand specific flag types for different nationalities, such as for the Dutch:\npublic class DutchFlag : IFlag\n{\npublic IList<Color> Colors\n{\nget\n{\nreturn new List<Color> { Color.Red, Color.White, Color.Blue };\n}\n}"}
{"60": "The GetFlagColors method now becomes even more concise and less error-prone:\nprivate static readonly Dictionary<Nationality, IFlag> FLAGS =\nnew Dictionary<Nationality, IFlag>();\nstatic FlagFactory()\n{\nFLAGS[Nationality.DUTCH] = new DutchFlag();\nFLAGS[Nationality.GERMAN] = new GermanFlag();\nFLAGS[Nationality.BELGIAN] = new BelgianFlag();\nFLAGS[Nationality.FRENCH] = new FrenchFlag();\nFLAGS[Nationality.ITALIAN] = new ItalianFlag();\n}\npublic IList<Color> GetFlagColors(Nationality nationality)\n{\nIFlag flag = FLAGS[nationality];\nflag = flag ?? new DefaultFlag();\nreturn flag.Colors;\n}\nThis refactoring offers the most flexible implementation. For example, it allows the\nflag type hierarchy to grow over time by implementing new flag types and testing\nthese types in isolation. A drawback of this refactoring is that it introduces more code\nspread out over more classes. The developer much choose between extensibility and\nconciseness.\nDealing with Nesting\nSuppose a unit has a deeply nested conditional, as in the following example. Given a\nbinary search tree root node and an integer, the CalculateDepth method determines\nwhether the integer occurs in the tree. If so, the method returns the depth of the inte\u2010\nger in the tree; otherwise, it throws a TreeException:\npublic static int CalculateDepth(BinaryTreeNode<int> t, int n)\n{"}
{"61": "{\nreturn 1 + CalculateDepth(left, n);\n}\n}\nelse\n{\nBinaryTreeNode<int> right = t.Right;\nif (right == null)\n{\nthrow new TreeException(\"Value not found in tree!\");\n}\nelse\n{\nreturn 1 + CalculateDepth(right, n);\n}\n}\n}\n}\nTo improve readability, we can get rid of the nested conditional by identifying the dis\u2010\ntinct cases and insert return statements for these. In terms of refactoring, this is\ncalled the Replace Nested Conditional with Guard Clauses pattern. The result will be\nthe following method:\npublic static int CalculateDepth(BinaryTreeNode<int> t, int n)\n{\nint depth = 0;\nif (t.Value == n)\n{\nreturn depth;\n}\nif ((n < t.Value) && (t.Left != null))\n{\nreturn 1 + CalculateDepth(t.Left, n);\n}\nif ((n > t.Value) && (t.Right != null))\n{"}
{"62": "}\nelse\n{\nreturn TraverseByValue(t, n);\n}\n}\nprivate static int TraverseByValue(BinaryTreeNode<int> t, int n)\n{\nBinaryTreeNode<int> childNode = GetChildNode(t, n);\nif (childNode == null)\n{\nthrow new TreeException(\"Value not found in tree!\");\n}\nelse\n{\nreturn 1 + CalculateDepth(childNode, n);\n}\n}\nprivate static BinaryTreeNode<int> GetChildNode(\nBinaryTreeNode<int> t, int n)\n{\nif (n < t.Value)\n{\nreturn t.Left;\n}\nelse\n{\nreturn t.Right;\n}\n}\nThis actually does decrease the complexity of the unit. Now we have achieved two\nthings: the methods are easier to understand, and they are easier to test in isolation\nsince we can now write unit tests for the distinct functionalities."}
{"63": "it is natural to think that the domain\u2019s complexity carries over to the implementation,\nand that this is an unavoidable fact of life.\nWe argue against this common interpretation. Complexity in the domain does not\nrequire the technical implementation to be complex as well. In fact, it is your respon\u2010\nsibility as a developer to simplify problems such that they lead to simple code. Even if\nthe system as a whole performs complex functionality, it does not mean that units on\nthe lowest level should be complex as well. In cases where a system needs to process\nmany conditions and exceptions (such as certain legislative requirements), one solu\u2010\ntion may be to implement a default, simple process and model the exceptions\nexplicitly.\nIt is true that the more demanding a domain is, the more effort the developer must\nexpend to build technically simple solutions. But it can be done! We have seen many\nhighly maintainable systems solving complex business problems. In fact, we believe\nthat the only way to solve complex business problems and keep them under control is\nthrough simple code.\nObjection: Splitting Up Methods Does Not Reduce Complexity\n\u201cReplacing one method with McCabe 15 by three methods with McCabe 5 each means\nthat overall McCabe is still 15 (and therefore, there are 15 control flow branches overall).\nSo nothing is gained.\u201d\nOf course, you will not decrease the overall McCabe complexity of a system by refac\u2010\ntoring a method into several new methods. But from a maintainability perspective,\nthere is an advantage to doing so: it will become easier to test and understand the\ncode that was written. So, as we already mentioned, newly written unit tests allow you\nto more easily identify the root cause of your failing tests.\nPut your code in simple units (at most four branch points) that\nhave carefully chosen names describing their function and cases."}
{"64": "How SIG Rates Unit Complexity\nThe complexity (McCabe) of units (methods and constructors in C#) is one of the\neight system properties of the SIG/T\u00dcViT Evaluation Criteria for Trusted Product\nMaintainability. To rate unit complexity, every unit of the system is categorized in one\nof four risk categories depending on its McCabe measurement. Table 3-1 lists the four\nrisk categories used in the 2015 version of the SIG/T\u00dcViT Evaluation Criteria.\nThe criteria (rows) in Table 3-1 are conjunctive: a codebase needs to comply with\nall four of them. For example, if 1.5% of all lines of code are in methods with a\nMcCabe over 25, it can still be rated at 4 stars. However, in that case, at most\n10.0% - 1.5% = 8.5% of all lines of code can be in methods that have a McCabe over\n10 but not over 25.\nTable 3-1. Minimum thresholds for a 4-star unit complexity rating (2015 version of the\nSIG/T\u00dcViT Evaluation Criteria)\nLines of code in methods with \u2026 Percentage allowed for 4 stars for unit complexity\n\u2026 a McCabe above 25 At most 1.5%\n\u2026 a McCabe above 10 At most 10.0%\n\u2026 a McCabe above 5 At most 25.2%\n\u2026 a McCabe of at most 5 At least 74.8%\nSee the three quality profiles in Figure 3-2 as an example:\n\u2022 Left: an open source system, in this case Jenkins\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for unit complexity\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic"}
{"65": "CHAPTER 4\nWrite Code Once\nNumber one in the stink parade is duplicated code.\n\u2014Kent Beck and Martin Fowler,\nBad Smells in Code\nGuideline:\n\u2022 Do not copy code.\n\u2022 Do this by writing reusable, generic code and/or calling\nexisting methods instead.\n\u2022 This improves maintainability because when code is copied,\nbugs need to be fixed at multiple places, which is inefficient\nand error-prone.\nCopying existing code looks like a quick win\u2014why write something anew when it"}
{"66": "if (amount.GreaterThan(this.transferLimit))\n{\nthrow new BusinessException(\"Limit exceeded!\");\n}\n// 2. Assuming result is 9-digit bank account number, validate 11-test:\nint sum = 0;\nfor (int i = 0; i < counterAccount.Length; i++)\n{\nsum = sum + (9 - i) * (int)Char.GetNumericValue(\ncounterAccount[i]);\n}\nif (sum % 11 == 0)\n{\n// 3. Look up counter account and make transfer object:\nCheckingAccount acct = Accounts.FindAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\nreturn result;\n}\nelse\n{\nthrow new BusinessException(\"Invalid account number!\");\n}\n}\n}\nGiven the account number of the account to transfer money to (as a string), the Make\nTransfer method creates a Transfer object. MakeTransfer first checks whether the\namount to be transferred does not exceed a certain limit. In this example, the limit is\nsimply hardcoded. MakeTransfer then checks whether the number of the account to\ntransfer the money to complies with a checksum (see the sidebar \u201cThe 11-Check for\nBank Account Numbers\u201d on page 12 for an explanation of the checksum used). If that\nis the case, the object that represents this account is retrieved, and a Transfer object\nis created and returned.\nNow assume the bank introduces a new account type, called a savings account. A sav\u2010\nings account does not have a transfer limit, but it does have a restriction: money can"}
{"67": "int sum = 0;\nfor (int i = 0; i < counterAccount.Length; i++)\n{\nsum = sum + (9 - i) * (int)Char.GetNumericValue(\ncounterAccount[i]);\n}\nif (sum % 11 == 0)\n{\n// 2. Look up counter account and make transfer object:\nCheckingAccount acct = Accounts.FindAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\n// 3. Check whether withdrawal is to registered counter account:\nif (result.CounterAccount.Equals(this.RegisteredCounterAccount))\n{\nreturn result;\n}\nelse\n{\nthrow new BusinessException(\"Counter-account not registered!\");\n}\n}\nelse\n{\nthrow new BusinessException(\"Invalid account number!!\");\n}\n}\n}\nStart of code clone.\nEnd of code clone.\nBoth classes exist in the same codebase. By copying and pasting an existing class, we\nhave introduced some duplicated code in the codebase. There are now two fragments\n(ten lines of code each) of consecutive lines of code that are exactly the same. These\nfragments are called code clones or duplicates."}
{"68": "Coding is about finding generic solutions for specific problems. Either reuse (by call\u2010\ning) an existing, generic method in your codebase, or make an existing method more\ngeneric.\nTypes of Duplication\nWe define a duplicate or code clone as an identical piece of code at least six lines long.\nThe line count excludes whitespace and comments, just like in the regular definition\nof \u201cline of code\u201d (see also Chapter 1). That means that the lines need to be exactly the\nsame to be considered a duplicate. Such clones are called Type 1 clones. It does not\nmatter where the duplicates occur. Two clones can be in the same method, in\ndifferent methods in the same class, or in different methods in different classes in the\nsame codebase. Code clones can cross method boundaries. For instance, if the follow\u2010\ning fragment appears twice in the codebase, it is considered one clone of six lines of\ncode, not two clones of three lines each:\npublic void SetGivenName(string givenName)\n{\nthis.givenName = givenName;\n}\npublic void SetFamilyName(string familyName)\n{\nthis.familyName = familyName;\n}\nThe following two methods are not considered duplicates of each other even though\nthey differ only in literals and the names of identifiers:\npublic void SetPageWidthInInches(float newWidth)\n{\nfloat cmPerInch = 2.54f;\nthis.pageWidthInCm = newWidth * cmPerInch;\n// A few more lines.\n}"}
{"69": "The guideline presented in this chapter is about Type 1 clones, for two reasons:\n\u2022 Source code maintenance benefits most from the removal of Type 1 clones.\n\u2022 Type 1 clones are easier to detect and recognize (both by humans and computers,\nas detecting Type 2 clones requires full parsing).\nThe limit of six lines of code may appear somewhat arbitrary, since other books and\ntools use a different limit. In our experience, the limit of six lines is the right balance\nbetween identifying too many and too few clones. As an example, a ToString method\ncould be three or four lines, and those lines may occur in many domain objects.\nThose clones can be ignored, as they are not what we are looking for\u2014namely, delib\u2010\nerate copies of functionality.\n4.1 Motivation\nTo understand the advantages of a codebase with little duplication, in this section we\ndiscuss the effects that duplication has on system maintainability.\nDuplicated Code Is Harder to Analyze\nIf you have a problem, you want to know how to fix it. And part of that \u201chow\u201d is\nwhere to locate the problem. When you are calling an existing method, you can easily\nfind the source. When you are copying code, the source of the problem may exist\nelsewhere as well. However, the only way to find out is by using a clone detection\ntool. A well-known tool for clone detection is CPD, which is included in a source\ncode analysis tool called PMD. Several editions of Visual Studio come with a clone\ndetection tool built-in.\nThe fundamental problem of duplication is not knowing whether\nthere is another copy of the code that you are analyzing, how many\ncopies exist, and where they are located."}
{"70": "The same problem holds for regular changes. When code is duplicated, changes may\nneed to be made in multiple places, and having many duplicates makes changing a\ncodebase unpredictable.\n4.2 How to Apply the Guideline\nTo avoid the problem of duplicated bugs, never reuse code by copying and pasting\nexisting code fragments. Instead, put it in a method if it is not already in one, so that\nyou can call it the second time that you need it. That is why, as we have covered in the\nprevious chapters, the Extract Method refactoring technique is the workhorse that\nsolves many duplication problems.\nIn the example presented at the beginning of the chapter, the code that implements\nthe checksum (which is part of the duplicate) is an obvious candidate for extraction.\nTo resolve duplication using Extract Method, the duplicate (or a part thereof) is\nextracted into a new method which is then called multiple times, once from each\nduplicate.\nIn Chapter 2, the new extracted method became a private method of the class in\nwhich the long method occurs. That does not work if duplication occurs across\nclasses, as in CheckingAccount and SavingsAccount. One option in that case is to\nmake the extracted method a method of a utility class. In the example, we already\nhave an appropriate class for that (Accounts). So the new static method, IsValid, is\nsimply a method of that class:\npublic static bool IsValid(string number)\n{\nint sum = 0;\nfor (int i = 0; i < number.Length; i++)\n{\nsum = sum + (9 - i) * (int)Char.GetNumericValue(number[i]);\n}\nreturn sum % 11 == 0;"}
{"71": "{\n// 2. Look up counter account and make transfer object:\nCheckingAccount acct = Accounts.FindAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\nreturn result;\n}\nelse\n{\nthrow new BusinessException(\"Invalid account number!\");\n}\n}\n}\nStart of short clone (three lines of code).\nEnd of short clone (three lines of code).\nAnd also in SavingsAccount:\npublic class SavingsAccount\n{\npublic CheckingAccount RegisteredCounterAccount { get; set; }\npublic Transfer MakeTransfer(string counterAccount, Money amount)\n{\n// 1. Assuming result is 9-digit bank account number, validate 11-test:\nif (Accounts.IsValid(counterAccount))\n{\n// 2. Look up counter account and make transfer object:\nCheckingAccount acct = Accounts.FindAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\nif (result.CounterAccount.Equals(this.RegisteredCounterAccount))\n{\nreturn result;\n}\nelse\n{"}
{"72": "Mission accomplished: according to the definition of duplication presented at the\nbeginning of this chapter, the clone has disappeared (because the repeated fragment is\nfewer than six lines of code). But the following issues remain:\n\u2022 Even though according to the definition, the clone has disappeared, there is still\nlogic repeated in the two classes.\n\u2022 The extracted fragment had to be put in a third class, just because in C# every\nmethod needs to be in a class (or struct). The class to which the extracted method\nwas added runs the risk of becoming a hodgepodge of unrelated methods. This\nleads to a large class smell and tight coupling. Having a large class is a smell\nbecause it signals that there are multiple unrelated functionalities within the\nclass. This tends to lead to tight coupling when methods need to know imple\u2010\nmentation details in order to interact with such a large class. (For elaboration, see\nChapter 6.)\nThe refactoring technique presented in the next section solves these problems.\nThe Extract Superclass Refactoring Technique\nIn the preceding code snippets, there are separate classes for a checking account and\na savings account. They are functionally related. However, they are not related in\nC# (they are just two classes that each derive directly from System.Object).\nBoth have common functionality (the checksum validation), which introduced a\nduplicate when we created SavingsAccount by copying and pasting (and modifying)\nCheckingAccount. One could say that a checking account is a special type of a (gen\u2010\neral) bank account, and that a savings account is also a special type of a (general)\nbank account. C# (and other object-oriented languages) has a feature to represent the\nrelationship between something general and something specific: inheritance from a\nsuperclass to a subclass.\nThe Extract Superclass refactoring technique uses this feature by extracting a frag\u2010"}
{"73": "{\n// 2. Look up counter account and make transfer object:\nCheckingAccount acct = Accounts.FindAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\nreturn result;\n}\nelse\n{\nthrow new BusinessException(\"Invalid account number!\");\n}\n}\n}\nStart of extracted clone.\nEnd of extracted clone.\nThe new superclass, Account, contains logic shared by the two types of special\naccounts. You can now turn both the CheckingAccount and SavingsAccount classes\ninto subclasses of this new superclass. For CheckingAccount, the result looks like this:\npublic class CheckingAccount : Account\n{\nprivate int transferLimit = 100;\npublic override Transfer MakeTransfer(string counterAccount, Money amount)\n{\nif (amount.GreaterThan(this.transferLimit))\n{\nthrow new BusinessException(\"Limit exceeded!\");\n}\nreturn base.MakeTransfer(counterAccount, amount);\n}\n}\nThe CheckingAccount class declares its own member, transferLimit, and overrides\nMakeTransfer. The MakeTransfer method first checks to be sure the amount to be"}
{"74": "return result;\n}\nelse\n{\nthrow new BusinessException(\"Counter-account not registered!\");\n}\n}\n}\nThe SavingsAccount class declares RegisteredCounterAccount and, just like Check\ningAccount, overrides MakeTransfer. The MakeTransfer method does not need to\ncheck a limit (because savings accounts do not have a limit). Instead, it calls Make\nTransfer directly in the superclass to create a transfer. It then checks whether the\ntransfer is actually with the registered counter account.\nAll functionality is now exactly where it belongs. The part of making a transfer that is\nthe same for all accounts is in the Account class, while the parts that are specific to\ncertain types of accounts are in their respective classes. All duplication has been\nremoved.\nAs the comments indicate, the MakeTransfer method in the Account superclass has\ntwo responsibilities. Although the duplication introduced by copying and pasting\nCheckingAccount has already been resolved, one more refactoring\u2014extracting the\n11-test to its own method\u2014makes the new Account class even more maintainable:\npublic class Account\n{\npublic Transfer MakeTransfer(string counterAccount, Money amount)\n{\nif (IsValid(counterAccount))\n{\nCheckingAccount acct = Accounts.FindAcctByNumber(counterAccount);\nreturn new Transfer(this, acct, amount);\n}\nelse\n{"}
{"75": "The Account class is now a natural place for IsValid, the extracted method.\n4.3 Common Objections to Avoiding Code Duplication\nThis section discusses common objections regarding code duplication. From our\nexperience, these are developers\u2019 arguments for allowing duplication, such as copying\nfrom other codebases, claiming there are \u201cunavoidable\u201d cases, and insisting that some\ncode will \u201cnever change.\u201d\nCopying from Another Codebase Should Be Allowed\n\u201cCopying and pasting code from another codebase is not a problem because it will not\ncreate a duplicate in the codebase of the current system.\u201d\nTechnically, that is correct: it does not create a duplicate in the codebase of the cur\u2010\nrent system. Copying code from another system may seem beneficial if the code\nsolves the exact same problem in the exact same context. However, in any of the fol\u2010\nlowing situations you will run into problems:\nThe other (original) codebase is still maintained\nYour copy will not benefit from the improvements made in the original codebase.\nTherefore, do not copy, but rather import the functionality needed (that is, add\nthe other codebase to your classpath).\nThe other codebase is no longer maintained and you are working on rebuilding this\ncodebase\nIn this case, you definitely should not copy the code. Often, rebuilds are caused\nby maintainability problems or technology renewals. In the case of maintainabil\u2010\nity issues, you would be defeating the purpose by copying code. You are introduc\u2010\ning code that is determined to be (on average) hard to maintain. In the case of\ntechnology renewals, you would be introducing limitations of the old technology\ninto the new codebase, such as an inability to use abstractions that are needed for"}
{"76": "model variations in the code in such a way that they are explicit, isolated, and\ntestable.\nThis Code Will Never Change\n\u201cThis code will never, ever change, so there is no harm in duplicating it.\u201d\nIf it is absolutely, completely certain that code will never, ever change, duplication\n(and every other aspect of maintainability) is not an issue. For a start, you have to be\nabsolutely, completely certain that the code in question also does not contain any\nbugs that need fixing. Apart from that, the reality is that systems change for many\nreasons, each of which may eventually lead to changes in parts deemed to never, ever\nchange:\n\u2022 The functional requirements of the system may change because of changing users,\nchanging behavior, or a change in the way the organization does business.\n\u2022 The organization may change in terms of ownership, responsibilities, develop\u2010\nment approach, development process, or legislative requirements.\n\u2022 Technology may change, typically in the system\u2019s environment, such as the operat\u2010\ning system, libraries, frameworks, or interfaces to other applications.\n\u2022 Code itself may change, because of bugs, refactoring efforts, or even cosmetic\nimprovements.\nThat is why we argue that most of the time the expectation that code never changes is\nunfounded. So accepting duplication is really nothing more than accepting the risk\nthat someone else will have to deal with it later if it happens.\nYour code will change. Really."}
{"77": "Unit Tests Are Covering Me\n\u201cUnit tests will sort out whether something goes wrong with a duplicate.\u201d\nThis is true only if the duplicates are in the same method, and the unit test of the\nmethod covers both. If the duplicates are in other methods, it can be true only if a\ncode analyzer alerts you if duplicates are changing. Otherwise, unit tests would not\nnecessarily signal that something is wrong if only one duplicate has changed. Hence,\nyou cannot rely only on the tests (identifying symptoms) instead of addressing the\nroot cause of the problem (using duplicate code). You should not assume that even\u2010\ntual problems will be fixed later in the development process, when you could avoid\nthem altogether right now.\nDuplication in String Literals Is Unavoidable and Harmless\n\u201cI need long string literals with a lot of duplication in them. Duplication is unavoidable\nand does not hurt because it is just in literals.\u201d\nThis is a variant of one of the objections discussed in Chapter 2 (\u201cThis unit is impos\u2010\nsible to split\u201d). We often see code that contains long SQL queries or XML or HTML\ndocuments appearing as string literals in C# code. Sometimes such literals are com\u2010\nplete clones, but more often parts of them are repeated. For instance, we have seen\nSQL queries of more than a hundred lines of code that differed only in the sorting\norder (order by asc versus order by desc). This type of duplication is not harmless\neven though technically they are not in the C# logic itself. It is also not unavoidable;\nin fact this type of duplication can be avoided in a straightforward fashion:\n\u2022 Extract to a method that uses string concatenation and parameters to deal with\nvariants.\n\u2022 Use a templating engine to generate HTML output from smaller, nonduplicated\nfragments that are kept in separate files."}
{"78": "How SIG Rates Duplication\nThe amount of duplication is one of the eight system properties of the SIG/T\u00dcViT\nEvaluation Criteria for Trusted Product Maintainability. To rate duplication, all Type\n1 (i.e., textually equal) code clones of at least six lines of code are considered, except\nclones consisting entirely of import statements. Code clones are then categorized in\ntwo risk categories: redundant clones and nonredundant clones, as follows. Take a\nfragment of 10 lines of code that appears three times in the codebase. In other words,\nthere is a group of three code clones, each 10 lines of code. Theoretically, two of these\ncan be removed: they are considered technically redundant. Consequently, 10 + 10 =\n20 lines of code are categorized as redundant. One clone is categorized as nonredun\u2010\ndant, and hence, 10 lines of code are categorized as nonredundant. To be rated at 4\nstars, at most 4.6% of the total number of lines of code in the codebase can be catego\u2010\nrized as redundant. See Table 4-1\nTable 4-1. Minimum thresholds for a 4-star duplication rating (2015 version of the SIG/\nT\u00dcViT Evaluation Criteria)\nLines of code categorized as \u2026 Percentage allowed for 4 stars\n\u2026 nonredundant At least 95.4%\n\u2026 redundant At most 4.6%\nSee the three quality profiles in Figure 4-1 as an example:\n\u2022 Left: an open source system, in this case Jenkins\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for duplication\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic"}
{"79": "CHAPTER 5\nKeep Unit Interfaces Small\nBunches of data that hang around together really ought to be made into their own\nobject.\n\u2014Martin Fowler\nGuideline:\n\u2022 Limit the number of parameters per unit to at most 4.\n\u2022 Do this by extracting parameters into objects.\n\u2022 This improves maintainability because keeping the number of\nparameters low makes units easier to understand and reuse.\nThere are many situations in the daily life of a programmer where long parameter\nlists seem unavoidable. In the rush of getting things done, you might add a few\nparameters more to that one method in order to make it work for exceptional cases."}
{"80": "/// <param name=\"g\">The graphics context to draw on.</param>\n/// <param name=\"x\">The x position to start drawing.</param>\n/// <param name=\"y\">The y position to start drawing.</param>\n/// <param name=\"w\">The width of this square (in pixels.)</param>\n/// <param name=\"h\">The height of this square (in pixels.)</param>\nprivate void Render(Square square, Graphics g, int x, int y, int w, int h)\n{\nsquare.Sprite.Draw(g, x, y, w, h);\nforeach (Unit unit in square.Occupants)\n{\nunit.Sprite.Draw(g, x, y, w, h);\n}\n}\nThis method exceeds the parameter limit of 4. Especially the last four arguments, all\nof type int, make the method harder to understand and its usage more error-prone\nthan necessary. It is not unthinkable that after a long day of writing code, even an\nexperienced developer could mix up the x,y,w and h parameters\u2014a mistake that the\ncompiler and possibly even the unit tests will not catch.\nBecause the x,y,w, and h variables are related (they define a rectangle with a 2D\nanchor point, a width and a height), and the render method does not manipulate\nthese variables independently, it makes sense to group them into an object of type\nRectangle. The next code snippets show the Rectangle class and the refactored\nrender method:\npublic class Rectangle\n{\npublic Point Position { get; set; }\npublic int Width { get; set; }\npublic int Height { get; set; }\npublic Rectangle(Point position, int width, int height)\n{"}
{"81": "{\nPoint position = r.Position;\nsquare.Sprite.Draw(g, position.X, position.Y, r.Width, r.Height);\nforeach (Unit unit in square.Occupants)\n{\nunit.Sprite.Draw(g, position.X, position.Y, r.Width, r.Height);\n}\n}\nNow the render method has only three parameters instead of six. Next to that, in the\nwhole system we now have the Rectangle class available to work with. This allows us\nto also create a smaller interface for the draw method:\nprivate void Render(Square square, Graphics g, Rectangle r)\n{\nPoint position = r.Position;\nsquare.Sprite.Draw(g, r);\nforeach (Unit unit in square.Occupants)\n{\nunit.Sprite.Draw(g, r);\n}\n}\nThe preceding refactorings are an example of the Introduce Parameter Object refac\u2010\ntoring pattern. Avoiding long parameter lists, as shown in the previous example,\nimproves the readability of your code. In the next section, we explain why small inter\u2010\nfaces contribute to the overall maintainability of a system.\n5.1 Motivation\nAs we already discussed in the introduction, there are good reasons to keep interfaces\nsmall and to introduce suitable objects for the parameters you keep passing around in\nconjunction. Methods with small interfaces keep their context simple and thus are\neasier to understand. Furthermore, they are easier to reuse and modify because they\ndo not depend on too much external input."}
{"82": "Methods with Small Interfaces Are Easier to Modify\nLarge interfaces do not only make your methods obscure, but in many cases also\nindicate multiple responsibilities (especially when you feel that you really cannot\ngroup your objects together anymore). In this sense, interface size correlates with unit\nsize and unit complexity. So it is pretty obvious that methods with large interfaces are\nhard to modify. If you have, say, a method with eight parameters and a lot is going on\nin the method body, it can be difficult to see where you can split your method into\ndistinct parts. However, once you have done so, you will have several methods with\ntheir own responsibility, and moreover, each method will have a small number of\nparameters! Now it will be much easier to modify each of these methods, because you\ncan more easily locate exactly where your modification needs to be done.\n5.2 How to Apply the Guideline\nBy the time you have read this, you should be convinced that having small interfaces\nis a good idea. How small should an interface be? In practice, an upper bound of four\nseems reasonable: a method with four parameters is still reasonably clear, but a\nmethod with five parameters is already getting difficult to read and has too many\nresponsibilities.\nSo how can you ensure small interfaces? Before we show you how you can fix meth\u2010\nods with large interfaces, keep in mind that large interfaces are not the problem, but\nrather are indicators of the actual problem\u2014a poor data model or ad hoc code modi\u2010\nfication. So, you can view interface size as a code smell, to see whether your data\nmodel needs improvement.\nLarge interfaces are usually not the main problem; rather, they are a\ncode smell that indicates a deeper maintainability problem."}
{"83": "message1 + message2 + message3);\n// Send message\nm.Send(mId, subject, mMessage);\n}\nThe buildAndSendMail method clearly has too many responsibilities; the construc\u2010\ntion of the email address does not have much to do with sending the actual email.\nFurthermore, you would not want to confuse your fellow programmer with five\nparameters that together will make up a message body! We propose the following\nrevision of the method:\npublic void DoBuildAndSendMail(MailMan m, MailAddress mAddress,\nMailBody mBody)\n{\n// Build the mail\nMail mail = new Mail(mAddress, mBody);\n// Send the mail\nm.SendMail(mail);\n}\npublic class Mail\n{\npublic MailAddress Address { get; set; }\npublic MailBody Body { get; set; }\npublic Mail(MailAddress mAddress, MailBody mBody)\n{\nthis.Address = mAddress;\nthis.Body = mBody;\n}\n}\npublic class MailBody\n{\npublic string Subject { get; set; }\npublic MailMessage Message { get; set; }"}
{"84": "$\"@{division.Substring(0, 5)}.compa.ny\";\n}\n}\nThe buildAndSendMail method is now considerably less complex. Of course, you\nnow have to construct the email address and message body before you invoke the\nmethod. But if you want to send the same message to several addresses, you only have\nto build the message once, and similarly for the case where you want to send a bunch\nof messages to one email address. In conclusion, we have now separated concerns,\nand while we did so we introduced some nice, structured classes.\nThe examples presented in this chapter all group parameters into objects. Such\nobjects are often called data transfer objects or parameter objects. In the examples,\nthese new objects actually represent meaningful concepts from the domain. A point, a\nwidth, and a height represent a rectangle, so grouping these in a class called Rectan\ngle makes sense. Likewise, a first name, a last name, and a division make an address,\nso grouping these in a class called MailAddress makes sense, too. It is not unlikely\nthat these classes will see a lot of use in the codebase because they are useful generali\u2010\nzations, not just because they may decrease the number of parameters of a method.\nWhat if we have a number of parameters that do not fit well together? We can always\nmake a parameter object out of them, but probably, it will be used only once. In such\ncases, another approach is often possible, as illustrated by the following example.\nSuppose we are creating a library that can draw charts, such as bar charts and pie\ncharts, on a System.Drawing.Graphics canvas. To draw a nice-looking chart, you\nusually need quite a bit of information, such as the size of the area to draw on, config\u2010\nuration of the category axis and value axis, the actual dataset to chart, and so forth.\nOne way to supply this information to the charting library is like this:\npublic static void DrawBarChart(Graphics g,\nCategoryItemRendererState state,\nRectangle graphArea,\nCategoryPlot plot,"}
{"85": "public static void DrawBarChart(Graphics g, CategoryDataset dataset)\n{\nCharts.DrawBarChart(g,\nCategoryItemRendererState.DEFAULT,\nnew Rectangle(new Point(0, 0), 100, 100),\nCategoryPlot.DEFAULT,\nCategoryAxis.DEFAULT,\nValueAxis.DEFAULT,\ndataset);\n}\nThis covers the case where we want to use defaults for all parameters whose data\ntypes have a default value defined. However, that is just one case. Before you know it,\nyou are defining more than a handful of alternatives like these. And the version with\nseven parameters is still there.\nAnother way to solve this is to use the Replace Method with Method Object refactoring\ntechnique presented in Chapter 2. This refactoring technique is primarily used to\nmake methods shorter, but it can also be used to reduce the number of method\nparameters.\nTo apply the Replace Method with Method Object technique to this example, we\ndefine a BarChart class like this:\npublic class BarChart\n{\nprivate CategoryItemRendererState state =\nCategoryItemRendererState.DEFAULT;\nprivate Rectangle graphArea = new Rectangle(new Point(0, 0), 100, 100);\nprivate CategoryPlot plot = CategoryPlot.DEFAULT;\nprivate CategoryAxis domainAxis = CategoryAxis.DEFAULT;\nprivate ValueAxis rangeAxis = ValueAxis.DEFAULT;\nprivate CategoryDataset dataset = CategoryDataset.DEFAULT;\npublic BarChart Draw(Graphics g)\n{\n// .."}
{"86": "}\nThe static method drawBarChart from the original version is replaced by the (non\u2010\nstatic) method draw in this class. Six of the seven parameters of drawBarChart have\nbeen turned into of BarChart class. All of these have default values. We have chosen\nto keep parameter g (of type System.Drawing.Graphics) as a parameter of draw. This\nis a sensible choice: draw always needs a Graphics object, and there is no sensible\ndefault value. But it is not necessary: we could also have made g into the seventh pri\u2010\nvate member and supplied a getter and setter for it.\nWe made another choice: all setters return this to create what is called a fluent inter\u2010\nface. The setters can then be called in a cascading style, like so:\nprivate void ShowMyBarChart()\n{\nGraphics g = this.CreateGraphics();\nBarChart b = new BarChart()\n.SetRangeAxis(myValueAxis)\n.SetDataset(myDataset)\n.Draw(g);\n}\nIn this particular call of draw, we provide values for the range axis, dataset, and g, and\nuse default values for the other members of BarChart. We could have used more\ndefault values or fewer, without having to define additional overloaded draw methods.\n5.3 Common Objections to Keeping Unit Interfaces Small\nIt may take some time to get rid of all large interfaces. Typical objections to this effort\nare discussed next.\nObjection: Parameter Objects with Large Interfaces"}
{"87": "Refactoring Large Interfaces Does Not Improve My Situation\n\u201cWhen I refactor my method, I am still passing a lot of parameters to another method.\u201d\nGetting rid of large interfaces is not always easy. It usually takes more than refactor\u2010\ning one method. Normally, you should continue splitting responsibilities in your\nmethods, so that you access the most primitive parameters only when you need to\nmanipulate them separately. For instance, the refactored version of the render\nmethod needs to access all parameters in the Rectangle object because they are input\nto the draw method. But it would be better, of course, to also refactor the draw\nmethod to access the x,y,w, and h parameters inside the method body. In this way,\nyou have just passed a Rectangle in the render method, because you do not actually\nmanipulate its class variables before you begin drawing!\nFrameworks or Libraries Prescribe Interfaces with Long Parameter\nLists\n\u201cThe interface of a framework we\u2019re using has nine parameters. How can I implement\nthis interface without creating a unit interface violation?\u201d\nSometimes frameworks/libraries define interfaces or classes with methods that have\nlong parameter lists. Implementing or overriding these methods will inevitably lead\nto long parameter lists in your own code. These types of violations are impossible to\nprevent, but their impact can be limited. To limit the impact of violations caused by\nthird-party frameworks or libraries, it is best to isolate these violations\u2014for instance,\nby using wrappers or adapters. Selecting a different framework/library is also a viable\nalternative, although this can have a large impact on other parts of the codebase.\n5.4 See Also\nMethods with multiple responsibilities are more likely when the methods are large"}
{"88": "How SIG Rates Unit Interfacing\nUnit interfacing is one of the eight system properties of the SIG/T\u00dcViT Evaluation\nCriteria for Trusted Product Maintainability. To rate unit interfacing, every unit of the\nsystem is categorized in one of four risk categories depending on its number of\nparameters. Table 5-1 lists the four risk categories used in the 2015 version of the SIG/\nT\u00dcViT Evaluation Criteria.\nTable 5-1. Minimum thresholds for a 4-star unit size rating (2015 version of the SIG/\nT\u00dcViT Evaluation Criteria)\nLines of code in methods with \u2026 Percentage allowed for 4 stars for unit interfacing\n\u2026 more than seven parameters At most 0.7%\n\u2026 five or more parameters At most 2.7%\n\u2026 three or more parameters At most 13.8%\n\u2026 at most two parameters At least 86.2%\nSee the three quality profiles shown in Figure 5-1 as an example:\n\u2022 Left: an open source system, in this case Jenkins\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for unit interfacing\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic"}
{"89": "CHAPTER 6\nSeparate Concerns in Modules\nIn a system that is both complex and tightly coupled, accidents are inevitable.\n\u2014Charles Perrow\u2019s Normal Accidents theory in one sentence\nGuideline:\n\u2022 Avoid large modules in order to achieve loose coupling\nbetween them.\n\u2022 Do this by assigning responsibilities to separate modules\nand hiding implementation details behind interfaces.\n\u2022 This improves maintainability because changes in a loosely\ncoupled codebase are much easier to oversee and execute\nthan changes in a tightly coupled codebase.\nThe guidelines presented in the previous chapters are all what we call unit guidelines:"}
{"90": "We will use a true story to illustrate what tight coupling between classes is and why it\nleads to maintenance problems. This story is about how a class called UserService in\nthe service layer of a web application started growing while under development and\nkept on growing until it violated the guideline of this chapter.\nIn the first development iteration, the UserService class started out as a class with\nonly three methods, the names and responsibilities of which are shown in this code\nsnippet:\npublic class UserService\n{\npublic User LoadUser(string userId)\n{\n// ...\n}\npublic bool DoesUserExist(string userId)\n{\n// ...\n}\npublic User ChangeUserInfo(UserInfo userInfo)\n{\n// ...\n}\n}\n// end::UserSerice[]\n}\nIn this case, the backend of the web application provides a REST interface to the\nfrontend code and other systems.\nA REST interface is an approach for providing web services in a simplified manner.\nREST is a common way to expose functionality outside of the system. The class in\nthe REST layer that implements user operations uses the UserService class like this:\npublic class UserController : System.Web.Http.ApiController"}
{"91": "}\n}\nDuring the second development iteration, the UserService class is not modified at\nall. In the third development iteration, new requirements were implemented that\nallowed a user to register to receive certain notifications. Three new methods were\nadded to the UserService class for this requirement:\npublic class UserService\n{\npublic User LoadUser(string userId)\n{\n// ...\n}\npublic bool DoesUserExist(string userId)\n{\n// ...\n}\npublic User ChangeUserInfo(UserInfo userInfo)\n{\n// ...\n}\npublic List<NotificationType> GetNotificationTypes(User user)\n{\n// ...\n}\npublic void RegisterForNotifications(User user, NotificationType type)\n{\n// ...\n}\npublic void UnregisterForNotifications(User user, NotificationType type)\n{"}
{"92": "string notificationType)\n{\nUser user = userService.LoadUser(id);\nuserService.RegisterForNotifications(user,\nNotificationType.FromString(notificationType));\nreturn Ok();\n}\n[System.Web.Http.HttpPost]\n[System.Web.Http.ActionName(\"unregister\")]\npublic System.Web.Http.IHttpActionResult Unregister(string id,\nstring notificationType)\n{\nUser user = userService.LoadUser(id);\nuserService.UnregisterForNotifications(user,\nNotificationType.FromString(notificationType));\nreturn Ok();\n}\n}\nIn the fourth development iteration, new requirements for searching users, blocking\nusers, and listing all blocked users were implemented (management requested that\nlast requirement for reporting purposes). All of these requirements caused new meth\u2010\nods to be added to the UserService class.\npublic class UserService\n{\npublic User LoadUser(string userId)\n{\n// ...\n}\npublic bool DoesUserExist(string userId)\n{\n// ...\n}"}
{"93": "public void UnregisterForNotifications(User user, NotificationType type)\n{\n// ...\n}\npublic List<User> SearchUsers(UserInfo userInfo)\n{\n// ...\n}\npublic void BlockUser(User user)\n{\n// ...\n}\npublic List<User> GetAllBlockedUsers()\n{\n// ...\n}\n}\n// end::UserSerice[]\n}\nAt the end of this development iteration, the class had grown to an impressive size. At\nthis point the UserService class had become the most used service in the service\nlayer of the system. Three frontend views (pages for Profile, Notifications, and\nSearch), connected through three REST API services, used the UserService class.\nThe number of incoming calls from other classes (the fan-in) has increased to over\n50. The size of class has increased to more than 300 lines of code.\nThese kind of classes have what is called the large class smell, briefly discussed in\nChapter 4. The code contains too much functionality and also knows implementation\ndetails about the code that surrounds it. The consequence is that the class is now\ntightly coupled. It is called from a large number of places in the code, and the class\nitself knows details on other parts of the codebase. For example, it uses different data"}
{"94": "find the UserService class increasingly more difficult to understand as it becomes\nlarge and unmanageable. Less experienced developers on the team will find the class\nintimidating and will hesitate to make changes to it.\nTwo principles are necessary to understand the significance of coupling between\nclasses.\n\u2022 Coupling is an issue on the class level of source code. Each of the methods in\nUserService complies with all guidelines presented in the preceding chapters.\nHowever, it is the combination of methods in the UserService class that makes\nUserService tightly coupled with the classes that use it.\n\u2022 Tight and loose coupling are a matter of degree. The actual maintenance conse\u2010\nquence of tight coupling is determined by the number of calls to that class and the\nsize of that class. Therefore, the more calls to a particular class that is tightly cou\u2010\npled, the smaller its size should be. Consider that even when classes are split up,\nthe number of calls may not necessarily be lower. However, the coupling is then\nlower, because less code is coupled.\n6.1 Motivation\nThe biggest advantage of keeping classes small is that it provides a direct path toward\nloose coupling between classes. Loose coupling means that your class-level design will\nbe much more flexible to facilitate future changes. By \u201cflexibility\u201d we mean that you\ncan make changes while limiting unexpected effects of those changes. Thus, loose\ncoupling allows developers to work on isolated parts of the codebase without creating\nchange ripples that affect the rest of the codebase. A third advantage, which cannot be\nunderestimated, is that the codebase as a whole will be much more open to less expe\u2010\nrienced developers.\nThe following sections discuss the advantages of having small, loosely coupled classes"}
{"95": "Small, Loosely Coupled Modules Ease Navigation Through the\nCodebase\nNot only does a good separation of concerns keep the codebase flexible to facilitate\nfuture changes, it also improves the analyzability of the codebase since classes encap\u2010\nsulate data and implement logic to perform a single task. Just as it is easier to name\nmethods that only do one thing, classes also become easier to name and understand\nwhen they have one responsibility. Making sure classes have only one responsibility is\nalso known as the single responsibility principle.\nSmall, Loosely Coupled Modules Prevent No-Go Areas for New\nDevelopers\nClasses that violate the single responsibility principle become tightly coupled and accu\u2010\nmulate a lot of code over time. As with the UserService example in the introduction\nof this chapter, these classes become intimidating to less experienced developers, and\neven experienced developers are hesitant to make changes to their implementation. A\ncodebase that has a large number of classes that lack a good separation of concerns is\nvery difficult to adapt to new requirements.\n6.2 How to Apply the Guideline\nIn general, this guideline prescribes keeping your classes small (by addressing only\none concern) and limiting the number of places where a class is called by code out\u2010\nside the class itself. Following are three development best practices that help to pre\u2010\nvent tight coupling between classes in a codebase.\nSplit Classes to Separate Concerns\nDesigning classes that collectively implement functionality of a software system is the\nmost essential step in modeling and designing object-oriented systems. In typical"}
{"96": "}\npublic void Register(User user, NotificationType type)\n{\n// ...\n}\npublic void Unregister(User user, NotificationType type)\n{\n// ...\n}\n}\npublic class UserBlockService\n{\npublic void BlockUser(User user)\n{\n// ...\n}\npublic IList<User> GetAllBlockedUsers()\n{\n// ...\n}\n}\npublic class UserService\n{\npublic User LoadUser(string userId)\n{\n// ...\n}\npublic bool DoesUserExist(string userId)\n{\n// ...\n}"}
{"97": "more likely to put new functionalities in separate classes instead of defaulting to the\nUserService class.\nHide Specialized Implementations Behind Interfaces\nWe can also achieve loose coupling by hiding specific and detailed implementations\nbehind a high-level interface. Consider the following class, which implements the\nfunctionality of a digital camera that can take snapshots with the flash on or off:\npublic class DigitalCamera\n{\npublic Image TakeSnapshot()\n{\n// ...\n}\npublic void FlashLightOn()\n{\n// ...\n}\npublic void FlashLightOff()\n{\n// ...\n}\n}\nAnd suppose this code runs inside an app on a smartphone device, like this:\npublic class SmartphoneApp\n{\nprivate static DigitalCamera camera = new DigitalCamera();\npublic static void Main(string[] args)\n{\n// ...\nImage image = camera.TakeSnapshot();"}
{"98": "public void FlashLightOn()\n{\n// ...\n}\npublic void FlaslLightOff()\n{\n// ...\n}\npublic Image TakePanoramaSnapshot()\n{\n// ...\n}\npublic Video Record()\n{\n// ...\n}\npublic void SetTimer(int seconds)\n{\n// ...\n}\npublic void ZoomIn()\n{\n// ...\n}\npublic void ZoomOut()\n{\n// ...\n}\n}\nFrom this example implementation, it is not difficult to imagine that the extended\nDigitalCamera"}
{"99": "public interface ISimpleDigitalCamera\n{\nImage TakeSnapshot();\nvoid FlashLightOn();\nvoid FlashLightOff();\n}\npublic class DigitalCamera : ISimpleDigitalCamera\n{\n// ...\n}\npublic class SmartphoneApp\n{\nprivate static ISimpleDigitalCamera camera = SDK.GetCamera();\npublic static void Main(string[] args)\n{\n// ...\nImage image = camera.TakeSnapshot();\n// ...\n}\n}\nThis change leads to lower coupling by a higher degree of encapsulation. In other\nwords, classes that use only basic digital camera functionalities now do not know\nabout all of the advanced digital camera functionalities. The SmartphoneApp class\naccesses only the SimpleDigitalCamera interface. This guarantees that Smart\nphoneApp does not use any of the methods of the more advanced camera.\nAlso, this way your system becomes more modular: it is composed such that a change\nto one class has minimal impact on other classes. This, in turn, increases modifiabil\u2010\nity: it is easier and less work to modify the system, and there is less risk that modifica\u2010\ntions introduce defects."}
{"100": "6.3 Common Objections to Separating Concerns\nThe following are typical objections to the principle explained in this chapter.\nObjection: Loose Coupling Conflicts With Reuse\n\u201cTight coupling is a side effect of code reuse, so this guideline conflicts with that best\npractice.\u201d\nOf course, code reuse can increase the number of calls to a method. However, there\nare two reasons why this should not lead to tight coupling:\n\u2022 Reuse does not necessarily lead to methods that are called from as many places as\npossible. Good software design\u2014for example, using inheritance and hiding\nimplementation behind interfaces\u2014will stimulate code reuse while keeping the\nimplementation loosely coupled, since interfaces hide implementation details.\n\u2022 Making your code more generic, to solve more problems with less code, does not\nmean it should become a tightly coupled codebase. Clearly, utility functionality is\nexpected to be called from more places than specific functionality. Utility func\u2010\ntionality should then also embody less source code. In that way, there may be\nmany incoming dependencies, but the dependencies refer to a small amount of\ncode.\nObjection: C# Interfaces Are Not Just for Loose Coupling\n\u201cIt doesn\u2019t make sense to use C# interfaces to prevent tight coupling.\u201d\nIndeed, using interfaces is a great way to improve encapsulation by hiding implemen\u2010\ntations, but it does not make sense to provide an interface for every class. As a rule of\nthumb, an interface should be implemented by at least two classes in your codebase.\nConsider splitting your class if the only reason to put an interface in front of your"}
{"101": "Objection: Not All Loose Coupling Solutions Increase Maintainability\n\u201cFrameworks that implement inversion of control (IoC) achieve loose coupling but make\nit harder to maintain the codebase.\u201d\nInversion of control is a design principle to achieve loose coupling. There are frame\u2010\nworks available that implement this for you. IoC makes a system more flexible for\nextension and decreases the amount of knowledge that pieces of code have of each\nother.\nThis objection holds when such frameworks add complexity for which the maintain\u2010\ning developers are not experienced enough. Therefore, in cases where this objection\nis true, it is not IoC that is the problem, but the framework that implements it.\nThus, the design decision to use a framework for implementing IoC should be con\u2010\nsidered with care. As with all engineering decisions, this is a trade-off that does not\npay off in all cases. Using these types of frameworks just to achieve loose coupling is a\nchoice that can almost never be justified."}
{"102": "How SIG Rates Module Coupling\nModule coupling is one of the eight system properties of the SIG/T\u00dcViT Evaluation\nCriteria for Trusted Product Maintainability. To rate module coupling, the fan-in of\nevery method is calculated. Each module (class in C#) is then categorized in one of\nfour risk categories depending on the total fan-in of all methods in the class. Table 6-1\nlists the four risk categories used in the 2015 version of the SIG/T\u00dcViT Evaluation\nCriteria. The table shows the maximum amount of code that may fall in the risk cate\u2010\ngories in order to achieve a 4-star rating. For example, a maximum of 21.8% of code\nvolume may be in classes with a fan-in in the moderate risk category, and likewise for\nthe other risk categories.\nTable 6-1. Module coupling risk categories (2015 version of the SIG/T\u00dcViT Evaluation\nCriteria)\nFan-in of modules in the category Percentage allowed for 4 stars\n51+ At most 6.6%\n21\u201350 At most 13.8%\n11\u201320 At most 21.6%\n1\u201310 No constraint\nSee the three quality profiles in Figure 6-1 as an example:\n\u2022 Left: an open source system, in this case Jenkins. Note that Jenkins does not fulfill\nthe 4-star requirement here for the highest risk category (in red).\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for module coupling.\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic."}
{"103": "CHAPTER 7\nCouple Architecture Components Loosely\nThere are two ways of constructing a software design: one way is to make it so simple\nthat there are obviously no deficiencies, and the other way is to make it so complicated\nthat there are no obvious deficiencies.\n\u2014C.A.R. Hoare\nGuideline:\n\u2022 Achieve loose coupling between top-level components.\n\u2022 Do this by minimizing the relative amount of code within\nmodules that is exposed to (i.e., can receive calls from)\nmodules in other components.\n\u2022 This improves maintainability because independent compo\u2010\nnents ease isolated maintenance."}
{"104": "However, the implementation of software architecture always remains the responsi\u2010\nbility of you as a developer.\nComponents should be loosely coupled; that is, they should be clearly separated by\nhaving few entry points for other components and a limited amount of information\nshared among components. In that case, implementation details of methods are hid\u2010\nden (or encapsulated) which makes the system more modular.\nSounds familiar? Yes, both as a general design principle and on a module level, loose\ncoupling has been discussed in Chapter 6. Component coupling applies the same rea\u2010\nsoning but at the higher level of components rather than modules. Module coupling\nfocuses on the exposure of individual modules (classes) to the rest of the codebase.\nComponent coupling focuses specifically on the exposure of modules in one compo\u2010\nnent (group of modules) to the modules in another component.\nSo a module being called from a module in the same component is\nconsidered to be an internal call if we assess at the component level,\nbut when we assess it at the module level, there is module coupling\nindeed.\nIn this chapter, we refer to the characteristic of being loosely coupled on a component\nlevel as component independence. The opposite of component independence is com\u2010\nponent dependence. In that case, the inner workings of components are exposed too\nmuch to other components that rely on them. That kind of entanglement makes it\nharder to oversee effects that code changes in one component may have on others,\nbecause it does not behave in an isolated manner. This complicates testing, when we\nmust make assumptions or simulations of what happens within another component.\n7.1 Motivation\nSystem maintenance is easier when changes within a component have effects that are"}
{"105": "Figure 7-1. Low component dependence (left) and high component dependence (right)\nThe left side of the figure shows a low level of component dependence. Most calls\nbetween modules are internal (within the component). Let us elaborate on internal\nand noninternal dependencies.\nCalls that improve maintainability:\n\u2022 Internal calls are healthy. Since the modules calling each other are part of the\nsame component, they should implement closely related functionality. Their\ninner logic is hidden from the outside.\n\u2022 Outgoing calls are also healthy. As they delegate tasks to other components, they\ncreate a dependency outward. In general, delegation of distinct concerns to other\ncomponents is a good thing. Delegation can be done from anywhere within a\ncomponent and does not need to be restricted to a limited set of modules within\nthe component."}
{"106": "Calls that have a negative impact on maintainability:\n\u2022 Incoming calls provide functionality for other components by offering an inter\u2010\nface. The code volume that is involved in this should be limited. Conversely, the\ncode within a component should be encapsulated as much as possible\u2014that is, it\nshould be shielded against direct invocations from other components. This\nimproves information hiding. Also, modifying code involved in incoming depen\u2010\ndencies potentially has a large impact on other components. By having a small\npercentage of code involved in incoming dependencies, you may dampen the\nnegative ripple effects of modifications to other components.\n\u2022 Throughput code is risky and must be avoided. Throughput code both receives\nincoming calls and delegates to other components. Throughput code accom\u2010\nplishes the opposite of information hiding: it exposes its delegates (implementa\u2010\ntion) to its clients. It is like asking a question to a help desk that does not\nformulate its own answer but instead forwards your question to another com\u2010\npany. Now you are dependent on two parties for the answer. In the case of code,\nthis indicates that responsibilities are not well divided over components. As it is\nhard to trace back the path that the request follows, it is also hard to test and\nmodify: tight coupling may cause effects to spill over to other components.\nThe right side of the figure shows a component with a high level of component\ndependence. The component has many dependencies with modules outside the com\u2010\nponent and is thus tightly coupled. It will be hard to make isolated changes, since the\neffects of changes cannot be easily overseen.\nNote that the effects of component independence are enhanced by\ncomponent balance. Component balance is achieved when the num\u2010\nber of components and their relative size are balanced. For elabora\u2010\ntion on this topic, see Chapter 8."}
{"107": "Figure 7-2. Designed versus implemented architecture\nLow Component Dependence Allows for Isolated Maintenance\nA low level of dependence means that changes can be made in an isolated manner.\nThis applies when most of a component\u2019s code volume is either internal or outgoing.\nIsolated maintenance means less work, as coding changes do not have effects outside\nthe functionality that you are modifying.\nNote that this reasoning about isolation applies to code on a\nsmaller level. For example, a system consisting of small, simple\nclasses signals a proper separation of concerns, but does not guar\u2010\nantee it. For that, you will need to investigate the actual dependen\u2010\ncies (see, for example, Chapter 6).\nLow Component Dependence Separates Maintenance Responsibilities\nIf all components are independent from each other, it is easier to distribute responsi\u2010\nbilities for maintenance among separate teams. This follows from the advantage of\nisolated modification. Isolation is in fact a prerequisite for efficient division of devel\u2010"}
{"108": "Low Component Dependence Eases Testing\nCode that has a low dependence on other components (modules with mainly internal\nand outgoing code) is easier to test. For internal calls, functionality can be traced and\ntested within the component. For outgoing calls, you do not need to mock or stub\nfunctionality that is provided by other components (given that functionality in that\nother component is finished).\nFor elaboration on (unit) testing, see also Chapter 10.\n7.2 How to Apply the Guideline\nThe goal for this chapter\u2019s guideline is to achieve loose coupling between compo\u2010\nnents. In practice, we find that you can help yourself by adhering to the following\nprinciples for implementing interfaces and requests between components.\nThe following principles help you apply the guideline of this chapter:\n\u2022 Limit the size of modules that are the component\u2019s interface.\n\u2022 Define component interfaces on a high level of abstraction. This limits the types\nof requests that cross component borders. That avoids requests that \u201cknow too\nmuch\u201d about the implementation details.\n\u2022 Avoid throughput code, because it has the most serious effect on testing func\u2010\ntionality. In other words, avoid interface modules that put through calls to other\ncomponents. If throughput code exists, analyze the concerned modules in order\nto solve calls that are put through to other components.\nAbstract Factory Design Pattern\nComponent independence reflects the high-level architecture of a software system."}
{"109": "The Abstract Factory design pattern hides (or encapsulates) the creation of specific\n\u201cproducts\u201d behind a generic \u201cproduct factory\u201d interface. In this context, products are\ntypically entities for which more than one variant exists. Examples are audio format\ndecoder/encoder algorithms or user interface widgets that have different themes\nfor \u201clook and feel.\u201d In the following example, we use the Abstract Factory design pat\u2010\nten to encapsulate the specifics of cloud hosting platforms behind a small factory\ninterface.\nSuppose our codebase contains a component, called PlatformServices, that imple\u2010\nments the management of services from a cloud hosting platform. Two specific cloud\nhosting providers are supported by the PlatformServices component: Amazon AWS\nand Microsoft Azure (more could be added in the future).\nTo start/stop servers and reserve storage space, we have to implement the following\ninterface for a cloud hosting platform:\npublic interface ICloudServerFactory\n{\nICloudServer LaunchComputeServer();\nICloudServer LaunchDatabaseServer();\nICloudStorage CreateCloudStorage(long sizeGb);\n}\nBased on this interface, we create two specific factory classes for AWS and Azure:\npublic class AWSCloudServerFactory : ICloudServerFactory\n{\npublic ICloudServer LaunchComputeServer()\n{\nreturn new AWSComputeServer();\n}\npublic ICloudServer LaunchDatabaseServer()\n{"}
{"110": "return new AzureDatabaseServer();\n}\npublic ICloudStorage CreateCloudStorage(long sizeGb) {\nreturn new AzureCloudStorage(sizeGb);\n}\n}\nNote that these factories make calls to specific AWS and Azure implementation\nclasses (which in turn do specific AWS and Azure API calls), but return generic inter\u2010\nface types for servers and storage.\nCode outside the PlatformServices component can now use the concise interface\nmodule ICloudServerFactory\u2014for example, like this:\npublic class ApplicationLauncher\n{\npublic static void Main(string[] args)\n{\nICloudServerFactory factory;\nif (args[1].Equals(\"-azure\"))\n{\nfactory = new AzureCloudServerFactory();\n}\nelse\n{\nfactory = new AWSCloudServerFactory();\n}\nICloudServer computeServer = factory.LaunchComputeServer();\nICloudServer databaseServer = factory.LaunchDatabaseServer();\nThe ICloudServerFactory interface of the PlatformServices provides a small inter\u2010\nface for other components in the codebase. This way, these other components can be\nloosely coupled to it.\n7.3 Common Objections to Loose Component Coupling"}
{"111": "Objection: Component Dependence Cannot Be Fixed Because the\nComponents Are Entangled\n\u201cWe cannot get component dependence right because of mutual dependencies between\ncomponents.\u201d\nEntangled components are a problem that you experience most clearly during main\u2010\ntenance. You should start by analyzing the modules in the throughput category, as it\nhas the most serious effect on the ease of testing and on predicting what exactly the\nfunctionality does.\nWhen you achieve clearer boundaries for component responsibilities, it improves the\nanalyzability and testability of the modules within. For example, modules with an\nextraordinary number of incoming calls may signal that they have multiple responsi\u2010\nbilities and can be split up. When they are split up, the code becomes easier to analyze\nand test. For elaboration, please refer to Chapter 6.\nObjection: No Time to Fix\n\u201cIn the maintenance team, we understand the importance of achieving low component\ndependence, but we are not granted time to fix it.\u201d\nWe understand how this is an issue. Development deadlines are real, and there may\nnot be time for refactoring, or what a manager may see as \u201ctechnical aesthetics.\u201d What\nis important is the trade-off. One should resolve issues that pose a real problem for\nmaintainability. So dependencies should be resolved if the team finds that they inhibit\ntesting, analysis, or stability. You can solidify your case by measuring what percentage\nof issues arises/maintenance effort is needed in components that are tightly coupled\nwith each other.\nFor example, throughput code follows complex paths that are hard to test for devel\u2010\nopers. There may be more elegant solutions that require less time and effort."}
{"112": "Objection: Throughput Is a Requirement\n\u201cWe have a requirement for a software architecture for a layer that puts through calls.\u201d\nIt is true that some architectures are designed to include an intermediate layer. Typi\u2010\ncally, this is a service layer that collects requests from one side (e.g., the user interface)\nand bundles them for passing on to another layer in the system. The existence of such\na layer is not necessarily a problem\u2014given that this layer implements loose coupling.\nIt should have a clear separation of incoming and outgoing requests. So the module\nthat receives requests in this layer:\n\u2022 Should not process the request itself.\n\u2022 Should not know where and how to process that request (its implementation\ndetails).\nIf both are true, the receiving module in the service layer has an incoming request\nand an outgoing request, instead of putting requests through to a specific module in\nthe receiving component.\nA large-volume service layer containing much logic is a typical code smell. In that\ncase, the layer does not merely abstract and pass on requests, but also transforms\nthem. Hence, for transformation, the layer knows about the implementation details.\nThat means that the layer does not properly encapsulate both request and implemen\u2010\ntation. If throughput code follows from software architecture requirements, you may\nraise the issue to the software or enterprise architect.\n7.4 See Also\nA related concept to component independence is that of component balance, dis\u2010\ncussed in Chapter 8. That chapter deals with achieving an overseeable number of\ncomponents that are balanced in size."}
{"113": "\u2022 Hidden code is composed of modules that have no incoming dependencies from\nmodules in other components: they call only within their own component (inter\u2010\nnal) and may have calls outside their own component (outgoing calls).\n\u2022 Interface code is composed of modules that have incoming dependencies from\nmodules in other components. They consist of code in modules with incoming\nand throughput code.\nFollowing the principle of loose coupling, a low level of dependence between modules\nis better than a high level of dependence. That signals the risk that changes within one\ncomponent propagate to other components.\nSIG measures component independence as the percentage of code that is classified as\nhidden code. To achieve a SIG/T\u00dcViT rating of 4 stars for highly-maintainable soft\u2010\nware, the percentage of code residing in modules with incoming dependencies from\nother components (incoming or throughput) should not exceed 14.2%.\nSee the three quality profiles in Figure 7-3 as an example:\n\u2022 Left: an open source system, in this case Jenkins\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for component independence\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic"}
{"114": ""}
{"115": "CHAPTER 8\nKeep Architecture Components Balanced\nBuilding encapsulation boundaries is a crucial skill in software architecture.\n\u2014George H. Fairbanks in Just Enough Architecture\nGuideline:\n\u2022 Balance the number and relative size of top-level compo\u2010\nnents in your code.\n\u2022 Do this by organizing source code in a way that the number\nof components is close to 9 (i.e., between 6 and 12) and that\nthe components are of approximately equal size.\n\u2022 This improves maintainability because balanced components\nease locating code and allow for isolated maintenance.\nA well-balanced software architecture is one with not too many and not too few com\u2010"}
{"116": "narily large, the architecture becomes monolithic; it becomes hard to navigate the\ncodebase and do isolated maintenance. In the third situation (bottom left), where the\narchitecture is scattered among many components, it becomes hard to keep a mental\nmap of the codebase and to grasp how components interact."}
{"117": "8.1 Motivation\nNow we know what component balance is, but not why it is important. The reason is\nsimple: software maintenance is easier when the software architecture is balanced.\nThis section discusses in what ways you can benefit from a good system component\nbalance: it makes it easier to find and analyze code, it better isolates effects of mainte\u2010\nnance, and it separates maintenance responsibilities.\nA Good Component Balance Eases Finding and Analyzing Code\nA clear code organization in components makes it easier to find the piece of code that\nyou want to change. Of course, proper code hygiene helps in this process as well, such\nas using a consistent naming convention (see Chapter 11). When the number of com\u2010\nponents is manageable (around nine) and their volume is consistent, they allow for a\ndrill-down each time that you need to analyze code to modify it.\nIn contrast, an unbalanced organization of components is more likely to have unclear\nfunctional boundaries. For example, a component that is very large compared to oth\u2010\ners is more likely to contain functionalities that are unrelated, and therefore that\ncomponent is harder to analyze.\nA Good Component Balance Better Isolates Maintenance Effects\nWhen a system\u2019s component balance clearly describes functional boundaries, it has a\nproper separation of concerns, which makes for isolated behavior in the system. Iso\u2010\nlated behavior within system components is relevant because it guards against unex\u2010\npected effects, such as regression.\nMore broadly, isolation of code within components has the general advantage of\nmodularity: components with clear functional and technical boundaries are easier to\nsubstitute, remove, and test than components with mixed functionalities and techni\u2010\ncal intertwinement."}
{"118": "This reasoning about isolation applies to code on a smaller level as\nwell. For example, a system that consists of small, simple classes\nsignals a proper separation of concerns, but does not guarantee it.\nFor that you will need to investigate the actual dependencies (see,\ne.g., Chapter 6).\nA Good Component Balance Separates Maintenance Responsibilities\nHaving clear functional boundaries between components makes it easier to distribute\nresponsibilities for maintenance among separate teams. The number of components\nof a system and their relative size should indicate the system\u2019s decomposition into\nfunctional groups.\nWhen a system has too many or too few components, it is considered more difficult\nto understand and harder to maintain. If the number of components is too low, it\ndoes not help you much to navigate through the functionalities of the system. On the\nother hand, too many components make it hard to get a clear overview of the entire\nsystem.\n8.2 How to Apply the Guideline\nThe two principles of component balance are:\n\u2022 The number of top-level system components should ideally be 9, and generally\nbetween 6 and 12.\n\u2022 The components\u2019 volume in terms of source code should be roughly equal.\nNote that component balance is an indicator for a clear component\nseparation, not a goal in itself. It should follow from the system"}
{"119": "Decide on the Right Conceptual Level for Grouping Functionality into\nComponents\nTo achieve a good system division that is easy to navigate for developers, you need to\nchoose the right conceptual level for grouping functionality. Usually, software systems\nare organized along high-level functional domains that describe what kind of func\u2010\ntions the system performs for the user. Alternatively, a division is made along the sep\u2010\narations of technical specialities.\nFor example, a system that bases component division on function domains might\nhave components like Data Retrieval, Invoice Administration, Reporting, Adminis\u2010\ntrator, and so on. Each component contains source code that offers end-to-end func\u2010\ntionality, ranging from the database to the frontend. A functional division has\nthe advantage of being available during design, before development starts. For devel\u2010\nopers, it has the advantage that they can analyze source code while thinking in\nhigh-level functionalities. A disadvantage can be that developers may need to be pro\u2010\nficient and comfortable in multiple technical domains to make changes to a single\ncomponent.\nAn example of a system that uses technical division might have components like\nFrontend, Backend, Interfaces, Logging, and so on. This approach has advantages for\nteams that have a division of responsibilities based on technology specialization. The\ncomponent division then reflects the division of labor among various specialists.\nChoosing the right concepts for grouping functionality within a system is part of the\nsoftware architect role. This role may be assigned to a single person, or it may be dis\u2010\ntributed over various people within the development team. When changes are needed\nto the component division, those in the architect role must be consulted.\nClarify the System\u2019s Domains and Apply Those Consistently\nOnce a choice for the type of system division into components has been made, you"}
{"120": "8.3 Common Objections to Balancing Components\nThis section discusses objections regarding component balance. Common objections\nare that component imbalance is not really a problem, or that it is a problem that can\u2010\nnot be fixed.\nObjection: Component Imbalance Works Just Fine\n\u201cOur system may seem to have bad component balance, but we\u2019re not having any prob\u2010\nlems with it.\u201d\nComponent balance, as we define it, is not binary. There are different degrees of bal\u2010\nance, and its definition allows for some deviation from the \u201cideal\u201d of nine compo\u2010\nnents of equal size. Whether a component imbalance is an actual maintenance\nburden depends on the degree of deviation, the experience of the maintenance team,\nand the cause of the imbalance.\nThe most important maintenance burden occurs when the imbalance is caused by\nlack of discipline during maintenance\u2014when developers do not put code in the com\u2010\nponent where it belongs. Since inconsistency is the enemy of predictability, that may\nlead to unexpected effects. Code that is placed in the wrong components may lead to\nunintended dependencies between components, which hurts testability and flexibility.\nObjection: Entanglement Is Impairing Component Balance\n\u201cWe cannot get component balance right because of entanglement among components.\u201d\nThis situation points to another problem: technical dependence between compo\u2010\nnents. Entanglement between components signals an improper separation of con\u2010\ncerns. This issue and guideline are further described in Chapter 7. In this case, it is\nmore important and urgent to fix component dependencies\u2014for example, by hiding\nimplementation details behind interfaces and fixing circular dependencies. After that,"}
{"121": "How SIG Rates Component Balance\nSIG defines and measures component balance as a combined calculation (i.e., multi\u2010\nplication) of the following:\n\u2022 The number of top-level system components\n\u2022 The uniformity of component size\nThe ideal number of top-level system components is nine, as SIG has identified that as\nthe median in its benchmark. The closer the actual number of components is to nine,\nthe better.\nThe score for the number of top-level system components ranges from 0 to 1. A sys\u2010\ntem with nine components gives a score of 1, linearly decreasing to 0 for a system\nwith one component. A correction is applied upward, to allow for a more lenient\ncount when the number of components is higher than 17, which would otherwise\nlead to a score of 0 with a linear model. The correction is based on the 95th percentile\nscores within the benchmark.\nUniformity of component size means the distribution of source code volume between\ncomponents. An equally sized distribution of top-level components is better than an\nunequal distribution.\nSIG uses the adjusted Gini coefficient as a measure of component size uniformity.\nThe Gini coefficient measures the inequality of distribution between things and\nranges from 0 (perfect equality) to 1 (perfect inequality).\nTo achieve a SIG/T\u00dcViT rating of 4 stars for highly-maintainable software, the num\u2010\nber of components should be close to the ideal of nine, and the adjusted Gini coeffi\u2010\ncient of the component sizes should be 0.71 maximum.\nSee the volume charts in Figure 8-2 as an example:"}
{"122": ""}
{"123": "CHAPTER 9\nKeep Your Codebase Small\nProgram complexity grows until it exceeds the capability of the programmer who must\nmaintain it.\n\u20147th Law of Computer Programming\nGuideline:\n\u2022 Keep your codebase as small as feasible.\n\u2022 Do this by avoiding codebase growth and actively reducing\nsystem size.\n\u2022 This improves maintainability because having a small prod\u2010\nuct, project, and team is a success factor.\nA codebase is a collection of source code that is stored in one repository, can be com\u2010\npiled and deployed independently, and is maintained by one team. A system has at"}
{"124": "Given two systems with the same functionality, in which one has a small codebase\nand the other has a large codebase, you surely would prefer the small system. In a\nsmall system it is easier to search through, analyze, and understand code. If you mod\u2010\nify something, it is easier to tell whether the change has effects elsewhere in the sys\u2010\ntem. This ease of maintenance leads to fewer mistakes and lower costs. That much is\nobvious.\n9.1 Motivation\nSoftware development and maintenance become increasingly hard with growing sys\u2010\ntem size. Building larger systems requires larger teams and longer-lasting projects,\nwhich bring additional overhead and risks of (project) failure. The rest of this section\ndiscusses the adverse effects of large software systems.\nA Project That Sets Out to Build a Large Codebase Is More Likely to\nFail\nThere is a strong correlation between project size and project risks. A large project\nleads to a larger team, complex design, and longer project duration. As a result, there\nis more complex communication and coordination among stakeholders and team\nmembers, less overview over the software design, and a larger number of require\u2010\nments that change during the project. This all increases the chance of reduced quality,\nproject delays, and project failure. The probabilities in the graph in Figure 9-1 are\ncumulative: for example, for all projects over 500 man-years of development effort,\nmore than 90% are indentified as \u201cpoor project quality.\u201d A subset of this is projects\nwith delays (80\u201390% of the total) and failed projects (50% of the total).\nFigure 9-1 illustrates the relationship between project size and project failure: it\nshows that as the size of a project increases, the chances of project failure (i.e., project\nis terminated or does not deliver results), of project delay, and of a project delivered\nwith poor quality are increasingly high."}
{"125": "Figure 9-1. Probability of project failures by project size1"}
{"126": "Large Codebases Are Harder to Maintain\nFigure 9-2 illustrates how codebase size affects maintainability.\nFigure 9-2. Distribution of system maintainability in SIG benchmark among different\nvolume groups\nThe graph is based on a set of codebases of over 1,500 systems in the SIG Software\nAnalysis Warehouse. Volume is measured as the amount of development effort in\nman-years to reproduce the system (see also \u201cHow SIG Rates Codebase Volume\u201d on\npage 110). Each bar shows the distribution of systems in different levels of maintaina\u2010\nbility (benchmarked in stars). As the graph shows, over 30% of systems in the small\u2010\nest volume category manage to reach 4- or 5-star maintainability, while in the largest"}
{"127": "Figure 9-3. Impact of code volume on the number of defects2\n9.2 How to Apply the Guideline\nAll other things being equal, a system that has less functionality will be smaller than a\nsystem that has more functionality. Then, the implementation of that functionality\nmay be either concise or verbose. Therefore, achieving a small codebase first requires\nkeeping the functionality of a system limited, and then requires attention to keep the\namount of code limited.\nFunctional Measures\nFunctionality-related measures are not always within your span of control, but when\u2010\never new or adapted functionality is being discussed with developers, the following\nshould be considered:"}
{"128": "Standardize functionality:\nBy standardization of functionality we mean consistency in the behavior and\ninteractions of the program. First of all, this is intended to avoid the implementa\u2010\ntion of the same core functionality in multiple, slightly different ways. Secondly,\nstandardization of functionality offers possibilities for reuse of code\u2014assuming\nthe code itself is written in a reusable way.\nTechnical Measures\nFor the technical implementation, the goal is to use less code to implement the same\nfunctionality. You can achieve this mainly through reusing code by referral (instead\nof writing or copying and pasting code again) or by avoiding coding altogether, but\nusing existing libraries or frameworks.\nDo not copy and paste code:\nReferring to existing code is always preferable to copying and pasting code in\npieces that will need to be maintained individually. If there are multiple copies of\na piece of code, maintenance needs to occur in multiple places, too. Mistakes\neasily crop up if an update in one piece of logic requires individual adjustment\n(or not) and testing of multiple, scattered copies. Note that the intention of the\nguideline presented in Chapter 4 is precisely to avoid copying and pasting.\nRefactor existing code:\nWhile refactoring has many merits for code maintainability, it can have an imme\u2010\ndiate and visible effect in reducing the codebase. Typically, refactoring involves\nrevisiting code, simplifying its structure, removing code redundancies, and\nimproving the amount of reuse. This may be as simple as removing unused/obso\u2010\nlete functionality. See, for example, the refactoring patterns in Chapter 4.\nUse third-party libraries and frameworks:\nMany applications share the same type of behavior for which a vast number of\nframeworks and libraries exist\u2014for example, UI behavior (e.g., jQuery), database"}
{"129": "Do not make changes to the source code of a third-party library. If\nyou do, essentially you have made the library code part of your\nown codebase. In particular, updates of changed libraries are cum\u2010\nbersome and can easily lead to bugs. Typically, difficulties arise\nwhen developers try to update the library to a newer version, since\nthey need to analyze what has been changed in the library code and\nhow that impacts the locally changed code.\nSplit up a large system:\nSplitting up a large system into multiple smaller systems is a way to minimize the\nissues that come with larger systems. A prerequisite is that the system can be\ndivided into parts that are independent, from a functional, technical, and lifecycle\nperspective. To the users, the systems (or plugins) must be clearly separated.\nTechnically, the code in the different systems must be loosely coupled; that is,\ntheir code is related via interfaces instead of direct dependencies. Systems are\nonly really independent if their lifecycles are decoupled (i.e., they are developed\nand released independently). Note that the split systems may well have some\nmutual or shared dependencies. There is an additional advantage. It might turn\nout that some of the new subsystems can be replaced by a third-party package,\ncompletely removing the need to have any codebase for this subsystem. An\nexample is a Linux distribution such as Ubuntu. The Linux kernel is a codebase\nthat lives at kernel.org and is maintained by a large team of volunteers headed by\nLinus Torvalds. Next to the actual Linux kernel, a distribution contains thou\u2010\nsands of other software applications, each of which has its own codebase. These\nare the types of plugins that we mean here.\nDecoupling (on a code level) is discussed in more detail in the chapters that deal with\nloose coupling, particularly Chapter 7.\n9.3 Common Objections to Keeping the Codebase Small"}
{"130": "The most visible improvements will appear once a system is big and parts of it can be\nremoved\u2014for example, when functionality is being replaced by third-party code or\nafter a system has been split into multiple parts.\nObjection: Reducing the Codebase Size Is Impeded by Productivity\nMeasures\n\u201cI cannot possibly reduce the size of my system, since my programming productivity is\nbeing measured in terms of added code volume.\u201d\nIf this is the case, we suggest escalating this issue. Measuring development productiv\u2010\nity in terms of added code volume is a bad practice. It provides a negative incentive,\nas it encourages the bad habit of copying and pasting code. Code reference is better\nbecause it improves analyzing, testing, and changing code.\nWe understand that the number of code additions can help managers monitor pro\u2010\ngress and predict timelines. However, productivity should be measured in terms of\nvalue added, not lines of code added. Experienced developers can often add function\u2010\nality with a minimum number of additional lines of code, and they will refactor the\ncode whenever they see an opportunity, often resulting in reduction of the code size.\nObjection: Reducing the Codebase Size is Impeded by the\nProgramming Language\n\u201cI work with a language that is more verbose than others, so I cannot achieve a small\ncodebase.\u201d\nIn most projects, the programming language is a given. It may very well be true that\nin some programming languages, it is impossible to get a small codebase (SQL-based\nlanguages come to mind). However, you can always strive to get a smaller codebase\nthan you currently have, in the same programming language. Every codebase benefits\nfrom decreasing its size, even those in low-level languages with little possibility for"}
{"131": "original functionality can be split up into multiple parts, then ideally you end up with\na piece of code that can be referred to independently by the new functionality, avoid\u2010\ning duplication and taming codebase growth. Write unit tests for the new units to\nverify that you understand the inner workings of the unit. Besides, it is recommended\npractice; see Chapter 10.\nObjection: Splitting the Codebase Is Impossible Because of Platform\nArchitecture\n\u201cWe cannot split the system into smaller parts because we are building for a platform\nwhere all functionality is tied to a common codebase.\u201d\nYes, platform-based software tends to grow large over time because it assimilates new\nfunctionality and rarely reduces functionality. One way to dramatically decrease the\nsize of the codebase is to decouple the system into a plug-in architecture. This leads\nto multiple codebases that are each smaller than the original one. There is a codebase\nfor the common core, and one or more codebases for the plugins. If those plugins are\ntechnically decoupled, they allow for separate release cycles. That means that small\nchanges in functionality do not need an update of the whole system. Keep in mind\nthat those small updates still need full integration/regression tests to ensure that the\nsystem as a whole still functions as expected.\nObjection: Splitting the Codebase Leads to Duplication\n\u201cSplitting the codebase forces me to duplicate code.\u201d\nThere may be cases in which decoupling a system into separate parts (such as plu\u2010\ngins/extensions) requires (interfaces to) common functionality or data structures to\nbe duplicated in those extensions.\nIn such a case, duplication is a bigger problem than having a large codebase, and the\nguideline of Chapter 4 prevails over this guideline of achieving a small codebase. It is\nthen preferable to code common functionality either as a separate extension or as"}
{"132": "Keep in mind that the goal is to have subsystems that can be\nmaintained independently, not necessarily systems that operate\nindependently.\nHow SIG Rates Codebase Volume\nThe metric for codebase volume does not have different risk categories, since it con\u2010\nsists of only one metric. To be rated at 4 stars, the codebase should be at most equiva\u2010\nlent to 20 man-years of rebuild value. If C# is the only technology in a system, this\ntranslates to at most 160,000 lines of code.\nMan-months and man-years\nThe total volume in a codebase is the volume in lines of code converted to man-\nmonths. A man-month is a standard measure of source code volume. It is the amount\nof source code that one developer with average productivity could write in one\nmonth. The advantage of \u201cman-month\u201d is that it allows for comparisons of source\ncode volume between technologies. This is relevant because programming languages\nhave different productivity measures, or \u201clevels of verbosity.\u201d Therefore, a system with\nmultiple programming languages can be converted to an aggregate measure that tells\nyou the approximate effort it would take to rebuild it: the \u201crebuild value.\u201d\nSIG\u2019s experience has shown that the man-month is an effective metric to assess the\nsize of a system and to compare systems with each other. A man-year is simply 12\nman-months. Of course, actual productivity is also dependent on skill and program\u2010\nming style. The volume metric does not tell you how many months or years of effort\nactually went into building the system."}
{"133": "CHAPTER 10\nAutomate Tests\nKeep the bar green to keep the code clean.\n\u2014The jUnit motto\nGuideline:\n\u2022 Automate tests for your codebase.\n\u2022 Do this by writing automated tests using a test framework.\n\u2022 This improves maintainability because automated testing\nmakes development predictable and less risky.\nIn Chapter 4, we have presented IsValid, a method to check whether bank account\nnumbers comply with a checksum. That method contains a small algorithm that\nimplements the checksum. It is easy to make mistakes in a method like this. That is\nwhy probably every programmer in the world at some point has written a little, one-"}
{"134": "Console.WriteLine(\"Type a bank account number on the next line.\");\nacct = Console.ReadLine();\nConsole.WriteLine($\"Bank account number '{acct}' is\" +\n(Accounts.IsValid(acct) ? \"\" : \" not\") + \" valid.\");\n} while (!String.IsNullOrEmpty(acct));\n}\n}\n}\nThis is a C# class with a Main method, so it can be run from the command line:\nC:\\> Program.exe\nType a bank account number on the next line.\n123456789\nBank account number '123456789' is valid.\nType a bank account number on the next line.\n123456788\nBank account number '123456788' is not valid.\nC:\\>\nA program like this can be called a manual unit test. It is a unit test because it is used\nto test just one unit, IsValid. It is manual because the user of this program has to\ntype in test cases manually, and manually assess whether the output of the program is\ncorrect.\nWhile better than having no unit testing at all, this approach has several problems:\n\u2022 Test cases have to be provided by hand, so the test cannot be executed automati\u2010\ncally in an easy way.\n\u2022 The developer who has written this test is focusing on logic to execute the test\n(the do \u2026 while loop, all input/output handling), not on the test itself.\n\u2022 The program does not show how IsValid is expected to behave.\n\u2022 The program is not recognizable as a test (although the rather generic name\nProgram is an indication it is meant as a one-off experiment)."}
{"135": "10.1 Motivation\nThis section describes the advantages of automating your tests as much as possible.\nAutomated Testing Makes Testing Repeatable\nJust like other programs and scripts, automated tests are executed in exactly the same\nway every time they are run. This makes testing repeatable: if a certain test executes at\ntwo different points in time yet gives different answers, it cannot be that the test exe\u2010\ncution itself was faulty. One can conclude that something has changed in the system\nthat has caused the different outcome. With manual tests, there is always the possibil\u2010\nity that tests are not performed consistently or that human errors are made.\nAutomated Testing Makes Development Efficient\nAutomated tests can be executed with much less effort than manual tests. The effort\nthey require is negligible and can be repeated as often as you see fit. They are also\nfaster than manual code review. You should also test as early in the development pro\u2010\ncess as possible, to limit the effort it takes to fix problems.\nPostponing testing to a late stage in the development pipeline risks\nlate identification of problems. That costs more effort to fix,\nbecause code needs to go back through the development pipeline\nand be merged, and tests must be rerun.\nAutomated Testing Makes Code Predictable\nTechnical tests can be automated to a high degree. Take unit tests and integration\ntests: they test the technical inner workings of code and the cohesion/integration of\nthat code. Without being sure of the inner workings of your system, you might get"}
{"136": "Thus, running automated tests provides certainty about how the code works. There\u2010\nfore, the predictability of automated tests also makes the quality of developed code\nmore predictable.\nTests Document the Code That Is Tested\nThe script or program code of a test contains assertions about the expected\nbehavior of the system under test. For example, as will be illustrated later in this\nchapter, an appropriate test of IsValid contains the following line of code:\nAssert.IsFalse(IsValid(\"\")). This documents, in C# code, that we expect IsValid\nto return false when checking the empty string. In this way, the Assert.IsFalse state\u2010\nment plays a double role: as the actual test, and as documentation of the expected\nbehavior. In other words, tests are examples of what the system does.\nWriting Tests Make You Write Better Code\nWriting tests helps you to write testable code. As a side effect, this leads to code con\u2010\nsisting of units that are shorter, are simpler, have fewer parameters, and are more\nloosely coupled (as the guidelines in the previous chapters advise). For example, a\nmethod is more difficult to test when it performs multiple functions instead of only\none. To make it easier to test, you move responsibilities to different methods, improv\u2010\ning the maintainability of the whole. That is why some development approaches\nadvocate writing a unit test before writing the code that conforms to the test. Such\napproaches are called test-driven development (TDD) approaches. You will see that\ndesigning a method becomes easier when you think about how you are going to test\nit: what are the valid arguments of the method, and what should the method return as\na result?\n10.2 How to Apply the Guideline\nHow you automate tests differs by the types of tests you want to automate. Test types"}
{"137": "Table 10-1. Types of testing\nType What it tests Why Who\nUnit test Functionality of one unit in isolation Verify that unit behaves as Developer (preferably\nexpected of the unit)\nIntegration test Functionality, performance, or other quality Verify that parts of the system Developer\ncharacteristic of at least two classes work together\nEnd-to-end test System interaction (with a user or another Verify that system behaves as Developer\nsystem) expected\nRegression test Previously erroneous behavior of a unit, class, Ensure that bugs do not re- Developer\nor system interaction appear\nAcceptance test System interaction (with a user or another Confirm the system behaves as End-user\nsystem) required representative (never\nthe developer)\nTable 10-1 shows that a regression test is a unit test, an integration test, or an end-to-\nend test that has been created when a bug was fixed. Acceptance tests are end-to-end\ntests executed by end user representatives.\nDifferent types of testing call for different automation frameworks. For unit testing,\nseveral well-known C# frameworks are available, such as NUnit. For automated\nend-to-end testing, you need a framework that can mimic user input and capture\noutput. A well-known framework that does just that for web development is Sele\u2010\nnium. For integration testing, it all depends on the environment in which you are\nworking and the quality characteristics you are testing. SoapUI is a framework for\nintegration tests that focuses on web services and messaging middleware. Apache\njMeter is a framework for testing the performance of C# applications under heavy\nworkloads.\nChoosing a test framework needs to be done at the team level. Writing integration\ntests is a specialized skill\u2014but unit testing is for each and every individual developer.\nThat is why the rest of this chapter focuses on writing unit tests using the most well-"}
{"138": "Writing unit tests also requires the smallest upfront investment:1 just download\nNUnit from http://nunit.org.\nGetting Started with NUnit Tests\nAs we noted in the introduction of this chapter, we want to test IsValid, a method of\nthe class Accounts. Accounts is called the class under test. In NUnit, tests are put in a\ndifferent class, the test class, or test fixture. This class is indicated as a test fixture by\nthe [TestFixture] attribute. By convention, the name of the test class is the name of\nthe class under test with the suffix Test added. In this case, that would mean the\nname of the test class is AccountsTest. It must be a public class, but apart from that,\nthere are no other requirements for a test class. In particular, it does not need to\nextend any other class. It is convenient, but not required, to place the test class in the\nsame namespace as the class under test. That way, the test class has access to all mem\u2010\nbers of the test class under test that have namespace (but not public) access.\nIn NUnit, a test itself is any method that has the [Test] attribute. To test IsValid,\nyou can use the following NUnit test class:\nusing NUnit.Framework;\nnamespace eu.sig.training.ch04.v1\n{\n[TestFixture]\npublic class AccountsTest\n{\n[Test]\npublic void TestIsValidNormalCases()\n{\nAssert.IsTrue(Accounts.IsValid(\"123456789\"));\nAssert.IsFalse(Accounts.IsValid(\"123456788\"));\n}\n}"}
{"139": "Unit tests can be run directly in Visual Studio. In addition, NUnit comes with test\nrunners to run tests from the command line. Tests can also be executed by Maven or\nAnt. Figure 10-1 shows the result of running the preceding test in Visual Studio. The\nred bar indicates that there are failed tests.\nFigure 10-1. All tests succeeded!\nThe test in the preceding test class only tests normal cases: two bank account num\u2010\nbers of the expected format (exactly nine characters, all digits). How about corner\ncases? One obvious special case is the empty string. The empty string is, of course,\nnot a valid bank account number, so we test it by calling Assert.IsFalse:\n[Test]\npublic void TestEmptyString()\n{\nAssert.IsFalse(Accounts.IsValid(\"\"));\n}\nAs Figure 10-2 shows, it turns out that this test fails! While the call to IsValid should\nreturn false, it actually returned something else (which, of course, must be true, as\nthere is no other option)."}
{"140": "This indeed returns true, while it should return false. This reminds us to add code\nto IsValid that checks the length of the bank account number.2\nThe NUnit runner reports this as a test failure and not as a test error. A test failure\nmeans that the test itself (the method TestEmptyString) is executed perfectly, but the\nassertion failed. A test error means that the test method itself did not execute cor\u2010\nrectly. The following code snippet illustrates this: the ShowError method raises a\ndivision-by-zero exception and never even executes Assert.IsTrue:\n[Test]\npublic void ShowError()\n{\nint tmp = 0, dummy = 1 / tmp;\n// Next line is never executed because the previous one raises an\n// exception.\n// If it were executed, you'll never see the assert message because\n// the test always succeeds.\nAssert.IsTrue(true);\n}\nNext, we present some basic principles that will help you write good unit tests. We\nstart with the most basic principles and then progress to more advanced ones that\napply when your test efforts become more mature.\nGeneral Principles for Writing Good Unit Tests\nWhen writing tests, it is important to keep in mind the following general principles:\nTest both normal and special cases\nAs in the examples given in this chapter, test two kinds of cases. Write tests that\nconfirm that a unit indeed behaves as expected on normal input (called happy\nflow or sunny-side testing). Also write tests that confirm that a unit behaves sensi\u2010\nbly on non-normal input and circumstances (called unhappy flow or rainy-side\ntesting). For instance, in NUnit it is possible to write tests to confirm that a"}
{"141": "Write tests that are isolated: their outcomes should reflect only the behavior of the sub\u2010\nject being tested\nThat is, each test should act independently of all other tests. For unit testing, this\nmeans that each test case should test only one functionality. No unit test should\ndepend on state, such as files written by other tests. That is why a unit test that,\nsay, causes the class under test to access the filesystem or a database server is not\na good unit test.\nConsequently, in unit testing you should simulate the state/input of other classes\nwhen those are needed (e.g., as arguments). Otherwise, the test is not isolated and\nwould test more than one unit. This was easy for the test of IsValid, because IsValid\ntakes a string as an argument, and it does not call other methods of our system. For\nother situations, you may need a technique like stubbing or mocking.\nIn Chapter 6, we introduced a C# interface for a simple digital camera, which is\nrepeated here for ease of reference:\npublic interface ISimpleDigitalCamera\n{\nImage TakeSnapshot();\nvoid FlashLightOn();\nvoid FlashLightOff();\n}\nSuppose this interface is used in an application that ensures people never forget to\nturn on the flash at night:\npublic const int DAYLIGHT_START = 6;\npublic Image TakePerfectPicture(int currentHour)\n{\nImage image;\nif (currentHour < PerfectPicture.DAYLIGHT_START)\n{"}
{"142": "automatic and independent. That means that the normal implementation of the digi\u2010\ntal camera interface cannot be used. On a typical device, the normal implementation\nrequires a (human) user to point the camera at something interesting and press a\nbutton. The picture taken can be any picture, so it is hard to test whether the (suppos\u2010\nedly perfect) picture taken is the one expected.\nThe solution is to use an implementation of the camera interface that has been made\nespecially for testing. This implementation is a fake object, called a test stub or simply\na stub.3 In this case, we want this fake object to behave in a preprogrammed (and\ntherefore predictable) way. We write a test stub like this:\nclass DigitalCameraStub : ISimpleDigitalCamera\n{\npublic Image TestImage;\npublic Image TakeSnapshot()\n{\nreturn this.TestImage;\n}\npublic void FlashLightOn()\n{\n}\npublic void FlashLightOff()\n{\n}\n}\nIn this stub, TakeSnapshot always returns the same image, which we can set simply\nby assigning to testImage (for reasons of simplicity, we have made testImage a pub\u2010\nlic field and do not provide a setter). This stub can now be used in a test:\n[Test]\npublic void TestDayPicture()\n{"}
{"143": "value of the call, 12, means that TakePerfectPicture assumes it is between noon\nand 1 p.m.\nNow suppose we want to test TakePerfectPicture for nighttime behavior; that is, we\nwant to ensure that if TakePerfectPicture is called with a value lower than Perfect\nPicture.DAYLIGHT_START, it indeed switches on the flash. So, we want to test whether\nTakePerfectPicture indeed calls FlashLightOn. However, FlashLightOn does not\nreturn any value, and the ISimpleDigitalCamera interface also does not provide any\nother way to know whether the flash has been switched on. So what to check?\nThe solution is to provide the fake digital camera implementation with some mecha\u2010\nnism to record whether the method we are interested in gets called. A fake object that\nrecords whether expected calls have taken place is called a mock object. So, a mock\nobject is a stub object with added test-specific behavior. The digital camera mock\nobject looks like this:\nclass DigitalCameraMock : ISimpleDigitalCamera\n{\npublic Image TestImage;\npublic int FlashOnCounter = 0;\npublic Image TakeSnapshot()\n{\nreturn this.TestImage;\n}\npublic void FlashLightOn()\n{\nthis.FlashOnCounter++;\n}\npublic void FlashLightOff()\n{\n}\n}"}
{"144": "Assert.AreEqual(1, cameraMock.FlashOnCounter);\n}\nIn these examples, we have written our own stub and mock objects. This leads to a lot\nof code. Generally, it is most efficient to use a mocking framework such as Moq.\nMocking frameworks use features of the .Net runtime to automatically create mock\nobjects from normal interfaces or classes. They also provide methods to test whether\nmethods of a mock object have been called, and with which arguments. Some mock\u2010\ning frameworks also provide ways to specify preprogrammed behavior of mock\nobjects, giving them the characteristics of both stubs and mocks.\nIndeed, using Moq as an example, you can write TestNightPicture without any need\nto write a class like DigitalCameraMock yourself:\n[Test]\npublic void TestNightPictureMoq()\n{\nImage image =\nImage.FromFile(\"../../../../test/resources/VanGoghStarryNight.jpg\");\nvar cameraMock = new Mock<ISimpleDigitalCamera>();\ncameraMock.Setup(foo => foo.TakeSnapshot()).Returns(image);\nPerfectPicture.camera = cameraMock.Object;\nAssert.AreSame(image, new PerfectPicture().TakePerfectPicture(0));\ncameraMock.Verify(foo => foo.FlashLightOn(), Times.AtMostOnce());\n}\nIn this test, Moq\u2019s Mock constructor is used to create cameraMock, the mock object\nused in this test. With Moq\u2019s Setup and Returns method, the desired behavior is\nspecified. Moq\u2019s Verify method is used to verify whether FlashLightOn has been\ncalled.\nMeasure Coverage to Determine Whether There Are Enough Tests\nHow many unit tests are needed? One way to assess whether you have written enough\nunit tests is to measure coverage of your unit tests. Coverage, or more precisely, line"}
{"145": "never test getters. Take a typical class that represents postal mail addresses. It typically\nhas two or three string fields that represent (additional) address lines. It is easy to\nmake a mistake like this one:\npublic string getAddressLine3() {\nreturn this.addressLine2;\n}\nA minimum of 80% coverage alone is not enough to ensure high-quality unit tests.\nIt is possible to get high coverage by testing just a few high-level methods (like Main,\nthe first method called by the .NET runtime) and not mock out lower-level methods.\nThat is why we advise a 1-to-1 ratio of production code versus test code.\nYou can measure coverage using a code coverage tool. Some editions of Visual Studio\nprovide a built-in code coverage tool. Figure 10-3 shows coverage of the examples of\nthis book, using Visual Studio 2015 Enterprise Edition.\nFigure 10-3. Coverage report of the examples of this book in Visual Studio 2015 Enter\u2010\nprise Edition.\n10.3 Common Objections to Automating Tests\nThis section discusses typical objections and limitations regarding automation. They\ndeal with the reasons and considerations to invest in test automation."}
{"146": "Manual acceptance testing can largely be automated with automa\u2010\nted regression tests. With those, the scope of remaining manual\ntests decreases. You may still need manual review or acceptance\ntests to verify that business logic is correct. This typically concerns\nthe process flow of a functionality.\nObjection: I Am Not Allowed to Write Unit Tests\n\u201cI am not allowed to write unit tests because they lower productivity according to my\nmanager.\u201d\nWriting unit tests during development actually improves productivity. It improves\nsystem code by shifting the focus from \u201cwhat code should do\u201d toward \u201cwhat it should\nnot do.\u201d If you never take into account how the code may fail, you cannot be sure\nwhether your code is resilient to unexpected situations.\nThe disadvantages of not having unit tests are mainly in uncertainty and rework.\nEvery time a piece of code is changed, it requires painstaking review to verify whether\nthe code does what it is supposed to do.\nObjection: Why Should We Invest in Unit Tests When the Current\nCoverage Is Low?\n\u201cThe current unit test coverage of my system is very low. Why should I invest time now\nin writing unit tests?\u201d\nWe have elaborated on the reasons why unit tests are useful and help you develop\ncode that works predictably. However, when a very large system has little to no unit\ntest code, this may be a burden. After all, it would be a significant investment to start\nwriting unit tests from scratch for an existing system because you would need to ana\u2010\nlyze all units again. Therefore, you should make a significant investment in unit tests\nonly if the added certainty is worth the effort. This especially applies to critical, cen\u2010\ntral functionality and when there is reason to believe that units are behaving in an"}
{"147": "10.4 See Also\nStandardization and consistency in applying it are important in achieving a well-\nautomated development environment. For elaboration, see Chapter 11.\nHow SIG Rates Testability\nTestability is one of the five subcharacteristics of maintainability according to ISO\n25010. SIG rates testability by aggregating the ratings of system properties unit com\u2010\nplexity (see Chapter 3), component independence (see Chapter 7), and system volume\n(see Chapter 9), using an aggregation mechanism as explained in Appendix A.\nThe rationale for this is that complex units are especially hard to test, poor compo\u2010\nnent independence increases the need for mocking and stubbing, and higher volumes\nof production code require higher volumes of test code."}
{"148": ""}
{"149": "CHAPTER 11\nWrite Clean Code\nWriting clean code is what you must do in order to call yourself a professional.\n\u2014Robert C. Martin\nGuideline:\n\u2022 Write clean code.\n\u2022 Do this by not leaving code smells behind after development\nwork.\n\u2022 This improves maintainability because clean code is main\u2010\ntainable code.\nCode smells are coding patterns that hint that a problem is present. Introducing or\nnot removing such patterns is bad practice, as they decrease the maintainability of\ncode. In this chapter we discuss guidelines for keeping the codebase clean from code"}
{"150": "11.2 How to Apply the Guideline\nTrying to be a clean coder is an ambitious goal, and there are many best practices that\nyou can follow. From our consultancy experience we have distilled seven developer\n\u201cBoy Scout rules\u201d that will help you to prevent code smells that impact maintainabil\u2010\nity most:\n1. Leave no unit-level code smells behind.\n2. Leave no bad comments behind.\n3. Leave no code in comments behind.\n4. Leave no dead code behind.\n5. Leave no long identifier names behind.\n6. Leave no magic constants behind.\n7. Leave no badly handled exceptions behind.\nThese seven rules are explained in the following sections.\nRule 1: Leave No-Unit Level Code Smells Behind\nAt this point in the book you are familiar with nine guidelines for building maintain\u2010\nable software, discussed in the previous nine chapters. Of those nine guidelines, three\ndeal with smells at the unit level: long units (Chapter 2), complex units (Chapter 3),\nand units with large interfaces (Chapter 5). For modern programming languages,\nthere is really no good reason why any of these guidelines should be violated when\nyou are writing new code.\nTo follow this rule is to refactor \u201csmelly\u201d code in time. By \u201cin time,\u201d we mean as soon\nas possible but certainly before the code is committed to the version control system.\nOf course, it is OK to have small violations when you are working on a development\nticket\u2014for example, a method of 20 lines of code or a method with 5 parameters. But"}
{"151": "Rule 2: Leave No Bad Comments Behind\nComments are sometimes considered the anti-pattern of good code. From our expe\u2010\nrience we can confirm that inline comments typically indicate a lack of elegant engi\u2010\nneering solutions. Consider the following method taken from the Jenkins codebase\n(which is in Java):\npublic HttpResponse doUploadPlugin(StaplerRequest req)\nthrows IOException, ServletException {\ntry {\nJenkins.getInstance().checkPermission(UPLOAD_PLUGINS);\nServletFileUpload upload = new ServletFileUpload(\nnew DiskFileItemFactory());\n// Parse the request\nFileItem fileItem = (FileItem)upload.parseRequest(req).get(0);\nString fileName = Util.getFileName(fileItem.getName());\nif (\"\".equals(fileName)) {\nreturn new HttpRedirect(\"advanced\");\n}\n// we allow the upload of the new jpi's and the legacy hpi's\nif (!fileName.endsWith(\".jpi\") && !fileName.endsWith(\".hpi\")) {\nthrow new Failure(\"Not a plugin: \" + fileName);\n}\n// first copy into a temporary file name\nFile t = File.createTempFile(\"uploaded\", \".jpi\");\nt.deleteOnExit();\nfileItem.write(t);\nfileItem.delete();\nfinal String baseName = identifyPluginShortName(t);\npluginUploaded = true;\n// Now create a dummy plugin that we can dynamically load"}
{"152": "Although the doUploadPlugin is not very hard to maintain (it has only 1 parameter,\n32 lines of code, and a McCabe index of 6), the inline comments indicate separate\nconcerns that could easily be addressed outside this method. For example, copying\nthe fileItem to a temporary file and creating the plugin configuration are tasks that\ndeserve their own methods (where they can be tested and potentially reused).\nComments in code may reveal many different problems:\n\u2022 Lack of understanding of the code itself\n// I don't know what is happening here, but if I remove this line\n// an infinite loop occurs\n\u2022 Issue tracking systems not properly used\n// JIRA-1234: Fixes a bug when summing negative numbers\n\u2022 Conventions or tooling are being bypassed\n// CHECKSTYLE:OFF\n// NOPMD\n\u2022 Good intentions\n// TODO: Make this method a lot faster some day\nComments are valuable in only a small number of cases. Helpful API documentation\ncan be such a case, but always be cautious to avoid dogmatic boilerplate commentary.\nIn general, the best advice we can give is to keep your code free of comments.\nRule 3: Leave No Code in Comments Behind\nAlthough there might be rare occasions where there is a good reason to use com\u2010\nments in your code, there is never an excuse for checking in code that is commented\nout. The version control system will always keep a record of old code, so it is perfectly\nsafe to delete it. Take a look at the following example, taken from the Apache Tomcat\ncodebase (which is in Java, but we present a C# translation here):"}
{"153": "// FIXME: Older spec revisions may still check this\n/*\nif ((servletNames.length != 0) && (urlPatterns.length != 0))\nthrow new IllegalArgumentException\n(sm.getString(\"standardContext.filterMap.either\"));\n*/\nfor (int i = 0; i < urlPatterns.Length; i++) {\nif (!ValidateURLPattern(urlPatterns[i])) {\nthrow new Exception(\nsm.GetString(\"standardContext.filterMap.pattern\",\nurlPatterns[i]));\n}\n}\n}\nThe FIXME note and accompanying code are understandable from the original develo\u2010\nper\u2019s perspective, but to a new developer they act as a distractor. The original devel\u2010\noper had to make a decision before leaving this commented-out code: either fix\nit at the spot, create a new ticket to fix it at some other time, or reject this corner case\naltogether.\nRule 4: Leave No Dead Code Behind\nDead code comes in different shapes. Dead code is code that is not executed at all or\nits output is \u201cdead\u201d: the code may be executed, but its output is not used elsewhere in\nthe system. Code in comments, as discussed in the previous section, is an example of\ndead code, but there are many other forms of dead code. In this section, we give three\nmore examples of dead code.\nUnreachable code in methods\npublic Transaction GetTransaction(long uid)\n{\nTransaction result = new Transaction(uid);\nif (result != null)"}
{"154": "Unused private methods\nPrivate methods can be called only from other code in the same class. If they are not,\nthey are dead code. Nonprivate methods that are not called by methods in the same\nclass may also be dead, but you cannot determine this by looking at the code of the\nclass alone.\nCode in comments\nThis is not to be confused with commented-out code. Sometimes it can be useful to\nuse short code snippets in API documentation (such as in C# XMLDOC tags), but\nremember that keeping those snippets in sync with the actual code is a task that is\nquickly overlooked. Avoid code in comments if possible.\nRule 5: Leave No Long Identifiers Behind\nGood identifiers make all the difference between code that is a pleasure to read and\ncode that is hard to wrap your head around. A famous saying by Phil Karlton is\n\u201cThere are only two hard problems in computer science: cache invalidation and nam\u2010\ning things.\u201d In this book we won\u2019t discuss the first, but we do want to say a few things\nabout long identifiers.\nIdentifiers name the items in your codebase, from units to modules to components to\neven the system itself. It is important to choose good names so that developers can\nfind their way through the codebase without great effort. The names of most of the\nidentifiers in a codebase will be dependent on the domain in which the system oper\u2010\nates. It is typical for teams to have a formal naming convention or an informal, but\nconsistent, use of domain-specific terminology.\nIt is not easy to choose the right identifiers in your code, and unfortunately there are\nno guidelines for what is and what isn\u2019t a good identifier. Sometimes it may even take\nyou a couple of iterations to find the right name for a method or class.\nAs a general rule, long identifiers must be avoided. A maximum length for an identi\u2010"}
{"155": "Rule 6: Leave No Magic Constants Behind\nMagic constants are number or literal values that are used in code without a clear def\u2010\ninition of their meaning (hence the name magic constant). Consider the following\ncode example:\nfloat CalculateFare(Customer c, long distance)\n{\nfloat travelledDistanceFare = distance * 0.10f;\nif (c.Age < 12)\n{\ntravelledDistanceFare *= 0.25f;\n}\nelse\nif (c.Age >= 65)\n{\ntravelledDistanceFare *= 0.5f;\n}\nreturn 3.00f + travelledDistanceFare;\n}\nAll the numbers in this code example could be considered magic numbers. For\ninstance, the age thresholds for children and the elderly may seem like familiar num\u2010\nbers, but remember they could be used at many other places in the codebase. The fare\nrates are constants that are likely to change over time by business demands.\nThe next snippet shows what the code looks like if we define all magic constants\nexplicitly. The code volume increased with six extra lines of code, which is a lot com\u2010\npared to the original source, but remember that these constants can be reused in\nmany other places in the code:\nprivate static readonly float BASE_RATE = 3.00f;\nprivate static readonly float FARE_PER_KM = 0.10f;\nprivate static readonly float DISCOUNT_RATE_CHILDREN = 0.25f;\nprivate static readonly float DISCOUNT_RATE_ELDERLY = 0.5f;"}
{"156": "return BASE_RATE + travelledDistanceFare;\n}\nRule 7: Leave No Badly Handled Exception Behind\nThree guidelines for good exception handling are discussed here specifically because\nin our practice we see many flaws in implementing exception handling:\n\u2022 Always catch exceptions. You are logging failures of the system to help you\nunderstand these failures and then improve the system\u2019s reaction to them. That\nmeans that exceptions must always be caught. Also, in some cases an empty\ncatch block compiles, but it is bad practice since it does not provide information\nabout the context of the exception.\n\u2022 Catch specific exceptions. To make exceptions traceable to a specific event,\nyou should catch specific exceptions. General exceptions that do not provide\ninformation specific to the state or event that triggered it fail to provide that\ntraceability. Therefore, you should not catch Exception or SystemException\ndirectly.\n\u2022 Translate specific exceptions to general messages before showing them to end\nusers. End users should not be \u201cbothered\u201d with detailed exceptions, since they\nare mostly confusing and a security bad practice (i.e., providing more informa\u2010\ntion than necessary about the inner workings of the system).\n11.3 Common Objections to Writing Clean Code\nThis section discusses typical objections regarding clean code. The most common\nobjections are reasons why commenting would be a good way to document code and\nwhether corners can be cut for doing exception handling.\nObjection: Comments Are Our Documentation"}
{"157": "Objection: Exception Handling Causes Code Additions\n\u201cImplementing exception classes forces me to add a lot of extra code without visible\nbenefits.\u201d\nException handling is an important part of defensive programming: coding to prevent\nunstable situations and unpredictable system behavior. Anticipating unstable situa\u2010\ntions means trying to foresee what can go wrong. This does indeed add to the burden\nof analysis and coding. However, this is an investment. The benefits of exception han\u2010\ndling may not be visible now, but they definitely will prove valuable in preventing and\nabsorbing unstable situations in the future.\nBy defining exceptions, you are documenting and safeguarding your assumptions.\nThey can later be adjusted when circumstances change.\nObjection: Why Only These Coding Guidelines?\n\u201cWe use a much longer list of coding conventions and quality checks in our team. This\nlist of seven seems like an arbitrary selection with many important omissions.\u201d\nHaving more guidelines and checks than the seven in this chapter is of course not a\nproblem. These seven rules are the ones we consider the most important for writing\nmaintainable code and the ones that should be adhered to by every member on the\ndevelopment team. A risk of having many guidelines and checks is that developers\ncan be overwhelmed by them and focus their efforts on the less critical issues. How\u2010\never, teams are obviously allowed to extend this list with items that they find indis\u2010\npensable for building a maintainable system."}
{"158": ""}
{"159": "CHAPTER 12\nNext Steps\nAt this point, you know a lot more about what maintainable code is, why it is impor\u2010\ntant, and how to apply the 10 guidelines in this book. But writing maintainable code\nis not something you learn from a book. You learn it by doing it! Therefore, here we\nwill discuss simple advice on practicing the 10 guidelines for achieving maintainable\nsoftware.\n12.1 Turning the Guidelines into Practice\nEnsuring that your code is easy to maintain depends on two behaviors in your daily\nroutine: discipline and setting priorities. Discipline helps you to constantly keep\nimproving your coding techniques, up to a point where any new code you write will\nalready be maintainable. As for priorities, some of the presented guidelines can seem\nto contradict each other. It takes consideration on your side about which guideline\nhas the most impact on the actual maintainability of your system. Be sure to take\nsome time to deliberate and ask your team for their opinion."}
{"160": "units into multiple smaller units slightly grows the total codebase. But the advantage\nof small units in terms of reusability will have a huge pay-off when more functionality\nis added to the system.\nThe same applies to the architecture-level guidelines (see Chapters 7 and 8): it makes\nno sense to reorganize your code structure when it makes your components highly\ndependent on each other. To put it succinctly: fix your dependencies before trying to\nbalance your components.\n12.3 Remember That Every Commit Counts\nThe hardest part of applying the guidelines in this book may be keeping the discipline\nto apply them. It is tempting to violate the guidelines when a \u201cquick fix\u201d seems more\nefficient. To keep this discipline, follow the Boy Scout rule presented in Chapter 11.\nThe Boy Scout rule is especially effective on large codebases. Unless you have the time\nto sort out your whole system and improve maintainability, you will have to do it\nstep-by-step while doing your regular work. This gradually improves maintainability\nand hones your refactoring skills. So, in the long run, you also have the skill to write\nhighly maintainable software.\n12.4 Development Process Best Practices Are Discussed in\nthe Follow-Up Book\nAs discussed in the preface, the process part of developing high-quality software is\ndiscussed in detail in the follow-up book in this series: Building Software Teams. It\nprovides 10 guidelines for managing and measuring the software development pro\u2010\ncess. It focuses on how to measure and manage best practices for software develop\u2010\nment (e.g., development tool support, automation, standardization)."}
{"161": "APPENDIX A\nHow SIG Measures Maintainability\nSIG measures system maintainability based on eight metrics. Those eight metrics are\ndiscussed in Chapters 2 through 9. Those chapters include sidebars explaining how\nSIG rates source code properties relevant to those guidelines. These ratings are\nderived from the SIG/T\u00dcViT1 Evaluation Criteria for Trusted Product Maintainabil\u2010\nity. In this appendix, we provide you with additional background.\nTogether with T\u00dcViT, SIG has determined eight properties of source code that can be\nmeasured automatically. See \u201cWhy These Ten Specific Guidelines?\u201d on page xi for\nhow these properties have been chosen.\nTo assess maintainability of a system, we measure these eight source code properties\nand summarize these measurements either in a single number (for instance, the per\u2010\ncentage of code duplication) or a couple of numbers (for instance, the percentage of\ncode in four categories of complexity, which we call a quality profile; see \u201cRating\nMaintainability\u201d).\nWe then compare these numbers against a benchmark containing several hundreds of"}
{"162": "Table A-1. SIG maintainability ratings\nRating Maintainability\n5 stars Top 5% of the systems in the benchmark\n4 stars Next 30% of the systems in the benchmark (above-average systems)\n3 stars Next 30% of the systems in the benchmark (average systems)\n2 stars Next 30% of the systems in the benchmark (below-average systems)\n1 star Bottom 5% least maintainable systems\nWe then aggregate the ratings to arrive at one overall rating. We do this in two steps.\nFirst, we determine the ratings for the subcharacteristics of maintainability as defined\nby ISO 25010 (i.e., analyzability, modifiability, etc.) by taking the weighted averages\naccording to the rows of Table A-2. Each cross in a given row indicates that the corre\u2010\nsponding system property (column) contributes to this subcharacteristic. Second, we\ntake a weighted average of the five subcharacteristics to determine an overall rating\nfor maintainability.\nTable A-2. Relation of subcharacteristics and system properties\nVolume Duplication Unit Unit Unit Module Component Component\nsize complexity interfacing coupling balance independence\nAnalyzability X X X X\nModifiability X X X\nTestability X X X\nModularity X X X\nReusability X X\nThis describes the SIG maintainability model in a nutshell, since there is more detail\nto it than what we can cover in this appendix. If you would like to learn more about\nthe details of the maintainability model, a good start for elaboration is the following\npublication:"}
{"163": "Background on the development of the model and its application is provided in the\nfollowing publications:\n\u2022 Heitlager, Ilja, Tobias Kuipers, and Joost Visser. \u201cA Practical Model for Measuring\nMaintainability.\u201d In Proceedings of the 6th International Conference on the Quality\nof Information and Communications Technology (QUATIC 2007), 30\u201339. IEEE\nComputer Society Press, 2007.\n\u2022 Baggen, Robert, Jos\u00e9 Pedro Correia, Katrin Schill, and Joost Visser. \u201cStandardized\ncode quality benchmarking for improving software maintainability.\u201d Software\nQuality Journal 20, no. 2 (2012): 287\u2013307.\n\u2022 Bijlsma, Dennis, Miguel Alexandre Ferreira, Bart Luijten, and Joost Visser.\n\u201cFaster issue resolution with higher technical quality of software.\u201d Software Qual\u2010\nity Journal 20, no. 2 (2012): 265\u2013285.\nDoes Maintainability Improve Over Time?\nA question we often get at SIG is whether maintainability improves over time across\nall systems we see. The answer is yes, but very slowly. The recalibration that we carry\nout every year consistently shows that the thresholds become stricter over time. This\nmeans that for one system to get a high maintainability rating, over time it must have\nfewer units that are overly long or complex, must have less duplication, lower cou\u2010\npling, and so on. Given the structure of our model, the reason for this must be that\nsystems in our benchmark over time have less duplication, less tight coupling, and so\non. One could argue that this means that maintainability across the systems we\nacquire for our benchmark is improving. We are not talking about big changes. In\nbroad terms, we can say this: it is about a tenth of a star per year.\nThe selection of systems within the SIG benchmark is a representative cross-cut of the\nsoftware industry, including both proprietary and open source systems, developed in\na variety of languages, functional domains, platforms, and so on. Therefore, the tenth"}
{"164": ""}
{"165": "Index\nSymbols C\n11-check, 12 C#\nconcept names in, xvi\nA namespaces vs. components, xvii\nC# interfaces, 78\nAbstract Factory design pattern, 86-88\ncalls, 83\nacceptance tests\nchains, conditional, 36-38\nautomation of, 124\nchange\ncharacteristics, 114\nand duplication, 54\nadaptive maintenance, 2\npossible reasons for, 54\nanalysis\nchecksum, 12\nand component balance, 95\nclasses, splitting, 73\nand duplication, 47\nclean coding\narchitecture components, balance of (see com\u2010\nand badly-handled exceptions, 134\nponent balance)\nand Boy Scout rule, 124\nautomation of tests (see test automation)\nand commented-out code, 130\nand comments as documentation, 134\nB\nand dead code, 131\nbackups, code duplication and, 54\nand exception class implementation, 135\nbalance of components (see component bal\u2010\nand number of coding guidelines, 135\nance)"}
{"166": "finding/analyzing, 95 of system as impediment to reducing code\u2010\nhidden vs. interface, 90 base volume, 108\nimprovement through test automation, 114 component balance, 93-99\nminimizing unit size, 11-26 advantages of, 95\nreading when spread out over multiple and component independence, 84\nunits, 23 and deceptive acceptability of imbalance, 98\nreplacing custom code with libraries/frame\u2010 and entanglements, 98\nworks, 77 applying guidelines for, 96\ntest automation and predictability of, 113 clarifying domains for, 97\nunreachable, 131 common objections to, 98\nwriting clean (see clean coding) deciding on proper conceptual level for\ncode clones grouping functionality, 97\nand SIG duplication ratings, 56 importance of, 93-99\ndefined, 46 isolation of maintenance effects, 95\ncodebase separation of maintenance responsibilities,\nBoy Scout rule with large, 138 96\ndefined, 101 SIG rating thresholds, 99\nnavigation of, 73 when finding/analyzing code, 95\nvolume (see codebase volume) component coupling, module coupling vs., 82\nworking on isolated parts of, 72 component dependence, 82\ncodebase volume component guidelines, unit guidelines vs., 137\nadvantages of minimizing, 102-104 component imbalance, 93, 98\nand defect density, 104 component independence\napplying guidelines for, 105-107 advantages of, 82-86\ncommon objections to minimizing of, and Abstract Factory design pattern, 86-88\n107-110 and entangled components, 89\nduplication as impediment to reducing, 109 and isolated maintenance, 85\nfunctionality-related measures to minimize, and separation of maintenance responsibili\u2010\n105 ties, 85\nimportance of minimizing, 101-110 and testing, 86\nmaintenance and, 104 applying guidelines for, 86-88\nplatform architecture and, 109 common objections to, 88\nproductivity measures and, 108 defined, 82\nprogramming languages and, 108 importance of, 81-91\nproject failures and, 102 SIG rating thresholds, 90"}
{"167": "reuse vs., 14 and unit tests, 55\nsystem complexity as cause of, 108 as impediment to reducing codebase vol\u2010\ncopying code (see duplication [code]) ume, 109\ncore classes, 59 avoiding, 43-56\ncorrective maintenance, 2 common objections to avoiding, 53-55\ncoupling, 71 guidelines for avoiding, 48-53\n(see also module coupling) SIG ratings, 56\nbetween classes, 72 types of, 46\ndefined, 71\nloose (see component independence) (see E\nloose coupling)\nembedded software, 6\ntight (see tight coupling)\nencapsulation, 73, 77, 78, 84, 87, 90\ncoverage\nend-to-end tests, 114\nmeasuring for test adequacy, 122\nentangled components\ntest automation in low-coverage situation,\nand component balance, 98\n124\nand component independence, 89\nCPD (clone detection tool), 47\nevolution over time, 84\ncyclomatic complexity, 31\nerrors, test, 118\n(see also McCabe complexity)\nexception handling\nand clean coding, 134\nD\nand exception class implementation, 135\ndata transfer objects, 62 execution paths, 30\ndead code, 131 Extract Method refactoring technique, 17, 48\ndeadlines, component independence and, 89 Extract Superclass refactoring technique, 50-53,\ndefect density, codebase volume and, 104 50\ndefensive programming, exception handling\nand, 135 F\ndependency injection, 86\nfailures, 118\ndiscipline, coding and, 137\n(see also exception handling)\ndocumentation\ncodebase volume and, 102\nand test automation, 114\ntest errors vs. test failures, 118\ncomments as, 134\nfluent interface, 64\ndomains\nfor loops, 13\nclarifying, 97\nformatting, unit size guidelines and, 23\ncomplex, 40\nframeworks"}
{"168": "importance of simple, 4 lower-level guidelines, 137\nlower-level vs. higher-level, 137\nmaintaining discipline to apply, 138 M\noverview, 9\nmagic constants, 133\npracticing, 137\nmaintainability\nprinciples of, 4\nand discipline during development, 5\nas enabler for other quality characteristics, 4\nH\nas industry-independent, 6\nhappy flow testing, 118 as language-independent, 6\nhidden code, 90 as nonbinary quantity, 7\nhigher-level guidelines, 137 business impact of, 3\ndefined, 2\nI guidelines (see guidelines, maintainability)\nimportance of, 3-4\nidentifiers, long, 132\nimportance of simple guidelines, 4\nif-then-else statements, 36-38\nimprovement over time, 141\nimplementations, specialized, 75-77\nmetrics for, xi\nincoming calls, 84\nmisunderstandings about, 6\nindustry-dependent software development, 6\nperformance vs., 22\nintegration tests, 114\nrating, 7-9\ninterface code, 90\nmaintenance, four types of, 2\ninterfaces, unit (see unit interfaces)\nman-months/years, 110\ninternal calls, 83\nmanual testing\nIntroduce Parameter Object refactoring pat\u2010\nand test automation, 123\ntern, 59\nlimitations of, 112\ninversion of control (IoC), 79\nunit test, 112\nisolated maintenance, 85\nMcCabe complexity\nas SIG/T\u00dcViT rating criteria, 42\nJ\ndefined, 31\nJMeter, 115\nmethod invocations, 22\nJPacman, 16\nmethod modification, 60\nmethod splitting, 41\nL\nmethods\nlarge class smell, 50, 71, 73 unreachable code in, 131\nlarge systems, splitting up of, 107 unused private, 132"}
{"169": "and utility code, 78 as impediment to reducing codebase vol\u2010\napplying guidelines for, 73-77 ume, 108\ncommon objections to separating concerns, as maintainability factor, 6\n78 project failures, codebase volume and, 102\ncomponent coupling vs., 82\nhiding specialized implementations behind Q\ninterfaces, 75-77\nquality profiles, 5, 8\nloose (see loose coupling)\nquality, maintainability as enabler for, 4\nreplacing custom code with libraries/frame\u2010\nquick fixes, 138\nworks, 77\nSIG rating thresholds, 80\nR\nsplitting classes to separate concerns, 73\nrainy-side testing, 118\ntight (see tight coupling)\nrefactoring\nto prevent no-go areas for new developers,\nand unit complexity, 41\n73\nand unit interfaces, 65\nwhen working on isolated parts of codebase,\nand unit size, 17-22\n72\ndifficulties as maintainability issue, 25\nMoq, 122\nExtract Method technique, 17, 48\nmutual dependencies, 89\nExtract Superclass technique, 50-53, 50\nfor improved maintainability, 127\nN\nIntroduce Parameter Object pattern, 59\nnamespaces, components vs., xvii\nReplace Method with Method Object tech\u2010\nnested conditionals, 38-40\nnique, 19-22, 63\nNUnit tests, 112, 116-118\nto reduce codebase, 106\nreferral, reuse by, 106\nO\nregression, 95, 109, 112-115, 124\nobservers, 15 automated testing to identify, 113\noutgoing calls, 83 bugs, 47\nregression tests\nP characteristics, 114\nfor automation of manual acceptance test\u2010\nparameter lists, 65\ning, 124\n(see also unit interfaces)\nrepeatability, test automation and, 113\nparameter objects, 62, 64\nReplace Conditional with Polymorphism pat\u2010\nperfective maintenance, 2\ntern, 37"}
{"170": "scope creep, 105 and SIG testability rating thresholds, 125\nSelenium, 115 applying guidelines for, 114\nself-taught developers, xi common objections to, 123\nseparation of concerns (see concerns, separa\u2010 general principles for writing good unit\ntion of) tests, 118-122\nSIG (Software Improvement Group), xiii in low-coverage situation, 124\nSIG/T\u00dcViT Evaluation Criteria Trusted Prod\u2010 measuring coverage of, 122\nuct Maintainability test stub, 120\ncodebase volume rating thresholds, 110 test-driven development (TDD), 114\ncomponent balance rating thresholds, 99 testability\ncomponent independence rating thresholds, and unit size, 14\n90 SIG rating thresholds, 125\nduplication rating thresholds, 56 tests/testing\nmetrics, xi, 139-140 and unit complexity, 35\nmodule coupling rating thresholds, 80 automation of (see test automation)\nstar ratings, 7-9 component independence and ease of, 86\ntestability rating thresholds, 125 dangers of postponing, 113\nunit complexity rating thresholds, 42 errors vs. failures, 118\nunit interface rating thresholds, 66 types of, 114\nunit size rating thresholds, 26 third-party libraries/frameworks (see frame\u2010\nsimple units (see unit complexity) works) (see libraries)\nsingle responsibility principle throughput code, 84\nclasses that violate, 73 throughput, component independence and, 90\ndefined, 73 tight coupling\nsmells, Boy Scout rules for preventing, 128-134 as impediment to reducing codebase vol\u2010\n(see also large class smell) ume, 109\nSoapUI, 115 as risk when removing clones, 50\nSoftware Improvement Group (see SIG) maintenance consequences of, 72\nSoftware Risk Monitoring service, xiii maintenance problems with, 68-72\nSQL queries, 24 of classes, 71\nstandardization of functionality, 106 Type 1 clones, 46\nstar rating system, 8 Type 2 clones, 46\nstring literals, 55\nstub (test stub), 120 U\nsunny-side testing, 118\nunhappy flow testing, 118"}
{"171": "in complex domains, 40 in real-world systems, 23\nminimizing, 29, 42 minimizing, 11-26\nSIG rating of, 42 perceived lack of advantage in splitting\nunit guidelines units, 25\ncomponent guidelines vs., 137 quantity of units and performance, 22\ndefined, 67 reading code when spread out over multiple\nunit interfaces units, 23\nadvantages of minimizing size of, 59 refactoring techniques for guideline applica\u2010\nand libraries/frameworks with long parame\u2010 tions, 17-22\nter lists, 65 SIG thresholds for, 26\nand method modification, 60 when extending unit with new functionality,\nand parameter objects, 64 16\napplying guidelines for, 60-64 when writing new unit, 15\nC# interfaces and loose coupling, 78 unit tests\ncommon objections to minimizing size of, and Boy Scout rule, 124\n64 and duplication, 55\nease of understanding, 59 characteristics, 114\nhiding specialized implementations behind, common objections to automating tests, 123\n75-77 failures vs. errors, 118\nminimizing size of, 57-66 general principles for writing, 118-122\nrefactoring, 65 measuring coverage to determine proper\nreuse, 59 number of, 124\nSIG rating thresholds, 66 NUnit tests, 116-118\nunit size unit-level code, clean coding and, 128\nadvantages of minimizing, 14 unreachable code, 131\nand ease of analysis, 14 unused private methods, 132\nand improper formatting, 23 utility code, module coupling and, 78\nand reuse, 14\nand testability, 14 V\napplying guidelines to, 15-22\nviolations\ncommon objections to writing short units,\nprioritizing, 5\n22-25\nwith frameworks/libraries, 65\ndifficulty of optimizing by splitting units, 24"}
{"172": "Colophon\nThe animal on the cover of Building Maintainable Software is a grey-headed wood\u2010\npecker (Picus canus). Like all woodpeckers, which consitute about half of the Pici\u2010\nformes order, grey-headed woodpeckers use strong bills to puncture the surface of\ntrees and seek small insects that inhabit the wood. Very long, bristly tongues coated\nwith an adhesive extend into deep cracks, holes, and crevices to gather food in the\nbird\u2019s bill. A membrane that closes over the woodpecker\u2019s eye protects it from the\ndebris that may result from each blow at the tree. Slit-like nostrils provide a similar\nprotection, as do feathers that cover them. Adaptations to the brain like small size\nand a position that maximizes its contact with the skull\u2014permitting optimal shock\nabsorption\u2014represent further guards against the violence of the woodpecker\u2019s drill\u2010\ning. The zygodactyl arrangement of the feet, putting two toes forward and two back,\nallow the woodpecker to maintain its position on the tree\u2019s trunk during this activity,\nas well as to traverse vertically up and down it.\nGrey-headed woodpeckers maintain a vast range across Eurasia, though individual\nmembers of the species tend to be homebodies to particular forest and woodland\nhabitats. As such, they rarely travel overseas and switch to a seed-based diet in winter.\nMating calls that begin with high-pitched whistles lead to monogamous pairs roost\u2010\ning with clutches of 5 to 10 eggs in the holes that males bore into the trunks of trees,\nwhere both parents remain to incubate eggs and nurse the hatchlings for the three to\nfour weeks in which the hatchlings progress to juveniles. At this point, the young can\nfly from the nest and gather their own food.\nIn their greenish back and tail plumage, grey-headed woodpeckers very much resem\u2010\nble the closely related green woodpecker, and males of the species will develop on\ntheir foreheads the red patch that appears on many other species of woodpecker.\nMany of the animals on O\u2019Reilly covers are endangered; all of them are important to\nthe world. To learn more about how you can help, go to animals.oreilly.com."}
{"45": "Objection: Code Is Harder to Read When Spread Out\n\u201cCode becomes harder to read when spread out over multiple units.\u201d\nWell, psychology says that is not the case. People have a working memory of about\nseven items, so someone who is reading a unit that is significantly longer than seven\nlines of code cannot process all of it. The exception is probably the original author of\na piece of source code while he or she is working on it (but not a week later).\nWrite code that is easy to read and understand for your successors\n(and for your future self).\nGuideline Encourages Improper Formatting\n\u201cYour guideline encourages improper source code formatting.\u201d\nDo not try to comply with guideline by cutting corners in the area of formatting. We\nare talking about putting multiple statements or multiple curly brackets on one line.\nIt makes the code slightly harder to read and thus decreases its maintainability. Resist\nthe temptation to do so.\nConsider what purpose the guideline really serves. We simply cannot leave unit\nlength unconstrained. That would be akin to removing speed limits in traffic because\nthey discourage being on time. It is perfectly possible to obey speed limits and arrive\non time: just leave home a bit earlier. It is equally possible to write short units. Our\nexperience is that 15 lines of properly formatted code is enough to write useful units.\nAs proof, Table 2-1 presents some data from a typical Java 2 Enterprise Edition sys\u2010\ntem, consisting of Java source files but also some XSD and XSLT. The system, cur\u2010\nrently in production at a SIG client, provides reporting functionality for its owner.\nThe Java part consists of about 28,000 lines of code (a medium-sized system). Of"}
{"46": "Out of the 3,220 units in this system, 3,071 (95.4%) are at most 15 lines of code, while\n149 units (4.6% of all units) are longer. This shows that it is very possible in practice\nto write short units\u2014at least for a vast majority of units.\nAgree on formatting conventions in your team. Keep units short\nand comply with these conventions.\nThis Unit Is Impossible to Split Up\n\u201cMy unit really cannot be split up.\u201d\nSometimes, splitting a method is indeed difficult. Take, for instance, a properly for\u2010\nmatted switch statement in C#. For each case of the switch statement, there is a line\nfor the case itself, at least one line to do anything useful, and a line for the break\nstatement. So, anything beyond four cases becomes very hard to fit into 15 lines of\ncode, and a case statement cannot be split. In Chapter 3, we present some guidelines\non how to deal specifically with switch statements.\nHowever, it is true that sometimes a source code statement simply cannot be split. A\ntypical example in enterprise software is SQL query construction. Consider the fol\u2010\nlowing example (adapted from a real-world system analyzed by the authors of this\nbook):\npublic static void PrintDepartmentEmployees(string department)\n{\nQuery q = new Query();\nforeach (Employee e in q.AddColumn(\"FamilyName\")\n.AddColumn(\"Initials\")\n.AddColumn(\"GivenName\")\n.AddColumn(\"AddressLine1\")\n.AddColumn(\"ZIPcode\")"}
{"47": "expression starting with q.AddColumn(\"FamilyName\") can be extracted into a new\nmethod. But before doing that (and seeing the newly created method grow to over 15\nlines when the query gets more complex in the future), rethink the architecture. Is it\nwise to create a SQL query piece by piece as in this snippet? Should the HTML\nmarkup really appear here? A templating solution such as ASP or Razor may be more\nsuitable for the job at hand.\nSo, if you are faced with a unit that seems impossible to refactor, do not ignore it and\nmove on to another programming task, but indeed raise the issue with your team\nmembers and team lead.\nWhen a refactoring seems possible but doesn\u2019t make sense, rethink\nthe architecture of your system.\nThere Is No Visible Advantage in Splitting Units\n\u201cPutting code in DoSomethingOne, DoSomethingTwo, DoSomethingThree has no benefit\nover putting the same code all together in one long DoSomething.\u201d\nActually, it does, provided you choose better names than DoSomethingOne, DoSome\nthingTwo, and so on. Each of the shorter units is, on its own, easier to understand\nthan the long DoSomething. More importantly, you may not even need to consider all\nthe parts, especially since each of the method names, when chosen carefully, serves as\ndocumentation indicating what the unit of code is supposed to do. Moreover, the long\nDoSomething typically will combine multiple tasks. That means that you can only\nreuse DoSomething if you need the exact same combination. Most likely, you can\nreuse each of DoSomethingOne, DoSomethingTwo, and so on much more easily.\nPut code in short units (at most 15 lines of code) that have carefully"}
{"48": "2.4 See Also\nSee Chapters 3, 4, and 5 for additional refactoring techniques. For a discussion on\nhow to test methods, see Chapter 10.\nHow SIG Rates Unit Size\nThe size (length) of units (methods and constructors in C#) is one of the eight system\nproperties of the SIG/T\u00dcViT Evaluation Criteria for Trusted Product Maintainability.\nTo rate unit size, every unit of the system is categorized in one of four risk categories\ndepending on the number of lines of code it contains. Table 2-2 lists the four risk cat\u2010\negories used in the 2015 version of the SIG/T\u00dcViT Evaluation Criteria.\nThe criteria (rows) in Table 2-2 are conjunctive: a codebase needs to comply with all\nfour of them. For example, if 6.9% of all lines of code are in methods longer than\n60 lines, the codebase can still be rated at 4 stars. However, in that case, at most\n22.3% \u2013 6.9% = 15.4% of all lines of code can be in methods that are longer than 30\nlines but not longer than 60 lines. To the contrary, if a codebase does not have any\nmethods of more than 60 lines of code, at most 22.3% of all lines of code can be in\nmethods that are longer than 30 lines but not longer than 60 lines.\nTable 2-2. Minimum thresholds for a 4-star unit size rating (2015 version of the SIG/\nT\u00dcViT Evaluation Criteria)\nLines of code in methods with \u2026 Percentage allowed for 4 stars for unit size\n\u2026 more than 60 lines of code At most 6.9%\n\u2026 more than 30 lines of code At most 22.3%\n\u2026 more than 15 lines of code At most 43.7%\n\u2026 at most 15 lines of code At least 56.3%\nSee the three quality profiles shown in Figure 2-2 as an example:"}
{"49": "Figure 2-2. Three quality profiles for unit size"}
{"50": ""}
{"51": "CHAPTER 3\nWrite Simple Units of Code\nEach problem has smaller problems inside.\n\u2014Martin Fowler\nGuideline:\n\u2022 Limit the number of branch points per unit to 4.\n\u2022 Do this by splitting complex units into simpler ones and\navoiding complex units altogether.\n\u2022 This improves maintainability because keeping the number of\nbranch points low makes units easier to modify and test.\nComplexity is an often disputed quality characteristic. Code that appears complex to\nan outsider or novice developer can appear straightforward to a developer that is inti\u2010\nmately familiar with it. To a certain extent, what is \u201ccomplex\u201d is in the eye of the"}
{"52": "later). Branch points can be counted for a complete codebase, a class, a namespace, or\na unit. The number of branch points of a unit is equal to the minimum number of\npaths needed to cover all branches created by all branch points of that unit. This is\ncalled branch coverage. However, when you consider all paths through a unit from the\nfirst line of the unit to a final statement, combinatory effects are possible. The reason\nis that it may matter whether a branch follows another in a particular order. All possi\u2010\nble combinations of branches are the execution paths of the unit\u2014that is, the maxi\u2010\nmum number of paths through the unit.\nConsider a unit containing two consecutive if statements. Figure 3-1 depicts the con\u2010\ntrol flow of the unit and shows the difference between branch coverage and execution\npath coverage."}
{"53": "In summary, the number of branch points is the number of paths that cover all\nbranches created by branch points. It is the minimum number of paths and can be\nzero (for a unit that has no branch points). The number of execution paths is a maxi\u2010\nmum, and can be very large due to combinatorial explosion. Which one to choose?\nThe answer is to take the number of branch points plus one. This is called cyclomatic\ncomplexity or McCabe complexity. Consequently, the guideline \u201climit the number of\nbranch points per unit to 4\u201d is equal to \u201climit code McCabe complexity to 5.\u201d This is\nthe minimum number of test cases that you need to cover a unit such that every path\nhas a part not covered by the other paths. The cyclomatic (McCabe) complexity of a\nunit is at least one, which is easy to understand as follows. Consider a unit with no\nbranch points. According to the definition, its cyclomatic complexity is one (number\nof branch points plus one). It also fits intuitively: a unit with no branch points has\none execution path, and needs at least one test.\nFor the sake of completeness: only for units with one exit point, the cyclomatic or\nMcCabe complexity is equal to the number of branch points plus one. It becomes\nmore complex for units with more than one exit point. Do not worry about that:\nfocus on limiting the number of branch points to four.\nThe minimum number of tests needed to cover all independent\nexecution paths of a unit is equal to the number of branch points\nplus one.\nNow consider the following example. Given a nationality, the GetFlagColors method\nreturns the correct flag colors:\npublic IList<Color> GetFlagColors(Nationality nationality)\n{\nList<Color> result;\nswitch (nationality)"}
{"54": "break;\ncase Nationality.UNCLASSIFIED:\ndefault:\nresult = new List<Color> { Color.Gray };\nbreak;\n}\nreturn result;\n}\nThe switch statement in the method body needs to handle all cases of the nationality\nenumeration type and return the correct flag colors. As there are five possible nation\u2010\nalities and the unclassified/default case, the number of isolated paths to be tested\n(control flow branches) is six.\nOn first sight, the GetFlagColors method might seem harmless. Indeed, the method\nis quite readable, and its behavior is as expected. Still, if we want to test the behavior\nof this method, we would need six unique test cases (one for each nationality plus one\nfor the default/unclassified case). Writing automated tests might seem excessive for\nthe GetFlagColors method, but suppose a developer adds the flag of Luxembourg\n(which is very similar to the Dutch flag) as a quick fix:\n...\ncase Nationality.DUTCH:\nresult = new List<Color> { Color.Red, Color.White, Color.Blue };\ncase Nationality.LUXEMBOURGER:\nresult = new List<Color> { Color.Red, Color.White, Color.LightBlue };\nbreak;\ncase Nationality.GERMAN:\n....\nBeing in a hurry, the developer copied the constructor call for the Dutch flag\nand updated the last argument to the right color. Unfortunately, the break statement\nescaped the developer\u2019s attention, and now all Dutch nationalities will see the flag\nfrom Luxembourg on their profile page!\nThis example looks like a forged scenario, but we know from our consultancy prac\u2010"}
{"55": "*/\nprivate static User getOrCreate(string id, string fullName, bool create)\n{\nstring idkey = idStrategy().keyFor(id);\nbyNameLock.readLock().doLock();\nUser u;\ntry\n{\nu = byName.get(idkey);\n}\nfinally\n{\nbyNameLock.readLock().unlock();\n}\nFileInfo configFile = getConfigFileFor(id);\nif (!configFile.Exists && !Directory.Exists(configFile.Directory.FullName))\n{\n// check for legacy users and migrate if safe to do so.\nFileInfo[] legacy = getLegacyConfigFilesFor(id);\nif (legacy != null && legacy.Length > 0)\n{\nforeach (FileInfo legacyUserDir in legacy)\n{\nXmlFile legacyXml = new XmlFile(XmlFile.XSTREAM,\nnew FileInfo(Path.Combine(\nlegacyUserDir.FullName, \"config.xml\")));\ntry\n{\nobject o = legacyXml.read();\nif (o is User)\n{\nif (idStrategy().equals(id, legacyUserDir.Name)\n&& !idStrategy()\n.filenameOf(legacyUserDir.Name)\n.Equals(legacyUserDir.Name))\n{"}
{"56": "}\nelse\n{\nLOGGER.log(Level.FINE,\n\"Unexpected object loaded from {0}: {1}\",\nnew object[] { legacyUserDir, o });\n}\n}\ncatch (IOException e)\n{\nLOGGER.log(Level.FINE,\nstring.Format(\n\"Exception trying to load user from {0}: {1}\",\nnew Object[] { legacyUserDir, e.Message }),\ne);\n}\n}\n}\n}\nif (u == null && (create || configFile.Exists))\n{\nUser tmp = new User(id, fullName);\nUser prev;\nbyNameLock.readLock().doLock();\ntry\n{\nprev = byName.putIfAbsent(idkey, u = tmp);\n}\nfinally\n{\nbyNameLock.readLock().unlock();\n}\nif (prev != null)\n{\nu = prev; // if some has already put a value in the map, use it\nif (LOGGER.isLoggable(Level.FINE)\n&& !fullName.Equals(prev.getFullName()))"}
{"57": "}\ncatch (IOException x)\n{\nLOGGER.log(Level.WARNING, null, x);\n}\n}\n}\nreturn u;\n}\n3.1 Motivation\nBased on the code examples in the previous section, keeping your units simple is\nimportant for two main reasons:\n\u2022 A simple unit is easier to understand, and thus modify, than a complex one.\n\u2022 Simple units ease testing.\nSimple Units Are Easier to Modify\nUnits with high complexity are generally hard to understand, which makes them hard\nto modify. The first code example of the first section was not overly complicated, but\nit would be when it checks for, say, 15 or more nationalities. The second code exam\u2010\nple covers many use cases for looking up or creating users. Understanding the second\ncode example in order to make a functional change is quite a challenge. The time it\ntakes to understand the code makes modification harder.\nSimple Units Are Easier to Test\nThere is a good reason you should keep your units simple: to make the process of\ntesting easier. If there are six control flow branches in a unit, you will need at least six\ntest cases to cover all of them. Consider the GetFlagColors method: six tests to cover"}
{"58": "\u2022 &&, ||\n\u2022 while\n\u2022 for , foreach\n\u2022 catch\nSo how can we limit the number of branch points? Well, this is mainly a matter of\nidentifying the proper causes of high complexity. In a lot of cases, a complex unit\nconsists of several code blocks glued together, where the complexity of the unit is the\nsum of its parts. In other cases, the complexity arises as the result of nested if-then-\nelse statements, making the code increasingly harder to understand with each level\nof nesting. Another possibility is the presence of a long chain of if-then-else state\u2010\nments or a long switch statement, of which the GetFlagColors method in the intro\u2010\nduction is an example.\nEach of these cases has its own problem, and thus, its own solution. The first case,\nwhere a unit consists of several code blocks that execute almost independently, is a\ngood candidate for refactoring using the Extract Method pattern. This way of reduc\u2010\ning complexity is similar to Chapter 2. But what to do when faced with the other\ncases of complexity?\nDealing with Conditional Chains\nA chain of if-then-else statements has to make a decision every time a conditional\nif is encountered. An easy-to-handle situation is the one in which the conditionals\nare mutually exclusive; that is, they each apply to a different situation. This is also the\ntypical use case for a switch statement, like the switch from the GetFlagColors\nmethod.\nThere are many ways to simplify this type of complexity, and selecting the best solu\u2010\ntion is a trade-off that depends on the specific situation. For the GetFlagColors"}
{"59": "Color.Red };\nFLAGS[Nationality.FRENCH] = new List<Color>{ Color.Blue, Color.White,\nColor.Red };\nFLAGS[Nationality.ITALIAN] = new List<Color>{ Color.Green, Color.White,\nColor.Red };\n}\npublic IList<Color> GetFlagColors(Nationality nationality)\n{\nIList<Color> colors = FLAGS[nationality];\nreturn colors ?? new List<Color> { Color.Gray };\n}\nA second, more advanced way to reduce the complexity of the GetFlagColors\nmethod is to apply a refactoring pattern that separates functionality for different flags\nin different flag types. You can do this by applying the Replace Conditional with Poly\u2010\nmorphism pattern: each flag will get its own type that implements a general interface.\nThe polymorphic behavior of the C# language will ensure that the right functionality\nis called during runtime.\nFor this refactoring, we start with a general IFlag interface:\npublic interface IFlag\n{\nIList<Color> Colors { get; }\n}\nand specific flag types for different nationalities, such as for the Dutch:\npublic class DutchFlag : IFlag\n{\npublic IList<Color> Colors\n{\nget\n{\nreturn new List<Color> { Color.Red, Color.White, Color.Blue };\n}\n}"}
{"60": "The GetFlagColors method now becomes even more concise and less error-prone:\nprivate static readonly Dictionary<Nationality, IFlag> FLAGS =\nnew Dictionary<Nationality, IFlag>();\nstatic FlagFactory()\n{\nFLAGS[Nationality.DUTCH] = new DutchFlag();\nFLAGS[Nationality.GERMAN] = new GermanFlag();\nFLAGS[Nationality.BELGIAN] = new BelgianFlag();\nFLAGS[Nationality.FRENCH] = new FrenchFlag();\nFLAGS[Nationality.ITALIAN] = new ItalianFlag();\n}\npublic IList<Color> GetFlagColors(Nationality nationality)\n{\nIFlag flag = FLAGS[nationality];\nflag = flag ?? new DefaultFlag();\nreturn flag.Colors;\n}\nThis refactoring offers the most flexible implementation. For example, it allows the\nflag type hierarchy to grow over time by implementing new flag types and testing\nthese types in isolation. A drawback of this refactoring is that it introduces more code\nspread out over more classes. The developer much choose between extensibility and\nconciseness.\nDealing with Nesting\nSuppose a unit has a deeply nested conditional, as in the following example. Given a\nbinary search tree root node and an integer, the CalculateDepth method determines\nwhether the integer occurs in the tree. If so, the method returns the depth of the inte\u2010\nger in the tree; otherwise, it throws a TreeException:\npublic static int CalculateDepth(BinaryTreeNode<int> t, int n)\n{"}
{"61": "{\nreturn 1 + CalculateDepth(left, n);\n}\n}\nelse\n{\nBinaryTreeNode<int> right = t.Right;\nif (right == null)\n{\nthrow new TreeException(\"Value not found in tree!\");\n}\nelse\n{\nreturn 1 + CalculateDepth(right, n);\n}\n}\n}\n}\nTo improve readability, we can get rid of the nested conditional by identifying the dis\u2010\ntinct cases and insert return statements for these. In terms of refactoring, this is\ncalled the Replace Nested Conditional with Guard Clauses pattern. The result will be\nthe following method:\npublic static int CalculateDepth(BinaryTreeNode<int> t, int n)\n{\nint depth = 0;\nif (t.Value == n)\n{\nreturn depth;\n}\nif ((n < t.Value) && (t.Left != null))\n{\nreturn 1 + CalculateDepth(t.Left, n);\n}\nif ((n > t.Value) && (t.Right != null))\n{"}
{"62": "}\nelse\n{\nreturn TraverseByValue(t, n);\n}\n}\nprivate static int TraverseByValue(BinaryTreeNode<int> t, int n)\n{\nBinaryTreeNode<int> childNode = GetChildNode(t, n);\nif (childNode == null)\n{\nthrow new TreeException(\"Value not found in tree!\");\n}\nelse\n{\nreturn 1 + CalculateDepth(childNode, n);\n}\n}\nprivate static BinaryTreeNode<int> GetChildNode(\nBinaryTreeNode<int> t, int n)\n{\nif (n < t.Value)\n{\nreturn t.Left;\n}\nelse\n{\nreturn t.Right;\n}\n}\nThis actually does decrease the complexity of the unit. Now we have achieved two\nthings: the methods are easier to understand, and they are easier to test in isolation\nsince we can now write unit tests for the distinct functionalities."}
{"63": "it is natural to think that the domain\u2019s complexity carries over to the implementation,\nand that this is an unavoidable fact of life.\nWe argue against this common interpretation. Complexity in the domain does not\nrequire the technical implementation to be complex as well. In fact, it is your respon\u2010\nsibility as a developer to simplify problems such that they lead to simple code. Even if\nthe system as a whole performs complex functionality, it does not mean that units on\nthe lowest level should be complex as well. In cases where a system needs to process\nmany conditions and exceptions (such as certain legislative requirements), one solu\u2010\ntion may be to implement a default, simple process and model the exceptions\nexplicitly.\nIt is true that the more demanding a domain is, the more effort the developer must\nexpend to build technically simple solutions. But it can be done! We have seen many\nhighly maintainable systems solving complex business problems. In fact, we believe\nthat the only way to solve complex business problems and keep them under control is\nthrough simple code.\nObjection: Splitting Up Methods Does Not Reduce Complexity\n\u201cReplacing one method with McCabe 15 by three methods with McCabe 5 each means\nthat overall McCabe is still 15 (and therefore, there are 15 control flow branches overall).\nSo nothing is gained.\u201d\nOf course, you will not decrease the overall McCabe complexity of a system by refac\u2010\ntoring a method into several new methods. But from a maintainability perspective,\nthere is an advantage to doing so: it will become easier to test and understand the\ncode that was written. So, as we already mentioned, newly written unit tests allow you\nto more easily identify the root cause of your failing tests.\nPut your code in simple units (at most four branch points) that\nhave carefully chosen names describing their function and cases."}
{"64": "How SIG Rates Unit Complexity\nThe complexity (McCabe) of units (methods and constructors in C#) is one of the\neight system properties of the SIG/T\u00dcViT Evaluation Criteria for Trusted Product\nMaintainability. To rate unit complexity, every unit of the system is categorized in one\nof four risk categories depending on its McCabe measurement. Table 3-1 lists the four\nrisk categories used in the 2015 version of the SIG/T\u00dcViT Evaluation Criteria.\nThe criteria (rows) in Table 3-1 are conjunctive: a codebase needs to comply with\nall four of them. For example, if 1.5% of all lines of code are in methods with a\nMcCabe over 25, it can still be rated at 4 stars. However, in that case, at most\n10.0% - 1.5% = 8.5% of all lines of code can be in methods that have a McCabe over\n10 but not over 25.\nTable 3-1. Minimum thresholds for a 4-star unit complexity rating (2015 version of the\nSIG/T\u00dcViT Evaluation Criteria)\nLines of code in methods with \u2026 Percentage allowed for 4 stars for unit complexity\n\u2026 a McCabe above 25 At most 1.5%\n\u2026 a McCabe above 10 At most 10.0%\n\u2026 a McCabe above 5 At most 25.2%\n\u2026 a McCabe of at most 5 At least 74.8%\nSee the three quality profiles in Figure 3-2 as an example:\n\u2022 Left: an open source system, in this case Jenkins\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for unit complexity\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic"}
{"65": "CHAPTER 4\nWrite Code Once\nNumber one in the stink parade is duplicated code.\n\u2014Kent Beck and Martin Fowler,\nBad Smells in Code\nGuideline:\n\u2022 Do not copy code.\n\u2022 Do this by writing reusable, generic code and/or calling\nexisting methods instead.\n\u2022 This improves maintainability because when code is copied,\nbugs need to be fixed at multiple places, which is inefficient\nand error-prone.\nCopying existing code looks like a quick win\u2014why write something anew when it"}
{"66": "if (amount.GreaterThan(this.transferLimit))\n{\nthrow new BusinessException(\"Limit exceeded!\");\n}\n// 2. Assuming result is 9-digit bank account number, validate 11-test:\nint sum = 0;\nfor (int i = 0; i < counterAccount.Length; i++)\n{\nsum = sum + (9 - i) * (int)Char.GetNumericValue(\ncounterAccount[i]);\n}\nif (sum % 11 == 0)\n{\n// 3. Look up counter account and make transfer object:\nCheckingAccount acct = Accounts.FindAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\nreturn result;\n}\nelse\n{\nthrow new BusinessException(\"Invalid account number!\");\n}\n}\n}\nGiven the account number of the account to transfer money to (as a string), the Make\nTransfer method creates a Transfer object. MakeTransfer first checks whether the\namount to be transferred does not exceed a certain limit. In this example, the limit is\nsimply hardcoded. MakeTransfer then checks whether the number of the account to\ntransfer the money to complies with a checksum (see the sidebar \u201cThe 11-Check for\nBank Account Numbers\u201d on page 12 for an explanation of the checksum used). If that\nis the case, the object that represents this account is retrieved, and a Transfer object\nis created and returned.\nNow assume the bank introduces a new account type, called a savings account. A sav\u2010\nings account does not have a transfer limit, but it does have a restriction: money can"}
{"67": "int sum = 0;\nfor (int i = 0; i < counterAccount.Length; i++)\n{\nsum = sum + (9 - i) * (int)Char.GetNumericValue(\ncounterAccount[i]);\n}\nif (sum % 11 == 0)\n{\n// 2. Look up counter account and make transfer object:\nCheckingAccount acct = Accounts.FindAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\n// 3. Check whether withdrawal is to registered counter account:\nif (result.CounterAccount.Equals(this.RegisteredCounterAccount))\n{\nreturn result;\n}\nelse\n{\nthrow new BusinessException(\"Counter-account not registered!\");\n}\n}\nelse\n{\nthrow new BusinessException(\"Invalid account number!!\");\n}\n}\n}\nStart of code clone.\nEnd of code clone.\nBoth classes exist in the same codebase. By copying and pasting an existing class, we\nhave introduced some duplicated code in the codebase. There are now two fragments\n(ten lines of code each) of consecutive lines of code that are exactly the same. These\nfragments are called code clones or duplicates."}
{"68": "Coding is about finding generic solutions for specific problems. Either reuse (by call\u2010\ning) an existing, generic method in your codebase, or make an existing method more\ngeneric.\nTypes of Duplication\nWe define a duplicate or code clone as an identical piece of code at least six lines long.\nThe line count excludes whitespace and comments, just like in the regular definition\nof \u201cline of code\u201d (see also Chapter 1). That means that the lines need to be exactly the\nsame to be considered a duplicate. Such clones are called Type 1 clones. It does not\nmatter where the duplicates occur. Two clones can be in the same method, in\ndifferent methods in the same class, or in different methods in different classes in the\nsame codebase. Code clones can cross method boundaries. For instance, if the follow\u2010\ning fragment appears twice in the codebase, it is considered one clone of six lines of\ncode, not two clones of three lines each:\npublic void SetGivenName(string givenName)\n{\nthis.givenName = givenName;\n}\npublic void SetFamilyName(string familyName)\n{\nthis.familyName = familyName;\n}\nThe following two methods are not considered duplicates of each other even though\nthey differ only in literals and the names of identifiers:\npublic void SetPageWidthInInches(float newWidth)\n{\nfloat cmPerInch = 2.54f;\nthis.pageWidthInCm = newWidth * cmPerInch;\n// A few more lines.\n}"}
{"69": "The guideline presented in this chapter is about Type 1 clones, for two reasons:\n\u2022 Source code maintenance benefits most from the removal of Type 1 clones.\n\u2022 Type 1 clones are easier to detect and recognize (both by humans and computers,\nas detecting Type 2 clones requires full parsing).\nThe limit of six lines of code may appear somewhat arbitrary, since other books and\ntools use a different limit. In our experience, the limit of six lines is the right balance\nbetween identifying too many and too few clones. As an example, a ToString method\ncould be three or four lines, and those lines may occur in many domain objects.\nThose clones can be ignored, as they are not what we are looking for\u2014namely, delib\u2010\nerate copies of functionality.\n4.1 Motivation\nTo understand the advantages of a codebase with little duplication, in this section we\ndiscuss the effects that duplication has on system maintainability.\nDuplicated Code Is Harder to Analyze\nIf you have a problem, you want to know how to fix it. And part of that \u201chow\u201d is\nwhere to locate the problem. When you are calling an existing method, you can easily\nfind the source. When you are copying code, the source of the problem may exist\nelsewhere as well. However, the only way to find out is by using a clone detection\ntool. A well-known tool for clone detection is CPD, which is included in a source\ncode analysis tool called PMD. Several editions of Visual Studio come with a clone\ndetection tool built-in.\nThe fundamental problem of duplication is not knowing whether\nthere is another copy of the code that you are analyzing, how many\ncopies exist, and where they are located."}
{"70": "The same problem holds for regular changes. When code is duplicated, changes may\nneed to be made in multiple places, and having many duplicates makes changing a\ncodebase unpredictable.\n4.2 How to Apply the Guideline\nTo avoid the problem of duplicated bugs, never reuse code by copying and pasting\nexisting code fragments. Instead, put it in a method if it is not already in one, so that\nyou can call it the second time that you need it. That is why, as we have covered in the\nprevious chapters, the Extract Method refactoring technique is the workhorse that\nsolves many duplication problems.\nIn the example presented at the beginning of the chapter, the code that implements\nthe checksum (which is part of the duplicate) is an obvious candidate for extraction.\nTo resolve duplication using Extract Method, the duplicate (or a part thereof) is\nextracted into a new method which is then called multiple times, once from each\nduplicate.\nIn Chapter 2, the new extracted method became a private method of the class in\nwhich the long method occurs. That does not work if duplication occurs across\nclasses, as in CheckingAccount and SavingsAccount. One option in that case is to\nmake the extracted method a method of a utility class. In the example, we already\nhave an appropriate class for that (Accounts). So the new static method, IsValid, is\nsimply a method of that class:\npublic static bool IsValid(string number)\n{\nint sum = 0;\nfor (int i = 0; i < number.Length; i++)\n{\nsum = sum + (9 - i) * (int)Char.GetNumericValue(number[i]);\n}\nreturn sum % 11 == 0;"}
{"71": "{\n// 2. Look up counter account and make transfer object:\nCheckingAccount acct = Accounts.FindAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\nreturn result;\n}\nelse\n{\nthrow new BusinessException(\"Invalid account number!\");\n}\n}\n}\nStart of short clone (three lines of code).\nEnd of short clone (three lines of code).\nAnd also in SavingsAccount:\npublic class SavingsAccount\n{\npublic CheckingAccount RegisteredCounterAccount { get; set; }\npublic Transfer MakeTransfer(string counterAccount, Money amount)\n{\n// 1. Assuming result is 9-digit bank account number, validate 11-test:\nif (Accounts.IsValid(counterAccount))\n{\n// 2. Look up counter account and make transfer object:\nCheckingAccount acct = Accounts.FindAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\nif (result.CounterAccount.Equals(this.RegisteredCounterAccount))\n{\nreturn result;\n}\nelse\n{"}
{"72": "Mission accomplished: according to the definition of duplication presented at the\nbeginning of this chapter, the clone has disappeared (because the repeated fragment is\nfewer than six lines of code). But the following issues remain:\n\u2022 Even though according to the definition, the clone has disappeared, there is still\nlogic repeated in the two classes.\n\u2022 The extracted fragment had to be put in a third class, just because in C# every\nmethod needs to be in a class (or struct). The class to which the extracted method\nwas added runs the risk of becoming a hodgepodge of unrelated methods. This\nleads to a large class smell and tight coupling. Having a large class is a smell\nbecause it signals that there are multiple unrelated functionalities within the\nclass. This tends to lead to tight coupling when methods need to know imple\u2010\nmentation details in order to interact with such a large class. (For elaboration, see\nChapter 6.)\nThe refactoring technique presented in the next section solves these problems.\nThe Extract Superclass Refactoring Technique\nIn the preceding code snippets, there are separate classes for a checking account and\na savings account. They are functionally related. However, they are not related in\nC# (they are just two classes that each derive directly from System.Object).\nBoth have common functionality (the checksum validation), which introduced a\nduplicate when we created SavingsAccount by copying and pasting (and modifying)\nCheckingAccount. One could say that a checking account is a special type of a (gen\u2010\neral) bank account, and that a savings account is also a special type of a (general)\nbank account. C# (and other object-oriented languages) has a feature to represent the\nrelationship between something general and something specific: inheritance from a\nsuperclass to a subclass.\nThe Extract Superclass refactoring technique uses this feature by extracting a frag\u2010"}
{"73": "{\n// 2. Look up counter account and make transfer object:\nCheckingAccount acct = Accounts.FindAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\nreturn result;\n}\nelse\n{\nthrow new BusinessException(\"Invalid account number!\");\n}\n}\n}\nStart of extracted clone.\nEnd of extracted clone.\nThe new superclass, Account, contains logic shared by the two types of special\naccounts. You can now turn both the CheckingAccount and SavingsAccount classes\ninto subclasses of this new superclass. For CheckingAccount, the result looks like this:\npublic class CheckingAccount : Account\n{\nprivate int transferLimit = 100;\npublic override Transfer MakeTransfer(string counterAccount, Money amount)\n{\nif (amount.GreaterThan(this.transferLimit))\n{\nthrow new BusinessException(\"Limit exceeded!\");\n}\nreturn base.MakeTransfer(counterAccount, amount);\n}\n}\nThe CheckingAccount class declares its own member, transferLimit, and overrides\nMakeTransfer. The MakeTransfer method first checks to be sure the amount to be"}
{"74": "return result;\n}\nelse\n{\nthrow new BusinessException(\"Counter-account not registered!\");\n}\n}\n}\nThe SavingsAccount class declares RegisteredCounterAccount and, just like Check\ningAccount, overrides MakeTransfer. The MakeTransfer method does not need to\ncheck a limit (because savings accounts do not have a limit). Instead, it calls Make\nTransfer directly in the superclass to create a transfer. It then checks whether the\ntransfer is actually with the registered counter account.\nAll functionality is now exactly where it belongs. The part of making a transfer that is\nthe same for all accounts is in the Account class, while the parts that are specific to\ncertain types of accounts are in their respective classes. All duplication has been\nremoved.\nAs the comments indicate, the MakeTransfer method in the Account superclass has\ntwo responsibilities. Although the duplication introduced by copying and pasting\nCheckingAccount has already been resolved, one more refactoring\u2014extracting the\n11-test to its own method\u2014makes the new Account class even more maintainable:\npublic class Account\n{\npublic Transfer MakeTransfer(string counterAccount, Money amount)\n{\nif (IsValid(counterAccount))\n{\nCheckingAccount acct = Accounts.FindAcctByNumber(counterAccount);\nreturn new Transfer(this, acct, amount);\n}\nelse\n{"}
{"75": "The Account class is now a natural place for IsValid, the extracted method.\n4.3 Common Objections to Avoiding Code Duplication\nThis section discusses common objections regarding code duplication. From our\nexperience, these are developers\u2019 arguments for allowing duplication, such as copying\nfrom other codebases, claiming there are \u201cunavoidable\u201d cases, and insisting that some\ncode will \u201cnever change.\u201d\nCopying from Another Codebase Should Be Allowed\n\u201cCopying and pasting code from another codebase is not a problem because it will not\ncreate a duplicate in the codebase of the current system.\u201d\nTechnically, that is correct: it does not create a duplicate in the codebase of the cur\u2010\nrent system. Copying code from another system may seem beneficial if the code\nsolves the exact same problem in the exact same context. However, in any of the fol\u2010\nlowing situations you will run into problems:\nThe other (original) codebase is still maintained\nYour copy will not benefit from the improvements made in the original codebase.\nTherefore, do not copy, but rather import the functionality needed (that is, add\nthe other codebase to your classpath).\nThe other codebase is no longer maintained and you are working on rebuilding this\ncodebase\nIn this case, you definitely should not copy the code. Often, rebuilds are caused\nby maintainability problems or technology renewals. In the case of maintainabil\u2010\nity issues, you would be defeating the purpose by copying code. You are introduc\u2010\ning code that is determined to be (on average) hard to maintain. In the case of\ntechnology renewals, you would be introducing limitations of the old technology\ninto the new codebase, such as an inability to use abstractions that are needed for"}
{"76": "model variations in the code in such a way that they are explicit, isolated, and\ntestable.\nThis Code Will Never Change\n\u201cThis code will never, ever change, so there is no harm in duplicating it.\u201d\nIf it is absolutely, completely certain that code will never, ever change, duplication\n(and every other aspect of maintainability) is not an issue. For a start, you have to be\nabsolutely, completely certain that the code in question also does not contain any\nbugs that need fixing. Apart from that, the reality is that systems change for many\nreasons, each of which may eventually lead to changes in parts deemed to never, ever\nchange:\n\u2022 The functional requirements of the system may change because of changing users,\nchanging behavior, or a change in the way the organization does business.\n\u2022 The organization may change in terms of ownership, responsibilities, develop\u2010\nment approach, development process, or legislative requirements.\n\u2022 Technology may change, typically in the system\u2019s environment, such as the operat\u2010\ning system, libraries, frameworks, or interfaces to other applications.\n\u2022 Code itself may change, because of bugs, refactoring efforts, or even cosmetic\nimprovements.\nThat is why we argue that most of the time the expectation that code never changes is\nunfounded. So accepting duplication is really nothing more than accepting the risk\nthat someone else will have to deal with it later if it happens.\nYour code will change. Really."}
{"77": "Unit Tests Are Covering Me\n\u201cUnit tests will sort out whether something goes wrong with a duplicate.\u201d\nThis is true only if the duplicates are in the same method, and the unit test of the\nmethod covers both. If the duplicates are in other methods, it can be true only if a\ncode analyzer alerts you if duplicates are changing. Otherwise, unit tests would not\nnecessarily signal that something is wrong if only one duplicate has changed. Hence,\nyou cannot rely only on the tests (identifying symptoms) instead of addressing the\nroot cause of the problem (using duplicate code). You should not assume that even\u2010\ntual problems will be fixed later in the development process, when you could avoid\nthem altogether right now.\nDuplication in String Literals Is Unavoidable and Harmless\n\u201cI need long string literals with a lot of duplication in them. Duplication is unavoidable\nand does not hurt because it is just in literals.\u201d\nThis is a variant of one of the objections discussed in Chapter 2 (\u201cThis unit is impos\u2010\nsible to split\u201d). We often see code that contains long SQL queries or XML or HTML\ndocuments appearing as string literals in C# code. Sometimes such literals are com\u2010\nplete clones, but more often parts of them are repeated. For instance, we have seen\nSQL queries of more than a hundred lines of code that differed only in the sorting\norder (order by asc versus order by desc). This type of duplication is not harmless\neven though technically they are not in the C# logic itself. It is also not unavoidable;\nin fact this type of duplication can be avoided in a straightforward fashion:\n\u2022 Extract to a method that uses string concatenation and parameters to deal with\nvariants.\n\u2022 Use a templating engine to generate HTML output from smaller, nonduplicated\nfragments that are kept in separate files."}
{"78": "How SIG Rates Duplication\nThe amount of duplication is one of the eight system properties of the SIG/T\u00dcViT\nEvaluation Criteria for Trusted Product Maintainability. To rate duplication, all Type\n1 (i.e., textually equal) code clones of at least six lines of code are considered, except\nclones consisting entirely of import statements. Code clones are then categorized in\ntwo risk categories: redundant clones and nonredundant clones, as follows. Take a\nfragment of 10 lines of code that appears three times in the codebase. In other words,\nthere is a group of three code clones, each 10 lines of code. Theoretically, two of these\ncan be removed: they are considered technically redundant. Consequently, 10 + 10 =\n20 lines of code are categorized as redundant. One clone is categorized as nonredun\u2010\ndant, and hence, 10 lines of code are categorized as nonredundant. To be rated at 4\nstars, at most 4.6% of the total number of lines of code in the codebase can be catego\u2010\nrized as redundant. See Table 4-1\nTable 4-1. Minimum thresholds for a 4-star duplication rating (2015 version of the SIG/\nT\u00dcViT Evaluation Criteria)\nLines of code categorized as \u2026 Percentage allowed for 4 stars\n\u2026 nonredundant At least 95.4%\n\u2026 redundant At most 4.6%\nSee the three quality profiles in Figure 4-1 as an example:\n\u2022 Left: an open source system, in this case Jenkins\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for duplication\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic"}
{"79": "CHAPTER 5\nKeep Unit Interfaces Small\nBunches of data that hang around together really ought to be made into their own\nobject.\n\u2014Martin Fowler\nGuideline:\n\u2022 Limit the number of parameters per unit to at most 4.\n\u2022 Do this by extracting parameters into objects.\n\u2022 This improves maintainability because keeping the number of\nparameters low makes units easier to understand and reuse.\nThere are many situations in the daily life of a programmer where long parameter\nlists seem unavoidable. In the rush of getting things done, you might add a few\nparameters more to that one method in order to make it work for exceptional cases."}
{"80": "/// <param name=\"g\">The graphics context to draw on.</param>\n/// <param name=\"x\">The x position to start drawing.</param>\n/// <param name=\"y\">The y position to start drawing.</param>\n/// <param name=\"w\">The width of this square (in pixels.)</param>\n/// <param name=\"h\">The height of this square (in pixels.)</param>\nprivate void Render(Square square, Graphics g, int x, int y, int w, int h)\n{\nsquare.Sprite.Draw(g, x, y, w, h);\nforeach (Unit unit in square.Occupants)\n{\nunit.Sprite.Draw(g, x, y, w, h);\n}\n}\nThis method exceeds the parameter limit of 4. Especially the last four arguments, all\nof type int, make the method harder to understand and its usage more error-prone\nthan necessary. It is not unthinkable that after a long day of writing code, even an\nexperienced developer could mix up the x,y,w and h parameters\u2014a mistake that the\ncompiler and possibly even the unit tests will not catch.\nBecause the x,y,w, and h variables are related (they define a rectangle with a 2D\nanchor point, a width and a height), and the render method does not manipulate\nthese variables independently, it makes sense to group them into an object of type\nRectangle. The next code snippets show the Rectangle class and the refactored\nrender method:\npublic class Rectangle\n{\npublic Point Position { get; set; }\npublic int Width { get; set; }\npublic int Height { get; set; }\npublic Rectangle(Point position, int width, int height)\n{"}
{"81": "{\nPoint position = r.Position;\nsquare.Sprite.Draw(g, position.X, position.Y, r.Width, r.Height);\nforeach (Unit unit in square.Occupants)\n{\nunit.Sprite.Draw(g, position.X, position.Y, r.Width, r.Height);\n}\n}\nNow the render method has only three parameters instead of six. Next to that, in the\nwhole system we now have the Rectangle class available to work with. This allows us\nto also create a smaller interface for the draw method:\nprivate void Render(Square square, Graphics g, Rectangle r)\n{\nPoint position = r.Position;\nsquare.Sprite.Draw(g, r);\nforeach (Unit unit in square.Occupants)\n{\nunit.Sprite.Draw(g, r);\n}\n}\nThe preceding refactorings are an example of the Introduce Parameter Object refac\u2010\ntoring pattern. Avoiding long parameter lists, as shown in the previous example,\nimproves the readability of your code. In the next section, we explain why small inter\u2010\nfaces contribute to the overall maintainability of a system.\n5.1 Motivation\nAs we already discussed in the introduction, there are good reasons to keep interfaces\nsmall and to introduce suitable objects for the parameters you keep passing around in\nconjunction. Methods with small interfaces keep their context simple and thus are\neasier to understand. Furthermore, they are easier to reuse and modify because they\ndo not depend on too much external input."}
{"82": "Methods with Small Interfaces Are Easier to Modify\nLarge interfaces do not only make your methods obscure, but in many cases also\nindicate multiple responsibilities (especially when you feel that you really cannot\ngroup your objects together anymore). In this sense, interface size correlates with unit\nsize and unit complexity. So it is pretty obvious that methods with large interfaces are\nhard to modify. If you have, say, a method with eight parameters and a lot is going on\nin the method body, it can be difficult to see where you can split your method into\ndistinct parts. However, once you have done so, you will have several methods with\ntheir own responsibility, and moreover, each method will have a small number of\nparameters! Now it will be much easier to modify each of these methods, because you\ncan more easily locate exactly where your modification needs to be done.\n5.2 How to Apply the Guideline\nBy the time you have read this, you should be convinced that having small interfaces\nis a good idea. How small should an interface be? In practice, an upper bound of four\nseems reasonable: a method with four parameters is still reasonably clear, but a\nmethod with five parameters is already getting difficult to read and has too many\nresponsibilities.\nSo how can you ensure small interfaces? Before we show you how you can fix meth\u2010\nods with large interfaces, keep in mind that large interfaces are not the problem, but\nrather are indicators of the actual problem\u2014a poor data model or ad hoc code modi\u2010\nfication. So, you can view interface size as a code smell, to see whether your data\nmodel needs improvement.\nLarge interfaces are usually not the main problem; rather, they are a\ncode smell that indicates a deeper maintainability problem."}
{"83": "message1 + message2 + message3);\n// Send message\nm.Send(mId, subject, mMessage);\n}\nThe buildAndSendMail method clearly has too many responsibilities; the construc\u2010\ntion of the email address does not have much to do with sending the actual email.\nFurthermore, you would not want to confuse your fellow programmer with five\nparameters that together will make up a message body! We propose the following\nrevision of the method:\npublic void DoBuildAndSendMail(MailMan m, MailAddress mAddress,\nMailBody mBody)\n{\n// Build the mail\nMail mail = new Mail(mAddress, mBody);\n// Send the mail\nm.SendMail(mail);\n}\npublic class Mail\n{\npublic MailAddress Address { get; set; }\npublic MailBody Body { get; set; }\npublic Mail(MailAddress mAddress, MailBody mBody)\n{\nthis.Address = mAddress;\nthis.Body = mBody;\n}\n}\npublic class MailBody\n{\npublic string Subject { get; set; }\npublic MailMessage Message { get; set; }"}
{"84": "$\"@{division.Substring(0, 5)}.compa.ny\";\n}\n}\nThe buildAndSendMail method is now considerably less complex. Of course, you\nnow have to construct the email address and message body before you invoke the\nmethod. But if you want to send the same message to several addresses, you only have\nto build the message once, and similarly for the case where you want to send a bunch\nof messages to one email address. In conclusion, we have now separated concerns,\nand while we did so we introduced some nice, structured classes.\nThe examples presented in this chapter all group parameters into objects. Such\nobjects are often called data transfer objects or parameter objects. In the examples,\nthese new objects actually represent meaningful concepts from the domain. A point, a\nwidth, and a height represent a rectangle, so grouping these in a class called Rectan\ngle makes sense. Likewise, a first name, a last name, and a division make an address,\nso grouping these in a class called MailAddress makes sense, too. It is not unlikely\nthat these classes will see a lot of use in the codebase because they are useful generali\u2010\nzations, not just because they may decrease the number of parameters of a method.\nWhat if we have a number of parameters that do not fit well together? We can always\nmake a parameter object out of them, but probably, it will be used only once. In such\ncases, another approach is often possible, as illustrated by the following example.\nSuppose we are creating a library that can draw charts, such as bar charts and pie\ncharts, on a System.Drawing.Graphics canvas. To draw a nice-looking chart, you\nusually need quite a bit of information, such as the size of the area to draw on, config\u2010\nuration of the category axis and value axis, the actual dataset to chart, and so forth.\nOne way to supply this information to the charting library is like this:\npublic static void DrawBarChart(Graphics g,\nCategoryItemRendererState state,\nRectangle graphArea,\nCategoryPlot plot,"}
{"85": "public static void DrawBarChart(Graphics g, CategoryDataset dataset)\n{\nCharts.DrawBarChart(g,\nCategoryItemRendererState.DEFAULT,\nnew Rectangle(new Point(0, 0), 100, 100),\nCategoryPlot.DEFAULT,\nCategoryAxis.DEFAULT,\nValueAxis.DEFAULT,\ndataset);\n}\nThis covers the case where we want to use defaults for all parameters whose data\ntypes have a default value defined. However, that is just one case. Before you know it,\nyou are defining more than a handful of alternatives like these. And the version with\nseven parameters is still there.\nAnother way to solve this is to use the Replace Method with Method Object refactoring\ntechnique presented in Chapter 2. This refactoring technique is primarily used to\nmake methods shorter, but it can also be used to reduce the number of method\nparameters.\nTo apply the Replace Method with Method Object technique to this example, we\ndefine a BarChart class like this:\npublic class BarChart\n{\nprivate CategoryItemRendererState state =\nCategoryItemRendererState.DEFAULT;\nprivate Rectangle graphArea = new Rectangle(new Point(0, 0), 100, 100);\nprivate CategoryPlot plot = CategoryPlot.DEFAULT;\nprivate CategoryAxis domainAxis = CategoryAxis.DEFAULT;\nprivate ValueAxis rangeAxis = ValueAxis.DEFAULT;\nprivate CategoryDataset dataset = CategoryDataset.DEFAULT;\npublic BarChart Draw(Graphics g)\n{\n// .."}
{"86": "}\nThe static method drawBarChart from the original version is replaced by the (non\u2010\nstatic) method draw in this class. Six of the seven parameters of drawBarChart have\nbeen turned into of BarChart class. All of these have default values. We have chosen\nto keep parameter g (of type System.Drawing.Graphics) as a parameter of draw. This\nis a sensible choice: draw always needs a Graphics object, and there is no sensible\ndefault value. But it is not necessary: we could also have made g into the seventh pri\u2010\nvate member and supplied a getter and setter for it.\nWe made another choice: all setters return this to create what is called a fluent inter\u2010\nface. The setters can then be called in a cascading style, like so:\nprivate void ShowMyBarChart()\n{\nGraphics g = this.CreateGraphics();\nBarChart b = new BarChart()\n.SetRangeAxis(myValueAxis)\n.SetDataset(myDataset)\n.Draw(g);\n}\nIn this particular call of draw, we provide values for the range axis, dataset, and g, and\nuse default values for the other members of BarChart. We could have used more\ndefault values or fewer, without having to define additional overloaded draw methods.\n5.3 Common Objections to Keeping Unit Interfaces Small\nIt may take some time to get rid of all large interfaces. Typical objections to this effort\nare discussed next.\nObjection: Parameter Objects with Large Interfaces"}
{"87": "Refactoring Large Interfaces Does Not Improve My Situation\n\u201cWhen I refactor my method, I am still passing a lot of parameters to another method.\u201d\nGetting rid of large interfaces is not always easy. It usually takes more than refactor\u2010\ning one method. Normally, you should continue splitting responsibilities in your\nmethods, so that you access the most primitive parameters only when you need to\nmanipulate them separately. For instance, the refactored version of the render\nmethod needs to access all parameters in the Rectangle object because they are input\nto the draw method. But it would be better, of course, to also refactor the draw\nmethod to access the x,y,w, and h parameters inside the method body. In this way,\nyou have just passed a Rectangle in the render method, because you do not actually\nmanipulate its class variables before you begin drawing!\nFrameworks or Libraries Prescribe Interfaces with Long Parameter\nLists\n\u201cThe interface of a framework we\u2019re using has nine parameters. How can I implement\nthis interface without creating a unit interface violation?\u201d\nSometimes frameworks/libraries define interfaces or classes with methods that have\nlong parameter lists. Implementing or overriding these methods will inevitably lead\nto long parameter lists in your own code. These types of violations are impossible to\nprevent, but their impact can be limited. To limit the impact of violations caused by\nthird-party frameworks or libraries, it is best to isolate these violations\u2014for instance,\nby using wrappers or adapters. Selecting a different framework/library is also a viable\nalternative, although this can have a large impact on other parts of the codebase.\n5.4 See Also\nMethods with multiple responsibilities are more likely when the methods are large"}
{"88": "How SIG Rates Unit Interfacing\nUnit interfacing is one of the eight system properties of the SIG/T\u00dcViT Evaluation\nCriteria for Trusted Product Maintainability. To rate unit interfacing, every unit of the\nsystem is categorized in one of four risk categories depending on its number of\nparameters. Table 5-1 lists the four risk categories used in the 2015 version of the SIG/\nT\u00dcViT Evaluation Criteria.\nTable 5-1. Minimum thresholds for a 4-star unit size rating (2015 version of the SIG/\nT\u00dcViT Evaluation Criteria)\nLines of code in methods with \u2026 Percentage allowed for 4 stars for unit interfacing\n\u2026 more than seven parameters At most 0.7%\n\u2026 five or more parameters At most 2.7%\n\u2026 three or more parameters At most 13.8%\n\u2026 at most two parameters At least 86.2%\nSee the three quality profiles shown in Figure 5-1 as an example:\n\u2022 Left: an open source system, in this case Jenkins\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for unit interfacing\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic"}
{"89": "CHAPTER 6\nSeparate Concerns in Modules\nIn a system that is both complex and tightly coupled, accidents are inevitable.\n\u2014Charles Perrow\u2019s Normal Accidents theory in one sentence\nGuideline:\n\u2022 Avoid large modules in order to achieve loose coupling\nbetween them.\n\u2022 Do this by assigning responsibilities to separate modules\nand hiding implementation details behind interfaces.\n\u2022 This improves maintainability because changes in a loosely\ncoupled codebase are much easier to oversee and execute\nthan changes in a tightly coupled codebase.\nThe guidelines presented in the previous chapters are all what we call unit guidelines:"}
{"90": "We will use a true story to illustrate what tight coupling between classes is and why it\nleads to maintenance problems. This story is about how a class called UserService in\nthe service layer of a web application started growing while under development and\nkept on growing until it violated the guideline of this chapter.\nIn the first development iteration, the UserService class started out as a class with\nonly three methods, the names and responsibilities of which are shown in this code\nsnippet:\npublic class UserService\n{\npublic User LoadUser(string userId)\n{\n// ...\n}\npublic bool DoesUserExist(string userId)\n{\n// ...\n}\npublic User ChangeUserInfo(UserInfo userInfo)\n{\n// ...\n}\n}\n// end::UserSerice[]\n}\nIn this case, the backend of the web application provides a REST interface to the\nfrontend code and other systems.\nA REST interface is an approach for providing web services in a simplified manner.\nREST is a common way to expose functionality outside of the system. The class in\nthe REST layer that implements user operations uses the UserService class like this:\npublic class UserController : System.Web.Http.ApiController"}
{"91": "}\n}\nDuring the second development iteration, the UserService class is not modified at\nall. In the third development iteration, new requirements were implemented that\nallowed a user to register to receive certain notifications. Three new methods were\nadded to the UserService class for this requirement:\npublic class UserService\n{\npublic User LoadUser(string userId)\n{\n// ...\n}\npublic bool DoesUserExist(string userId)\n{\n// ...\n}\npublic User ChangeUserInfo(UserInfo userInfo)\n{\n// ...\n}\npublic List<NotificationType> GetNotificationTypes(User user)\n{\n// ...\n}\npublic void RegisterForNotifications(User user, NotificationType type)\n{\n// ...\n}\npublic void UnregisterForNotifications(User user, NotificationType type)\n{"}
{"92": "string notificationType)\n{\nUser user = userService.LoadUser(id);\nuserService.RegisterForNotifications(user,\nNotificationType.FromString(notificationType));\nreturn Ok();\n}\n[System.Web.Http.HttpPost]\n[System.Web.Http.ActionName(\"unregister\")]\npublic System.Web.Http.IHttpActionResult Unregister(string id,\nstring notificationType)\n{\nUser user = userService.LoadUser(id);\nuserService.UnregisterForNotifications(user,\nNotificationType.FromString(notificationType));\nreturn Ok();\n}\n}\nIn the fourth development iteration, new requirements for searching users, blocking\nusers, and listing all blocked users were implemented (management requested that\nlast requirement for reporting purposes). All of these requirements caused new meth\u2010\nods to be added to the UserService class.\npublic class UserService\n{\npublic User LoadUser(string userId)\n{\n// ...\n}\npublic bool DoesUserExist(string userId)\n{\n// ...\n}"}
{"93": "public void UnregisterForNotifications(User user, NotificationType type)\n{\n// ...\n}\npublic List<User> SearchUsers(UserInfo userInfo)\n{\n// ...\n}\npublic void BlockUser(User user)\n{\n// ...\n}\npublic List<User> GetAllBlockedUsers()\n{\n// ...\n}\n}\n// end::UserSerice[]\n}\nAt the end of this development iteration, the class had grown to an impressive size. At\nthis point the UserService class had become the most used service in the service\nlayer of the system. Three frontend views (pages for Profile, Notifications, and\nSearch), connected through three REST API services, used the UserService class.\nThe number of incoming calls from other classes (the fan-in) has increased to over\n50. The size of class has increased to more than 300 lines of code.\nThese kind of classes have what is called the large class smell, briefly discussed in\nChapter 4. The code contains too much functionality and also knows implementation\ndetails about the code that surrounds it. The consequence is that the class is now\ntightly coupled. It is called from a large number of places in the code, and the class\nitself knows details on other parts of the codebase. For example, it uses different data"}
{"94": "find the UserService class increasingly more difficult to understand as it becomes\nlarge and unmanageable. Less experienced developers on the team will find the class\nintimidating and will hesitate to make changes to it.\nTwo principles are necessary to understand the significance of coupling between\nclasses.\n\u2022 Coupling is an issue on the class level of source code. Each of the methods in\nUserService complies with all guidelines presented in the preceding chapters.\nHowever, it is the combination of methods in the UserService class that makes\nUserService tightly coupled with the classes that use it.\n\u2022 Tight and loose coupling are a matter of degree. The actual maintenance conse\u2010\nquence of tight coupling is determined by the number of calls to that class and the\nsize of that class. Therefore, the more calls to a particular class that is tightly cou\u2010\npled, the smaller its size should be. Consider that even when classes are split up,\nthe number of calls may not necessarily be lower. However, the coupling is then\nlower, because less code is coupled.\n6.1 Motivation\nThe biggest advantage of keeping classes small is that it provides a direct path toward\nloose coupling between classes. Loose coupling means that your class-level design will\nbe much more flexible to facilitate future changes. By \u201cflexibility\u201d we mean that you\ncan make changes while limiting unexpected effects of those changes. Thus, loose\ncoupling allows developers to work on isolated parts of the codebase without creating\nchange ripples that affect the rest of the codebase. A third advantage, which cannot be\nunderestimated, is that the codebase as a whole will be much more open to less expe\u2010\nrienced developers.\nThe following sections discuss the advantages of having small, loosely coupled classes"}
{"95": "Small, Loosely Coupled Modules Ease Navigation Through the\nCodebase\nNot only does a good separation of concerns keep the codebase flexible to facilitate\nfuture changes, it also improves the analyzability of the codebase since classes encap\u2010\nsulate data and implement logic to perform a single task. Just as it is easier to name\nmethods that only do one thing, classes also become easier to name and understand\nwhen they have one responsibility. Making sure classes have only one responsibility is\nalso known as the single responsibility principle.\nSmall, Loosely Coupled Modules Prevent No-Go Areas for New\nDevelopers\nClasses that violate the single responsibility principle become tightly coupled and accu\u2010\nmulate a lot of code over time. As with the UserService example in the introduction\nof this chapter, these classes become intimidating to less experienced developers, and\neven experienced developers are hesitant to make changes to their implementation. A\ncodebase that has a large number of classes that lack a good separation of concerns is\nvery difficult to adapt to new requirements.\n6.2 How to Apply the Guideline\nIn general, this guideline prescribes keeping your classes small (by addressing only\none concern) and limiting the number of places where a class is called by code out\u2010\nside the class itself. Following are three development best practices that help to pre\u2010\nvent tight coupling between classes in a codebase.\nSplit Classes to Separate Concerns\nDesigning classes that collectively implement functionality of a software system is the\nmost essential step in modeling and designing object-oriented systems. In typical"}
{"96": "}\npublic void Register(User user, NotificationType type)\n{\n// ...\n}\npublic void Unregister(User user, NotificationType type)\n{\n// ...\n}\n}\npublic class UserBlockService\n{\npublic void BlockUser(User user)\n{\n// ...\n}\npublic IList<User> GetAllBlockedUsers()\n{\n// ...\n}\n}\npublic class UserService\n{\npublic User LoadUser(string userId)\n{\n// ...\n}\npublic bool DoesUserExist(string userId)\n{\n// ...\n}"}
{"97": "more likely to put new functionalities in separate classes instead of defaulting to the\nUserService class.\nHide Specialized Implementations Behind Interfaces\nWe can also achieve loose coupling by hiding specific and detailed implementations\nbehind a high-level interface. Consider the following class, which implements the\nfunctionality of a digital camera that can take snapshots with the flash on or off:\npublic class DigitalCamera\n{\npublic Image TakeSnapshot()\n{\n// ...\n}\npublic void FlashLightOn()\n{\n// ...\n}\npublic void FlashLightOff()\n{\n// ...\n}\n}\nAnd suppose this code runs inside an app on a smartphone device, like this:\npublic class SmartphoneApp\n{\nprivate static DigitalCamera camera = new DigitalCamera();\npublic static void Main(string[] args)\n{\n// ...\nImage image = camera.TakeSnapshot();"}
{"98": "public void FlashLightOn()\n{\n// ...\n}\npublic void FlaslLightOff()\n{\n// ...\n}\npublic Image TakePanoramaSnapshot()\n{\n// ...\n}\npublic Video Record()\n{\n// ...\n}\npublic void SetTimer(int seconds)\n{\n// ...\n}\npublic void ZoomIn()\n{\n// ...\n}\npublic void ZoomOut()\n{\n// ...\n}\n}\nFrom this example implementation, it is not difficult to imagine that the extended\nDigitalCamera"}
{"99": "public interface ISimpleDigitalCamera\n{\nImage TakeSnapshot();\nvoid FlashLightOn();\nvoid FlashLightOff();\n}\npublic class DigitalCamera : ISimpleDigitalCamera\n{\n// ...\n}\npublic class SmartphoneApp\n{\nprivate static ISimpleDigitalCamera camera = SDK.GetCamera();\npublic static void Main(string[] args)\n{\n// ...\nImage image = camera.TakeSnapshot();\n// ...\n}\n}\nThis change leads to lower coupling by a higher degree of encapsulation. In other\nwords, classes that use only basic digital camera functionalities now do not know\nabout all of the advanced digital camera functionalities. The SmartphoneApp class\naccesses only the SimpleDigitalCamera interface. This guarantees that Smart\nphoneApp does not use any of the methods of the more advanced camera.\nAlso, this way your system becomes more modular: it is composed such that a change\nto one class has minimal impact on other classes. This, in turn, increases modifiabil\u2010\nity: it is easier and less work to modify the system, and there is less risk that modifica\u2010\ntions introduce defects."}
{"100": "6.3 Common Objections to Separating Concerns\nThe following are typical objections to the principle explained in this chapter.\nObjection: Loose Coupling Conflicts With Reuse\n\u201cTight coupling is a side effect of code reuse, so this guideline conflicts with that best\npractice.\u201d\nOf course, code reuse can increase the number of calls to a method. However, there\nare two reasons why this should not lead to tight coupling:\n\u2022 Reuse does not necessarily lead to methods that are called from as many places as\npossible. Good software design\u2014for example, using inheritance and hiding\nimplementation behind interfaces\u2014will stimulate code reuse while keeping the\nimplementation loosely coupled, since interfaces hide implementation details.\n\u2022 Making your code more generic, to solve more problems with less code, does not\nmean it should become a tightly coupled codebase. Clearly, utility functionality is\nexpected to be called from more places than specific functionality. Utility func\u2010\ntionality should then also embody less source code. In that way, there may be\nmany incoming dependencies, but the dependencies refer to a small amount of\ncode.\nObjection: C# Interfaces Are Not Just for Loose Coupling\n\u201cIt doesn\u2019t make sense to use C# interfaces to prevent tight coupling.\u201d\nIndeed, using interfaces is a great way to improve encapsulation by hiding implemen\u2010\ntations, but it does not make sense to provide an interface for every class. As a rule of\nthumb, an interface should be implemented by at least two classes in your codebase.\nConsider splitting your class if the only reason to put an interface in front of your"}
{"101": "Objection: Not All Loose Coupling Solutions Increase Maintainability\n\u201cFrameworks that implement inversion of control (IoC) achieve loose coupling but make\nit harder to maintain the codebase.\u201d\nInversion of control is a design principle to achieve loose coupling. There are frame\u2010\nworks available that implement this for you. IoC makes a system more flexible for\nextension and decreases the amount of knowledge that pieces of code have of each\nother.\nThis objection holds when such frameworks add complexity for which the maintain\u2010\ning developers are not experienced enough. Therefore, in cases where this objection\nis true, it is not IoC that is the problem, but the framework that implements it.\nThus, the design decision to use a framework for implementing IoC should be con\u2010\nsidered with care. As with all engineering decisions, this is a trade-off that does not\npay off in all cases. Using these types of frameworks just to achieve loose coupling is a\nchoice that can almost never be justified."}
{"102": "How SIG Rates Module Coupling\nModule coupling is one of the eight system properties of the SIG/T\u00dcViT Evaluation\nCriteria for Trusted Product Maintainability. To rate module coupling, the fan-in of\nevery method is calculated. Each module (class in C#) is then categorized in one of\nfour risk categories depending on the total fan-in of all methods in the class. Table 6-1\nlists the four risk categories used in the 2015 version of the SIG/T\u00dcViT Evaluation\nCriteria. The table shows the maximum amount of code that may fall in the risk cate\u2010\ngories in order to achieve a 4-star rating. For example, a maximum of 21.8% of code\nvolume may be in classes with a fan-in in the moderate risk category, and likewise for\nthe other risk categories.\nTable 6-1. Module coupling risk categories (2015 version of the SIG/T\u00dcViT Evaluation\nCriteria)\nFan-in of modules in the category Percentage allowed for 4 stars\n51+ At most 6.6%\n21\u201350 At most 13.8%\n11\u201320 At most 21.6%\n1\u201310 No constraint\nSee the three quality profiles in Figure 6-1 as an example:\n\u2022 Left: an open source system, in this case Jenkins. Note that Jenkins does not fulfill\nthe 4-star requirement here for the highest risk category (in red).\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for module coupling.\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic."}
{"103": "CHAPTER 7\nCouple Architecture Components Loosely\nThere are two ways of constructing a software design: one way is to make it so simple\nthat there are obviously no deficiencies, and the other way is to make it so complicated\nthat there are no obvious deficiencies.\n\u2014C.A.R. Hoare\nGuideline:\n\u2022 Achieve loose coupling between top-level components.\n\u2022 Do this by minimizing the relative amount of code within\nmodules that is exposed to (i.e., can receive calls from)\nmodules in other components.\n\u2022 This improves maintainability because independent compo\u2010\nnents ease isolated maintenance."}
{"104": "However, the implementation of software architecture always remains the responsi\u2010\nbility of you as a developer.\nComponents should be loosely coupled; that is, they should be clearly separated by\nhaving few entry points for other components and a limited amount of information\nshared among components. In that case, implementation details of methods are hid\u2010\nden (or encapsulated) which makes the system more modular.\nSounds familiar? Yes, both as a general design principle and on a module level, loose\ncoupling has been discussed in Chapter 6. Component coupling applies the same rea\u2010\nsoning but at the higher level of components rather than modules. Module coupling\nfocuses on the exposure of individual modules (classes) to the rest of the codebase.\nComponent coupling focuses specifically on the exposure of modules in one compo\u2010\nnent (group of modules) to the modules in another component.\nSo a module being called from a module in the same component is\nconsidered to be an internal call if we assess at the component level,\nbut when we assess it at the module level, there is module coupling\nindeed.\nIn this chapter, we refer to the characteristic of being loosely coupled on a component\nlevel as component independence. The opposite of component independence is com\u2010\nponent dependence. In that case, the inner workings of components are exposed too\nmuch to other components that rely on them. That kind of entanglement makes it\nharder to oversee effects that code changes in one component may have on others,\nbecause it does not behave in an isolated manner. This complicates testing, when we\nmust make assumptions or simulations of what happens within another component.\n7.1 Motivation\nSystem maintenance is easier when changes within a component have effects that are"}
{"105": "Figure 7-1. Low component dependence (left) and high component dependence (right)\nThe left side of the figure shows a low level of component dependence. Most calls\nbetween modules are internal (within the component). Let us elaborate on internal\nand noninternal dependencies.\nCalls that improve maintainability:\n\u2022 Internal calls are healthy. Since the modules calling each other are part of the\nsame component, they should implement closely related functionality. Their\ninner logic is hidden from the outside.\n\u2022 Outgoing calls are also healthy. As they delegate tasks to other components, they\ncreate a dependency outward. In general, delegation of distinct concerns to other\ncomponents is a good thing. Delegation can be done from anywhere within a\ncomponent and does not need to be restricted to a limited set of modules within\nthe component."}
{"106": "Calls that have a negative impact on maintainability:\n\u2022 Incoming calls provide functionality for other components by offering an inter\u2010\nface. The code volume that is involved in this should be limited. Conversely, the\ncode within a component should be encapsulated as much as possible\u2014that is, it\nshould be shielded against direct invocations from other components. This\nimproves information hiding. Also, modifying code involved in incoming depen\u2010\ndencies potentially has a large impact on other components. By having a small\npercentage of code involved in incoming dependencies, you may dampen the\nnegative ripple effects of modifications to other components.\n\u2022 Throughput code is risky and must be avoided. Throughput code both receives\nincoming calls and delegates to other components. Throughput code accom\u2010\nplishes the opposite of information hiding: it exposes its delegates (implementa\u2010\ntion) to its clients. It is like asking a question to a help desk that does not\nformulate its own answer but instead forwards your question to another com\u2010\npany. Now you are dependent on two parties for the answer. In the case of code,\nthis indicates that responsibilities are not well divided over components. As it is\nhard to trace back the path that the request follows, it is also hard to test and\nmodify: tight coupling may cause effects to spill over to other components.\nThe right side of the figure shows a component with a high level of component\ndependence. The component has many dependencies with modules outside the com\u2010\nponent and is thus tightly coupled. It will be hard to make isolated changes, since the\neffects of changes cannot be easily overseen.\nNote that the effects of component independence are enhanced by\ncomponent balance. Component balance is achieved when the num\u2010\nber of components and their relative size are balanced. For elabora\u2010\ntion on this topic, see Chapter 8."}
{"107": "Figure 7-2. Designed versus implemented architecture\nLow Component Dependence Allows for Isolated Maintenance\nA low level of dependence means that changes can be made in an isolated manner.\nThis applies when most of a component\u2019s code volume is either internal or outgoing.\nIsolated maintenance means less work, as coding changes do not have effects outside\nthe functionality that you are modifying.\nNote that this reasoning about isolation applies to code on a\nsmaller level. For example, a system consisting of small, simple\nclasses signals a proper separation of concerns, but does not guar\u2010\nantee it. For that, you will need to investigate the actual dependen\u2010\ncies (see, for example, Chapter 6).\nLow Component Dependence Separates Maintenance Responsibilities\nIf all components are independent from each other, it is easier to distribute responsi\u2010\nbilities for maintenance among separate teams. This follows from the advantage of\nisolated modification. Isolation is in fact a prerequisite for efficient division of devel\u2010"}
{"108": "Low Component Dependence Eases Testing\nCode that has a low dependence on other components (modules with mainly internal\nand outgoing code) is easier to test. For internal calls, functionality can be traced and\ntested within the component. For outgoing calls, you do not need to mock or stub\nfunctionality that is provided by other components (given that functionality in that\nother component is finished).\nFor elaboration on (unit) testing, see also Chapter 10.\n7.2 How to Apply the Guideline\nThe goal for this chapter\u2019s guideline is to achieve loose coupling between compo\u2010\nnents. In practice, we find that you can help yourself by adhering to the following\nprinciples for implementing interfaces and requests between components.\nThe following principles help you apply the guideline of this chapter:\n\u2022 Limit the size of modules that are the component\u2019s interface.\n\u2022 Define component interfaces on a high level of abstraction. This limits the types\nof requests that cross component borders. That avoids requests that \u201cknow too\nmuch\u201d about the implementation details.\n\u2022 Avoid throughput code, because it has the most serious effect on testing func\u2010\ntionality. In other words, avoid interface modules that put through calls to other\ncomponents. If throughput code exists, analyze the concerned modules in order\nto solve calls that are put through to other components.\nAbstract Factory Design Pattern\nComponent independence reflects the high-level architecture of a software system."}
{"109": "The Abstract Factory design pattern hides (or encapsulates) the creation of specific\n\u201cproducts\u201d behind a generic \u201cproduct factory\u201d interface. In this context, products are\ntypically entities for which more than one variant exists. Examples are audio format\ndecoder/encoder algorithms or user interface widgets that have different themes\nfor \u201clook and feel.\u201d In the following example, we use the Abstract Factory design pat\u2010\nten to encapsulate the specifics of cloud hosting platforms behind a small factory\ninterface.\nSuppose our codebase contains a component, called PlatformServices, that imple\u2010\nments the management of services from a cloud hosting platform. Two specific cloud\nhosting providers are supported by the PlatformServices component: Amazon AWS\nand Microsoft Azure (more could be added in the future).\nTo start/stop servers and reserve storage space, we have to implement the following\ninterface for a cloud hosting platform:\npublic interface ICloudServerFactory\n{\nICloudServer LaunchComputeServer();\nICloudServer LaunchDatabaseServer();\nICloudStorage CreateCloudStorage(long sizeGb);\n}\nBased on this interface, we create two specific factory classes for AWS and Azure:\npublic class AWSCloudServerFactory : ICloudServerFactory\n{\npublic ICloudServer LaunchComputeServer()\n{\nreturn new AWSComputeServer();\n}\npublic ICloudServer LaunchDatabaseServer()\n{"}
{"110": "return new AzureDatabaseServer();\n}\npublic ICloudStorage CreateCloudStorage(long sizeGb) {\nreturn new AzureCloudStorage(sizeGb);\n}\n}\nNote that these factories make calls to specific AWS and Azure implementation\nclasses (which in turn do specific AWS and Azure API calls), but return generic inter\u2010\nface types for servers and storage.\nCode outside the PlatformServices component can now use the concise interface\nmodule ICloudServerFactory\u2014for example, like this:\npublic class ApplicationLauncher\n{\npublic static void Main(string[] args)\n{\nICloudServerFactory factory;\nif (args[1].Equals(\"-azure\"))\n{\nfactory = new AzureCloudServerFactory();\n}\nelse\n{\nfactory = new AWSCloudServerFactory();\n}\nICloudServer computeServer = factory.LaunchComputeServer();\nICloudServer databaseServer = factory.LaunchDatabaseServer();\nThe ICloudServerFactory interface of the PlatformServices provides a small inter\u2010\nface for other components in the codebase. This way, these other components can be\nloosely coupled to it.\n7.3 Common Objections to Loose Component Coupling"}
{"111": "Objection: Component Dependence Cannot Be Fixed Because the\nComponents Are Entangled\n\u201cWe cannot get component dependence right because of mutual dependencies between\ncomponents.\u201d\nEntangled components are a problem that you experience most clearly during main\u2010\ntenance. You should start by analyzing the modules in the throughput category, as it\nhas the most serious effect on the ease of testing and on predicting what exactly the\nfunctionality does.\nWhen you achieve clearer boundaries for component responsibilities, it improves the\nanalyzability and testability of the modules within. For example, modules with an\nextraordinary number of incoming calls may signal that they have multiple responsi\u2010\nbilities and can be split up. When they are split up, the code becomes easier to analyze\nand test. For elaboration, please refer to Chapter 6.\nObjection: No Time to Fix\n\u201cIn the maintenance team, we understand the importance of achieving low component\ndependence, but we are not granted time to fix it.\u201d\nWe understand how this is an issue. Development deadlines are real, and there may\nnot be time for refactoring, or what a manager may see as \u201ctechnical aesthetics.\u201d What\nis important is the trade-off. One should resolve issues that pose a real problem for\nmaintainability. So dependencies should be resolved if the team finds that they inhibit\ntesting, analysis, or stability. You can solidify your case by measuring what percentage\nof issues arises/maintenance effort is needed in components that are tightly coupled\nwith each other.\nFor example, throughput code follows complex paths that are hard to test for devel\u2010\nopers. There may be more elegant solutions that require less time and effort."}
{"112": "Objection: Throughput Is a Requirement\n\u201cWe have a requirement for a software architecture for a layer that puts through calls.\u201d\nIt is true that some architectures are designed to include an intermediate layer. Typi\u2010\ncally, this is a service layer that collects requests from one side (e.g., the user interface)\nand bundles them for passing on to another layer in the system. The existence of such\na layer is not necessarily a problem\u2014given that this layer implements loose coupling.\nIt should have a clear separation of incoming and outgoing requests. So the module\nthat receives requests in this layer:\n\u2022 Should not process the request itself.\n\u2022 Should not know where and how to process that request (its implementation\ndetails).\nIf both are true, the receiving module in the service layer has an incoming request\nand an outgoing request, instead of putting requests through to a specific module in\nthe receiving component.\nA large-volume service layer containing much logic is a typical code smell. In that\ncase, the layer does not merely abstract and pass on requests, but also transforms\nthem. Hence, for transformation, the layer knows about the implementation details.\nThat means that the layer does not properly encapsulate both request and implemen\u2010\ntation. If throughput code follows from software architecture requirements, you may\nraise the issue to the software or enterprise architect.\n7.4 See Also\nA related concept to component independence is that of component balance, dis\u2010\ncussed in Chapter 8. That chapter deals with achieving an overseeable number of\ncomponents that are balanced in size."}
{"113": "\u2022 Hidden code is composed of modules that have no incoming dependencies from\nmodules in other components: they call only within their own component (inter\u2010\nnal) and may have calls outside their own component (outgoing calls).\n\u2022 Interface code is composed of modules that have incoming dependencies from\nmodules in other components. They consist of code in modules with incoming\nand throughput code.\nFollowing the principle of loose coupling, a low level of dependence between modules\nis better than a high level of dependence. That signals the risk that changes within one\ncomponent propagate to other components.\nSIG measures component independence as the percentage of code that is classified as\nhidden code. To achieve a SIG/T\u00dcViT rating of 4 stars for highly-maintainable soft\u2010\nware, the percentage of code residing in modules with incoming dependencies from\nother components (incoming or throughput) should not exceed 14.2%.\nSee the three quality profiles in Figure 7-3 as an example:\n\u2022 Left: an open source system, in this case Jenkins\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for component independence\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic"}
{"114": ""}
{"115": "CHAPTER 8\nKeep Architecture Components Balanced\nBuilding encapsulation boundaries is a crucial skill in software architecture.\n\u2014George H. Fairbanks in Just Enough Architecture\nGuideline:\n\u2022 Balance the number and relative size of top-level compo\u2010\nnents in your code.\n\u2022 Do this by organizing source code in a way that the number\nof components is close to 9 (i.e., between 6 and 12) and that\nthe components are of approximately equal size.\n\u2022 This improves maintainability because balanced components\nease locating code and allow for isolated maintenance.\nA well-balanced software architecture is one with not too many and not too few com\u2010"}
{"116": "narily large, the architecture becomes monolithic; it becomes hard to navigate the\ncodebase and do isolated maintenance. In the third situation (bottom left), where the\narchitecture is scattered among many components, it becomes hard to keep a mental\nmap of the codebase and to grasp how components interact."}
{"117": "8.1 Motivation\nNow we know what component balance is, but not why it is important. The reason is\nsimple: software maintenance is easier when the software architecture is balanced.\nThis section discusses in what ways you can benefit from a good system component\nbalance: it makes it easier to find and analyze code, it better isolates effects of mainte\u2010\nnance, and it separates maintenance responsibilities.\nA Good Component Balance Eases Finding and Analyzing Code\nA clear code organization in components makes it easier to find the piece of code that\nyou want to change. Of course, proper code hygiene helps in this process as well, such\nas using a consistent naming convention (see Chapter 11). When the number of com\u2010\nponents is manageable (around nine) and their volume is consistent, they allow for a\ndrill-down each time that you need to analyze code to modify it.\nIn contrast, an unbalanced organization of components is more likely to have unclear\nfunctional boundaries. For example, a component that is very large compared to oth\u2010\ners is more likely to contain functionalities that are unrelated, and therefore that\ncomponent is harder to analyze.\nA Good Component Balance Better Isolates Maintenance Effects\nWhen a system\u2019s component balance clearly describes functional boundaries, it has a\nproper separation of concerns, which makes for isolated behavior in the system. Iso\u2010\nlated behavior within system components is relevant because it guards against unex\u2010\npected effects, such as regression.\nMore broadly, isolation of code within components has the general advantage of\nmodularity: components with clear functional and technical boundaries are easier to\nsubstitute, remove, and test than components with mixed functionalities and techni\u2010\ncal intertwinement."}
{"118": "This reasoning about isolation applies to code on a smaller level as\nwell. For example, a system that consists of small, simple classes\nsignals a proper separation of concerns, but does not guarantee it.\nFor that you will need to investigate the actual dependencies (see,\ne.g., Chapter 6).\nA Good Component Balance Separates Maintenance Responsibilities\nHaving clear functional boundaries between components makes it easier to distribute\nresponsibilities for maintenance among separate teams. The number of components\nof a system and their relative size should indicate the system\u2019s decomposition into\nfunctional groups.\nWhen a system has too many or too few components, it is considered more difficult\nto understand and harder to maintain. If the number of components is too low, it\ndoes not help you much to navigate through the functionalities of the system. On the\nother hand, too many components make it hard to get a clear overview of the entire\nsystem.\n8.2 How to Apply the Guideline\nThe two principles of component balance are:\n\u2022 The number of top-level system components should ideally be 9, and generally\nbetween 6 and 12.\n\u2022 The components\u2019 volume in terms of source code should be roughly equal.\nNote that component balance is an indicator for a clear component\nseparation, not a goal in itself. It should follow from the system"}
{"119": "Decide on the Right Conceptual Level for Grouping Functionality into\nComponents\nTo achieve a good system division that is easy to navigate for developers, you need to\nchoose the right conceptual level for grouping functionality. Usually, software systems\nare organized along high-level functional domains that describe what kind of func\u2010\ntions the system performs for the user. Alternatively, a division is made along the sep\u2010\narations of technical specialities.\nFor example, a system that bases component division on function domains might\nhave components like Data Retrieval, Invoice Administration, Reporting, Adminis\u2010\ntrator, and so on. Each component contains source code that offers end-to-end func\u2010\ntionality, ranging from the database to the frontend. A functional division has\nthe advantage of being available during design, before development starts. For devel\u2010\nopers, it has the advantage that they can analyze source code while thinking in\nhigh-level functionalities. A disadvantage can be that developers may need to be pro\u2010\nficient and comfortable in multiple technical domains to make changes to a single\ncomponent.\nAn example of a system that uses technical division might have components like\nFrontend, Backend, Interfaces, Logging, and so on. This approach has advantages for\nteams that have a division of responsibilities based on technology specialization. The\ncomponent division then reflects the division of labor among various specialists.\nChoosing the right concepts for grouping functionality within a system is part of the\nsoftware architect role. This role may be assigned to a single person, or it may be dis\u2010\ntributed over various people within the development team. When changes are needed\nto the component division, those in the architect role must be consulted.\nClarify the System\u2019s Domains and Apply Those Consistently\nOnce a choice for the type of system division into components has been made, you"}
{"120": "8.3 Common Objections to Balancing Components\nThis section discusses objections regarding component balance. Common objections\nare that component imbalance is not really a problem, or that it is a problem that can\u2010\nnot be fixed.\nObjection: Component Imbalance Works Just Fine\n\u201cOur system may seem to have bad component balance, but we\u2019re not having any prob\u2010\nlems with it.\u201d\nComponent balance, as we define it, is not binary. There are different degrees of bal\u2010\nance, and its definition allows for some deviation from the \u201cideal\u201d of nine compo\u2010\nnents of equal size. Whether a component imbalance is an actual maintenance\nburden depends on the degree of deviation, the experience of the maintenance team,\nand the cause of the imbalance.\nThe most important maintenance burden occurs when the imbalance is caused by\nlack of discipline during maintenance\u2014when developers do not put code in the com\u2010\nponent where it belongs. Since inconsistency is the enemy of predictability, that may\nlead to unexpected effects. Code that is placed in the wrong components may lead to\nunintended dependencies between components, which hurts testability and flexibility.\nObjection: Entanglement Is Impairing Component Balance\n\u201cWe cannot get component balance right because of entanglement among components.\u201d\nThis situation points to another problem: technical dependence between compo\u2010\nnents. Entanglement between components signals an improper separation of con\u2010\ncerns. This issue and guideline are further described in Chapter 7. In this case, it is\nmore important and urgent to fix component dependencies\u2014for example, by hiding\nimplementation details behind interfaces and fixing circular dependencies. After that,"}
{"121": "How SIG Rates Component Balance\nSIG defines and measures component balance as a combined calculation (i.e., multi\u2010\nplication) of the following:\n\u2022 The number of top-level system components\n\u2022 The uniformity of component size\nThe ideal number of top-level system components is nine, as SIG has identified that as\nthe median in its benchmark. The closer the actual number of components is to nine,\nthe better.\nThe score for the number of top-level system components ranges from 0 to 1. A sys\u2010\ntem with nine components gives a score of 1, linearly decreasing to 0 for a system\nwith one component. A correction is applied upward, to allow for a more lenient\ncount when the number of components is higher than 17, which would otherwise\nlead to a score of 0 with a linear model. The correction is based on the 95th percentile\nscores within the benchmark.\nUniformity of component size means the distribution of source code volume between\ncomponents. An equally sized distribution of top-level components is better than an\nunequal distribution.\nSIG uses the adjusted Gini coefficient as a measure of component size uniformity.\nThe Gini coefficient measures the inequality of distribution between things and\nranges from 0 (perfect equality) to 1 (perfect inequality).\nTo achieve a SIG/T\u00dcViT rating of 4 stars for highly-maintainable software, the num\u2010\nber of components should be close to the ideal of nine, and the adjusted Gini coeffi\u2010\ncient of the component sizes should be 0.71 maximum.\nSee the volume charts in Figure 8-2 as an example:"}
{"122": ""}
{"123": "CHAPTER 9\nKeep Your Codebase Small\nProgram complexity grows until it exceeds the capability of the programmer who must\nmaintain it.\n\u20147th Law of Computer Programming\nGuideline:\n\u2022 Keep your codebase as small as feasible.\n\u2022 Do this by avoiding codebase growth and actively reducing\nsystem size.\n\u2022 This improves maintainability because having a small prod\u2010\nuct, project, and team is a success factor.\nA codebase is a collection of source code that is stored in one repository, can be com\u2010\npiled and deployed independently, and is maintained by one team. A system has at"}
{"124": "Given two systems with the same functionality, in which one has a small codebase\nand the other has a large codebase, you surely would prefer the small system. In a\nsmall system it is easier to search through, analyze, and understand code. If you mod\u2010\nify something, it is easier to tell whether the change has effects elsewhere in the sys\u2010\ntem. This ease of maintenance leads to fewer mistakes and lower costs. That much is\nobvious.\n9.1 Motivation\nSoftware development and maintenance become increasingly hard with growing sys\u2010\ntem size. Building larger systems requires larger teams and longer-lasting projects,\nwhich bring additional overhead and risks of (project) failure. The rest of this section\ndiscusses the adverse effects of large software systems.\nA Project That Sets Out to Build a Large Codebase Is More Likely to\nFail\nThere is a strong correlation between project size and project risks. A large project\nleads to a larger team, complex design, and longer project duration. As a result, there\nis more complex communication and coordination among stakeholders and team\nmembers, less overview over the software design, and a larger number of require\u2010\nments that change during the project. This all increases the chance of reduced quality,\nproject delays, and project failure. The probabilities in the graph in Figure 9-1 are\ncumulative: for example, for all projects over 500 man-years of development effort,\nmore than 90% are indentified as \u201cpoor project quality.\u201d A subset of this is projects\nwith delays (80\u201390% of the total) and failed projects (50% of the total).\nFigure 9-1 illustrates the relationship between project size and project failure: it\nshows that as the size of a project increases, the chances of project failure (i.e., project\nis terminated or does not deliver results), of project delay, and of a project delivered\nwith poor quality are increasingly high."}
{"125": "Figure 9-1. Probability of project failures by project size1"}
{"126": "Large Codebases Are Harder to Maintain\nFigure 9-2 illustrates how codebase size affects maintainability.\nFigure 9-2. Distribution of system maintainability in SIG benchmark among different\nvolume groups\nThe graph is based on a set of codebases of over 1,500 systems in the SIG Software\nAnalysis Warehouse. Volume is measured as the amount of development effort in\nman-years to reproduce the system (see also \u201cHow SIG Rates Codebase Volume\u201d on\npage 110). Each bar shows the distribution of systems in different levels of maintaina\u2010\nbility (benchmarked in stars). As the graph shows, over 30% of systems in the small\u2010\nest volume category manage to reach 4- or 5-star maintainability, while in the largest"}
{"127": "Figure 9-3. Impact of code volume on the number of defects2\n9.2 How to Apply the Guideline\nAll other things being equal, a system that has less functionality will be smaller than a\nsystem that has more functionality. Then, the implementation of that functionality\nmay be either concise or verbose. Therefore, achieving a small codebase first requires\nkeeping the functionality of a system limited, and then requires attention to keep the\namount of code limited.\nFunctional Measures\nFunctionality-related measures are not always within your span of control, but when\u2010\never new or adapted functionality is being discussed with developers, the following\nshould be considered:"}
{"128": "Standardize functionality:\nBy standardization of functionality we mean consistency in the behavior and\ninteractions of the program. First of all, this is intended to avoid the implementa\u2010\ntion of the same core functionality in multiple, slightly different ways. Secondly,\nstandardization of functionality offers possibilities for reuse of code\u2014assuming\nthe code itself is written in a reusable way.\nTechnical Measures\nFor the technical implementation, the goal is to use less code to implement the same\nfunctionality. You can achieve this mainly through reusing code by referral (instead\nof writing or copying and pasting code again) or by avoiding coding altogether, but\nusing existing libraries or frameworks.\nDo not copy and paste code:\nReferring to existing code is always preferable to copying and pasting code in\npieces that will need to be maintained individually. If there are multiple copies of\na piece of code, maintenance needs to occur in multiple places, too. Mistakes\neasily crop up if an update in one piece of logic requires individual adjustment\n(or not) and testing of multiple, scattered copies. Note that the intention of the\nguideline presented in Chapter 4 is precisely to avoid copying and pasting.\nRefactor existing code:\nWhile refactoring has many merits for code maintainability, it can have an imme\u2010\ndiate and visible effect in reducing the codebase. Typically, refactoring involves\nrevisiting code, simplifying its structure, removing code redundancies, and\nimproving the amount of reuse. This may be as simple as removing unused/obso\u2010\nlete functionality. See, for example, the refactoring patterns in Chapter 4.\nUse third-party libraries and frameworks:\nMany applications share the same type of behavior for which a vast number of\nframeworks and libraries exist\u2014for example, UI behavior (e.g., jQuery), database"}
{"129": "Do not make changes to the source code of a third-party library. If\nyou do, essentially you have made the library code part of your\nown codebase. In particular, updates of changed libraries are cum\u2010\nbersome and can easily lead to bugs. Typically, difficulties arise\nwhen developers try to update the library to a newer version, since\nthey need to analyze what has been changed in the library code and\nhow that impacts the locally changed code.\nSplit up a large system:\nSplitting up a large system into multiple smaller systems is a way to minimize the\nissues that come with larger systems. A prerequisite is that the system can be\ndivided into parts that are independent, from a functional, technical, and lifecycle\nperspective. To the users, the systems (or plugins) must be clearly separated.\nTechnically, the code in the different systems must be loosely coupled; that is,\ntheir code is related via interfaces instead of direct dependencies. Systems are\nonly really independent if their lifecycles are decoupled (i.e., they are developed\nand released independently). Note that the split systems may well have some\nmutual or shared dependencies. There is an additional advantage. It might turn\nout that some of the new subsystems can be replaced by a third-party package,\ncompletely removing the need to have any codebase for this subsystem. An\nexample is a Linux distribution such as Ubuntu. The Linux kernel is a codebase\nthat lives at kernel.org and is maintained by a large team of volunteers headed by\nLinus Torvalds. Next to the actual Linux kernel, a distribution contains thou\u2010\nsands of other software applications, each of which has its own codebase. These\nare the types of plugins that we mean here.\nDecoupling (on a code level) is discussed in more detail in the chapters that deal with\nloose coupling, particularly Chapter 7.\n9.3 Common Objections to Keeping the Codebase Small"}
{"130": "The most visible improvements will appear once a system is big and parts of it can be\nremoved\u2014for example, when functionality is being replaced by third-party code or\nafter a system has been split into multiple parts.\nObjection: Reducing the Codebase Size Is Impeded by Productivity\nMeasures\n\u201cI cannot possibly reduce the size of my system, since my programming productivity is\nbeing measured in terms of added code volume.\u201d\nIf this is the case, we suggest escalating this issue. Measuring development productiv\u2010\nity in terms of added code volume is a bad practice. It provides a negative incentive,\nas it encourages the bad habit of copying and pasting code. Code reference is better\nbecause it improves analyzing, testing, and changing code.\nWe understand that the number of code additions can help managers monitor pro\u2010\ngress and predict timelines. However, productivity should be measured in terms of\nvalue added, not lines of code added. Experienced developers can often add function\u2010\nality with a minimum number of additional lines of code, and they will refactor the\ncode whenever they see an opportunity, often resulting in reduction of the code size.\nObjection: Reducing the Codebase Size is Impeded by the\nProgramming Language\n\u201cI work with a language that is more verbose than others, so I cannot achieve a small\ncodebase.\u201d\nIn most projects, the programming language is a given. It may very well be true that\nin some programming languages, it is impossible to get a small codebase (SQL-based\nlanguages come to mind). However, you can always strive to get a smaller codebase\nthan you currently have, in the same programming language. Every codebase benefits\nfrom decreasing its size, even those in low-level languages with little possibility for"}
{"131": "original functionality can be split up into multiple parts, then ideally you end up with\na piece of code that can be referred to independently by the new functionality, avoid\u2010\ning duplication and taming codebase growth. Write unit tests for the new units to\nverify that you understand the inner workings of the unit. Besides, it is recommended\npractice; see Chapter 10.\nObjection: Splitting the Codebase Is Impossible Because of Platform\nArchitecture\n\u201cWe cannot split the system into smaller parts because we are building for a platform\nwhere all functionality is tied to a common codebase.\u201d\nYes, platform-based software tends to grow large over time because it assimilates new\nfunctionality and rarely reduces functionality. One way to dramatically decrease the\nsize of the codebase is to decouple the system into a plug-in architecture. This leads\nto multiple codebases that are each smaller than the original one. There is a codebase\nfor the common core, and one or more codebases for the plugins. If those plugins are\ntechnically decoupled, they allow for separate release cycles. That means that small\nchanges in functionality do not need an update of the whole system. Keep in mind\nthat those small updates still need full integration/regression tests to ensure that the\nsystem as a whole still functions as expected.\nObjection: Splitting the Codebase Leads to Duplication\n\u201cSplitting the codebase forces me to duplicate code.\u201d\nThere may be cases in which decoupling a system into separate parts (such as plu\u2010\ngins/extensions) requires (interfaces to) common functionality or data structures to\nbe duplicated in those extensions.\nIn such a case, duplication is a bigger problem than having a large codebase, and the\nguideline of Chapter 4 prevails over this guideline of achieving a small codebase. It is\nthen preferable to code common functionality either as a separate extension or as"}
{"132": "Keep in mind that the goal is to have subsystems that can be\nmaintained independently, not necessarily systems that operate\nindependently.\nHow SIG Rates Codebase Volume\nThe metric for codebase volume does not have different risk categories, since it con\u2010\nsists of only one metric. To be rated at 4 stars, the codebase should be at most equiva\u2010\nlent to 20 man-years of rebuild value. If C# is the only technology in a system, this\ntranslates to at most 160,000 lines of code.\nMan-months and man-years\nThe total volume in a codebase is the volume in lines of code converted to man-\nmonths. A man-month is a standard measure of source code volume. It is the amount\nof source code that one developer with average productivity could write in one\nmonth. The advantage of \u201cman-month\u201d is that it allows for comparisons of source\ncode volume between technologies. This is relevant because programming languages\nhave different productivity measures, or \u201clevels of verbosity.\u201d Therefore, a system with\nmultiple programming languages can be converted to an aggregate measure that tells\nyou the approximate effort it would take to rebuild it: the \u201crebuild value.\u201d\nSIG\u2019s experience has shown that the man-month is an effective metric to assess the\nsize of a system and to compare systems with each other. A man-year is simply 12\nman-months. Of course, actual productivity is also dependent on skill and program\u2010\nming style. The volume metric does not tell you how many months or years of effort\nactually went into building the system."}
{"133": "CHAPTER 10\nAutomate Tests\nKeep the bar green to keep the code clean.\n\u2014The jUnit motto\nGuideline:\n\u2022 Automate tests for your codebase.\n\u2022 Do this by writing automated tests using a test framework.\n\u2022 This improves maintainability because automated testing\nmakes development predictable and less risky.\nIn Chapter 4, we have presented IsValid, a method to check whether bank account\nnumbers comply with a checksum. That method contains a small algorithm that\nimplements the checksum. It is easy to make mistakes in a method like this. That is\nwhy probably every programmer in the world at some point has written a little, one-"}
{"134": "Console.WriteLine(\"Type a bank account number on the next line.\");\nacct = Console.ReadLine();\nConsole.WriteLine($\"Bank account number '{acct}' is\" +\n(Accounts.IsValid(acct) ? \"\" : \" not\") + \" valid.\");\n} while (!String.IsNullOrEmpty(acct));\n}\n}\n}\nThis is a C# class with a Main method, so it can be run from the command line:\nC:\\> Program.exe\nType a bank account number on the next line.\n123456789\nBank account number '123456789' is valid.\nType a bank account number on the next line.\n123456788\nBank account number '123456788' is not valid.\nC:\\>\nA program like this can be called a manual unit test. It is a unit test because it is used\nto test just one unit, IsValid. It is manual because the user of this program has to\ntype in test cases manually, and manually assess whether the output of the program is\ncorrect.\nWhile better than having no unit testing at all, this approach has several problems:\n\u2022 Test cases have to be provided by hand, so the test cannot be executed automati\u2010\ncally in an easy way.\n\u2022 The developer who has written this test is focusing on logic to execute the test\n(the do \u2026 while loop, all input/output handling), not on the test itself.\n\u2022 The program does not show how IsValid is expected to behave.\n\u2022 The program is not recognizable as a test (although the rather generic name\nProgram is an indication it is meant as a one-off experiment)."}
{"135": "10.1 Motivation\nThis section describes the advantages of automating your tests as much as possible.\nAutomated Testing Makes Testing Repeatable\nJust like other programs and scripts, automated tests are executed in exactly the same\nway every time they are run. This makes testing repeatable: if a certain test executes at\ntwo different points in time yet gives different answers, it cannot be that the test exe\u2010\ncution itself was faulty. One can conclude that something has changed in the system\nthat has caused the different outcome. With manual tests, there is always the possibil\u2010\nity that tests are not performed consistently or that human errors are made.\nAutomated Testing Makes Development Efficient\nAutomated tests can be executed with much less effort than manual tests. The effort\nthey require is negligible and can be repeated as often as you see fit. They are also\nfaster than manual code review. You should also test as early in the development pro\u2010\ncess as possible, to limit the effort it takes to fix problems.\nPostponing testing to a late stage in the development pipeline risks\nlate identification of problems. That costs more effort to fix,\nbecause code needs to go back through the development pipeline\nand be merged, and tests must be rerun.\nAutomated Testing Makes Code Predictable\nTechnical tests can be automated to a high degree. Take unit tests and integration\ntests: they test the technical inner workings of code and the cohesion/integration of\nthat code. Without being sure of the inner workings of your system, you might get"}
{"136": "Thus, running automated tests provides certainty about how the code works. There\u2010\nfore, the predictability of automated tests also makes the quality of developed code\nmore predictable.\nTests Document the Code That Is Tested\nThe script or program code of a test contains assertions about the expected\nbehavior of the system under test. For example, as will be illustrated later in this\nchapter, an appropriate test of IsValid contains the following line of code:\nAssert.IsFalse(IsValid(\"\")). This documents, in C# code, that we expect IsValid\nto return false when checking the empty string. In this way, the Assert.IsFalse state\u2010\nment plays a double role: as the actual test, and as documentation of the expected\nbehavior. In other words, tests are examples of what the system does.\nWriting Tests Make You Write Better Code\nWriting tests helps you to write testable code. As a side effect, this leads to code con\u2010\nsisting of units that are shorter, are simpler, have fewer parameters, and are more\nloosely coupled (as the guidelines in the previous chapters advise). For example, a\nmethod is more difficult to test when it performs multiple functions instead of only\none. To make it easier to test, you move responsibilities to different methods, improv\u2010\ning the maintainability of the whole. That is why some development approaches\nadvocate writing a unit test before writing the code that conforms to the test. Such\napproaches are called test-driven development (TDD) approaches. You will see that\ndesigning a method becomes easier when you think about how you are going to test\nit: what are the valid arguments of the method, and what should the method return as\na result?\n10.2 How to Apply the Guideline\nHow you automate tests differs by the types of tests you want to automate. Test types"}
{"137": "Table 10-1. Types of testing\nType What it tests Why Who\nUnit test Functionality of one unit in isolation Verify that unit behaves as Developer (preferably\nexpected of the unit)\nIntegration test Functionality, performance, or other quality Verify that parts of the system Developer\ncharacteristic of at least two classes work together\nEnd-to-end test System interaction (with a user or another Verify that system behaves as Developer\nsystem) expected\nRegression test Previously erroneous behavior of a unit, class, Ensure that bugs do not re- Developer\nor system interaction appear\nAcceptance test System interaction (with a user or another Confirm the system behaves as End-user\nsystem) required representative (never\nthe developer)\nTable 10-1 shows that a regression test is a unit test, an integration test, or an end-to-\nend test that has been created when a bug was fixed. Acceptance tests are end-to-end\ntests executed by end user representatives.\nDifferent types of testing call for different automation frameworks. For unit testing,\nseveral well-known C# frameworks are available, such as NUnit. For automated\nend-to-end testing, you need a framework that can mimic user input and capture\noutput. A well-known framework that does just that for web development is Sele\u2010\nnium. For integration testing, it all depends on the environment in which you are\nworking and the quality characteristics you are testing. SoapUI is a framework for\nintegration tests that focuses on web services and messaging middleware. Apache\njMeter is a framework for testing the performance of C# applications under heavy\nworkloads.\nChoosing a test framework needs to be done at the team level. Writing integration\ntests is a specialized skill\u2014but unit testing is for each and every individual developer.\nThat is why the rest of this chapter focuses on writing unit tests using the most well-"}
{"138": "Writing unit tests also requires the smallest upfront investment:1 just download\nNUnit from http://nunit.org.\nGetting Started with NUnit Tests\nAs we noted in the introduction of this chapter, we want to test IsValid, a method of\nthe class Accounts. Accounts is called the class under test. In NUnit, tests are put in a\ndifferent class, the test class, or test fixture. This class is indicated as a test fixture by\nthe [TestFixture] attribute. By convention, the name of the test class is the name of\nthe class under test with the suffix Test added. In this case, that would mean the\nname of the test class is AccountsTest. It must be a public class, but apart from that,\nthere are no other requirements for a test class. In particular, it does not need to\nextend any other class. It is convenient, but not required, to place the test class in the\nsame namespace as the class under test. That way, the test class has access to all mem\u2010\nbers of the test class under test that have namespace (but not public) access.\nIn NUnit, a test itself is any method that has the [Test] attribute. To test IsValid,\nyou can use the following NUnit test class:\nusing NUnit.Framework;\nnamespace eu.sig.training.ch04.v1\n{\n[TestFixture]\npublic class AccountsTest\n{\n[Test]\npublic void TestIsValidNormalCases()\n{\nAssert.IsTrue(Accounts.IsValid(\"123456789\"));\nAssert.IsFalse(Accounts.IsValid(\"123456788\"));\n}\n}"}
{"139": "Unit tests can be run directly in Visual Studio. In addition, NUnit comes with test\nrunners to run tests from the command line. Tests can also be executed by Maven or\nAnt. Figure 10-1 shows the result of running the preceding test in Visual Studio. The\nred bar indicates that there are failed tests.\nFigure 10-1. All tests succeeded!\nThe test in the preceding test class only tests normal cases: two bank account num\u2010\nbers of the expected format (exactly nine characters, all digits). How about corner\ncases? One obvious special case is the empty string. The empty string is, of course,\nnot a valid bank account number, so we test it by calling Assert.IsFalse:\n[Test]\npublic void TestEmptyString()\n{\nAssert.IsFalse(Accounts.IsValid(\"\"));\n}\nAs Figure 10-2 shows, it turns out that this test fails! While the call to IsValid should\nreturn false, it actually returned something else (which, of course, must be true, as\nthere is no other option)."}
{"140": "This indeed returns true, while it should return false. This reminds us to add code\nto IsValid that checks the length of the bank account number.2\nThe NUnit runner reports this as a test failure and not as a test error. A test failure\nmeans that the test itself (the method TestEmptyString) is executed perfectly, but the\nassertion failed. A test error means that the test method itself did not execute cor\u2010\nrectly. The following code snippet illustrates this: the ShowError method raises a\ndivision-by-zero exception and never even executes Assert.IsTrue:\n[Test]\npublic void ShowError()\n{\nint tmp = 0, dummy = 1 / tmp;\n// Next line is never executed because the previous one raises an\n// exception.\n// If it were executed, you'll never see the assert message because\n// the test always succeeds.\nAssert.IsTrue(true);\n}\nNext, we present some basic principles that will help you write good unit tests. We\nstart with the most basic principles and then progress to more advanced ones that\napply when your test efforts become more mature.\nGeneral Principles for Writing Good Unit Tests\nWhen writing tests, it is important to keep in mind the following general principles:\nTest both normal and special cases\nAs in the examples given in this chapter, test two kinds of cases. Write tests that\nconfirm that a unit indeed behaves as expected on normal input (called happy\nflow or sunny-side testing). Also write tests that confirm that a unit behaves sensi\u2010\nbly on non-normal input and circumstances (called unhappy flow or rainy-side\ntesting). For instance, in NUnit it is possible to write tests to confirm that a"}
{"141": "Write tests that are isolated: their outcomes should reflect only the behavior of the sub\u2010\nject being tested\nThat is, each test should act independently of all other tests. For unit testing, this\nmeans that each test case should test only one functionality. No unit test should\ndepend on state, such as files written by other tests. That is why a unit test that,\nsay, causes the class under test to access the filesystem or a database server is not\na good unit test.\nConsequently, in unit testing you should simulate the state/input of other classes\nwhen those are needed (e.g., as arguments). Otherwise, the test is not isolated and\nwould test more than one unit. This was easy for the test of IsValid, because IsValid\ntakes a string as an argument, and it does not call other methods of our system. For\nother situations, you may need a technique like stubbing or mocking.\nIn Chapter 6, we introduced a C# interface for a simple digital camera, which is\nrepeated here for ease of reference:\npublic interface ISimpleDigitalCamera\n{\nImage TakeSnapshot();\nvoid FlashLightOn();\nvoid FlashLightOff();\n}\nSuppose this interface is used in an application that ensures people never forget to\nturn on the flash at night:\npublic const int DAYLIGHT_START = 6;\npublic Image TakePerfectPicture(int currentHour)\n{\nImage image;\nif (currentHour < PerfectPicture.DAYLIGHT_START)\n{"}
{"142": "automatic and independent. That means that the normal implementation of the digi\u2010\ntal camera interface cannot be used. On a typical device, the normal implementation\nrequires a (human) user to point the camera at something interesting and press a\nbutton. The picture taken can be any picture, so it is hard to test whether the (suppos\u2010\nedly perfect) picture taken is the one expected.\nThe solution is to use an implementation of the camera interface that has been made\nespecially for testing. This implementation is a fake object, called a test stub or simply\na stub.3 In this case, we want this fake object to behave in a preprogrammed (and\ntherefore predictable) way. We write a test stub like this:\nclass DigitalCameraStub : ISimpleDigitalCamera\n{\npublic Image TestImage;\npublic Image TakeSnapshot()\n{\nreturn this.TestImage;\n}\npublic void FlashLightOn()\n{\n}\npublic void FlashLightOff()\n{\n}\n}\nIn this stub, TakeSnapshot always returns the same image, which we can set simply\nby assigning to testImage (for reasons of simplicity, we have made testImage a pub\u2010\nlic field and do not provide a setter). This stub can now be used in a test:\n[Test]\npublic void TestDayPicture()\n{"}
{"143": "value of the call, 12, means that TakePerfectPicture assumes it is between noon\nand 1 p.m.\nNow suppose we want to test TakePerfectPicture for nighttime behavior; that is, we\nwant to ensure that if TakePerfectPicture is called with a value lower than Perfect\nPicture.DAYLIGHT_START, it indeed switches on the flash. So, we want to test whether\nTakePerfectPicture indeed calls FlashLightOn. However, FlashLightOn does not\nreturn any value, and the ISimpleDigitalCamera interface also does not provide any\nother way to know whether the flash has been switched on. So what to check?\nThe solution is to provide the fake digital camera implementation with some mecha\u2010\nnism to record whether the method we are interested in gets called. A fake object that\nrecords whether expected calls have taken place is called a mock object. So, a mock\nobject is a stub object with added test-specific behavior. The digital camera mock\nobject looks like this:\nclass DigitalCameraMock : ISimpleDigitalCamera\n{\npublic Image TestImage;\npublic int FlashOnCounter = 0;\npublic Image TakeSnapshot()\n{\nreturn this.TestImage;\n}\npublic void FlashLightOn()\n{\nthis.FlashOnCounter++;\n}\npublic void FlashLightOff()\n{\n}\n}"}
{"144": "Assert.AreEqual(1, cameraMock.FlashOnCounter);\n}\nIn these examples, we have written our own stub and mock objects. This leads to a lot\nof code. Generally, it is most efficient to use a mocking framework such as Moq.\nMocking frameworks use features of the .Net runtime to automatically create mock\nobjects from normal interfaces or classes. They also provide methods to test whether\nmethods of a mock object have been called, and with which arguments. Some mock\u2010\ning frameworks also provide ways to specify preprogrammed behavior of mock\nobjects, giving them the characteristics of both stubs and mocks.\nIndeed, using Moq as an example, you can write TestNightPicture without any need\nto write a class like DigitalCameraMock yourself:\n[Test]\npublic void TestNightPictureMoq()\n{\nImage image =\nImage.FromFile(\"../../../../test/resources/VanGoghStarryNight.jpg\");\nvar cameraMock = new Mock<ISimpleDigitalCamera>();\ncameraMock.Setup(foo => foo.TakeSnapshot()).Returns(image);\nPerfectPicture.camera = cameraMock.Object;\nAssert.AreSame(image, new PerfectPicture().TakePerfectPicture(0));\ncameraMock.Verify(foo => foo.FlashLightOn(), Times.AtMostOnce());\n}\nIn this test, Moq\u2019s Mock constructor is used to create cameraMock, the mock object\nused in this test. With Moq\u2019s Setup and Returns method, the desired behavior is\nspecified. Moq\u2019s Verify method is used to verify whether FlashLightOn has been\ncalled.\nMeasure Coverage to Determine Whether There Are Enough Tests\nHow many unit tests are needed? One way to assess whether you have written enough\nunit tests is to measure coverage of your unit tests. Coverage, or more precisely, line"}
{"145": "never test getters. Take a typical class that represents postal mail addresses. It typically\nhas two or three string fields that represent (additional) address lines. It is easy to\nmake a mistake like this one:\npublic string getAddressLine3() {\nreturn this.addressLine2;\n}\nA minimum of 80% coverage alone is not enough to ensure high-quality unit tests.\nIt is possible to get high coverage by testing just a few high-level methods (like Main,\nthe first method called by the .NET runtime) and not mock out lower-level methods.\nThat is why we advise a 1-to-1 ratio of production code versus test code.\nYou can measure coverage using a code coverage tool. Some editions of Visual Studio\nprovide a built-in code coverage tool. Figure 10-3 shows coverage of the examples of\nthis book, using Visual Studio 2015 Enterprise Edition.\nFigure 10-3. Coverage report of the examples of this book in Visual Studio 2015 Enter\u2010\nprise Edition.\n10.3 Common Objections to Automating Tests\nThis section discusses typical objections and limitations regarding automation. They\ndeal with the reasons and considerations to invest in test automation."}
{"146": "Manual acceptance testing can largely be automated with automa\u2010\nted regression tests. With those, the scope of remaining manual\ntests decreases. You may still need manual review or acceptance\ntests to verify that business logic is correct. This typically concerns\nthe process flow of a functionality.\nObjection: I Am Not Allowed to Write Unit Tests\n\u201cI am not allowed to write unit tests because they lower productivity according to my\nmanager.\u201d\nWriting unit tests during development actually improves productivity. It improves\nsystem code by shifting the focus from \u201cwhat code should do\u201d toward \u201cwhat it should\nnot do.\u201d If you never take into account how the code may fail, you cannot be sure\nwhether your code is resilient to unexpected situations.\nThe disadvantages of not having unit tests are mainly in uncertainty and rework.\nEvery time a piece of code is changed, it requires painstaking review to verify whether\nthe code does what it is supposed to do.\nObjection: Why Should We Invest in Unit Tests When the Current\nCoverage Is Low?\n\u201cThe current unit test coverage of my system is very low. Why should I invest time now\nin writing unit tests?\u201d\nWe have elaborated on the reasons why unit tests are useful and help you develop\ncode that works predictably. However, when a very large system has little to no unit\ntest code, this may be a burden. After all, it would be a significant investment to start\nwriting unit tests from scratch for an existing system because you would need to ana\u2010\nlyze all units again. Therefore, you should make a significant investment in unit tests\nonly if the added certainty is worth the effort. This especially applies to critical, cen\u2010\ntral functionality and when there is reason to believe that units are behaving in an"}
{"147": "10.4 See Also\nStandardization and consistency in applying it are important in achieving a well-\nautomated development environment. For elaboration, see Chapter 11.\nHow SIG Rates Testability\nTestability is one of the five subcharacteristics of maintainability according to ISO\n25010. SIG rates testability by aggregating the ratings of system properties unit com\u2010\nplexity (see Chapter 3), component independence (see Chapter 7), and system volume\n(see Chapter 9), using an aggregation mechanism as explained in Appendix A.\nThe rationale for this is that complex units are especially hard to test, poor compo\u2010\nnent independence increases the need for mocking and stubbing, and higher volumes\nof production code require higher volumes of test code."}
{"148": ""}
{"149": "CHAPTER 11\nWrite Clean Code\nWriting clean code is what you must do in order to call yourself a professional.\n\u2014Robert C. Martin\nGuideline:\n\u2022 Write clean code.\n\u2022 Do this by not leaving code smells behind after development\nwork.\n\u2022 This improves maintainability because clean code is main\u2010\ntainable code.\nCode smells are coding patterns that hint that a problem is present. Introducing or\nnot removing such patterns is bad practice, as they decrease the maintainability of\ncode. In this chapter we discuss guidelines for keeping the codebase clean from code"}
{"150": "11.2 How to Apply the Guideline\nTrying to be a clean coder is an ambitious goal, and there are many best practices that\nyou can follow. From our consultancy experience we have distilled seven developer\n\u201cBoy Scout rules\u201d that will help you to prevent code smells that impact maintainabil\u2010\nity most:\n1. Leave no unit-level code smells behind.\n2. Leave no bad comments behind.\n3. Leave no code in comments behind.\n4. Leave no dead code behind.\n5. Leave no long identifier names behind.\n6. Leave no magic constants behind.\n7. Leave no badly handled exceptions behind.\nThese seven rules are explained in the following sections.\nRule 1: Leave No-Unit Level Code Smells Behind\nAt this point in the book you are familiar with nine guidelines for building maintain\u2010\nable software, discussed in the previous nine chapters. Of those nine guidelines, three\ndeal with smells at the unit level: long units (Chapter 2), complex units (Chapter 3),\nand units with large interfaces (Chapter 5). For modern programming languages,\nthere is really no good reason why any of these guidelines should be violated when\nyou are writing new code.\nTo follow this rule is to refactor \u201csmelly\u201d code in time. By \u201cin time,\u201d we mean as soon\nas possible but certainly before the code is committed to the version control system.\nOf course, it is OK to have small violations when you are working on a development\nticket\u2014for example, a method of 20 lines of code or a method with 5 parameters. But"}
{"151": "Rule 2: Leave No Bad Comments Behind\nComments are sometimes considered the anti-pattern of good code. From our expe\u2010\nrience we can confirm that inline comments typically indicate a lack of elegant engi\u2010\nneering solutions. Consider the following method taken from the Jenkins codebase\n(which is in Java):\npublic HttpResponse doUploadPlugin(StaplerRequest req)\nthrows IOException, ServletException {\ntry {\nJenkins.getInstance().checkPermission(UPLOAD_PLUGINS);\nServletFileUpload upload = new ServletFileUpload(\nnew DiskFileItemFactory());\n// Parse the request\nFileItem fileItem = (FileItem)upload.parseRequest(req).get(0);\nString fileName = Util.getFileName(fileItem.getName());\nif (\"\".equals(fileName)) {\nreturn new HttpRedirect(\"advanced\");\n}\n// we allow the upload of the new jpi's and the legacy hpi's\nif (!fileName.endsWith(\".jpi\") && !fileName.endsWith(\".hpi\")) {\nthrow new Failure(\"Not a plugin: \" + fileName);\n}\n// first copy into a temporary file name\nFile t = File.createTempFile(\"uploaded\", \".jpi\");\nt.deleteOnExit();\nfileItem.write(t);\nfileItem.delete();\nfinal String baseName = identifyPluginShortName(t);\npluginUploaded = true;\n// Now create a dummy plugin that we can dynamically load"}
{"152": "Although the doUploadPlugin is not very hard to maintain (it has only 1 parameter,\n32 lines of code, and a McCabe index of 6), the inline comments indicate separate\nconcerns that could easily be addressed outside this method. For example, copying\nthe fileItem to a temporary file and creating the plugin configuration are tasks that\ndeserve their own methods (where they can be tested and potentially reused).\nComments in code may reveal many different problems:\n\u2022 Lack of understanding of the code itself\n// I don't know what is happening here, but if I remove this line\n// an infinite loop occurs\n\u2022 Issue tracking systems not properly used\n// JIRA-1234: Fixes a bug when summing negative numbers\n\u2022 Conventions or tooling are being bypassed\n// CHECKSTYLE:OFF\n// NOPMD\n\u2022 Good intentions\n// TODO: Make this method a lot faster some day\nComments are valuable in only a small number of cases. Helpful API documentation\ncan be such a case, but always be cautious to avoid dogmatic boilerplate commentary.\nIn general, the best advice we can give is to keep your code free of comments.\nRule 3: Leave No Code in Comments Behind\nAlthough there might be rare occasions where there is a good reason to use com\u2010\nments in your code, there is never an excuse for checking in code that is commented\nout. The version control system will always keep a record of old code, so it is perfectly\nsafe to delete it. Take a look at the following example, taken from the Apache Tomcat\ncodebase (which is in Java, but we present a C# translation here):"}
{"153": "// FIXME: Older spec revisions may still check this\n/*\nif ((servletNames.length != 0) && (urlPatterns.length != 0))\nthrow new IllegalArgumentException\n(sm.getString(\"standardContext.filterMap.either\"));\n*/\nfor (int i = 0; i < urlPatterns.Length; i++) {\nif (!ValidateURLPattern(urlPatterns[i])) {\nthrow new Exception(\nsm.GetString(\"standardContext.filterMap.pattern\",\nurlPatterns[i]));\n}\n}\n}\nThe FIXME note and accompanying code are understandable from the original develo\u2010\nper\u2019s perspective, but to a new developer they act as a distractor. The original devel\u2010\noper had to make a decision before leaving this commented-out code: either fix\nit at the spot, create a new ticket to fix it at some other time, or reject this corner case\naltogether.\nRule 4: Leave No Dead Code Behind\nDead code comes in different shapes. Dead code is code that is not executed at all or\nits output is \u201cdead\u201d: the code may be executed, but its output is not used elsewhere in\nthe system. Code in comments, as discussed in the previous section, is an example of\ndead code, but there are many other forms of dead code. In this section, we give three\nmore examples of dead code.\nUnreachable code in methods\npublic Transaction GetTransaction(long uid)\n{\nTransaction result = new Transaction(uid);\nif (result != null)"}
{"154": "Unused private methods\nPrivate methods can be called only from other code in the same class. If they are not,\nthey are dead code. Nonprivate methods that are not called by methods in the same\nclass may also be dead, but you cannot determine this by looking at the code of the\nclass alone.\nCode in comments\nThis is not to be confused with commented-out code. Sometimes it can be useful to\nuse short code snippets in API documentation (such as in C# XMLDOC tags), but\nremember that keeping those snippets in sync with the actual code is a task that is\nquickly overlooked. Avoid code in comments if possible.\nRule 5: Leave No Long Identifiers Behind\nGood identifiers make all the difference between code that is a pleasure to read and\ncode that is hard to wrap your head around. A famous saying by Phil Karlton is\n\u201cThere are only two hard problems in computer science: cache invalidation and nam\u2010\ning things.\u201d In this book we won\u2019t discuss the first, but we do want to say a few things\nabout long identifiers.\nIdentifiers name the items in your codebase, from units to modules to components to\neven the system itself. It is important to choose good names so that developers can\nfind their way through the codebase without great effort. The names of most of the\nidentifiers in a codebase will be dependent on the domain in which the system oper\u2010\nates. It is typical for teams to have a formal naming convention or an informal, but\nconsistent, use of domain-specific terminology.\nIt is not easy to choose the right identifiers in your code, and unfortunately there are\nno guidelines for what is and what isn\u2019t a good identifier. Sometimes it may even take\nyou a couple of iterations to find the right name for a method or class.\nAs a general rule, long identifiers must be avoided. A maximum length for an identi\u2010"}
{"155": "Rule 6: Leave No Magic Constants Behind\nMagic constants are number or literal values that are used in code without a clear def\u2010\ninition of their meaning (hence the name magic constant). Consider the following\ncode example:\nfloat CalculateFare(Customer c, long distance)\n{\nfloat travelledDistanceFare = distance * 0.10f;\nif (c.Age < 12)\n{\ntravelledDistanceFare *= 0.25f;\n}\nelse\nif (c.Age >= 65)\n{\ntravelledDistanceFare *= 0.5f;\n}\nreturn 3.00f + travelledDistanceFare;\n}\nAll the numbers in this code example could be considered magic numbers. For\ninstance, the age thresholds for children and the elderly may seem like familiar num\u2010\nbers, but remember they could be used at many other places in the codebase. The fare\nrates are constants that are likely to change over time by business demands.\nThe next snippet shows what the code looks like if we define all magic constants\nexplicitly. The code volume increased with six extra lines of code, which is a lot com\u2010\npared to the original source, but remember that these constants can be reused in\nmany other places in the code:\nprivate static readonly float BASE_RATE = 3.00f;\nprivate static readonly float FARE_PER_KM = 0.10f;\nprivate static readonly float DISCOUNT_RATE_CHILDREN = 0.25f;\nprivate static readonly float DISCOUNT_RATE_ELDERLY = 0.5f;"}
{"156": "return BASE_RATE + travelledDistanceFare;\n}\nRule 7: Leave No Badly Handled Exception Behind\nThree guidelines for good exception handling are discussed here specifically because\nin our practice we see many flaws in implementing exception handling:\n\u2022 Always catch exceptions. You are logging failures of the system to help you\nunderstand these failures and then improve the system\u2019s reaction to them. That\nmeans that exceptions must always be caught. Also, in some cases an empty\ncatch block compiles, but it is bad practice since it does not provide information\nabout the context of the exception.\n\u2022 Catch specific exceptions. To make exceptions traceable to a specific event,\nyou should catch specific exceptions. General exceptions that do not provide\ninformation specific to the state or event that triggered it fail to provide that\ntraceability. Therefore, you should not catch Exception or SystemException\ndirectly.\n\u2022 Translate specific exceptions to general messages before showing them to end\nusers. End users should not be \u201cbothered\u201d with detailed exceptions, since they\nare mostly confusing and a security bad practice (i.e., providing more informa\u2010\ntion than necessary about the inner workings of the system).\n11.3 Common Objections to Writing Clean Code\nThis section discusses typical objections regarding clean code. The most common\nobjections are reasons why commenting would be a good way to document code and\nwhether corners can be cut for doing exception handling.\nObjection: Comments Are Our Documentation"}
{"157": "Objection: Exception Handling Causes Code Additions\n\u201cImplementing exception classes forces me to add a lot of extra code without visible\nbenefits.\u201d\nException handling is an important part of defensive programming: coding to prevent\nunstable situations and unpredictable system behavior. Anticipating unstable situa\u2010\ntions means trying to foresee what can go wrong. This does indeed add to the burden\nof analysis and coding. However, this is an investment. The benefits of exception han\u2010\ndling may not be visible now, but they definitely will prove valuable in preventing and\nabsorbing unstable situations in the future.\nBy defining exceptions, you are documenting and safeguarding your assumptions.\nThey can later be adjusted when circumstances change.\nObjection: Why Only These Coding Guidelines?\n\u201cWe use a much longer list of coding conventions and quality checks in our team. This\nlist of seven seems like an arbitrary selection with many important omissions.\u201d\nHaving more guidelines and checks than the seven in this chapter is of course not a\nproblem. These seven rules are the ones we consider the most important for writing\nmaintainable code and the ones that should be adhered to by every member on the\ndevelopment team. A risk of having many guidelines and checks is that developers\ncan be overwhelmed by them and focus their efforts on the less critical issues. How\u2010\never, teams are obviously allowed to extend this list with items that they find indis\u2010\npensable for building a maintainable system."}
{"158": ""}
{"159": "CHAPTER 12\nNext Steps\nAt this point, you know a lot more about what maintainable code is, why it is impor\u2010\ntant, and how to apply the 10 guidelines in this book. But writing maintainable code\nis not something you learn from a book. You learn it by doing it! Therefore, here we\nwill discuss simple advice on practicing the 10 guidelines for achieving maintainable\nsoftware.\n12.1 Turning the Guidelines into Practice\nEnsuring that your code is easy to maintain depends on two behaviors in your daily\nroutine: discipline and setting priorities. Discipline helps you to constantly keep\nimproving your coding techniques, up to a point where any new code you write will\nalready be maintainable. As for priorities, some of the presented guidelines can seem\nto contradict each other. It takes consideration on your side about which guideline\nhas the most impact on the actual maintainability of your system. Be sure to take\nsome time to deliberate and ask your team for their opinion."}
{"160": "units into multiple smaller units slightly grows the total codebase. But the advantage\nof small units in terms of reusability will have a huge pay-off when more functionality\nis added to the system.\nThe same applies to the architecture-level guidelines (see Chapters 7 and 8): it makes\nno sense to reorganize your code structure when it makes your components highly\ndependent on each other. To put it succinctly: fix your dependencies before trying to\nbalance your components.\n12.3 Remember That Every Commit Counts\nThe hardest part of applying the guidelines in this book may be keeping the discipline\nto apply them. It is tempting to violate the guidelines when a \u201cquick fix\u201d seems more\nefficient. To keep this discipline, follow the Boy Scout rule presented in Chapter 11.\nThe Boy Scout rule is especially effective on large codebases. Unless you have the time\nto sort out your whole system and improve maintainability, you will have to do it\nstep-by-step while doing your regular work. This gradually improves maintainability\nand hones your refactoring skills. So, in the long run, you also have the skill to write\nhighly maintainable software.\n12.4 Development Process Best Practices Are Discussed in\nthe Follow-Up Book\nAs discussed in the preface, the process part of developing high-quality software is\ndiscussed in detail in the follow-up book in this series: Building Software Teams. It\nprovides 10 guidelines for managing and measuring the software development pro\u2010\ncess. It focuses on how to measure and manage best practices for software develop\u2010\nment (e.g., development tool support, automation, standardization)."}
{"161": "APPENDIX A\nHow SIG Measures Maintainability\nSIG measures system maintainability based on eight metrics. Those eight metrics are\ndiscussed in Chapters 2 through 9. Those chapters include sidebars explaining how\nSIG rates source code properties relevant to those guidelines. These ratings are\nderived from the SIG/T\u00dcViT1 Evaluation Criteria for Trusted Product Maintainabil\u2010\nity. In this appendix, we provide you with additional background.\nTogether with T\u00dcViT, SIG has determined eight properties of source code that can be\nmeasured automatically. See \u201cWhy These Ten Specific Guidelines?\u201d on page xi for\nhow these properties have been chosen.\nTo assess maintainability of a system, we measure these eight source code properties\nand summarize these measurements either in a single number (for instance, the per\u2010\ncentage of code duplication) or a couple of numbers (for instance, the percentage of\ncode in four categories of complexity, which we call a quality profile; see \u201cRating\nMaintainability\u201d).\nWe then compare these numbers against a benchmark containing several hundreds of"}
{"162": "Table A-1. SIG maintainability ratings\nRating Maintainability\n5 stars Top 5% of the systems in the benchmark\n4 stars Next 30% of the systems in the benchmark (above-average systems)\n3 stars Next 30% of the systems in the benchmark (average systems)\n2 stars Next 30% of the systems in the benchmark (below-average systems)\n1 star Bottom 5% least maintainable systems\nWe then aggregate the ratings to arrive at one overall rating. We do this in two steps.\nFirst, we determine the ratings for the subcharacteristics of maintainability as defined\nby ISO 25010 (i.e., analyzability, modifiability, etc.) by taking the weighted averages\naccording to the rows of Table A-2. Each cross in a given row indicates that the corre\u2010\nsponding system property (column) contributes to this subcharacteristic. Second, we\ntake a weighted average of the five subcharacteristics to determine an overall rating\nfor maintainability.\nTable A-2. Relation of subcharacteristics and system properties\nVolume Duplication Unit Unit Unit Module Component Component\nsize complexity interfacing coupling balance independence\nAnalyzability X X X X\nModifiability X X X\nTestability X X X\nModularity X X X\nReusability X X\nThis describes the SIG maintainability model in a nutshell, since there is more detail\nto it than what we can cover in this appendix. If you would like to learn more about\nthe details of the maintainability model, a good start for elaboration is the following\npublication:"}
{"163": "Background on the development of the model and its application is provided in the\nfollowing publications:\n\u2022 Heitlager, Ilja, Tobias Kuipers, and Joost Visser. \u201cA Practical Model for Measuring\nMaintainability.\u201d In Proceedings of the 6th International Conference on the Quality\nof Information and Communications Technology (QUATIC 2007), 30\u201339. IEEE\nComputer Society Press, 2007.\n\u2022 Baggen, Robert, Jos\u00e9 Pedro Correia, Katrin Schill, and Joost Visser. \u201cStandardized\ncode quality benchmarking for improving software maintainability.\u201d Software\nQuality Journal 20, no. 2 (2012): 287\u2013307.\n\u2022 Bijlsma, Dennis, Miguel Alexandre Ferreira, Bart Luijten, and Joost Visser.\n\u201cFaster issue resolution with higher technical quality of software.\u201d Software Qual\u2010\nity Journal 20, no. 2 (2012): 265\u2013285.\nDoes Maintainability Improve Over Time?\nA question we often get at SIG is whether maintainability improves over time across\nall systems we see. The answer is yes, but very slowly. The recalibration that we carry\nout every year consistently shows that the thresholds become stricter over time. This\nmeans that for one system to get a high maintainability rating, over time it must have\nfewer units that are overly long or complex, must have less duplication, lower cou\u2010\npling, and so on. Given the structure of our model, the reason for this must be that\nsystems in our benchmark over time have less duplication, less tight coupling, and so\non. One could argue that this means that maintainability across the systems we\nacquire for our benchmark is improving. We are not talking about big changes. In\nbroad terms, we can say this: it is about a tenth of a star per year.\nThe selection of systems within the SIG benchmark is a representative cross-cut of the\nsoftware industry, including both proprietary and open source systems, developed in\na variety of languages, functional domains, platforms, and so on. Therefore, the tenth"}
{"164": ""}
{"165": "Index\nSymbols C\n11-check, 12 C#\nconcept names in, xvi\nA namespaces vs. components, xvii\nC# interfaces, 78\nAbstract Factory design pattern, 86-88\ncalls, 83\nacceptance tests\nchains, conditional, 36-38\nautomation of, 124\nchange\ncharacteristics, 114\nand duplication, 54\nadaptive maintenance, 2\npossible reasons for, 54\nanalysis\nchecksum, 12\nand component balance, 95\nclasses, splitting, 73\nand duplication, 47\nclean coding\narchitecture components, balance of (see com\u2010\nand badly-handled exceptions, 134\nponent balance)\nand Boy Scout rule, 124\nautomation of tests (see test automation)\nand commented-out code, 130\nand comments as documentation, 134\nB\nand dead code, 131\nbackups, code duplication and, 54\nand exception class implementation, 135\nbalance of components (see component bal\u2010\nand number of coding guidelines, 135\nance)"}
{"166": "finding/analyzing, 95 of system as impediment to reducing code\u2010\nhidden vs. interface, 90 base volume, 108\nimprovement through test automation, 114 component balance, 93-99\nminimizing unit size, 11-26 advantages of, 95\nreading when spread out over multiple and component independence, 84\nunits, 23 and deceptive acceptability of imbalance, 98\nreplacing custom code with libraries/frame\u2010 and entanglements, 98\nworks, 77 applying guidelines for, 96\ntest automation and predictability of, 113 clarifying domains for, 97\nunreachable, 131 common objections to, 98\nwriting clean (see clean coding) deciding on proper conceptual level for\ncode clones grouping functionality, 97\nand SIG duplication ratings, 56 importance of, 93-99\ndefined, 46 isolation of maintenance effects, 95\ncodebase separation of maintenance responsibilities,\nBoy Scout rule with large, 138 96\ndefined, 101 SIG rating thresholds, 99\nnavigation of, 73 when finding/analyzing code, 95\nvolume (see codebase volume) component coupling, module coupling vs., 82\nworking on isolated parts of, 72 component dependence, 82\ncodebase volume component guidelines, unit guidelines vs., 137\nadvantages of minimizing, 102-104 component imbalance, 93, 98\nand defect density, 104 component independence\napplying guidelines for, 105-107 advantages of, 82-86\ncommon objections to minimizing of, and Abstract Factory design pattern, 86-88\n107-110 and entangled components, 89\nduplication as impediment to reducing, 109 and isolated maintenance, 85\nfunctionality-related measures to minimize, and separation of maintenance responsibili\u2010\n105 ties, 85\nimportance of minimizing, 101-110 and testing, 86\nmaintenance and, 104 applying guidelines for, 86-88\nplatform architecture and, 109 common objections to, 88\nproductivity measures and, 108 defined, 82\nprogramming languages and, 108 importance of, 81-91\nproject failures and, 102 SIG rating thresholds, 90"}
{"167": "reuse vs., 14 and unit tests, 55\nsystem complexity as cause of, 108 as impediment to reducing codebase vol\u2010\ncopying code (see duplication [code]) ume, 109\ncore classes, 59 avoiding, 43-56\ncorrective maintenance, 2 common objections to avoiding, 53-55\ncoupling, 71 guidelines for avoiding, 48-53\n(see also module coupling) SIG ratings, 56\nbetween classes, 72 types of, 46\ndefined, 71\nloose (see component independence) (see E\nloose coupling)\nembedded software, 6\ntight (see tight coupling)\nencapsulation, 73, 77, 78, 84, 87, 90\ncoverage\nend-to-end tests, 114\nmeasuring for test adequacy, 122\nentangled components\ntest automation in low-coverage situation,\nand component balance, 98\n124\nand component independence, 89\nCPD (clone detection tool), 47\nevolution over time, 84\ncyclomatic complexity, 31\nerrors, test, 118\n(see also McCabe complexity)\nexception handling\nand clean coding, 134\nD\nand exception class implementation, 135\ndata transfer objects, 62 execution paths, 30\ndead code, 131 Extract Method refactoring technique, 17, 48\ndeadlines, component independence and, 89 Extract Superclass refactoring technique, 50-53,\ndefect density, codebase volume and, 104 50\ndefensive programming, exception handling\nand, 135 F\ndependency injection, 86\nfailures, 118\ndiscipline, coding and, 137\n(see also exception handling)\ndocumentation\ncodebase volume and, 102\nand test automation, 114\ntest errors vs. test failures, 118\ncomments as, 134\nfluent interface, 64\ndomains\nfor loops, 13\nclarifying, 97\nformatting, unit size guidelines and, 23\ncomplex, 40\nframeworks"}
{"168": "importance of simple, 4 lower-level guidelines, 137\nlower-level vs. higher-level, 137\nmaintaining discipline to apply, 138 M\noverview, 9\nmagic constants, 133\npracticing, 137\nmaintainability\nprinciples of, 4\nand discipline during development, 5\nas enabler for other quality characteristics, 4\nH\nas industry-independent, 6\nhappy flow testing, 118 as language-independent, 6\nhidden code, 90 as nonbinary quantity, 7\nhigher-level guidelines, 137 business impact of, 3\ndefined, 2\nI guidelines (see guidelines, maintainability)\nimportance of, 3-4\nidentifiers, long, 132\nimportance of simple guidelines, 4\nif-then-else statements, 36-38\nimprovement over time, 141\nimplementations, specialized, 75-77\nmetrics for, xi\nincoming calls, 84\nmisunderstandings about, 6\nindustry-dependent software development, 6\nperformance vs., 22\nintegration tests, 114\nrating, 7-9\ninterface code, 90\nmaintenance, four types of, 2\ninterfaces, unit (see unit interfaces)\nman-months/years, 110\ninternal calls, 83\nmanual testing\nIntroduce Parameter Object refactoring pat\u2010\nand test automation, 123\ntern, 59\nlimitations of, 112\ninversion of control (IoC), 79\nunit test, 112\nisolated maintenance, 85\nMcCabe complexity\nas SIG/T\u00dcViT rating criteria, 42\nJ\ndefined, 31\nJMeter, 115\nmethod invocations, 22\nJPacman, 16\nmethod modification, 60\nmethod splitting, 41\nL\nmethods\nlarge class smell, 50, 71, 73 unreachable code in, 131\nlarge systems, splitting up of, 107 unused private, 132"}
{"169": "and utility code, 78 as impediment to reducing codebase vol\u2010\napplying guidelines for, 73-77 ume, 108\ncommon objections to separating concerns, as maintainability factor, 6\n78 project failures, codebase volume and, 102\ncomponent coupling vs., 82\nhiding specialized implementations behind Q\ninterfaces, 75-77\nquality profiles, 5, 8\nloose (see loose coupling)\nquality, maintainability as enabler for, 4\nreplacing custom code with libraries/frame\u2010\nquick fixes, 138\nworks, 77\nSIG rating thresholds, 80\nR\nsplitting classes to separate concerns, 73\nrainy-side testing, 118\ntight (see tight coupling)\nrefactoring\nto prevent no-go areas for new developers,\nand unit complexity, 41\n73\nand unit interfaces, 65\nwhen working on isolated parts of codebase,\nand unit size, 17-22\n72\ndifficulties as maintainability issue, 25\nMoq, 122\nExtract Method technique, 17, 48\nmutual dependencies, 89\nExtract Superclass technique, 50-53, 50\nfor improved maintainability, 127\nN\nIntroduce Parameter Object pattern, 59\nnamespaces, components vs., xvii\nReplace Method with Method Object tech\u2010\nnested conditionals, 38-40\nnique, 19-22, 63\nNUnit tests, 112, 116-118\nto reduce codebase, 106\nreferral, reuse by, 106\nO\nregression, 95, 109, 112-115, 124\nobservers, 15 automated testing to identify, 113\noutgoing calls, 83 bugs, 47\nregression tests\nP characteristics, 114\nfor automation of manual acceptance test\u2010\nparameter lists, 65\ning, 124\n(see also unit interfaces)\nrepeatability, test automation and, 113\nparameter objects, 62, 64\nReplace Conditional with Polymorphism pat\u2010\nperfective maintenance, 2\ntern, 37"}
{"170": "scope creep, 105 and SIG testability rating thresholds, 125\nSelenium, 115 applying guidelines for, 114\nself-taught developers, xi common objections to, 123\nseparation of concerns (see concerns, separa\u2010 general principles for writing good unit\ntion of) tests, 118-122\nSIG (Software Improvement Group), xiii in low-coverage situation, 124\nSIG/T\u00dcViT Evaluation Criteria Trusted Prod\u2010 measuring coverage of, 122\nuct Maintainability test stub, 120\ncodebase volume rating thresholds, 110 test-driven development (TDD), 114\ncomponent balance rating thresholds, 99 testability\ncomponent independence rating thresholds, and unit size, 14\n90 SIG rating thresholds, 125\nduplication rating thresholds, 56 tests/testing\nmetrics, xi, 139-140 and unit complexity, 35\nmodule coupling rating thresholds, 80 automation of (see test automation)\nstar ratings, 7-9 component independence and ease of, 86\ntestability rating thresholds, 125 dangers of postponing, 113\nunit complexity rating thresholds, 42 errors vs. failures, 118\nunit interface rating thresholds, 66 types of, 114\nunit size rating thresholds, 26 third-party libraries/frameworks (see frame\u2010\nsimple units (see unit complexity) works) (see libraries)\nsingle responsibility principle throughput code, 84\nclasses that violate, 73 throughput, component independence and, 90\ndefined, 73 tight coupling\nsmells, Boy Scout rules for preventing, 128-134 as impediment to reducing codebase vol\u2010\n(see also large class smell) ume, 109\nSoapUI, 115 as risk when removing clones, 50\nSoftware Improvement Group (see SIG) maintenance consequences of, 72\nSoftware Risk Monitoring service, xiii maintenance problems with, 68-72\nSQL queries, 24 of classes, 71\nstandardization of functionality, 106 Type 1 clones, 46\nstar rating system, 8 Type 2 clones, 46\nstring literals, 55\nstub (test stub), 120 U\nsunny-side testing, 118\nunhappy flow testing, 118"}
{"171": "in complex domains, 40 in real-world systems, 23\nminimizing, 29, 42 minimizing, 11-26\nSIG rating of, 42 perceived lack of advantage in splitting\nunit guidelines units, 25\ncomponent guidelines vs., 137 quantity of units and performance, 22\ndefined, 67 reading code when spread out over multiple\nunit interfaces units, 23\nadvantages of minimizing size of, 59 refactoring techniques for guideline applica\u2010\nand libraries/frameworks with long parame\u2010 tions, 17-22\nter lists, 65 SIG thresholds for, 26\nand method modification, 60 when extending unit with new functionality,\nand parameter objects, 64 16\napplying guidelines for, 60-64 when writing new unit, 15\nC# interfaces and loose coupling, 78 unit tests\ncommon objections to minimizing size of, and Boy Scout rule, 124\n64 and duplication, 55\nease of understanding, 59 characteristics, 114\nhiding specialized implementations behind, common objections to automating tests, 123\n75-77 failures vs. errors, 118\nminimizing size of, 57-66 general principles for writing, 118-122\nrefactoring, 65 measuring coverage to determine proper\nreuse, 59 number of, 124\nSIG rating thresholds, 66 NUnit tests, 116-118\nunit size unit-level code, clean coding and, 128\nadvantages of minimizing, 14 unreachable code, 131\nand ease of analysis, 14 unused private methods, 132\nand improper formatting, 23 utility code, module coupling and, 78\nand reuse, 14\nand testability, 14 V\napplying guidelines to, 15-22\nviolations\ncommon objections to writing short units,\nprioritizing, 5\n22-25\nwith frameworks/libraries, 65\ndifficulty of optimizing by splitting units, 24"}
{"172": "Colophon\nThe animal on the cover of Building Maintainable Software is a grey-headed wood\u2010\npecker (Picus canus). Like all woodpeckers, which consitute about half of the Pici\u2010\nformes order, grey-headed woodpeckers use strong bills to puncture the surface of\ntrees and seek small insects that inhabit the wood. Very long, bristly tongues coated\nwith an adhesive extend into deep cracks, holes, and crevices to gather food in the\nbird\u2019s bill. A membrane that closes over the woodpecker\u2019s eye protects it from the\ndebris that may result from each blow at the tree. Slit-like nostrils provide a similar\nprotection, as do feathers that cover them. Adaptations to the brain like small size\nand a position that maximizes its contact with the skull\u2014permitting optimal shock\nabsorption\u2014represent further guards against the violence of the woodpecker\u2019s drill\u2010\ning. The zygodactyl arrangement of the feet, putting two toes forward and two back,\nallow the woodpecker to maintain its position on the tree\u2019s trunk during this activity,\nas well as to traverse vertically up and down it.\nGrey-headed woodpeckers maintain a vast range across Eurasia, though individual\nmembers of the species tend to be homebodies to particular forest and woodland\nhabitats. As such, they rarely travel overseas and switch to a seed-based diet in winter.\nMating calls that begin with high-pitched whistles lead to monogamous pairs roost\u2010\ning with clutches of 5 to 10 eggs in the holes that males bore into the trunks of trees,\nwhere both parents remain to incubate eggs and nurse the hatchlings for the three to\nfour weeks in which the hatchlings progress to juveniles. At this point, the young can\nfly from the nest and gather their own food.\nIn their greenish back and tail plumage, grey-headed woodpeckers very much resem\u2010\nble the closely related green woodpecker, and males of the species will develop on\ntheir foreheads the red patch that appears on many other species of woodpecker.\nMany of the animals on O\u2019Reilly covers are endangered; all of them are important to\nthe world. To learn more about how you can help, go to animals.oreilly.com."}
{"45": "Now one, two, or all of you might get called back for the next round.\nWhile you act as part of a team during the presentation of the case, you\nare each judged individually.\nHow do you become a leader in a situation like that? The best way to take\ncharge is to assign positions. Say, \u201cWho is the finance geek?\u201d or \u201cWho\nknows PowerPoint really well?\u201d Allocate positions based on strengths.\nAssign roles that include who opens the presentation, who closes, and\nwho is the timekeeper. The downside of this is that you get stuck with\nwhatever\u2019s left, but remember the recruiter is watching and you have\nalready made a big impression. Also determine up front how you\u2019re going\nto make a decision. Do all four of you have to agree, or just the majority?\nScenario 6: Presentation cases. You\u2019re at home minding your own\nbusiness when you get a call from a recruiter from your favorite\nconsulting firm. She says, \u201cEveryone loved you. We want you to come\nback for your final round. Can you come in at 4pm tomorrow?\u201d Of\ncourse! \u201cGood. At 4pm today you\u2019ll receive an email containing a case.\nYou have 24 hours to research and solve the case. We want you to put\ntogether a 15-minute presentation to be given to senior management.\u201d\nPrepare to be grilled. This is just like a case competition. How are your\nPowerPoint skills? Spend a lot of time on the defense of your answer.\nRefer to Case in Point: Case Competitions for detailed information on"}
{"46": "exercise is supposed to feel like a case interview, but with multiple-choice\nresponses.\u201d\nLast year the McKinsey Problem Solving Test (McK PST) contained 26\nquestions, and candidates had an hour to complete it. Candidates received\na bit more information about the business, the environment, and the\nproblem with each question.\nStudents can\u2019t bring in calculators or scratch paper. The test was\ndeveloped internally by McKinsey and validated by Applied\nPsychological Techniques Inc. (APT). \u201cIt was fun, now that it\u2019s over,\u201d\nrecounts a non-MBA Harvard graduate student. \u201cThere are some ratios\nand percentages, a couple of formulas, but nothing too overwhelming.\nAlso, a few charts are used to present some of the information, but again\nfairly basic, in my opinion.\u201d\nA McKinsey recruiter said, \u201cYou need to be comfortable with calculating\nsome percentages, basic equations, understanding relationships between\ndata, but nothing terribly advanced.\u201d\nSome international offices have a math section that one student says is\nmore like the GMAT than the GRE. You have 18 questions and 30\nminutes to complete them. \u201cYou start with probability and it gets harder\nfrom there,\u201d recounts a Harvard graduate student."}
{"47": "time-consuming and sometimes confusing. The test case is about one\ncompany, so you\u2019ll have to often go back in the case to get information.\nIt\u2019s basically broken down into three parts, math and word problems\n(about 40% of the test), logic and data interpretation (35%), and reading\ncomprehension (25%). Like a case interviewer, you\u2019re not allowed to use\na calculator. One MBA told me that what they are testing is your ability\nto do these calculations quickly and accurately \u2013 how do you handle time\npressure? BCG is also testing reaction time, thought process, and\nmemory. The scoring system awards you three points for a correct answer\nand zero points if you don\u2019t answer, but you are penalized one point for a\nwrong answer. Therefore, don\u2019t pick answers randomly if you start\nrunning out of time.\nVideo Games: In the last several years, McKinsey also asked students to\nplay an online video game. The firm is working with Imbellus to \u201ccreate\ngame-based tests that measure prospective employees\u2019 decision making,\nadaptability, and critical thinking.\u201d The goal is to deliver a quantitative\npicture of how an applicant thinks. The skills they\u2019re testing are how\nquickly can you synthesize a lot of information and data, and understand\nhow it applies to the overall question. Part of it is thinking ahead and\nplanning a strategy to make it successful.\nThe video games are ecologically themed. You\u2019re on an island where"}
{"48": "will need to choose a collection of species that would create a self-\nsustaining ecosystem.\nTo prevent applicants from gaming the system, no two candidates/test\ntakers experience the game/test the same way. There are tens of thousands\nof variations of each scenario, each at the same level of difficulty. There\nis no way to prepare, you will need to improvise. You\u2019ll feel as if you are\nplaying a video game and might forget that it\u2019s a test. A student told me\nthat the major challenge was being given many data points and deciding\nwhich were essential to use/move forward for a particular scenario.\n+ Reading graphs\nOne of the biggest changes we\u2019ve seen in case interviewing is the\nincreased use of graphs. You will be given charts, sometimes at the outset\nof the case. Sometimes they\u2019re slipped into the middle of the case,\nchanging the dynamics of the question. You are expected to analyze them\nquickly, extract the most important information, and then apply it to your\nanswer all the while drawing meaningful insights. It is easy to get\noverwhelmed. Eleven different types of graphs are used in consulting and\ncase interviews, and you should be familiar with all of them.\nMany students will make the mistake of just walking the interviewer\nthrough the graph. While that\u2019s helpful to make sure you are interrupting\nthe graph correctly, it is more important to analyze that information. If"}
{"49": "To fine-tune your graph analytics, look at the graphs printed in the Wall\nStreet Journal and The Economist and draw some conclusions. Then read\nthe article and compare your thoughts with the main points of the article.\nTo hone your graph analysis skills, read Case in Point: Graph Analysis for\nConsulting and Case Interviewing.\nMaking slides and graphs: There is another twist: making slides and\ngraphs. The recruiter hands you a few pages of data and asks that you\ncreate (on paper) three or four PowerPoint slides, one of which should be\na graph. You are then expected to present these slides during the case as\nyou of which should be a graph. You are then expected to present these\nslides during the case as you would during a presentation to a client.\n+ Silence\nOne of the questions I\u2019m asked most often is about silence. Is it ever okay\nto be silent? Yes and no. The simple rule to remember is that silence is\nokay when you are doing something like calculating, writing down your\nthoughts, or drawing a graph or decision tree. It is not okay to be silent\nwhen you are just thinking while staring out the window or looking down\nat your shoes, particularly in the beginning of the case. If the interviewer\ngives you the case and you just sit there thinking, you\u2019ve lost your\nmomentum. You\u2019re sitting dead in the water."}
{"50": "irrelevant often becomes relevant as you work through the case. Third,\nquickly run through the 5Cs (client, company, cost, channels,\ncompetition) in your head to see if there is something obvious that you\nmissed. Finally, if you are still stuck, ask for help. There is no shame in\nasking for help. If we were working on a project together, I would much\nrather have you ask me for help than have you waste a lot of time banging\nyour head against a wall. But I wouldn\u2019t recommend asking for help more\nthan once.\nTHE TROUBLE WITH MATH\nThere will be math! There are three kinds of people in the world, those\ngood at math and those not so good. Wait, what?\nSometimes there will be a little, sometimes a lot, but nothing that you\nhaven\u2019t seen before. The most common math you\u2019ll see in a case\ninterview are percentages, ROI, breakevens, weighted averages, net\npresent value, and multiplication and division with lots of zeros. The big\ndifference is that you can\u2019t use a calculator. The two reasons they make\nyou do math without a calculator are to see how you think and to see if\nyou think.\nHow you think: I\u2019ve seen students do the same math problem different\nways. Do you use scientific notation? Do you turn everything into a"}
{"51": "be more than 25 percent but less than 30 percent. My best guess is around\n28 percent. So, I\u2019m going to calculate 30 times 1.28 and get \u20ac 38.4.\nIf you think: I don\u2019t care whether you are an applied math undergrad or\nhave a PhD in string theory, most students have trouble with zeros. If you\nmultiply two numbers together and the answer is supposed to be 320\nmillion and you tell me it is 3.2 billion and 3.2 billion makes no sense at\nall, it\u2019s going to raise some eyebrows \u2013 because it is showing me that\nyou\u2019re not thinking before you speak. And if you do that in something as\nimportant as an interview, what will you do in front of a client? I can\u2019t\ntrust you, and if I can\u2019t trust you, I\u2019m not going to hire you. A good rule\nof thumb is that whenever you do a calculation, ask yourself, \u201cDoes this\nnumber make sense?\u201d If yes, then say it. If no, then go back and\nrecalculate it because you know that they\u2019re going to make you\nrecalculate. And once you say it, you can\u2019t un-ring the bell. Study the\nfollowing table."}
{"52": "\u201cNo, it isn\u2019t,\u201d then you don\u2019t know where you made the mistake. You\u2019ll\nhave to go back to the beginning of the calculations \u2013 and no one wants to\ndo that. Below are some math questions to do in your head:\n\u2022 A ) The total widget market is $170 million and our sales are $30\nmillion. What percentage of the market share do we hold?\n\u2022 B ) Our total manufacturing costs are $20 million. With that, we\ncan make 39,379 units. What is our approximate cost per unit?\n\u2022 C ) Our total costs are $75,000. Labor costs make up 25 percent of\nthe total costs. How much are our labor costs?\n\u2022 D ) You bought a stock for $36 a share. Today it jumped 6 percent.\nHow much is your stock worth?\n\u2022 E ) You raised $3.5 million for a startup. Your commission is 2.5\npercent. What\u2019s your commission in dollars?\n\u2022 F ) What\u2019s 7 x 45?\n\u2022 G ) The number of current outstanding shares for UKL Inc. is\n41,084,000. Institutional investors hold 25,171,000 shares. What is\nthe approximate percentage of shares held by institutions?"}
{"53": "Go figure: Try to estimate some of the percentages in your head, and\nthen work out the others without a calculator. Round off the answers as\nyou would during a case question. (Worth noting: These are from a fifth-\ngrade math test.)\nP) 60 percent of 70 = ___\nQ) 25 percent of 124 = ___\nR) 68 percent of 68 = ___\nS) 12 percent of 83 = ___\nT) 23 percent of 60 = ___\nU) 27 percent of 54 = ___\nANSWERS: A) about 18% B) about $500 C) $18,750 D) $38.16 E)\n$87,500 F) 315 G) 60% H) 2% I) 2% J) 2.5% K) 4% L) 17% M) 5%\nN) about $2.3 billion O) Europe\u2019s population drops by about 5% P) 42\nQ) 31 R) 46 S) 10 T) 14 U) 15\nHere are some formulas you should know inside and out:\nNet Income = Revenues - expenses\nBreakeven Point in Units = Fixed Costs/(Price - Variable Costs)\nBreakeven Point in Price = (Total fixed cost/Production unit volume) +\nVariable cost per unit\nProfit Margin = Net Income/Revenue"}
{"54": "case is that the CEO of Coors is thinking of entering the bottled water\nmarket. Is this a good idea? So you draw a line, put the object across the\ntop, then lay out your structure. If you ask clarifying questions you can\nwrite the answers in the left-hand column underneath the original\nquestion.\nThe use of plain white printer paper is fine; however, some consulting\nfirms prefer that you use graph paper. There are several reasons for this:"}
{"55": "when you are off by a zero when your answer has eight zeros at the\nend of it.\n\u2022 Graph paper organizes your notes. Well-organized notes make it\neasier for the interviewer to follow. When he isn\u2019t looking into your\neyes, he\u2019s looking at your notes to see what you wrote down\nbecause he knows what you should be writing down. In addition,\nthere is a good chance that the interviewer will collect your notes at\nthe end of the interview. He uses your notes as one more data point\n\u2013 what did you write down? How did you write it? How did you do\nyour math? Can he read your handwriting?\nWhen it comes time for the case portion of your interview, rip out five\npages of paper and number them (you can do this before the interview to\nsave time and look well-organized). Remember to write on just one side\nof the page. Flipping pages back and forth can be disruptive and makes it\nhard to find important data at a glance. Using bullet points will make your\nnotes seem better organized and make it easier to go back to find\ninformation. Star or highlight important points that you think will make\nthe summary. This way, points will jump out at you when it comes time to\nsummarize the case.\nAs you fill up the pages (while leaving plenty of white space on your\nnotes), spread your notes out in front of you. That way, you can see the"}
{"56": "Often, the interviewer will hand you a chart. Always ask for permission\nbefore you write on the chart. As strange as it may seem, sometimes\ninterviewers show up with only one copy of a chart.\n( Idea box )\nAfter hearing the initial problem statement, candidates will often think of\nan idea, solution, or strategy that seems inappropriate to state at the\nbeginning of the case for fear that the interviewer might think that you are\nshooting from the hip and not analyzing the problem. Candidates write it\ndown in their notes and then often forget about it during the case, even if\nit becomes relevant. Then they wind up kicking themselves because they\nhad the solution, but forgot about it. I tell students to draw an \u201cidea box\u201d\non the first page of their notes. If anything pops into your head, write it in\nthe idea box. Then you know where it is, and you\u2019ll get in the habit of\nchecking the box when you review your first page of notes (which you\nshould do several times during the case).\nRECOMMENDATIONS, SUMMARIES, AND THE\nFINAL SLIDE\nIn most cases, you will be asked to either make a recommendation or\nsummarize the case.\nIf you are asked to make a recommendation, the interviewer will either"}
{"57": "risk is going to happen and the severity if it does, or use decision trees.\nThe idea is to bring your interviewer forward to go over it with you, thus\nmaking him feel like a client. If you can end the case with the interviewer\nfeeling like a client, it doesn\u2019t get much better than that. Many sins will\nbe forgiven if they feel like a client in the end.\nWhen you\u2019re ready, lead with the recommendation. State a clear yes or\nno. Yes, they should enter the market or no, they shouldn\u2019t buy that piece"}
{"58": "term and the long term. Once you lay out the next steps, simply add, \u201cand\nwe can help you with that.\u201d This shows the interviewer that you\nunderstand how it all works, that consultants are always looking for more\nwork, and that your recommendation is more likely to be successful if\nyou are there to help implement it. Also, offer recommendations that are\nimplementable in a relatively short period of time (between 18 and 24\nmonths unless otherwise stated), have reasonable budgets, and that move\nthe needle. If your idea increases profits by $10 million, that\u2019s great if\nyour company has profits of $50 million, but not so great if it has profits\nof of $10 billion. Be careful of grand schemes. Most important, you\nneed to sell your recommendation. You need to close the deal. You\nspent 40 minutes listening to the problem and coming up with a solution.\nIf the interviewer doesn\u2019t buy your recommendation, the chances of your\nmoving on are a lot less.\nBe prepared for the interviewer to come back at you and say, \u201cLet me tell\nyou why you\u2019re wrong.\u201d If he comes back with an argument that brings\nup something that you didn\u2019t consider, it\u2019s okay to admit that you were\nwrong. \u201cI didn\u2019t think about the inventory issue, and now that I\u2019m\nconsidering it I think you are right.\u201d There is no shame in admitting you\nare wrong. What he doesn\u2019t want you to do is to change your answer\nbecause he told you that you were wrong \u2013 or to defend an answer that\nyou know is wrong because you don\u2019t want to admit it. Corporations hire"}
{"59": "job? The recommendation allows you to stand out, to be creative, to\ndifferentiate yourself from the rest, and to close the deal.\nRecommendation/RECAP\n\u2022 Put your recommendation on a separate sheet of paper and make it\nvisual.\n\u2022 Turn your recommendation toward the interviewer.\n\u2022 Lead with the recommendation, no backstory needed.\n\u2022 Be definitive, state it with confidence.\n\u2022 Don\u2019t hedge your answer.\n\u2022 If your recommendation is a \u201cno,\u201d then you should come up with\nan alternative plan to help the company reach its goal.\n\u2022 Tell the interviewer why \u2013 the reasons behind your decision.\n\u2022 State and prioritize risks based on impact and likelihood of\noccurrence.\n\u2022 Offer recommendations that are implementable in a relatively short\nperiod of time, have reasonable budgets, are creative, and that\nmove the needle.\n\u2022 Sell your recommendation, close the deal.\n\u2022 Close with \u201cand we can help you with that.\u201d\n\u2022 Be prepared for the interviewer to come back at you and say, \u201cLet\nme tell you why you\u2019re wrong.\u201d\n\u2022 Defend your answer."}
{"60": "In some cases \u2013 particularly those that ask for a list of numbers the\ninterviewer wants you to figure out, or those comparing two or more\nstrategies, ideas, or options using the same criteria \u2013 you can create the\n\u201cfinal slide\u201d right at the beginning of the case. Almost no one remembers\nto do this, so if you do think of it, you\u2019ll score big points with the\ninterviewer.\nOn a separate sheet of paper, draw a chart listing the product or markets\n(whatever it is that you are comparing) and below that, the criteria. As\nyou calculate the numbers, fill them in on the final slide; this keeps all\nrelevant information in one place and makes it easier for the interviewer\nto follow (think of it as a scorecard). Once all the information is filled in,\nturn the final slide toward the interviewer and walk him through it. This is\nthe best summary. It is similar to the final slide of a deck that a consultant\nwould present to a client.\nCASE JOURNAL"}
{"61": "the Wall Street Journal, Bloomberg Businessweek, or McKinsey\nQuarterly, she would add to her journal. It never left her side.\nThis student ended up at a top firm and took the journal with her. With\nevery engagement, she learned something new and added it. When she\nand her coworkers sat around brainstorming problems, she would flip\nthrough her journal and throw out ideas, which often sparked discussions.\nI saw her five years after she had graduated, and she still had her journal\nwith her. Although it was as beaten up as Indiana Jones\u2019s, it held just as\nmany treasures. She was headed to a new job and the journal was the first\nthing she packed.\nSince then, whenever I speak at schools, I recommend creating a journal.\nBesides keeping all your notes in one place, it becomes a single source\nfor case material that is also extremely helpful for your classes. If you are\ntruly serious about case interviewing, then you will continue to read and\npractice cases all summer long. Recruiting events start as soon as you get\nback to campus, so if you take the time over the summer to practice, life\nis going to be easier in the fall."}
{"62": "4 : IVY CASE SYSTEM\nBEST CASE THINKING \u2013 BUILDING YOUR CASE\nCRED\nMany students have a hard time getting started. Sometimes they are\noverwhelmed, sometimes they are nervous, and sometimes they just\nTM\ndon\u2019t have a clue. So we developed The Ivy Case System .\nThe Ivy Case System is a two-part system made up of five easy steps to\nget you going and four popular case scenarios, each with a collection of\nideas and questions that will help you structure the remainder of your\nresponse. If you follow the outline for each scenario, you can be\nconfident that your response will be logical and cohesive. And because it\nis all based on business sense and common sense, you\u2019ll find that there is\nnothing in there that you don\u2019t already know \u2026 it\u2019s just organized a little\ndifferently.\nThe first five steps will provide you with a quick start (no long awkward\npause between question and answer). They will get you five minutes into"}
{"63": "THE FIRST FIVE STEPS\n1. Summarize the question.\n2. Verify the objective(s). Ask if there are any other goals or\nobjectives.\n3. Ask clarifying questions.\n4. Label the case and lay out your structure.\n5. State your hypothesis.\n( 1. Summarize the Question )\nSometimes the question is short: \u201cHow do we increase sales at the\ncampus bookstore?\u201d Other times the question is long and involved and is\nfilled with data that comes at you as if from a fire hose. It\u2019s difficult to\ncapture all that information in your notes, so the first step is to\nsummarize the question. That way, if you missed something or wrote it\ndown wrong, the interviewer will correct you before allowing you to\nmove on. It puts you and the interviewer on the same page from the\nbeginning, thus keeping you from answering the wrong question.\nSummarizing the question shows the interviewer that you listened, and it\nfills the gap of silence between the end of the question and the beginning\nof your answer. Make sure you streamline the summary; don\u2019t repeat it\nback word for word. If there are numbers in the problem, always repeat\nthe numbers because they may be relevant, and you want to make sure"}
{"64": "You can bet that when consultants first meet with clients, they always\nask about objectives and goals. What are the client\u2019s expectations, and\nare those expectations realistic? In the client\u2019s mind, what constitutes\nsuccess? Even if the objective of the case seems obvious, there is always\nthe possibility of an additional underlying objective. So you (the\ninterviewee) should say, \u201cOne objective is to raise profits. Are there any\nother objectives or goals that I should know about?\u201d If the interviewer\nsays, \u201cNo, higher profits is the only objective,\u201d you just focus on that one\nobjective. If there is an additional objective, you may need to break the\ncase in two and tackle one objective at a time. If you don\u2019t ask about\nother objectives or goals and there is one, the interviewer will have to\nfeed you that information during the case and you\u2019ll lose points. By\nasking, you might receive key information that will affect the way you\nlay out your structure.\n( 3. Ask Clarifying Questions )\nThese are questions to ask only when you don\u2019t understand something or\nneed additional information to lay out your structure. If the interviewer\nthrows out industry jargon, slang, or a string of initials or acronyms and\nyou don\u2019t know what they are, ask. You don\u2019t lose any points for asking.\nIf you need additional information to lay out your structure, example\nmight be, if it\u2019s an entering a new market case, a good clarifying\nquestion might be \u201cWhy does the company want to enter this market?\u201d"}
{"65": "through the bullet points in your mind and decide which are the most\nrelevant to this particular question. You then just need to tell the\ninterviewer how you plan to proceed. Keep in mind that you are not\nmarried to your structure. Because new information is constantly added,\nas well as twists and turns in the case, your structure could stand through\nthe whole case, or it might become obsolete rather quickly. A structure is\n\u2013 given how you plan to proceed. Keep in mind that you are not married\nto your structure. Because new information is constantly added, as well\nas twists and turns in the case, your structure could stand through the\nwhole case, or it might become obsolete rather quickly. A structure is \u2013\ngiven the limited information you have \u2013 what would you analyze, and in\nwhich order, to solve this problem?\nYou want to make sure that your structure is MECE, or Mutually\nExclusive, Collectively Exhaustive. This is a consulting term that is\nthrown around a lot. All it means is that there shouldn\u2019t be any overlap in\nyour structure. For example, if you get an entering a new market\nquestion, you might want to break your structure down into three\nbuckets: first analyze the client, then analyze the new market, then look\nat the different ways to enter the market. Each of those buckets is\nmutually exclusive of the others, but collectively exhaustive of\neverything we need to look at to solve this case."}
{"66": "it seems crazy to give them an answer to the case before you have any\ninformation. But the reason consulting firms like it is that when they go\ninto a company, they often don\u2019t know what the problem is, so they state\nan initial hypothesis and try to prove it. Chances are they are going to\ndisprove the first three, but the process narrows the scope of the problem.\nFor example, if you get a P&L case, you might say, \u201cMy hypothesis is\n(or my thoughts are) that profits have fallen because costs have risen.\u201d\nThat is something you can try to prove as you move through the case. If\nyou disprove it, you\u2019ll want to update your hypothesis as you proceed\nthrough the case.\nThe advantages to stating a hypothesis up front are that it will help you\nask the right questions, make your analysis more linear, and force you to\nfocus on issues that you can either prove or disprove. It also helps you\nidentify which structure to use, thus defining the starting point.\nI\u2019ve known plenty of candidates who received job offers without ever\nstating a hypothesis. Think of it as icing on the cake. If you can\nremember to state one, they\u2019ll love it. If you forget, though, it\u2019s not the\nend of the world. But because the interviews are so competitive, you\u2019ll\nwant to take every advantage. When you are doing practice cases, a\nsimple trick to remind yourself to state a hypothesis is to lightly write the\nletter H in the middle of your first page of notes. That way, as you are"}
{"67": "You know you\u2019re going to get a lot of cases that ask you to analyze a\ncompany (the client) and cases that ask you to analyze the\nmarket/industry. Below are two lists of core questions you should be\nthinking and asking about the company and market/industry. You need to\nlearn these. It will speed up your structure (because you won\u2019t have to\nstop and think) and ensure that you don\u2019t miss anything critical. You will\nprobably add to or subtract from the list depending on the case, but it\nwill eventually become second nature to you and thus build up your\nconfidence.\nCore questions to ask about the company: Memorize these!\n1. Revenues and profits for the last three years?\na. What are the major revenue streams, and what percentage of\nthe total revenue does each stream represent?\nb. What are the major costs? Do any seem out of line?\n2. Customer segmentations?\na. Characteristics?\nb. Changing needs?\nc. Profitability by segment?\n3. Product mix?\na. Cost/margins?\nb. Product differentiations?"}
{"68": "\u2022 It\u2019s important to ask about the revenue and profit trends so you can\nget a feel for the size of the company and how it\u2019s been doing.\nMany of these companies are made up so you have no idea about\nanything. You may even want to ask whether it is a publicly traded\nor private company.\n\u2022 Customer segmentation is critical. You need to know who the\nclient\u2019s customers are so you can craft a strategy.\n\u2022 Characteristics and changing needs of each segment.\n\u2022 Profitability of each segment.\n\u2022 Product mix. What products or services does the company\noffer, and what are the costs and margins associated with each\nproduct? Have there been any recent changes in the product\nmix? Product differentiation and market share.\n\u2022 Production capabilities and capacity allow you to access several\nkey areas. Does the company have the ability to expand? Are\nthey running at full capacity now and if not, why not?\n\u2022 Brand. How strong is its brand? Is it a market leader or has the\nbrand faded? Since the company in the case is often made up,\nwe have no idea about the strength of the brand.\n\u2022 Distribution channels. How are the products and services\ncurrently distributed? How can the company expand its\nchannels? How can it reach new markets?\n\u2022 WCS. What constitutes success? This is different from the"}
{"69": "4. Customer segmentation(s)?\n5. Margins?\n6. Industry changes? (M&A? New players? Change in\ntechnology? New regulations?)\n7. Distribution channels?\n8. Major players and market share?\n9. Product differentiation?\n10.Access to suppliers?\n11.Barriers to entry/exit?\n+ Market\n\u2022 Market size, growth rate, and trends. Ask for three years of data.\nHow is the industry doing overall, and how is the company\ngrowing compared with the industry?\n\u2022 Industry drivers. Is it brand, street cred, price, content, size,\nendorsements, fads/culture, marketing, economics, technology,\ngeopolitical events, bargaining power of buyers, bargaining power\nof suppliers, or distribution channels?\n\u2022 Customer segmentation(s). There are often a number of segments\nwithin an industry.\n\u2022 Customer segmentation(s). There are often a number of segments\nwithin an industry. Which is the company going after? How big"}
{"70": "\u2022 Product differentiation. This often ties into the one above. How do\nthe competitors differentiate their products or services? Match it\nup to market share.\n\u2022 Access to suppliers. How difficult will it be to gain access? How\nmany suppliers and who supplies them?\n\u2022 Barriers. Barriers to entry can be access to capital, distribution\nchannels, raw materials, technical knowledge, or human talent.\nBarriers could also be government regulations, customer loyalty,\nsticky features (making it hard to leave one product for another),\nor market domination by one or two major players. Barriers to exit\ncould be massive investment and nontransferable fixed assets,\ncontract requirements with suppliers, or government requirements\n(e.g. a company receives major tax breaks from a state government\nto employ a certain number of people from that state). Another\nbarrier would be that the costs of leaving a market are higher than\nthose incurred to continue competing in the market, barriers of\nemotions (\u201cWe built our house on this market\u201d).\nTHE FOUR KEY CASE SCENARIOS \u2013 DEVELOPING\nYOUR STRUCTURE\nWhile interviewers give dozens of different types of cases, the four\nscenarios listed below are the basic building blocks of many of the cases."}
{"71": "brilliance, and thus make it harder for you to set yourself apart from the\nnext candidate. You\u2019ll lose points for lack of creativity, imagination, and\nintellectual curiosity. The best interviewers use the bicameral mind \u2013 the\ntwo-lobed brain. The left side is analytical, and the right side is creative.\nThe four key case scenarios are:\n1. Profit and loss\n2. Entering a new market\n3. Pricing\n4. Growth and increasing sales\n( 1. Profit and loss )\nQuestion: Our client manufactures high-end athletic footwear. Sales are\nup but profits are down in Europe. What\u2019s going on, and how do we fix\nit?\nProfit-and-loss questions have been the most popular type of question for\nthe last 25 years. Whenever you hear the words bottom line, profits,\ncosts, or revenues, you should immediately think: \u201cProfits = (Revenues \u2013\nCosts).\u201d We know that Revenue = Quantity x Price, and\nCosts = (Quantity x Variable Costs) + Fixed Costs.\nHowever, I\u2019m going to change this formula to the structure of E(P=R-"}
{"72": "\u2013 increased costs, lower revenues, or outdated products. Another\nclarifying question could be, \u201cBy how much have profits fallen?\u201d\nE(P=R-C)M. If you suspect that it is an industry-wide problem, start by\nspelling out your take on external factors. Then look inside. I want two\nor three things that are important to the company inside the parentheses.\nIf it is a retailer, then we\u2019d like to look at consumer confidence,\ndisposable income, unemployment rate, and maybe petrol prices. If it\u2019s a\nmanufacturer, then we\u2019d want to look at how the dollar is doing against"}
{"73": "The E can also stand for the environment. In 2017 the US suffered major\nhurricanes, tornadoes, mudslides, and wildfires. As a result of those\nevents, some companies saw their profits drop, while others saw their\nprofits rise. These are factors that most students don\u2019t take into\nconsideration when analyzing a case. Look at everything, touch on\neverything, analyze everything.\nThe M stands for the market or industry. No one is going expect you to\nknow what\u2019s going on in all industries. The interviewer has a lot of\ninformation to give you, but he\u2019s not going to give it to you unless you\nask for it. Ask about industry trends and competitors to determine\nwhether they are facing the same problems as the client: How has our\nclient been doing compared with the rest of the industry? If you have\nmemorized the industry core from the beginning of the chapter these\nquestions will seem natural, and you\u2019ll come off projecting confidence.\nThe company. Going inside the parentheses is the same as going inside\nthe company. Start by asking questions about the company. Who are\nthey? What do they do? What are their products? Write down the core\ncompany questions relevant to this case, such as:\n\u2022 Market leader?\n\u2022 Size in terms of revenues and profits, public or private? (Ask for 3\nyears.)"}
{"74": "good to ask for trends. This is how consultants think, and it\u2019s how they\nwant you to think.\nThe other parts to the P&L formula are price and volume. Price and\nvolume are interdependent. You need to find the best mix, because\nchanging one isn\u2019t always the best answer. If you cut places to drive up\nvolume, what happens to the profit and next-year sales? Do profits\nincrease or decrease? There must be a balance. The reason behind the\ndecision must make sense.\nOnce you have an understanding of the market and the company, you\u2019ll\nneed to come up with some solutions to raise profits \u2013 that, after all, is\nthe objective. The first thing you want to do is ask for a moment to write\ndown your thoughts. Start with headings, revenue-based strategies, and\ncost-based strategies. You do this for several reasons: It shows the\ninterviewer that you are well organized and thinking ahead. It\u2019s also\neasier to come up with ideas when you are looking at a heading instead\nof a blank sheet of paper. And it keeps you from commingling your\nideas. Dividing ideas into short-term and long-term solutions is also\ndesirable, particularly if there is a time constraint to raising profits. You\nwant to present all the revenue-based ideas first, then all the cost-based\nideas. If appropriate, divide your cost-based ideas into production, labor,\nand finance. It will show the interviewer that you are well organized and"}
{"75": "Don\u2019t let that happen to you. If you take the time to write down your\nanswers first, and the interviewer cuts you off, just look at your notes\nand offer another idea. It takes a lot of the stress out of the situation and\nmakes you look more professional.\nQuestions to think about:\nRevenues:\n1. What are the major revenue streams, and what percentage of the\ntotal revenue does each stream represent?\n2. Does anything seem unusual in the balance of percentages?\n3. Have the percentages changed lately? If so, why?\nCosts:\n1. Have there been any major shifts in costs?\n2. Do any costs seem out of line?\n3. If we benchmarked our costs against our competitors\u2019 costs,\nwhat would we find?\nProducts:\nWith new products, make sure you ask about the advantages and\ndisadvantages. Everyone forgets to ask about the disadvantages, but\ndisadvantages can drive the case more than the advantages.\nDisadvantages might be things like product cannibalization, layoffs, or in"}
{"76": "( 2. Entering a New Market )\nQuestion: Your client, Company Z manufactures hair products. The\ncompany is thinking about entering the sunscreen market. Is this a good\nidea?\nEntering a new market questions can be as straightforward as the one\nabove, or they can involve mergers, acquisitions, joint ventures, starting\na new business, or the development of a new product. So it\u2019s not just \u201cDo\nwe enter?\u201d but also \u201cHow do we enter and what are the advantages and"}
{"77": "eyes. If they don\u2019t understand the client, they really can\u2019t tell if the\nmarket is \u201cworth it.\u201d\nthe market. This way, they can look at the new market through the\ncompany\u2019s eyes and not just their own eyes. If they don\u2019t understand the\nclient, they really can\u2019t tell if the market is \u201cworth it.\u201d\nYour notes might include something like the following, covering the\ncompany, the current market, and how best to enter the market. This can\nbe laid out in three buckets, or you can structure it as internal (company\nand how to enter) and external (market and competition). Either way, it is\nbasically all the same information. Structure it whichever way makes\nyou most comfortable.\nStep 1: Start off with questions about the company. (Remember the core\nlist.)\n\u2022 What are the company\u2019s profits and revenues for the last three\nyears?\n\u2022 What is the company\u2019s product mix?\n\u2022 If this is about a new product:\n\u2022 Will it cannibalize an existing product?\n\u2022 Is the customer segmentation(s) the same?\n\u2022 Can we use the same distribution channels?"}
{"78": "\u2022 Where is the industry in its life cycle? (Stage of development:\nEmerging? Mature? Declining?)\n\u2022 Who are the customers, and how are they segmented?\n\u2022 What role does technology play in the industry, and how quickly\nwill it change?\n\u2022 How will the competition respond?\nStep 3: Investigate the market to determine whether entry makes good\nbusiness sense.\n\u2022 Who are the competitors, and what size market share do they\nhave?\n\u2022 How do their products differ from ours?\n\u2022 How will we price our products or services?\n\u2022 Are substitutions available?\n\u2022 Are there any barriers to entry? Examples might include lack of\nbrand or street cred, capital requirements, access to raw materials,\naccess to distribution channels, lack of human capital, and\ngovernment policy. Industries dominated by a small number of big\nplayers can also be a barrier.\n\u2022 Are there barriers to an exit? How would we exit if this market\nsours?\n\u2022 What are the risks? (For example, changing market regulations or\ntechnology.)"}
{"79": "Cost-benefit analysis. Analyze the pros and cons of each strategy. You\ncan use this whenever you are trying to decide whether to proceed with a\ndecision.\nYour notes might look something like this:\nRecommendation for entering a new market: Lead with the answer.\nIf yes, say, \u201cYes, the client should do it.\u201d This is why, this is how, and\nthese are the risks, remember to prioritize the risks based on impact and\nthe likelihood of occurrence, and the next steps \u2013 in both the short-term"}
{"80": "Many questions about entering a new market are also M&A questions.\nThe two most important factors about a merger are whether it increases\nshareholder value and whether the two cultures will mesh well. Cultural\nmismatch is the biggest reason mergers fail or don\u2019t live up to their\npotential. Larry Fink, the CEO of BlackRock, says, \u201cBuy companies\nwith cultures your company can learn from, not just businesses that will\nlet you cut costs.\u201d\nHere are some key points about an M&A question:\n\u2022 If the buyer is a private equity firm, ask \u201cWhy does the PE firm\nwant to buy the company? What else does it own? And what does\nit plan to do with it (hold, flip, or break apart)?\u201d Many students\nforget to ask this and they miss out on the countless synergies\namong portfolio companies.\n\u2022 If the M&A involves one company acquiring another, ask not only\n\u201cWhy?\u201d but also, \u201cWhat other products do they sell?\u201d and \u201cWhat\nare the synergies involved?\u201d The acquisition needs to make good\nbusiness sense.\n\u2022 Reasons to purchase:\n\u2022 Increase market access, boost the brand, and increase market\nshare."}
{"81": "\u2022 Due diligence. Research the company and industry:\n\u2022 What kind of shape is the target company in? Management?\nProducts? Profitability? Brand? What is the stand-alone value?\nWhat has been its growth rate? Why is it on the market?\nConsider all the items in the core box about analyzing a\ncompany.\n\u2022 How secure are its markets, customers, and suppliers?\n\u2022 What are the margins like? Are they high volume, low margin,\nor low-volume/high-margin?\n\u2022 How is the industry doing overall? And how is the company\ndoing compared with the industry? Is it a leader in the field?\n\u2022 How will competitors respond?\n\u2022 Are there any legal reasons that we can\u2019t or shouldn\u2019t acquire\nthe target company?\n\u2022 Are there technology risks?\n\u2022 How much will it cost? Will the client overpay?\n\u2022 Does the buyer have enough cash or access to capital markets?\n( 3. Pricing Strategies )\nQuestion: Company S is coming out with a new tablet. How should they\nprice it?\nPricing questions can sometimes be stand-alone questions, but just as"}
{"82": "The company strategy or objective is the first and most important\ncomponent of a pricing case. Let\u2019s take the tablet market as an\nexample. Apple came out with the iPad. It was first to market; and it\u2019s a\nbeautiful piece of technology, but kind of pricey. We know Apple likes\nbig margins. Apple was going for profits. Next, Samsung came out with\nthe Galaxy. Another beautiful piece of technology, but the company\u2019s\nmargins were smaller and the price lower. Samsung was going for\nmarket share. Then the Amazon Kindle Fire was launched. Amazon sold\nit for what it cost them to make it. The strategy was to get as many Fires\nas possible out into the market, because the company makes more off the\nancillary sales \u2013 books, movies, songs, and Prime membership \u2013 than it\nwould ever make from the sale of a single device. These are three very\nsuccessful companies, with three very different pricing strategies for\nbasically the same product.\nStep 2: Investigate the product. How does it compare with that of the\ncompetition? Are there substitutions or alternatives? Where is the\nproduct in its growth cycle? Is there a supply-and-demand issue at work?\nStep 3: Determine a pricing strategy. There are three main pricing\nstrategies to think about: competitive analysis, cost-based pricing, and\nprice\u2013based costing."}
{"83": "good way to price anything, though, because if you misjudge the market,\nyou\u2019ll have to cut prices, which will squeeze margins \u2013 but the company\nneeds to know what its costs are.\nPrice-based Costing: What are people willing to pay for this product? If\nthey\u2019re not willing to pay more than what it costs you to make it, then it\nmight not be worth making. On the other hand, consumers may be\nwilling to pay much more than you could get by just adding a profit\nmargin. Profit margins vary greatly by industry: Grocery stores have a\nvery thin profit margin, while drug companies traditionally have a huge\nmargin. Also consider what your product will be worth to the buyer.\nCompare it with other products or services in their lives. What did they\npay in those cases?\nIn short, when solving a pricing problem, you need to look at all these\nstrategies and see where, or if, they intersect.\nNOTE: Pricing questions become more difficult/interesting when you\nget a case about partition pricing \u2013 meaning that you charge separately\nfor things like delivery, shipping, installation, and warranties versus\nbundling everything into one price. If you run an airline, do you\nadvertise lower ticket prices and charge for baggage or do you advertise\nthat bags fly free and charge a higher ticket price? If you\u2019re a large"}
{"84": "I teach at 50 schools a year. Any fee I quote includes my speaking fee\nand my travel expenses, combined. My competitor charges a fee and\nthen charges travel expenses separately. My thought is that my pricing\nstrategy makes it easier for both the school and me because I don\u2019t have\nto collect, copy, and submit a travel report with receipts to the school\u2019s\naccounts payable department. It\u2019s just one flat fee. That way, I get paid\nsooner and don\u2019t have to wait for my credit card bill to submit my plane\nticket cost. I already have an idea what my expenses will be, and that\nallows the school to budget in advance, which is what they like. With my\nway, there are no surprises. Whose way is right? Well, the norm in\nconsulting is to charge a fee plus expenses. That might work well when\nyou have Apple as a client, but not when a client is a cash-strapped\nuniversity.\n( 4. Growth and Increasing Sales )\nQuestion: BBB Electronics wants to increase its sales so it can claim it is\nthe largest distributor of the K6 double-prong lightning rod. How can\nBBB reach its goal?\nIncreasing sales or growing the company are not the same as increasing\nprofits. In the former case, you are less interested in costs. Still, you want\nto have an understanding of the company and its objective, its products,\nand the industry."}
{"85": "Step 2. Investigate the industry. Is it growing, and how is the client\ngrowing compared with the industry? Are the client\u2019s prices in line with\nits competitors?\nIncrease volume/revenues:\n\u2022 Expand the number of distribution channels.\n\u2022 Increase product line through diversification of products or\nservices (particularly with products that won\u2019t cannibalize sales\nfrom existing products).\n\u2022 Analyze the segments of the business with the highest future\npotential and margins.\n\u2022 Invest in a marketing campaign.\n\u2022 Acquire a competitor (particularly if the question is about\nincreasing market share).\n\u2022 Adjust prices. (Take into account the price sensitivity of the\ncustomer. Lower prices to increase volume, and raise them to\ndecrease demand or increase profits per unit).\n\u2022 Create a seasonal balance. (Increase sales in every quarter \u2013 if you\nown a nursery, sell flowers in the spring, herbs in the summer,\npumpkins in the fall, and Christmas trees and garlands in the\nwinter).\nAnother way for a company to grow is to find niches in developing"}
{"86": "\u2022 How have we been doing compared with the industry?\n\u2022 Who are the major players, and what market share does each have?\nWho has the rest?\n\u2022 Has the industry seen any major changes lately? These might\ninclude new players, new technology, and increased regulation.\n\u2022 What drives the industry? Brand/street cred, price, quality,\nendorsements, fads/culture, marketing, products, size, economics,\nor technology?\n\u2022 Profitability. What are the margins?\nSuppliers:\n\u2022 How many are there?\n\u2022 What is their product availability?\n\u2022 What\u2019s going on in their market?\n\u2022 How is the supply chain? Are the companies that supply you\ngetting what they need from their suppliers?\nThe Future:\n\u2022 Are players entering or leaving the market?\n\u2022 Are any mergers or acquisitions going on?\n\u2022 Are there any barriers to entry or exit?\n\u2022 Substitutions: What alternatives are there?"}
{"87": "\u2022 How does this strategy affect our existing product line?\n\u2022 Are we cannibalizing our own sales from an existing product?\n\u2022 Are we replacing an existing product?\n\u2022 How will this strategy expand our customer base and increase our\nsales?\n\u2022 What will the competitive response be?\n\u2022 If we are entering a new market, what are the barriers to entry?\n\u2022 Who are the major players, and what are their respective market\nshares?\nThink about customers:\n\u2022 Who are our customers, and what is important to them?\n\u2022 How are they segmented?\n\u2022 How can we best reach them?\n\u2022 How can we ensure that we retain them?\n\u2022 What is the cost per customer?\nFunding:\n\u2022 How is this product being funded? Does our company have the\ncash, or are we taking on debt? And can we support the debt under\nvarious economic conditions?\n\u2022 What is the best allocation of funds?"}
{"88": "Rogers Adoption/Innovation Curve. Draw the curve, then turn your notes\ntoward the interviewer and walk her through your thought process.\nStarting a New Business\nStarting a new business encompasses entering a new market as well \u2013 the\nfirst step is the same. Investigate the market to determine whether\nentering it makes good business sense:\n\u2022 Who is our competition?\n\u2022 What market share does each competitor have?\n\u2022 How do competitors\u2019 products or services differ from ours?\n\u2022 Are there any barriers to entry or exit?\nOnce we determine that there are no significant barriers to entry, we\nshould then look at the company from a venture capitalist point of view."}
{"89": "\u2022 Who are the major players and what are their respective market\nshares?\n\u2022 What will the competitive response be?\n\u2022 Distribution channels\n\u2022 Which, and how many, distribution channels can we rely on?\n\u2022 Products and services\n\u2022 What is the product, service, or technology?\n\u2022 What is the competitive edge?\n\u2022 What are the disadvantages?\n\u2022 Is the technology proprietary?\n\u2022 Customers\n\u2022 Who are the customers?\n\u2022 How can we best reach them?\n\u2022 How can we ensure that we retain them?\n\u2022 Finance\n\u2022 How is the project being funded?\n\u2022 What is the best allocation of funds?\n\u2022 Can we support the debt under various economic conditions?\nCompetitive Response\nThere are two sides to the competitive response coin. What will you do if\na competitor comes out with a new product or service and starts to steal\nyour market share? Or what will your competitor do if you come out"}
{"90": "\u2022 Have the consumers\u2019 needs changed?\n\u2022 Has the competitor increased or expanded into new channels?\nResponses might include:\n\u2022 Analyze our current product and redesign, repackage, or move\nupmarket.\n\u2022 Introduce a new product.\n\u2022 Increase our profile with a marketing and public relations\ncampaign.\n\u2022 Build customer loyalty.\n\u2022 Cut prices.\n\u2022 Lock up raw materials and talent.\n\u2022 Acquire the competitor or another player in the same market.\n\u2022 Merge with a competitor to create a strategic advantage and\nbecome more powerful.\n\u2022 Copy the competitor.\nWhen planning a product launch or making a price change, you should\ntake into account competitive response. Too many firms seem to have a\nwait-and-see attitude, which may erase much of the advantage they had\nhoped to gain with the new strategy.\nTurnarounds"}
{"91": "\u2022 Analyze services, products, and finances.\n\u2022 Secure sufficient financing so your plan has a chance.\n\u2022 Review the talent and temperament of all employees, and get\nrid of the deadwood.\n\u2022 Determine short-term and long-term goals.\n\u2022 Devise a business plan.\n\u2022 Visit clients, suppliers, and distributors \u2013 and reassure them.\n\u2022 Prioritize goals and get some small successes under your belt\nASAP to build confidence.\nOn the next couple of pages, I\u2019ve listed key points that you should study,\nthink about, and memorize, ways to cut costs, risks, synergies, and\ndisrupting external factors. Not all of these will work for every case, but\nif you make yourself aware of them, it will give you more confidence\ngoing into the case and you\u2019ll be less likely to miss anything big. I want\nto thank Matt Chambers, a UCLA MBA student, for suggesting the\nidea and providing me with his well-thought-out lists to which I have\nadded some thoughts of my own.\n21 WAYS TO CUT COSTS\nBelow are a number of ways to cut costs in the areas of labor,\nproduction, and finance. Consider the client\u2019s strategic needs and think\nabout the long-term consequences under a range of various economic"}
{"92": "6. Convert workers into owners (if they have a stake in the\ncompany, they will work longer and harder and constantly think\nof ways to cut costs in ways they may not have done before).\n7. Contemplate layoffs.\n8. Institute across-the-board pay decreases.\nProduction\n9. Invest in technology.\n10. Consolidate production space to gain scale and create\naccountability.\n11. Create flexible production lines.\n12. Reduce inventories (JIT).\n13. Outsource.\n14. Renegotiate with suppliers.\n15. Consolidate suppliers.\n16. Import parts.\nFinance\n17. Have customers pay sooner.\n18. Refinance your debt.\n19. Sell nonessential assets.\n20. Hedge currency rates.\n21. Redesign health insurance."}
{"93": "7. Increased fuel or commodity prices\n8. Currency shifts\n9. Geopolitical events\n10. Workforce gaps, not enough skilled workers, higher wages\nbecause of low unemployment\n11. Population shifts\n12. Environmental disasters\nControlled risks\n13. Salesforce risks, shifting product priorities based on\ncommission, capping commissions\n14. Systems risks, particularly during a merger or conversion\n15. Inventory risks, too much or too little on hand, changing needs,\nstorage, tied up capital\n16. Public image after a crisis or an endorser\u2019s mishaps\n17. Other revenue streams collapsing\n18. Cannibalization\n19. Losing brand loyalty and consumer trust\n20. Lower product quality because of cost cuts\n21. Workforce turnover\n11 SYNERGIES\nSynergies \u2013 Broken down by revenues, costs, and financials"}
{"94": "a. Transport\nb. Warehouses\nc. Plant/production\nd. Suppliers\ne. Raw materials\n8. Bulk ordering\n9. Negotiating and buying power\nFinancials\n10. Selling off redundant assets\n11. Tax advantages\n7 DISRUPTING EXTERNAL/MACRO FACTORS\nDon\u2019t solve cases in a vacuum. Take into account disrupting external\nfactors that move markets, shrink market share, and play havoc with\nP&Ls. In the last few years we have seen \u2026\n1. Weather: Hurricanes, nor\u2019easters, polar vortex, wildfires,\nmudslides, floods\n2. Worldwide health pandemics: coronavirus, flu, measles\n3. Geopolitical events\na. Climate change\nb. Brexit\nc. Tariffs"}
{"95": "6 FACTORS THAT MOVE THE MARKETS\nWhile there are many factors that move individual stocks, these are five\nfactors that move the stock market, along with many of the disrupting\nfactors listed above:\n1. The Fed \u2013 change in interest rates, quant easing\n2. Higher prices\n3. Change in inflation\n4. Recession expectations\n5. Earnings\n6. Momentum/FOMO (Fear Of Missing Out)\n\u201cIF\u201d SCENARIOS TO REMEMBER\n( Sales Scenarios )\n\u2022 If sales are flat and profits are taking a header, you need to\nexamine both revenues and costs. Always start with the revenue\nside first. Until you identify and understand the revenue streams,\nyou can\u2019t make educated decisions on the cost side.\n\u2022 If sales are flat but market share remains relatively constant, that\ncould indicate industry sales are flat \u2013 and that your competitors\nare experiencing similar problems.\n\u2022 If your case includes a decline-in-sales problem, analyze these\nthree things:"}
{"96": "are climbing. You should investigate changes in product mix and\nmargins and if players are leaving the market.\n( Profit Scenarios )\n\u2022 If profits are declining because of a drop in revenues, concentrate\non marketing and distribution issues.\n\u2022 If profits are declining because of rising expenses, though,\nconcentrate on operational and financial issues \u2013 e.g., COGS (cost\nof goods sold), labor, rent, and marketing costs.\n\u2022 If profits are declining, yet revenues have gone up, then review:\n\u2022 Changes in costs\n\u2022 Any additional expenses\n\u2022 Changes in prices\n\u2022 The product mix\n\u2022 Changes in customers\u2019 needs\n( Product Scenarios )\n\u2022 If a product is in its emerging growth stage, concentrate on R&D,\ncompetition, and pricing.\n\u2022 If a product is in its growth stage, then emphasize marketing and\ncompetition.\n\u2022 If a product is in its mature stage, focus on manufacturing, costs,\nand competition."}
{"97": "Volume (the amount you produce) and costs are easier to change than the\nindustry price levels, unless all parties change their prices together (e.g.,\nairline tickets or gas prices).\nThe perfect strategy for the high-cost producer is one that convinces\ncompetitors that market shares cannot be shifted except over long\nperiods of time. The highest practical industry prices are to everyone\u2019s\nadvantage \u2013 i.e., price wars are detrimental, and everyone will profit\nmore by keeping prices high."}
{"98": "5 : PRACTICE CASES\nThis part of the book is divided into four sections. The first is the\n\u201canatomy\u201d section, including a breakdown of the two most popular types\nof cases, profit and loss (Harley-Davidson) and entering a new market\n(Coors Brewing Company). Motorcycles and beer: Some might say this\nis a recipe for disaster, but not us. As you go through these two cases, not\nonly will you read the dialogue between the interviewer and the student,\nbut you\u2019ll also read my analysis of how the interviewee is doing.\nThe second section is made up of five \u201ccase starts.\u201d Read the problem\nstatement, then take a minute to lay out your structure and make a list of\nquestions and concerns that you would want to ask the interviewer in a\nreal interview. Then compare your structure with mine. Remember, there\nare no right answers, so it\u2019s okay if our structures don\u2019t match up. Do\none or two at first, then save the rest for when you feel more confident.\nIn the third section, you\u2019ll find seventeen \u201cdialogue cases,\u201d similar to\nthose in the first section, but without my analysis. Read the problem"}
{"99": "public. The information concerning these companies may not be accurate\nand should not be used as reliable up-to-date data."}
{"100": "HARLEY-DAVIDSON\nInterviewer: Our client is Harley-Davidson. On Tuesday, its stock fell\nfrom $36 a share to $34 a share on news of declining profits and sales.\nWhat\u2019s going on and how can we turn this around?\nStudent: Our client is Harley-Davidson. Its stock fell from $36 to $34 a\nshare on news of declining profits and sales. We need to figure out\nwhat\u2019s going on and how to fix it. Are there any other objectives I should\nbe aware of?\nInterviewer: Yes, maintaining market share.\nAnalysis: The student was right to summarize the case; however, she\nwould have made a better impression if she had tried to quantify the\ncase. Instead of saying the stock price dropped from $36 to $34 a share,\nshe should have said that the stock dropped about 5 percent. Remember,\nso much of this is how you think \u2013 what goes through your mind when\nyou hear some numbers. She was also right to verify the objective and to\nask whether there were any other objectives she should be concerned\nwith. Without asking, she never would have known that maintaining\nmarket share was an issue.\nStudent: I have a clarifying question. Has Harley lost market share?"}
{"101": "Interviewer: Its revenues have dropped 8 percent last year. Three years\nago, profits were down 5 percent; two years ago they were down 8\npercent; and last year, they were down 6 percent.\nStudent: What constitutes success to the client?\nInterviewer: It wants to stop the bleeding and then some \u2013 a 5 percent\nincrease every year for the next five years.\nStudent: My initial thoughts are that revenues and profits are down\nbecause our demographics are aging out and that the company hasn\u2019t\nadjusted to changes in the market. I just want to take a moment to jot\ndown some notes.\nInterviewer: That\u2019s fine.\n[The student writes on her paper E(P=R\u2013C)M.]\nAnalysis: This is the framework you want to use for a P&L case. Inside\nthe parentheses is the classic \u201cprofits equals revenues minus costs.\u201d That\ntells us what\u2019s going on inside the company. But you want to look at\nexternal factors first. Is this a Harley problem or an industry-wide\nproblem? She starts with the E.\nStudent: I\u2019d like to start with external factors first. Can you tell me"}
{"102": "frame it yourself, there are fewer surprises. When you talk about the\neconomy, pick out the main factors that will affect Harley\u2019s business.\nLet\u2019s try it again \u2026\nStudent: I\u2019d like to start with some external factors. I\u2019d like to begin\nwith the economy. I know that the US unemployment remains low and\ngas prices are around $2.50 a gallon, so people should have disposable\nincome, which is good news. I know that the US dollar has been gaining\nstrength against the euro and pound, but is still fairly weak against Asian\ncurrencies, particularly the yen. And I know that interest rates are now\nclose to a 30-year low. (Note: US economy in 2020.) We also want to\nconsider other external factors. I assume that the coronavirus has had an\neffect on production and sales in China.\nAnalysis: Much better. Do you need to go into this much detail? Yes.\nYou\u2019ll see how everything she brought up will tie into her answer later\non. Make sure that you write everything down; it will give you\nsomeplace to go if you get stuck.\nInterviewer: Good. What\u2019s next?\nStudent: I\u2019d like to know about the motorcycle industry. Can you tell me\nthe size and growth rate of the industry, and what the trends have been?\nI\u2019d like to know about the different segmentations of the industry and"}
{"103": "the student to extract the information. Sometimes it takes only one\nquestion, and the interviewer does a data dump. It is then up to the\nstudent to sort through what\u2019s relevant now, what\u2019s just smoke, and what\nmight become relevant later. In this case, the interviewer is going to\nprovide a data dump.\nInterviewer: I have some industry information. Last year, the industry\ngrew by 5 percent; Harley shrunk by 6 percent; the small, less expensive\nmotorcycles and scooters grew by 8 percent. Female riders were up 12\npercent and now make up 10 percent of all motorcycle riders, but they\nconstitute only 2 percent of Harley riders. I have some market share for\nyou, but I want you to assume that each of these companies makes only\none model. For Harley, it is the big Harley Hog.\nStudent: Okay.\nInterviewer: The market leader is Honda, with 27 percent; Harley, with\n24 percent; Yamaha, 17 percent; Suzuki, 10 percent; Kawasaki, 8\npercent; BMW, 6 percent. The remaining 9 percent is made up of two\nscooter companies, Vespa and Suzuki. What else do you want to know\nabout the industry?\nStudent: It looks as though Harley is not growing as fast as the industry\noverall. That might be because its current customer base is aging out, and"}
{"104": "by 10 percent the year before and by 5 percent this year, then the 5\npercent looks very different to me than if the industry had gone from 2\npercent to 5 percent. Very few students ever ask for trends. Ask for them,\nand you\u2019ll stand out from your peers. Again, they are trying to learn how\nyou think. If you don\u2019t ask for trends, you\u2019re not thinking like a\nconsultant.\nInterviewer: What\u2019s next?\nStudent: I\u2019d like to look inside the parentheses to see what\u2019s going on\ninside the company. I want to look at revenues and profits. Then I\u2019d like\nto look at product mix. The client\u2019s production capabilities and locations,\ndistribution channels, and how strong the brand is across the world. But\nI\u2019d like to start with the revenues first.\n[The interviewer hands her this chart. She studies it.]"}
{"105": "have one motorcycle product, the Hog. I\u2019d be interested to know whether\nthe customer segmentation is the same internationally as it is in the US.\nInterviewer: What do you think?\nStudent: I think not. I would assume that the international rider is\nyounger than in the US.\nInterviewer: That\u2019s right.\nStudent: Do they have any other revenue streams? What are the major\nrevenue streams and how have they changed over time?\nInterviewer: Okay. I\u2019m going to give the four major revenue streams for\nY1 and Y2. The four major revenue streams are domestic motorcycle\nsales, international motorcycle sales, replacement parts, and garb.\nStudent: \u201cGarb\u201d being merchandise?\nAnalysis: If you ever get a phrase, industry jargon, or a string of initials\nthat you don\u2019t understand, ask for clarification. You don\u2019t lose any points\nfor clarification questions up front.\nInterviewer: Yes, garb is merchandise. For Year 1, domestic\nmotorcycles made up 45 percent, international 40 percent, replacement"}
{"106": "Domestic 45% 35%\nInternational 40% 40%\nReplacement Parts 10% 15%\nGarb 5% 10%\nStudent: It looks as if Harley customers are buying fewer new bikes,\nfixing up their old bikes, and buying some garb to make themselves feel\ngood and look bad.\nInterviewer: [Smiles]\nAnalysis: She did a great job. She kept it to one sentence and added a\nlittle humor to the interview as well.\nInterviewer: Okay, good. Let\u2019s talk about costs.\nStudent: Before we do, can I ask about volume? Do we have any\nnumbers on volume of bikes sold?\nInterviewer: We do. In Year 1, Harley sold 350,000 bikes and in Year 2,\nit sold 330,000 bikes."}
{"107": "currently have a good deal, but we are concerned about getting slammed\nwith high steel costs in 24 months. I just want you to keep that in the\nback of your mind.\nWhat I\u2019d like you to do now is to come up with some short-term\nstrategies that will help turn Harley around. By short term, I mean 18\nmonths or less.\nStudent: Okay. The first thing they should do is market to women.\nInterviewer: What would they market?\nStudent: They could design a new bike \u2026\nInterviewer: It is going to take more than 18 months to design,\nmanufacture, and distribute a new bike. Let\u2019s leave that for the long\nterm.\nStudent: They could market the Hog to women.\nInterviewer: They\u2019d have to be pretty big women. The Hog is a hard\nbike to handle. That\u2019s one reason only 2 percent of Harley owners are\nwomen.\nStudent: Then they could market the garb. Women like to look \u2026"}
{"108": "Analysis: Whenever you are answering a P&L case and they ask you for\nstrategies, you want to do two things: (1) write revenue-based strategies\nand cost-based strategies on your paper, and (2) ask for a moment to jot\ndown some ideas.\nBy writing down revenue-based strategies, you are showing the\ninterviewer that you are well organized and thinking two steps ahead. It\nis easier to think of some ideas if you are looking at a heading on a piece\nof paper rather than at a blank page. It also keeps you from ping-ponging\nbetween revenue-based and cost-based ideas. You want to present all the\nrevenue-based ideas first, and then the cost-based ideas.\nThe reason to ask for a moment to note down a few ideas is that it allows\nyou to think of ideas in any order, but present them in the right order. It\nalso gives you some place to go. Keep in mind that the interviewer has\nprobably given this case ten times. He knows every possible answer that\nyou can think of, and he\u2019s heard them all before. There is a good chance\nhe will cut you off as soon as he knows where your answer is headed. It\nis very difficult for people to drop a thought and then come up with a\nnew one right away. When cut off in mid-thought, people tend to panic,\nscramble, and then shut down. They can\u2019t think of another idea to save\ntheir lives.\nIf you take the time to jot down some ideas and you are cut off, you can"}
{"109": "Student: Can I take a moment to jot down some ideas?\nInterviewer: Absolutely.\nThe student notes revenue-based and cost-based strategies on her paper,\nthen writes out a few ideas.\nStudent: Okay. I\u2019d like to break these down by revenue-based and cost-\nbased. I\u2019ll start with revenue-based. Harley can raise the prices of its\ngarb and replacement parts because we know people will buy them\nanyway. It can start learn-to-ride programs to get new customers\ninterested. Then offer a rental program, if they don\u2019t already. We can also\nincrease international distribution channels.\nInterviewer: Where?\nStudent: In Asia, where the dollar is still weak.\nInterviewer: Okay, what else?\nStudent: Because interest rates are so low, we can offer financing\npackages and give high trade-in values to encourage customers to buy\nnew bikes.\nInterviewer: What else?"}
{"110": "more important, why you want to do it. If we leave the price the same,\nHarley will sell 330,000 motorcycles and make a net profit of $10,000\neach. If we discount the price, Harley will sell 440,000 motorcycles and\nnet $7,000 each. And if they raise the price, they will sell only 275,000\nmotorcycles but net $12,000 each.\nAnalysis: The student should take some time here to run the numbers.\nYou are better off taking a little extra time and getting the right answer,\nrather than rushing through and getting the wrong answer. Before you\ngive your answer, ask yourself if the number makes sense. If it doesn\u2019t,\ngo back and figure it out. You can\u2019t un-ring the bell. I\u2019d hate to see you\nlose a great opportunity over a silly math mistake.\nRemember, silence is okay as long as you are doing calculations, writing\nnotes, or drawing a chart.\nStudent: If we keep the price the same, then our net profit will be $3.3\nbillion. If we lower the price, our net profit will be $3.08 billion. If we\nraise the price, our net profit will also be $3.3 billion.\nInterviewer: So if we raise the price or leave it alone, we\u2019ll make the\nsame net profit. What do you want to do, and why?\nStudent: I\u2019d like to keep the price the same. You said market share was a"}
{"111": "be able to lay people off. In addition, the higher price will enhance the\nbrand. As far as your other concerns go, you told me that you plan to\nincrease international distribution channels where the dollar is still weak.\nIf we do that, any extra inventory can be shipped overseas and sold at the\nnew higher price. Garb sales tend to be higher when you enter a new\nterritory. So market share shouldn\u2019t be an issue.\nAnalysis: Ouch. The interviewer got right in her face even though she\u2019d\ngiven a well-thought-out answer. Luckily, she knows that this \u201cLet me\ntell you why you\u2019re wrong\u201d business is just a test. She keeps her\nemotions in check and does what the interviewer wants her to do: stick\nwith and defend her answer.\nStudent: You make an interesting argument; however, I don\u2019t find it\ncompelling enough. I can\u2019t believe that you can do all that within an 18-\nmonth period. Therefore, I think the best option in these economic\nconditions would be to keep the price the same, particularly if you want\nto increase market share.\nInterviewer: Okay, good. Give me two long-term revenue-based ideas\nand two long-term cost-based ideas.\nStudent: Can I take a moment to make a couple of notes?"}
{"112": "concerned about the price of steel. We can buy some steel futures to\nhedge against a steel increase. We could stockpile some steel at the\ncurrent price, and because we are developing a new bike, we can make\nmore parts from composites instead of steel. We could modernize the\nplant with new technologies and increase our production operations\noverseas. I\u2019m assuming that we manufacture some bikes in Asia.\nInterviewer: Yes, Thailand. Why don\u2019t you take a moment and\nsummarize the case?\nStudent: Our client is Harley-Davidson. Its stock dropped around 5\npercent on news of declining profits and sales. We looked at external\nfactors first and determined that it was more of a Harley problem rather\nthan an industry problem. Harley is out of step with the two fast-growing\nsegments of the industry, women and scooters. So we came up with some\nshort-term and long-term strategies on both the revenue and cost sides.\nAn example of a short-term revenue-based strategy is offering low\nfinancing to customers. On the cost side, we could refinance our debt. In\nthe long-term, we could produce a new bike geared toward women and\nyounger men, and acquire a scooter maker. Also on the cost side, we\ncould hedge steel prices and have certain new parts made out of\ncomposite instead of steel. If Harley follows these strategies as well as\nsome of the others we talked about, it should be on its way to higher"}
{"113": "COORS BREWING COMPANY\nInterviewer: Our client is Coors Brewing Company. For the last 60\nyears it\u2019s been advertising the fact that a key ingredient in Coors beer is\nRocky Mountain spring water. The CEO calls you in to his office and\nsays that Coors is considering entering the bottled water market. I want\nyou to analyze the market, identify any key issues, and make a\nrecommendation.\nStudent: Our client is Coors, and it\u2019s thinking of entering the bottled\nwater industry. Besides conducting an industry analysis and identifying\nmajor issues, are there any other objectives I should be concerned with?\nInterviewer: Yes. The CEO told the board of directors that he would\nincrease revenues by 50 percent in five years or he would resign.\nAnalysis: It was important that the student asked about other objective,\nor he would never have learned about the needed 50 percent increase in\nrevenues. If he hadn\u2019t asked about this, the interviewer would have had\nto feed him the information during the case, and he would have lost\npoints.\nStudent: I\u2019m assuming that\u2019s overall company revenues. So my initial\nhypothesis is that by entering the bottled water industry, Coors will"}
{"114": "Student: First I\u2019d like to look at Coors, then the water market and finally\nthe best ways for Coors to enter the market. And if we decide that Coors\nshouldn\u2019t enter the market, I\u2019d like to come up with an alternative plan\nthat will help the company reach its goal of increasing revenues by 50\npercent cumulatively over five years.\nI\u2019d like to start by looking at the company. Why does it want to enter this\nmarket? If it needs to increase revenues by 50 percent, I need to know\nwhat its revenues are today. I\u2019d like to know what its product line looks"}
{"115": "Next, I\u2019d like to look at the bottled water industry. What were its\nrevenues last year and how has it been trending over the last three years?\nDo we have any forecasts? I\u2019d like to know who the major players are\nand what market share they have? How do their products differ from\nCoors\u2019s? Are there any other companies that sell Rocky Mountain spring\nwater? Then the last thing I\u2019d like to know about the industry is whether\nthere are any barriers to entry or barriers to exit.\nFinally, I\u2019d like to think about how best to enter the market. Coors can\nstart from scratch and grow organically, it can buy its way in, or it can do\na joint venture. I\u2019d like to look at the advantages and disadvantages of\neach.\nAnalysis: He did a great job. By turning his notes toward the\ninterviewer, he brought her into the case and made her feel more like a\nclient and less like an interviewer.\nThe student started off analyzing the client company. Many students\nmake the mistake of starting off with the water industry. It\u2019s important to\nunderstand why Coors wants to enter. He also made a pretty exhaustive\nlist of company issues that he wanted to investigate, particularly what\nconstitutes success in the eyes of the client. It\u2019s critical to know this, so\nyou have a goal and can reality-test the client\u2019s response."}
{"116": "are forecast to remain that way for the next five years. Any new product\nwould only cannibalize sales from an existing product.\nStudent: What were its revenues last year?\nInterviewer: $5.2 billion.\nStudent: It would have to increase its revenues by [quick calculation]\n$2.6 billion for a total of $7.8 billion.\nInterviewer: That\u2019s right.\nStudent: I know it produces Coors and Coors Light; what other products\ndoes it have?\nInterviewer: Besides those two, it has Keystone and Keystone Light.\nThat\u2019s the beer you buy when you have only $1.50 left to your name. It\nalso produces Killian\u2019s Irish Red and Blue Moon.\nStudent: No non-beer products?\nInterviewer: Assume no. You asked about brand and distribution\nchannels, and we\u2019ll talk about them in a little bit. Who do you think\ndrinks Coors?"}
{"117": "Interviewer: Last year, the bottled water industry did $11 billion in\nrevenues. It\u2019s forecast to grow 5 percent a year for the next five years.\nFlat water makes up 96 percent of the market compared with sparkling\nwater\u2019s 4 percent. The three major players are Coke, Pepsi, and Nestl\u00e9.\nAssume that together they share 60 percent of the overall water market.\nThere are about a dozen other players, some international and some\nregional. Some of the regional players produce private-label water as\nwell as producing under their own names. They make up 36 percent of\nthe market. The final 4 percent is the sparkling water. The major players\nthere are San Pellegrino, Perrier, Voss, and Poland Spring Sparkling.\nAlso assume that there are three levels of water. The premium level\nconsists of brands like Fiji, Evian, and San Pellegrino, to name a few.\nThe big three, Coke, Pepsi, and Nestl\u00e9, dominate the national mid-level\nwater market. This is where Coors wants to enter, at mid-level. The\nlower tier is made up of regional waters, which are usually priced lower.\nWhat\u2019s next?\nStudent: I just want to take a minute and figure something out. Coors\nneeds $2.6 billion in five years. It thinks it can take 10 percent of the\nmarket in five years. Is 10 percent of the bottled water market greater\nthan or equal to $2.6 billion? If the market was $11 billion last year and\nit is growing 5 percent a year for the next five years, we need to figure"}
{"118": "Analysis: He almost made the math more complicated than it needed to\nbe. Interviewers make you do math without a calculator for two main\nreasons: to see how you think, and to see if you think before you speak.\nNo interviewer would want to sit there and watch that student struggle\nwith the exponents or do the same calculation over and over again if he\nhad decided to take 5 percent of 11 billion, then 5 percent of that number,\nand so on.\nThe other concern would be if he had trouble with the zeros and came up\nwith $140 billion instead of $14 billion; $140 billion doesn\u2019t make any\nsense, and even if he caught his mistake right after he had said it, you\ncan\u2019t un-ring a bell like that. You\u2019re not thinking before you speak \u2013 and\nif you do that in an interview, what are you going to do in front of a\nclient? I can\u2019t trust you. And if I can\u2019t trust you, I\u2019m not going to hire\nyou.\nInterviewer: What percentage of the bottled water market would we\nneed to get in order to reach the goal of $2.6 billion?\nStudent: Well, 10 percent is 1.4, and 20 percent is 2.8, so a little less\nthan 20 percent. Around 18 percent.\nInterviewer: Good. There is no way we\u2019re getting 18 percent of the\nbottled water market in five years. We\u2019ll be lucky to get 10 percent."}
{"119": "Analysis: He lost his confidence. The interviewer asked him a question,\ndidn\u2019t like the answer, and asked him the same question two more times.\nWhile the student was confident about his first answer, he stumbled\nwhen he was asked again and again. If you take this whole case\ninterviewing process and boil it down, the two most important things are\nstructure and confidence. Usually one follows the other.\nInterviewer: Good. Say that Coors is going to buy a regional player\ncalled Bulldog Water, out of Athens, Georgia, with 4 percent national\nmarket share. Now, Bulldog\u2019s water source is Athens tap water, and\nCoors\u2019s water is Rocky Mountain spring water. Two very different water\nsources about 2,000 miles apart. Coors will run two separate companies,\nwhich is fine. Coors will be in the middle tier and Bulldog in the lower\ntier. Besides the increase in market share, what are two other advantages\nto buying Bulldog?\nStudent: Expertise. Coors is new to the water \u2026\nInterviewer: Good. What else?\nStudent: Production facilities \u2026\nInterviewer: Coors isn\u2019t going to ship water by tanker 2,000 miles to\nsave on production costs. What else?"}
{"120": "Student: What will be the market acceptance of a beer company selling\nwater? That\u2019s the question. Is the Coors name too closely associated with\nbeer? I\u2019m betting that it is. Water is seen as pure and healthy, beer is not.\nI don\u2019t think mothers would like to see their kids drinking Coors.\nBesides that, Coke, which is one of the best-known brands in the world,\ndidn\u2019t call its product Coke Water; they called it Dasani.\nInterviewer: Coors ships beer to independent beer distributors all over\nthe US; should they use their beer distributors to distribute water?\nStudent: Yes. It will save on shipping and the cost of building a new\nnetwork.\nInterviewer: Let me tell you why you\u2019re wrong. Water is sold in three\ntimes as many places as beer. You can\u2019t buy beer in a vending machine,\nyou can\u2019t buy it at McDonald\u2019s, and you can\u2019t buy it at school. If Coors\nwants 10 percent of the water market, it has to have its product in every\npossible venue. Besides, I don\u2019t want a beer truck pulling up in front of\nan elementary school and unloading water. It sends a bad message. And\nmy last point is that if Coors is interested in increasing revenues, why are\nyou focusing on costs?\nStudent: Those are all valid points. I think you\u2019re right. They should\nbuild a new network of distributors."}
{"121": "national marketing campaign, maybe even gets a celebrity spokesperson\nand offers an introductory low price. How will Coke respond, or will it?\nStudent: To quote Churchill, I think Coke will try to strangle the baby in\nthe cradle. Coors is too much of a \u2026\nInterviewer: Wait a minute. You\u2019re comparing Coors to fascism?\nStudent: The quote was about Bolshevism: Strangle Bolshevism in its\ncradle.\nInterviewer: Are you sure? I was a history major at Williams.\nStudent: Positive. Coke will respond, and quickly. It won\u2019t want Coors\nto get a foothold in the market and start stealing market share.\nAnalysis: The case almost went off track there, but the student kept the\nfocus. It would have been easy to go off on a tangent about Churchill or\nWorld War II, but he took control and steered it back.\nInterviewer: Say Coors does all that. We are still $1.2 billion short. How\nare we going to close the gap? And keep in mind that we already spent\nour allowance and can\u2019t buy another company, and the margins aren\u2019t\nthere for exporting."}
{"122": "What are you going to tell him?\nStudent: I\u2019d like to take a moment. Because if I go in now, I\u2019m going to\nhave to tell him that we\u2019ll be short of his $2.6 billion revenue increase.\nI\u2019d like to think about how I can go in and tell him yes to everything.\nFirst, the economy will be in better shape five years from now. So most\nof that $50 million will come from increased beer sales. Second, we can\nlook at other alcoholic products, such as vodka, tequila, or alcopops \u2013\nlike Mike\u2019s Hard Lemonade. I\u2019d also like to point out that the original\nassignment was to analyze the bottled water industry and determine\nwhether it was a good idea for Coors to enter. The water takes us 95\npercent of the way there. Do we throw the baby out with the bathwater\neven if we are $50 million short?\nInterviewer: The CEO just walked in and wants your recommendation.\nWhat do you tell him?\nStudent: Yes, we enter the water market. We do it because the bottled\nwater market will continue to grow 5 percent a year, while beer sales will\nmost likely remain sluggish. We do it under a new brand, Rocky\nMountain Spring. There are risks involved. The biggest risk is that we\ndon\u2019t reach 10 percent market share because of competitive response and\nbecause of the association with the beer company. The likelihood of not"}
{"123": "to get there. Most students would have left it at \u201cIt looks promising, but\nwe\u2019re going to be $50 million short.\u201d In addition, he stated his\nrecommendation first, then backed it up with a plan. He got beat up\nalong the way, but he kept his cool and regained control of the interview."}
{"124": "CASE STARTS\nMany people struggle with getting started: thinking about the key issues,\nasking the right questions, and crafting their structure. This section was\ndesigned to help you with your case starts.\nKeep in mind that some frameworks are as simple as short-term and\nlong-term, and internal and external. Take a step back and look at the big\npicture. Don\u2019t try to force it and don\u2019t use the same structure all the time.\nI\u2019ve seen many students use P = R \u2013 C as their structure for every case.\nTo me, that\u2019s the same as using one of the cookie-cutter frameworks like\nthe 5Cs or 4Ps. These students have, in fact, turned the profit formula\ninto a checklist. By doing so, they\u2019ll lose points for lack of creativity,\nimagination, and intellectual curiosity.\nWith all cases, you should summarize the case, verify the objective, ask\nclarifying questions, and then lay out your structure. (See the \u201cThe First\nFive Steps\u201d section in the Ivy Case System chapter.) To get the most out\nof this chapter, read the case out loud, pretending it is a real interview."}
{"125": "CASE STARTS \u2013 CASES\nCS1: Power plants and chemical, paper, and textile factories have\npolluted about 70 percent of China\u2019s lakes and rivers. Water quality is a\nmajor concern for Chinese consumers. They are turning toward bottled\nwater as a safer alternative. The cities continue to grow, as does the\nbottled water market. Nestl\u00e9 wants to know how best to grow its market\nshare.\nCS2: Hacker Guard is the industry leader in the US identity-theft\nmonitoring market. It wants to do an IPO but has seen inconsistent\nprofits and losses six out of the last 10 quarters. How can it stabilize the\nturmoil and increase its profits? (The interviewer hands you a chart.)\nCS3: A Hong Kong company is acquiring a US video game maker. What\nconsiderations should be made?\nCS4: The CEO of a large Italian electronics firm has come out with a\nnew computer tablet, which is much like the iPad. How should it price its\nproduct?\nCS5: Our client is a global automaker headquartered in Detroit. Its motor\nparts division, with 20 percent industry market share, carries almost\n500,000 parts, options, and accessories for vehicle customization. The\nclient has not been profitable for several years and the CEO suspects that"}
{"126": "CS1: CHINA WATER\nPower plants and chemical, paper and textile factories have polluted\nabout 70 percent of China\u2019s lakes and rivers. Water quality is a major\nconcern for Chinese consumers. They are turning toward bottled water as\na safer alternative. The cities continue to grow, as does the bottled water\nmarket. Nestl\u00e9 wants to know how to grow its market share.\nSummarize the case. China\u2019s drinking water, particularly in the cities, is\nhorrible. The bottled water market is growing, and Nestl\u00e9 wants to\nincrease its market share. Are there any other objectives I should be\nconcerned with?\nAssume no.\nLay out your structure. First, you should ask about Nestl\u00e9 and its\nposition in the Chinese bottled water market \u2013 current market share;\ngrowth rate and WCS (what constitutes success in the mind of the\nclient?). What percentage of the market is the company hoping to get and\nis that realistic? How many brands does it sell under? What is its pricing\nstrategy? How is it priced compared with the market leaders?\nSecond, investigate the Chinese water market: growth rate, major players\nand their market share, changes in the industry (mergers? new players?"}
{"127": "competitors; and create seasonal balance (e.g., Vitamin C water in\nwinter).\nAnd fourth, I\u2019d like to know about Nestl\u00e9\u2019s market share in other key\nwater markets, including North America and Europe. How are those\nmarkets growing, and would it be wiser to allocate more resources there\nor in China?\nYour notes should look something like this:"}
{"128": "CS2: HACKER GUARD\nHacker Guard is the industry leader in the US identity-theft monitoring\nmarket. It wants to do an IPO but has seen inconsistent profits and losses\nin six of the last ten quarters. How can it stabilize the turmoil and\nincrease its profits? (The interviewer hands you a chart.)\nSummarize the case. Hacker Guard wants to do an IPO but needs to"}
{"129": "sizeable. What would cause Hacker Guard to lose $12 million in one\nquarter and make $18 million in another?\nLay out your structure. Because this is a P&L problem, immediately\nlay out the P&L framework and then build on it. E(P=R-C)M is the\ninitial framework. To start, look at external factors. Is this just Hacker\nGuard\u2019s problem, or are all companies in this space having a similar\nproblem? Touch on a few things in the domestic economy: the\nunemployment rate, interest rates, and anything else that might affect the\ndisposable income of a US household.\nNext, look at the identity-theft industry. What market share does the\ncompany have? What are the industry revenues, and what have the trends\nbeen in the last three to five years? Are other firms\u2019 P&Ls all over the\nplace, or is it just Hacker Guard\u2019s? Were there profit spikes at the same\ntime with all firms, meaning something happened in the market, such as\na large corporate breach like those at Sony, T.J.Maxx or Equifax?\nOnce you have an understanding of the industry, look inside the\ncompany. Start with the revenue streams. What are the major revenue\nstreams, and how have they changed over time? Customer segmentation:\nWho are its clients? Middle-class households? How many clients does it\nhave, and how much do they pay per month or per year? How long do"}
{"130": ""}
{"131": "CS3: HONG KONG VIDEO GAMES\nA Hong Kong company is acquiring a US video game maker. What\nconsiderations should be made?\nSummarize the case and verify the objective. This is a pretty short\ncase, so instead of repeating the question, just ask, \u201cBesides determining\nthe key factors to this acquisition, are there any other objectives I should\nbe concerned with?\u201d\nAssume no other objectives.\nLay out your structure. This is a merger and acquisition case. Break it\ninto five main buckets: company, current market analysis, due diligence\nand risks, costs, and exit strategies.\nYour notes might look something like this:"}
{"132": "Company. I\u2019d first ask about the Hong Kong Company. Who are they?\nIs it a private equity firm or a competitor? Why does it want to buy this\ncompany? Is it for access to the US market? A special patented\ntechnology? To increase its market share? Or to take advantage of cost-\nsaving synergies? What else does it own? Does it own other gaming\ncompanies?\nIndustry. What\u2019s the current industry like? Is it growing, and what are\nthe trends and forecasts? Who are the major players, and what market\nshare do they have? How are their products different from those of the\ntargeted company? What is the industry segmentation, and have there\nbeen any changes taking place in the industry? Mergers? New players?\nDue diligence. I\u2019d want to look at the overall economy. Is the targeted\ncompany an industry leader? How secure are its customers, suppliers,\ndistributors, and top management? Will the cultures of the two\ncompanies mesh well? Are the margins what we anticipated? What will\nbe the competitive response to our buying this video game maker?\nExit strategies. What\u2019s the game plan if this fails? Do we sell, spin off\nthe company and do an IPO, or break it up and sell off the pieces?"}
{"133": "CS4: ITALIAN COMPUTER TABLET\nThe CEO of a large Italian electronics firm has come out with a new\ncomputer tablet, which is much like the iPad. How should it price its\nproduct?\nSummarize the case and verify the objective. Ask if there are any\nother objectives \u2013 such as the reason the CEO wants to enter this market.\nLay out your structure. Your notes might look something like this:\nFirst, I\u2019d like to know about the company. Why does it want to enter this\nmarket? How big is the company revenue-wise, and what have the trends"}
{"134": "Third, I\u2019d look at four pricing strategies. The first one is company\nobjective. This ties in with what constitutes success in the mind of the\nclient. Is it looking for profits and wanting to price the tablet at a\npremium (like Apple)? Is it looking for market share (like Samsung) and\nthus pricing it a little lower? Or is it interested in market positioning (like\nAmazon), selling at around cost and hoping to make profits on the\nancillary products? The second is competitive analysis. Who is our\ncompetition, how do their tablets differ from ours, and how much do\nthey charge? Are there any substitutions? How will the competitors\nrespond to our entering their market? Third is cost-based pricing. What\ndoes it cost our client to manufacture, distribute, and market? Make sure\nall those costs are baked into your number. What sort of margin does the\ncompany make on its other products? The fourth is price-based costing.\nWhat is the market willing to pay? Is it more than our costs?"}
{"135": "CS5: GLOBAL MOTOR CARS\nOur client is a global automaker headquartered in Detroit. Its motor parts\ndivision, with 20 percent industry market share, carries almost 500,000\nparts, options, and accessories for vehicle customization. The client has\nnot been profitable for several years, and the CEO suspects that the\ncompany\u2019s high degree of vertical integration is hurting it. The client\nmakes about 80 percent of its own parts compared with 40 percent at its\nprimary competitors. The CEO has asked for our help. How would you\napproach this issue?\nSummarize the case and verify the objective. Repeat the question to\nmake sure you and the interviewer are on the same page. Then verify the\nobjectives. In this case the objectives are to come up with a strategic plan\nto make the company profitable. While you might think that this is a\nP&L case, I\u2019d approach it a little differently.\nLay out your structure. Your notes might look something like this:"}
{"136": "the parts division revenues and their trends, and what percentage of the\noverall company revenues do they account for? Has the parts division\nbeen gaining or losing market share over the last three years? Have there\nbeen changes in costs? What constitutes success in the mind of the\nclient?\nAsk questions about the industry. What have been the overall trends\nfor the auto industry? Have there been any changes in the parts industry,\nnew players, or new technology? Strategies. I\u2019d break this down into\nfour buckets: costs, strategic value, alternatives, and exit strategy.\nCosts. I\u2019d look at both external costs (the overall economy, interest rates,\nfuel costs, transportations costs, etc.) and internal costs (union wages,\nraw materials, etc.).\nStrategic value. What is the true strategic value of making our own\nparts? It might have been cost-saving initially, but that may have\nchanged. Is there a patented technology that only our cars have?\nAlternatives: This could mean reducing the number of individual parts\nper car or having new models using as many common parts as possible.\nThis is tricky; some believe this is what got GM into trouble because all\nits cars seemed the same. Would it be cheaper to buy our parts from\nanother vendor?"}
{"137": "CASE PRACTICE\nWhile nothing beats live practice, the next best thing is reading as many\ncases as possible. This builds up an archive in the back of your mind and\nallows you to draw from one case to help you answer another. The best\nway to use this section is to read the problem statement, then write out\nyour structure. Your answer and my answer will probably not match up.\nThere is no way for you to predict the added variables and twists and\nturns in the case. Once you draw your structure, sit back and read the\ncase, making notes in your journal of anything you may not have thought\nabout. Then reread it. You\u2019ll pick up more the second time through.\nCASE LIST\n+ THE SHOW\u2019S OVER \u2013 NETFLIX\n+ XHEAD\n+ SNOW JOB\n+ FLATLINE\n+ BLADE TO BLADE\n+ DISPOSABLE REVENUES"}
{"138": "+ THE BEST SHOT"}
{"139": "THE SHOW\u2019S OVER \u2013 NETFLIX\nInterviewer: Our client is Netflix. The company wants us to develop a\ngame plan for how and when it should shut down its DVD-by-mail\nmembership services.\nStudent: Besides coming up with a game plan to shutter its DVD-by-mail\nservice, are there any other objectives I should consider?\nInterviewer: No.\nStudent: I have two clarifying questions. Why do they want to shut it\ndown?\nInterviewer: Why do you think?\nStudent: I can think of a number of reasons. It\u2019s not profitable anymore.\nThe technology is becoming obsolete. It wants to consolidate its customer\nbase to streaming. Maybe to reduce its footprint and focus on its core\nbusiness. Or perhaps it needs warehouse space for other projects.\nInterviewer: All good reasons. Then why do you think Netflix hasn\u2019t\nshut it down already?\nStudent: Because it is still profitable."}
{"140": "[The student takes two minutes to draw out three buckets.]\nStudent: I\u2019d like to break this down into three buckets. Current status,\nhow to shut it down, and when to shut it down.\nUnder current status, I want to know how the program works, the\ndifferent pricing options, the number of members, and the membership\ntrends. The revenues and profits for the last three years for both DVD-by-\nmail and domestic streaming. The margins. The percentage of revenues\nthat the DVD rentals make up of Netflix\u2019s total domestic revenues, and\nhow that has changed over time. Have we consolidated or closed any of\nthe warehouses?\nUnder how to shut it down, I have five areas to consider: customers, staff,\nthe warehouses and equipment, the DVDs, and barriers to exit.\nCustomers. Is it a shrinking customer base? Where are they going to go?\nCan we convert them to streaming? How can we make up the lost revenue\nfrom the program? What are the steps we could take to unwind the\nbusiness, such as not renewing or signing up new memberships?\nStaff. Can we transfer or reassign workers to other areas? How many\nworkers would we have to lay off and what would that cost in terms of\ndollars and company morale?"}
{"141": "Student: This is the product that built the house of Netflix. Is Reed\nHasting going to hesitate to pull the trigger because he has an emotional\ntie to this product?\nInterviewer: Interesting.\nStudent: And finally, when?\nIf R=C, can we calculate the time when it will no longer be profitable?\nIf it needs resources currently allocated to the DVD program to fund\nother projects or develop new content.\nCan we encourage members to leave, either by limiting new titles or\nraising prices? What would be the consequences of either move?\nInterviewer: You wanted prices and membership. The price per plan for\nDVD-by-mail varies from $4.99 to $14.99 per month according to the\nplan chosen by the member. DVD-by-mail plans differ by the number of\nDVDs that a member may have out at any point.\nThe interviewer hands the student a chart."}
{"142": "Student: Wow. The DVD-by-mail program is bleeding members. But I\u2019m\nalso thinking that each member must be profitable, maybe even more\nprofitable than the streaming member. Otherwise Netflix would have shut\nit down by now. Any information on the profit margins?\nInterviewer: Now what do you think?\nHe hands the student the chart.\nStudent: Profit margins are 55 percent for DVD members. That\u2019s\nsurprising. It looks like the streaming member profit margins are around\n37 percent, which is also very good. What surprises me is that the profit"}
{"143": "Interviewer: Anything else?\nStudent: Yes. There was probably a decrease in content expense. Fewer\nmembers means fewer copies of the movie are needed. Also, if fewer\nDVDs are mailed out, that means Netflix can reduce the number of\nwarehouses and staff.\nInterviewer: It has reduced the number of warehouses \u2013 from 50 to 17 \u2013\nwhich has led to longer delivery times, with fewer DVDs and fewer\nwarehouses. How do you think Netflix decides who gets the new DVD\nfirst?\nStudent: First come, first served?\nInterviewer: Are you asking me or telling me?\nStudent: Telling you.\nInterviewer: Well, you\u2019re wrong. Traditionally Netflix gave priority to\nsubscribers who rented the fewest number of discs in a given period, with\nthe rationale that because they paid the same price for the service but used\nit less, they should have high priority to get the disc they wanted. But this\ndoesn\u2019t work anymore. Can you tell me why?\nStudent: Because this group would probably be slower returning the"}
{"144": "Student: That\u2019s money that could be funneled toward building content on\nthe streaming side.\nInterviewer: When it does shutter the service, what should Netflix do\nwith all the DVDs?\nStudent: I can think of a few ideas. Netflix could send them to its best\nDVD customers if they sign up for the streaming service. It could donate\nthem to libraries across the country or sell them online either in the US or\nabroad. Or simply destroy them so they don\u2019t end up on the black market.\nInterviewer: I want you to look at my last chart, take two minutes, and\nmake a recommendation about when Netflix should shutter its DVD-by-\nmail service.\n[He hands the student the chart.]\nAll numbers are in millions."}
{"145": "will be reduced to a point that it is not profitable or can\u2019t make a\nmeaningful contribution to the bottom line. In the short term, the next\nsteps would be to continue cutting content and adjusting the algorithm\nthat determines which subscribers get new releases first. In the long term,\nI\u2019d recommend revisiting this issue in three years to reassess the situation.\nAnd we can help you with that.\n( Case Takeaways )\n\u2022 The student\u2019s structure was good and complete. It wasn\u2019t a\ntraditional structure but one crafted to the situation, as it should be.\nHe would have looked ridiculous if he used the one-structure-fits-\nall approach.\n\u2022 He didn\u2019t state a hypothesis.\n\u2022 The student handled it well when the interviewer threw his question\nback at him and when the interviewer told the student he was\nwrong.\n\u2022 His recommendation was straightforward, definitive, and no-\nnonsense.\n\u2022 His math was solid during the analysis of the last graph."}
{"146": "XHEAD\nInterviewer: Our client is XHead, a youth-oriented company that\ndesigns and sells edgy headphones, earbuds, and docking speakers, as\nwell as apparel and accessories geared toward action sports enthusiasts.\nThey are thinking about getting into the wearables market. We\u2019ve been\ntasked to analyze the market and make a recommendation.\nStudent: Our client is XHead, an electronic and apparel company geared\ntoward action oriented youth. It is thinking about entering the wearables\nmarket. We\u2019ve been tasked to analyze the market and make a\nrecommendation. Are there any other objectives or goals I should be\nconcerned with?\nInterviewer: It wants to increase its revenues by 10 percent, and get 2\npercent market share.\nStudent: What are its current revenues?\nInterviewer: $160 million.\nStudent: That means a $16 million increase, while getting a 2 percent\nmarket share."}
{"147": "I\u2019d like to get an understanding of the company, then I\u2019ll look at the\nwearable market. And finally look at the different ways to enter the\nmarket.\nThe company. While we know that the revenues were $160 million last\nyear, I\u2019d like to know the growth rate of the company \u2013 so, revenues and\nprofits for the last three years. I\u2019d like to know about its customers. You\nsaid youth. What ages and genders are we talking about? Product mix\nand production capabilities? What type of wearables is it thinking about?\nHow strong is its brand, and what are its main distribution channels?\nNext, I\u2019ll look at the wearables market. Market size, growth rate, and\ntrends. I need to know the market size, so I can see if 2 percent is greater\nthan the 16 million. What are the industry drivers? I\u2019d like to understand\nthe different products within the industry and their customer\nsegmentations by product. Margins are always important. Any major\nindustry changes, mergers, or new technology? We should look at\nindustry distribution channels to see if they match up with the client\u2019s\ncurrent distribution channels. Of course, who are the major players, what\nmarket share do they have and how will our product be different?\nFinally, are there any barriers to entry?\nWhat is the best way for XHead to enter the market? By organic growth?"}
{"148": "29. Most of their products are outsourced. They have limited\nmanufacturing capability.\nIts brand is well known. XHead and Skullcandy are two of the biggest\nnames in this sector. XHead retains its street cred by sponsoring both\nskate- and snow boarders and BMX bikers. Distribution channels. The\ngear was originally sold at specialty shops but can also be found\nnationwide at Target, Walmart, Best Buy, and of course through the\ncompany\u2019s website. Its products are sold in 82 countries.\nStudent: That tells me that it is a successful niche marketing company.\nThat R&D costs should be low because they know their base, they\nprobably aren\u2019t out to develop a new product, just put its logo on existing\nproducts that can be outsourced. I\u2019m I missing something?\nInterviewer: We\u2019ll see. Let me tell you about the market. The smart\nwearables market was valued at 216 million units last year and is\nexpected to reach 615 million units by 2025. Demand will be driven\nmostly by increased innovation and new product categories, like smart\nfabrics and hearables.\nMajor players: Apple, Fitbit, Fossil, Medtronic and Transcend\nInformation, which makes product for six of its brands."}
{"149": "Student: The client\u2019s market is all of the 12.7 percent plus five years into\nthe second category at 3.3 percent a year, which is about 16 or 17\npercent. I\u2019m going to round it up to 20 percent and then cut it in half\nbecause most of our customers are male. Overall, our customer base\nwould make up 10 percent of the market. And the client is trying to get 2\npercent. Do we have a dollar amount for the market?\nInterviewer: $25 billion.\nStudent: Great. Ten percent equals 2.5 billion, so 1 percent equals 250\nmillion. Two percent is $500 million. Something is off. We need to do\nsome reality testing. The $16 million might not be a problem, but the 2"}
{"150": "Student: Okay, out of 216 million units, 141 million units were\nsmartwatches. That\u2019s around \u2013 well, half would be 108 million and 10\npercent would be 21 million, so it would be around 65 percent. I\u2019m not\nsure of the client\u2019s product mix for wearables but certainly a smartwatch\nof some sort would be important.\nInterviewer: What else?\nStudent: You said that the client was in 82 countries. And the\nsmartwatch numbers are global. Where are the hottest markets?\nInterviewer: They fall into three categories of growth rates \u2013 high,\nmedium, and low. Asia is high; North American and Europe are medium;"}
{"151": "Interviewer: Assume around the same.\nStudent: One more market question. Do you have any information\nbroken down by income level?\nInterviewer: Yes. Low income makes up 45 percent, middle income 30\npercent, and high income 25 percent.\nStudent: Okay, that\u2019s good news. Seventy-five percent of the market is\nlow- to middle income. Which is where I\u2019d guess is our market. Let\u2019s\ntalk product mix. Does the client have an idea what products it wants to\noffer?\nInterviewer: Yes. Smartwatches, fitness trackers, and helmets.\nStudent: Helmets?\nInterviewer: Remember that a big portion of our clients are boarders\nand bikers. Measuring the impact on your skull if you go down. The\ntechnology is called Multi-Directional Impact Protection System (MIPS).\nYou see it in football helmets. They would also measure a variety of\nbiometrics. Really cool stuff. They would also have a built-in camera\nand headphones. The one concern is that the helmets would be pricey.\nStudent: My third bucket was how best to enter the market. You"}
{"152": "Student: Even though Asia, particularly China, is 2.5 times bigger than\nthe US market, I\u2019d focus on North America and Europe. I\u2019m assuming\nthat there are many wearables manufacturers in China, not to mention the\nknockoffs. It would be hard to compete, and it would be expensive. If we\nwere able to gain a foothold, the pirates would jump in. I\u2019d focus on\nNorth America and Europe. They make up 75 percent of our market, and\nChinese brands don\u2019t sell particularly well outside of Asia.\nInterviewer: Currently, the prices range from $40 to $160. Assume that\nthe average price of the new products would range from $60 to $240,\nwith most of the sales being on the lower range. Could we reach our\ngoals?\nStudent: I\u2019m going to make the assumption that the average sale price of\nthe wearables would be $120. To get $16 million in sales, we would have\nto sell around (scratches out some math) 130,000 items per year, \u2013\n11,000 units per month. If we estimate the average price is $100, that\nmeans they sell around 1.6 million units of current products a year or\naround 130,000 per month. Ten percent of that is 13,000, and we\u2019re\nlooking at 11,000 units. So while it might be 10 percent of the revenues\nit is only around 8 percent of the units sold.\nInterviewer: The CEO just walked in. What would be your"}
{"153": "The risks are, one, it is a highly competitive market with several\ndominant brands, including Apple. This is a high risk, but our unique\ncustomer base would be looking for something edgier geared toward\nthem. We won\u2019t get the 2 percent market share; we determined this to be\nunrealistic. Another risk is that we are not focusing on the fast-growing\nChinese market. I think that this is less of a concern because the\ncompetition in that market is even higher and the piracy rate even\ngreater. However, American products enjoy a certain cachet in the Asian\nmarkets, particularly China. The final risk is finding a manufacturing\nplant to outsource our production to. We need to carefully vet the\ncompany to make sure that it can meet our high-quality standards and the\nexpected demand. This is high risk because of the growing market, and\nothers will be looking to outsource as well. Because we outsource all our\nproducts, we should be great at vetting potential manufacturing partners\nand have strong industry connections.\nNext steps. In the short term, we need to start designing the products,\nfinding suitable manufacturers for each product, and developing a\nmarketing plan for the North American and European markets. In the\nlong term, we need to investigate our other markets \u2013 Africa, the Middle\nEast and South America \u2013 for possible distribution.\n( Case Takeaways )"}
{"154": "SNOW JOB\nInterviewer: Snow Shovels Inc. (SSI) imports and distributes snow\nshovels. The snow shovel market is relatively stable. As expected, sales\ndepend on demand, and demand depends on weather. SSI has to order its\nshovels four months in advance. How many shovels should they order?\nStudent: SSI imports and distributes snow shovels. They have to order\ntheir product four months in advance. They want to know how many\nshovels they should order.\nInterviewer: Yes.\nStudent: Besides deciding how many shovels to order, are there any\nother objectives I should be concerned about?\nInterviewer: Yes. The goal is to maximize profits with the lowest level\nof risk and the least amount of inventory on hand.\nStudent: What areas of the country does the company cover?\nInterviewer: Just Wellesley, Massachusetts.\nStudent: I\u2019d look at expanding into other areas."}
{"155": "Interviewer: Two thousand.\nStudent: What was the weather like last year?\nInterviewer: Cold, with lots of snow.\nStudent: Did they have any inventory left over from the year before?\nInterviewer: Yes, 500 shovels.\nStudent: Is it fair to assume that they sold all 2,500 shovels this past\nyear?\nInterviewer: Yes.\nStudent: So there is no leftover inventory?\nInterviewer: That\u2019s right. SSI hates to carry over inventory.\nStudent: Could we have sold more? Were there orders left unfilled?\nInterviewer: Yes. It\u2019s fair to say that if it\u2019s a cold winter, SSI will sell\n3,000 shovels. If it\u2019s a mild winter, they will sell only 1,000.\nStudent: Do we know what the forecast is for the coming winter?"}
{"156": "Student: So we make $10 a shovel. Let\u2019s figure that 40 percent of 3,000\nequals 1,200 and 60 percent of 1,000 equals 600. If you add them\ntogether, that equals 1,800 shovels.\nInterviewer: That\u2019s it? That\u2019s your answer? Why does everyone come\nup with 1,800 shovels? I\u2019ve given this case five times today, and\neveryone has come up with 1,800 shovels. Think about the information I\ngave you. Think about the objective.\nStudent: I\u2019d like to look at the estimated value. If we order 1,000\nshovels and assume that no matter what kind of winter we had, we would\nstill sell 1,000 shovels, then the estimated value would be \u2026\nIf we order 2,000 shovels and there is a 60 percent chance of a mild\nwinter in which we would sell only 1,000 shovels, and a 40 percent\nchance of a cold winter in which we would sell all 2,000 of them, the\nvalue would be \u2026"}
{"157": "Based on the numbers above, and assuming that you\u2019re relatively risk\nadverse, I would have to suggest that you order 1,000 shovels. You are\npretty much guaranteed a $10,000 profit. If you order 3,000 shovels, you\nhave only a 40 percent chance of making $12,000 and a 60 percent\nchance of losing $6,000.\nInterviewer: Good point. Can you graph it?\nStudent: Sure. It would look like this."}
{"158": "Comments: This case is all about risk. The student tried to come to a\nfast answer, then pulled back and quickly re-thought this strategy based\non the interviewer\u2019s reaction. Estimated value may not be common\nknowledge to a lot of non-MBAs, so go back and reread the answer.\nNote: I\u2019ve given this case 40 times to students, and only two got the\ncorrect answer. On a scale of 1 to 10, this is probably a 9."}
{"159": "FLATLINE\nInterviewer: Our client is a $1 billion-dollar US-based medical device\nmanufacturer with products strictly in the cardiovascular space. Their\nmost profitable product \u2013 in terms of margin \u2013 is an artificial heart valve\nfor a procedure called Transcatheter Aortic Valve Replacement (TAVR),\nwhich is used to treat severe aortic stenosis, a disease that affects\nindividuals over the age of 85. TAVR is used by only a few hundred\nspecially trained doctors. Revenues associated with this device totaled\n$250 million last year, but profits have remained flat over the past two\nyears. The CEO has asked us to investigate why profits have been flat,\ndetermine whether there is any upside, and brainstorm about how we can\nincrease profits.\nStudent: Could I have a few minutes to restate the case facts as well as\nmy understanding of our objectives? The CEO of this medical device\nmanufacturer wants us to explain why profits have remained flat for a\nhigh-margin product that generates about $250 million in revenues\nannually, or about 25 percent of the company\u2019s revenues. He also wants\nus to determine whether there is any upside, what that upside is, and how\nwe can capture it. Are there any other goals that we should be concerned\nwith?"}
{"160": "[Student takes 60 seconds to draw out his notes and then turns them\ntoward the interviewer.]\nOkay, I don\u2019t see any external factors at play here, but I\u2019ll want to take a\nlook at the market. And because the primary goal is to increase profits,\nour framework requires us to explore revenues and costs because profits\nequals revenues minus costs. Revenues, in turn, are price times quantity.\nCosts can be broken into fixed and variable. I\u2019d like to explore these five\nlevers.\nInterviewer: That looks like a good framework.\nStudent: I\u2019d like to start by looking at the price. How do our prices\ncompare with our competitors\u2019 prices?\nInterviewer: Actually, our client is the only manufacturer of a device for\nTAVR. There are two other competitors on the horizon, though; one is\nabout 12 months away from launching its product, and the other is about\n16 months away.\nStudent: That\u2019s a great place to be. What opportunity do we have to raise\nour prices? Given that the audience is over the age of 85, I assume\nMedicare pays for it. Is the device fully reimbursable or partly\nreimbursable by Medicare?"}
{"161": "doctors who are trained in the procedure. I suspect the number of devices\nis therefore limited because of the number of trained doctors.\nInterviewer: Exactly. Here\u2019s a chart of the number of devices we sold\nlast year.\nStudent: Thank you. Is it accurate to say that one procedure equates to\none device?\nInterviewer: Yes, that\u2019s correct."}
{"162": "[The student walks the interviewer through the math of 20 x 60 = 1,200\nplus 30 x 30 = 900, plus 20 x 40 = 800, etc. Then he adds them all up.]\nThat makes 5,000 procedures or devices. Revenues of $250 million, that\u2019s\n$50,000 per device.\nInterviewer: That\u2019s correct. What else does this chart tell you?\nStudent: It appears that there are a lot of procedures performed by a few\ndoctors and a lot of doctors who perform very few procedures. It\u2019s quite\nskewed.\nInterviewer: Exactly. Why do you think that is?\nStudent: I think this might be a function of two things: the geographic\ndistribution of 85-year-olds and the availability of trained doctors in those\ngeographies.\nInterviewer: That\u2019s probably true. So what do you want to do?\nStudent: Let\u2019s table quantity for a second and focus on costs. Has\nanything changed on the cost side that could be affecting our profits?\nInterviewer: Actually, we\u2019ve already compressed both fixed and variable\ncosts as much as possible. There really isn\u2019t any more room there."}
{"163": "Interviewer: Good question. Here is a chart that shows the distribution\nby city.\nStudent: Interesting. So this shows the number of cities that have our\npopulation. In other words, there are 20 cities that have 10,000 people\nover the age of 85, 20 cities with 20,000 people, and so on. Is that\ncorrect?\nInterviewer: You\u2019re correct. What else does this chart tell you?\nStudent: As one would expect, as the population of adults over 85 goes\nup, the number of cities with that population goes down. This tells us the\npopulation, but it doesn\u2019t tell us the number of devices we can expect."}
{"164": "are 20 cities with 10,000 people. We expect 10 devices in each of these\ncities for a total of 10 x 20 = 200 devices.\n[The student takes two minutes to write up the chart, walking the\ninterviewer through the calculations. He turns the chart toward the\ninterviewer.]\nHere is a chart with that data:\nA > 85 Total Predicted\nPop. # Procedure\n(000) Cities Volume\n10 20 200\n20 20 400\n30 10 300\n40 14 560\n50 16 800\n60 9 540"}
{"165": "Interviewer: What do you make of this?\nStudent: There are two things that could explain this. First, the ratio you\nmentioned (10 devices for every 10,000 85-year-olds) might not be\naccurate. Secondly, it\u2019s possible that there are some cities where we\u2019ve\nbeen able to exceed this ratio. Have we analyzed what our penetration is\nin the cities where doctors have performed the procedure in terms of\ndevices per 10,000 targets? That is, are we above or below the 10 per\n10,000 ratio?\nInterviewer: Great question. We looked at this and came up with the\nfollowing. What does this tell you?\n[Interviewer shows the student this chart.]"}
{"166": "each, and three cities where we\u2019re doing zero devices. Is it safe to assume\nthat there are sufficient doctors in those geographies?\nInterviewer: Yes, assume that the supply of doctors is evenly distributed\nper 10,000 85-year-olds nationwide. How would you calculate\nheadroom?\nStudent: I would calculate headroom by \u201cmoving\u201d all the under-\npenetrating cities up to the line.\nInterviewer: Based on this information, what\u2019s the headroom for growth?"}
{"167": "Student: I would start by looking at what we did in the geographies\nwhere we\u2019ve over-penetrated. For example, did we do any marketing in\nthose geographies?\nInterviewer: Actually, we partnered with hospitals in those geographies\non outreach efforts to encourage the elderly to come in and get tested for\naortic stenosis. That way, we could get them the procedure quickly. Can\nyou think of other things we can do to drive procedures?\nStudent: One thing I would do is focus on educating referring physicians.\nBecause someone would go to a specially trained doctor only after seeing\na referring physician, it\u2019s important to educate them on this procedure.\nInterviewer: So can you sum things up for the CEO?\nStudent: We have an opportunity to increase revenues by 25 percent, or\napproximately $50 million. Two ways we can achieve this growth are by\nincreasing patient outreach efforts in cities where we are under-penetrated\nand by increasing awareness among referring physicians.\n( Case Takeaways )\nThis was a math-heavy case. The student \u2026\n\u2022 quantified related numbers as percentages.\n\u2022 walked the interviewer through his calculations."}
{"168": "BLADE TO BLADE\nInterviewer: Our client is a small, privately held lawn mower\nmanufacturer in lower Alabama. The company makes low-end mowers,\nthe type marketed to low-income households. It has 1 percent of the\nnational market. There are 25 national competitors.\nLike the rest of the industry, the company\u2019s sales have been flat. But its\nbigger problem is that the company it has been purchasing its engines\nfrom for the last 40 years suddenly called up and said it was filing for\nbankruptcy and closing its doors.\nThe client has looked around for another engine manufacturer, but the\nonly one it could find is charging 40 percent more than what our client\nhas been paying. The reason for this is that the client has been using a\nside-mounted engine rather than the flat-top engine most other mowers\nuse. They\u2019ve been using this side-mounted engine for 40 years. It\u2019s\ninexpensive and reliable, a real workhorse. You know the old saying,\n\u201cThey don\u2019t make things like they used to.\u201d Well, this engine is \u201clike\nthey used to.\u201d In addition, the side-mounted engine has become our\nclient\u2019s signature over the years.\nHere is a chart of the lawn mower industry. As you can see, there are 25"}
{"169": "ENG\nOur client is Company X. Its biggest competitor is Company Y. Together\nthey share the lowest 2 percent of the market. Company Y buys its\nengines from the same engine maker (ENG), so it is in the same boat. X\nand Y have been competing against each other in this market for the last\n40 years, and there is no love lost between them. If both mowers were in\nthe room, you couldn\u2019t tell them apart, except that one has a big X on it.\nThese are basic mowers. You pull the cord to start the mower, push it,\nand the grass blows out the side. It\u2019s one step above Fred Flintstone\npushing Dino around the front yard.\nIf the client raises its price by 12 percent, that increase will push it up\ninto the next level, where there are already ten companies fighting over a\n40 percent market share. More important, there are big names in that\ngroup, including Honda, Lawn-Boy, and John Deere. Their mowers have\na lot more bells and whistles than ours. We can\u2019t compete in that market\nwithout drastic redesign and upgrades.\nI know I have given you a lot of information, but just one last thing: The\nclient uses just in time inventory, which is normally a good thing;\nhowever, because it didn\u2019t see this bankruptcy coming, it has only"}
{"170": "objective, I imagine, is not only to survive this crisis but also to maintain\nor increase sales and market share. Are there any other objectives I\nshould be concerned with?\nInterviewer: No.\nStudent: Can I ask a few clarifying questions?\nInterviewer: Certainly.\nStudent: What are its margins?\nInterviewer: Twenty percent.\nStudent: What is the manufacturing cost of the mower, and what\npercentage does the engine account for?\nInterviewer: You can assume that it costs us $100 to manufacture and\nwe sell it to the distributor for $125. The cost of the engine makes up 60\npercent of the cost of the mower. The other parts \u2013 the wheels, shell, and\nhandle \u2013 make up 25 percent, and labor accounts for 15 percent.\nStudent: If the engine makes up 60 percent of the cost of the mower,\nthat\u2019s $60. If that component were to jump up 40 percent, that means\n[student does a quick calculation on his graph paper \u2013 60 x .4 = 24] \u2026"}
{"171": "going to solve this problem.\nStudent: Do you mind if I take a moment to lay out my structure?\nInterviewer: Go ahead.\nStudent: One last question. How much does the flat-top engine cost\ncompared with the old side-mounted engine?\nInterviewer: The price is the same, although some would argue that the\nquality isn\u2019t as good.\nThe student draws a line down the middle of his page and starts writing.\nStudent: I want to divide this into short-term solutions and long-term\nsolutions. In the short term, the company has several options. It can buy\nthe new engine and raise its prices, but that would pit it against Honda,\nso that\u2019s not ideal. It could close its doors and liquidate the company, or\neven sell its company to Honda, which wants to enter this lower market.\nAgain, not an ideal solution. It could eat its margins for the busy season\nand then switch to the flat-top engines when things slow down. Do you\nknow how long it would take to redesign the mower and retool the\nfactory?\nInterviewer: Eight weeks."}
{"172": "Interviewer: They went bankrupt for two reasons. Bad management and\n$2 million in debt. The engine company itself was a cash cow, always\nhas been. It too, has 25 percent margins. However, the owner took out all\nthe cash and signed a big loan to start a new company in another\nindustry, which failed. Now it can\u2019t pay back its loan and the bank won\u2019t\nextend it because of the company\u2019s bad credit history and the tight credit\nmarkets.\nStudent: Do we have $2 million in cash reserves?\nInterviewer: No, we have $1 million. But the bank is willing to take 40\ncents on the dollar based on the value of the company, and it values the\ncompany at $2 million. So you get the building and the equipment,\nwhich is ten years old and has about two to three years of life left on it.\nThe owners would have been better off investing in new equipment than\nin racehorses or whatever they threw their money at. With new\nequipment, they could have built the side-mounted engines for less and\nbuilt other engines as well.\nStudent: So the bank will take $800,000. I\u2019m concerned about laying out\n80 percent of our cash reserves for an acquisition. How\u2019s our bank\ncredit?\nInterviewer: It\u2019s good. If we put $500,000 down, we could take out a"}
{"173": "Interviewer: This is lower Alabama, and they were just laid off, so we\ncould get the workers back. And on one side you have guys building\nengines and on the other side guys making lawn mowers. They probably\nplay in the same softball league and drink out of the same keg. What\nelse?\nStudent: Is the side-mounted engine the only product we make? And are\nthe engine company\u2019s suppliers in good shape? While there is nothing to\nindicate that they are in trouble, we need to find out. This is an old\ndesign, and some of the parts might be tough to find.\nInterviewer: Good. Yes, we have only that one product, and the\nsuppliers are in good shape. What else?\nStudent: We would need to review an exit strategy. What happens if this\nfails, and what happens if we are successful? While an exit strategy\nmight not be that critical, in this case it is something you want to think\nabout.\nInterviewer: Okay, good. Anything else?\nStudent: One last thing, competitive response; 99 percent of the industry\nisn\u2019t going to care one way or the other. We need to think about\nCompany Y. How would it react? Would it still buy its engines from us?\nDo we even want them to buy their engines from us?"}
{"174": "raises its prices, and we can gobble up its market share, or it might eat its\nmargins for a while and then switch to a top-mounted engine. It would\nhave little choice. It can\u2019t live on 1 percent margins.\nSecond, we can sell to it but increase the price by 30 percent. It would\ncontinue to buy our engines but that would still cut deeply into its\nmargins, and I think that it would buy from us to get through the busy\nseason, then switch to the flat-top engine. The third option is to sell to it\nat a 5 percent increase. This would keep it buying from us. This is\nimportant because we just laid out $800,000 for this engine company and\ncan ill afford to cut off half our revenue stream. It would also keep\nCompany Y from manufacturing a flat-top mower. This is good because\nwe would still be supplying engines to this entire sector of the market,\nand it would also keep us from having to produce a flat-top mower\nourselves. If we did this, I\u2019d probably insist on a two-year contract. But\nbefore I did anything, I\u2019d like to run the numbers. We know the engine\ncompany\u2019s margins are 25 percent. Do we know how many engines they\nsell in a year?\nInterviewer: Good questions. But I don\u2019t want to get into that. Say we\ncontinue to sell to them for the next two years. What additional steps\nwould you take to ensure company X\u2019s success?"}
{"175": "and/or paying down the $300,000 we owe to the bank. There would be\nsome synergies and cost savings we could take advantage of.\nInterviewer: Give me three ways to cut costs in this case, besides\nmoving everything under one roof.\nStudent: Now that we have the new equipment, we can produce other\nengines to sell, as well as help our own company expand our product\nline. We could build weed-whackers and leaf blowers. We could even\ncreate some seasonal balance to our production line by building engines\nfor snow blowers.\nInterviewer: Okay, that\u2019s one. What else?\nStudent: We could cross-train the workers to build both engines and\nlawn mowers. Also, we could retool the assembly line to be able to\nquickly switch from one product to the other as demand dictates. The\nthird way is to go back down the supply chain and renegotiate with our\nsuppliers. Maybe sign longer contracts or get bulk discounts if the\nengines and mowers share parts. In addition, interest rates are at a near-\n40-year low, so maybe we could refinance our debt.\nInterviewer: Nice. Can you summarize the case for me?"}
{"176": "Interviewer: Okay, good.\nType of Case: Merger and acquisition\nComments: The student did very well. The opening prompt was long and\ninvolved, and the student did a great job summarizing the question. He\nasked some good questions up front, quickly realizing that cost-cutting\nwasn\u2019t going to solve this problem. He broke the problem down into\nshort-term and long-term solutions. Once he did that, he was quick to\nrealize that this was a mergers and acquisitions question. He asked a lot\nof good questions about the engine company, and he analyzed the Y\nsituation by laying out his three options rather than just automatically\ncutting Company Y off, which would have been an emotional response."}
{"177": "DISPOSABLE REVENUES\nInterviewer: Our client is a leading disposable consumer goods\nmanufacturer that makes disposable plates, cups, bowls, utensils, and\nnapkins. Each of these categories has seen growth over the past four\nyears with the exception of the cups category. There, revenues have been\nflat over the past two years. The head of the business unit has hired us to\ndetermine the reason for the decline and recommend ways to arrest the\ndecline and drive growth once again.\nStudent: I\u2019d like to reiterate the facts of the case as well as my\nunderstanding of our objectives. Our client manufacturers disposable\nproducts and while the other categories are growing, the cups segment\nhas been flat for the past two years. They want our thoughts on why this\nis happening and recommendations on what can be done to return the\ncategory to positive growth. Is that correct?\nInterviewer: Yes, that\u2019s correct.\nStudent: Okay. Can I take a minute to think about my approach?\nInterviewer: Sure.\nStudent: [Student takes 30 seconds to write out her thoughts.] To"}
{"178": "disposable distribution channels the same? If cups and plates use the\nsame distribution channels and plates are growing, then channels\nprobably isn\u2019t the issue \u2013 but it never hurts to take a look. Also, how are\nthe competitors\u2019 cups different from ours?\nInterviewer: Take a look at this graph and tell me what you think.\nStudent: Thanks. Based on the graph, there is nothing systemic that"}
{"179": "What can you tell me about our competitors\u2019 cups? Are they superior in\nquality? Do our competitors have better distribution channels? Better\npricing? Is there anything about them that would encourage a consumer\nto purchase a competitor\u2019s cup rather than ours?\nInterviewer: Good questions. Actually, there is nothing unique about\nour competitors\u2019 cups. They are virtually identical to our client\u2019s cups in\nterms of quality, pricing, and appearance, and all three competitors have\nthe same distribution channels.\nStudent: Okay. Cups come in all sizes. How many different sizes are\nthere? Is there growth in all sizes?\nInterviewer: All good questions. Cups come in three distinct categories\nbased primarily on size and material. The \u201cCasual\u201d category consists of\nthe traditional 8-ounce paper cup. Next is the \u201cParty\u201d category, which\nincludes both 12-ounce and 16-ounce cups that are usually red in color\nand made of plastic. The third category is called \u201cSpecial,\u201d and the cups\nthere are also red and made of plastic, but just larger \u2013 usually 20 ounces.\nThis chart illustrates our client revenues each year in the various\ncategories. Are there any immediate conclusions you can draw from\nthis?"}
{"180": "Student: There are a couple of conclusions I can draw from this. First,\nthere is a seismic shift in what\u2019s driving our growth. From Y1 to Y3,\nthere was steady growth in the Casual category, but from Y2 onward,\ngrowth is largely the result of the Party and Special categories. Next, the\ndecline in the Casual category revenues from Y3 to Y4 leads me to\nbelieve that there is something going on with consumption patterns. In\nother words, people need and want bigger cups as evidenced by the\ngrowth in the Party and Special categories. To validate this inference, I\u2019d"}
{"181": "Student: I think my inference might be correct. Neither competitor\noffers a cup in the Casual category \u2013 perhaps because they might have\nanticipated growth coming in the Party size. What\u2019s interesting to me is\nwhy neither offers a cup in the Special category. Based on our client\u2019s\ngrowth, there appears to be an opportunity there.\nInterviewer: You bring up a good point. What do you think our client\nshould do?\nStudent: Well, I think there are several things. In terms of\nmanufacturing, I think they should investigate phasing out the Casual\noffering and ramping up production of the Party and Special category\ncups. They will have to understand the economics of the decision.\nInterviewer: That\u2019s an interesting strategy, since Casual cups are still the"}
{"182": "should assume the same volume of the larger cups, or does shelf space\ndictate a smaller quantity?\nInterviewer: All great questions. First, the fixed costs would remain the\nsame at $10 million. The variable cost for the Special cup is 3 times the\nvariable cost for the Casual cup ($0.001 vs. $0.003). The smaller cup has\na unit price of $0.02 whereas the larger cup has a unit price of $0.05.\nAssume that because the Special cup is larger, replacing the Casual cups\nwith the Special cups cuts the volume of Special cups by half.\nStudent: This is very helpful. So based on this information, since we did\n$50 million in revenues in 2015 for the Casual cups, we sold 2.5 billion\ncups ($50m / $0.02). Our total cost of producing these was $10m (fixed)\n+ 2.5 billion x $0.001 (variable) = $12.5 million. The total cost of\nproducing the Special cups requires determining the quantity of cups we\ncan shelve. You said we can shelve only half the number of Casual cups,\nso that means 1.25 billion cups (2.5B / 2). So our total cost of producing\nthese will be $10m (fixed) + 1.25b x $0.003 (variable) = $13.75m. It\nlooks like making the replacement could increase our costs by $1.25\nmillion ($13.75m - $12.5m).\nLet\u2019s look at the revenue side. With 1.25 billion cups at $0.05, that\nmeans we would achieve $62,500,000 in revenues. In other words, our"}
{"183": "Student: This is interesting. [She takes a minute to make her own chart\nand turns it toward the interviewer.] Based on the graph, the underlying\ndata looks like this:"}
{"184": "be possible for our client to swap out the Casual cups for Party or Special\ncups in the Grocery and Drug channels. Is that a possibility?\nInterviewer: Sure. What else would you like to know before you sum\nthings up?\nStudent: I would like to know our shelf space in the warehouse channel.\nOne way would be to know what percentage of the $100 million of Party\nrevenues is our client\u2019s and what percentage is our competitors\u2019.\nKnowing this would allow us to estimate what share we have to take.\nInterviewer: Why don\u2019t you sum things up? What is your\nrecommendation to our client?\nStudent: I would recommend that our client reduce or eliminate\nmanufacturing of Casual cups and accelerate manufacturing of Party and\nSpecial cup sizes, given consumer demand and revenue growth. We\u2019ve\nseen that replacing the Casual with Special cups generates an\nincremental $12.5 million in revenues. To support growth in these\ncategories and maximize their chances of success, I\u2019d leverage our\ncurrent distribution channels and expand our shelf space through\npromotions, better pricing, and better cup design.\n( Case Takeaways )"}
{"185": "him through it.\n\u2022 Her summary was short and to the point"}
{"186": "JAMAICAN BATTERY ENTERPRISE\nInterviewer: Our client is the Jamaican Battery Enterprise. Currently,\nthey sell car batteries throughout the Caribbean, Africa, and Central and\nSouth America. Over the past two decades they have been eyeing the\nCuban battery market. However, Cuban Battery Enterprise, a state-\nowned battery company, currently has 100 percent of the secondary\nmarket. The reason they have 100 percent of the secondary market is that\nthe Cuban government imposes a 50 percent tariff on the manufacturing\ncosts and shipping costs on all imported batteries.\nThe Cuban government has just announced it will be lowering the tariff\non batteries by 5 percent a year for the next 10 years until the tariff\nreaches zero.\nThe Jamaican Battery board of directors wants to know the size of the\nCuban market and if, when, and how they should enter it.\nStudent: The board of directors of the Jamaican Battery Enterprise\nwants to know the size of the Cuban market and if, when, and how they\nshould enter it. We know that the Cuban battery market is now\ndominated by the Cuban Battery Enterprise because of a 50 percent tariff\non the manufacturing and shipping costs on all imported batteries. But"}
{"187": "Interviewer: One hundred percent.\nStudent: Let me rephrase. What is the market share that they can\nreasonably expect to gain and under what timeframe?\nInterviewer: Twenty-five percent within five years of entering.\nStudent: Let\u2019s start by estimating the size of the Cuban secondary car\nbattery market. I\u2019ll assume that there are 10 million people in Cuba.\nInterviewer: That\u2019s a little low but a good figure to use.\nStudent: I\u2019ll also assume that disposable income is limited and that only\none in ten households has a car. So if we estimate that the average Cuban\nhousehold is made up of five people \u2026\nInterviewer: Where did you get five from?\nStudent: I\u2019m assuming that there are two generations living in a number\nof the homes.\nInterviewer: Okay.\nStudent: So, if there are 2 million households and if only one in ten has\na car, that means that there are 200,000 cars. I would also like to add in"}
{"188": "Student: I was assuming that this is a monopoly in a communist country,\nthus the quality of the battery might not be competitive with a Jamaican\nbattery, which probably lasts five years.\nInterviewer: Go on.\nStudent: So, 210,000 vehicles will need a new battery every three years.\nBut there are two factors we need to figure in. First, let\u2019s say that half of\nthe 10,000 \u201cother\u201d vehicles we mentioned are government or military\nvehicles. So we need to subtract 5,000 from the total. Now it is 205,000\ndivided by every three years, which equals around 68,000 batteries. Also,\nthe number is going to be reduced over the long run because our batteries\nwill last five years, not three. I\u2019m not sure how to factor that in.\nInterviewer: That\u2019s okay. It\u2019s just important that you brought it up.\nStudent: If we want 25 percent of that market, we\u2019re talking 17,000\nbatteries a year.\nInterviewer: Okay, what\u2019s next?\nStudent: I\u2019d like to know some costs and prices. What are our costs and\nprices compared with theirs?\nInterviewer: Prices are irrelevant, but costs aren\u2019t. It costs the Cuban"}
{"189": "Cuban Battery Jamaican Battery\nEnterprise Company\nProduction costs: $12 Production costs: $9\nRaw material 20% Raw material 20%\nLabor 50% Labor 25%\nAll other costs 30% All other costs 55%\nShipping costs $0 Shipping costs $1\nTariff $0 Tariff $5\nTotal cost $12 Total cost $15\nThat means it costs us $9 manufacturing plus $1 shipping, which equals\n$10. Add in the 50 percent tariff, and we\u2019re talking $15 a battery.\nWe now need to figure out when we will be competitive. In five years,\nthe tariff will drop from 50 percent to 25 percent, which is half. So it will\nstill cost us $10 to manufacture and ship the battery; however, the tariff\nwill be only $2.50. That makes our total cost $12.50. I would say, based"}
{"190": "Interviewer: Their minds are made up. The tariff will be reduced.\nStudent: Next, I would want to find out why our labor costs are so high.\nInterviewer: Why do you think?\nStudent: The two things that jump to mind are technology and medical\ncosts. Maybe our technology is old and our manufacturing process is\nvery labor intensive.\nInterviewer: Yes, that\u2019s part of it. What else?\nStudent: We are in a communist country where healthcare is free. That\u2019s\nthe hidden cost in everything that\u2019s done \u2013 every service and every\nmanufactured item. Even a country like Canada, with its national\nhealthcare program, has higher prices. If the Canadian dollar weren\u2019t so\nweak compared with the US dollar, the Canadians would price\nthemselves right out of the market in many items.\nInterviewer: We\u2019ll save that discussion for another time.\nStudent: Well, we can\u2019t do much about the healthcare costs, but we can\nupgrade our technology. The upgrade would also make our batteries\nmore competitive and able to last five years instead of three years."}
{"191": "Interviewer: Good point. Our customer service is pitiful and our\ndistribution channels are restricted to two major warehouses, one in\nHavana and the other in Nuevitas. You said that you would launch a\nmarketing campaign, and I\u2019ll assume that there will be a customer\nservice aspect to that. What would you do about the distribution\nchannels?\nStudent: I\u2019ll make two assumptions. First, I\u2019ll assume that we have at\nleast two years before the Jamaican Battery Enterprise enters our market.\nSecond, I\u2019ll assume that other non-American battery companies will also\nenter our market, probably about the same time and with a strategy\nsimilar to the Jamaican company\u2019s.\nInterviewer: Both fair assumptions.\nStudent: First, I would go to every gas station on the island, both in the\ncities and in the countryside. I would front each one the cost of the\nbatteries, give them a nice display rack, free logo T-shirts, and maybe\nsome cash. In return, they would have to sign an exclusive agreement to\nsell only our batteries.\nLet me ask you this: Does the government make its own tires? And if\nyes, how\u2019s the quality?"}
{"192": "Enterprise has upgraded its plant, increased its distribution channels,\nformed a joint venture with the Cuban Tire Enterprise, and launched a\nnationalistic marketing campaign. Do you now enter the Cuban battery\nmarket, and if so, how?\nStudent: Whenever you enter a new market, there are several things you\nneed to examine. Who are the major players? What size market share do\nthey have? How are their products or services different from ours? And\nare there any barriers to entry? The major player is the Cuban Battery\nEnterprise. They now have 100 percent of the market. Two years ago,\ntheir products were inferior, but today they are very similar. The tariff\nwas a barrier to entry, but now it looks as if access to distribution\nchannels could be a threat.\nI\u2019ve learned that there are three main ways to enter a market. Start from\nscratch, buy your way in, or form a joint venture. I\u2019d like to do a quick\ncost-benefit analysis of each. Starting from scratch would be a fine\nstrategy if we can define our distribution channels. If the Cuban\ncompany has all the gas stations tied up and has built tire and battery\nstores, then our distribution means are limited. Plus, selling 17,000\nbatteries a year might not justify an investment of building our own\nbattery stores."}
{"193": "Interviewer: So it all boils down to \u2026\nStudent: So it all boils down to distribution channels.\nInterviewer: Great job.\nType of Case: Strategy/entering a new market/market-sizing\nComments: This was a long case and one that you\u2019d get in the final\nrounds, where you have about an hour to answer it. It had a market-\nsizing component to it, but probably the hardest thing was the switching\nof the hats. It forced the student to come up with counterstrategies to the\nstrategies he had just developed.\nMost students would have tried to figure out the reduction in tariff fees\nyear by year, but this student saved time and impressed the interviewer\nby picking a point in the middle and working from there. He made the\nmath simple and was able to do the calculations in his head.\nThe student was very well organized \u2013 he even wrote out the costs and\npercentages in a little chart. This impressed the interviewer and made\neverything easy for the student to find when flipping back through his\nnotes."}
{"194": "POWERSPORTS POWERTHROUGH\nInterviewer: The client is the number-three player in the North American\npowersports industry. It makes mainly snowmobiles, motorcycles, and\nATVs. Over the past year, its stock fell from $40 to $25 and net income\nwas down 18 percent while gross profits were down 13 percent. What\u2019s\ngoing on? How do we fix it?\nStudent: Just to make sure I got this right: Our client is the number-three\nplayer in the North American powersports market. Its stock fell from $40\nto $25, which is a drop of a little less than 40 percent. Its net income was\ndown 18 percent, and gross profits fell 13 percent. We\u2019ve been tasked to\nfigure out why and fix it.\nInterviewer: That\u2019s right.\nStudent: Any other objectives or goals I should consider?\nInterviewer: No.\nStudent: Are our competitors having the same problem?\nInterviewer: Yes.\nStudent: Has our market share stayed the same?"}
{"195": "Student: Because if the entire industry is having this problem, it\u2019s\nprobably external factors causing this drop, and that won\u2019t change\novernight.\nInterviewer: Point taken.\nStudent: Can I have a moment to think about my structure?\nInterviewer: Certainly.\n[The student takes a minute to write out his structure and then turns it\ntoward the interviewer. He writes E(P=R-C)M]\nStudent: I\u2019d like to look at external factors first \u2013 economic factors, as\nwell as changes in the industry. Then I\u2019d like to look at the company \u2013 its\nproduct mix, revenues, and costs for the last three years \u2013 and then come\nup with a plan to raise revenues and profits both in the short term and\nlong term.\nInterviewer: Talk to me about the external factors first.\nStudent: The overall worldwide economic slowdown. The strengthening\nUS dollar and falling Canadian dollar played roles. The weakening oil\nmarkets combined with an unseasonably warm winter, I believe,\nconstrained demand for off-road vehicles and snowmobiles."}
{"196": "Student: Let\u2019s touch on the market. You said not only that the rest of the\nindustry is having the same issues, but also that our market share has\nincreased. That means to me that we are either making acquisitions or\ngaining market share organically, maybe as a result of a player exiting the\nmarket.\nInterviewer: All our businesses increased market share despite a weak\npowersports industry. We\u2019ve made acquisitions in accessories such as\nhelmets, trailers, goggles, and gloves. But we also gained organically in\nthe snowmobile and motorcycle markets.\nStudent: You said our net income was down by 18 percent. How\u2019d the\nindustry do overall?\nInterviewer: Down 25 percent.\nStudent: Next I\u2019d like to focus on the company. Do you have any\ninformation on the product mix and their revenues for the last three\nyears? As well as overall profits?\nInterviewer: This is what I do have. [The interviewer hands this\ninformation to the student.] Take a minute and look this over and tell me\nwhat you think."}
{"197": "Student: ATV and snowmobile sales are crushing us. Not only are they\ndown 18 percent, but they also make up around [quick calculation] 80\npercent of the revenues. While motorcycles are up 33 percent, they make\nup only about 15 percent of sales. That\u2019s nice, but it\u2019s not going to move"}
{"198": "Interviewer: Of that 80 percent of revenues, 90 percent is ATVs and 10\npercent is snowmobiles.\nStudent: ATVs make up 72 percent of all sales, and we sell twice as\nmany motorcycles as snowmobiles. I imagine we have excess inventory\non hand. Have they reduced their prices?\nInterviewer: The short answer is \u201cno.\u201d They have run promotions but are\nreluctant to cut prices. Neither have the competitors, but I think if one\ndoes it they all will, and it will cause a price war.\nStudent: The volume and the costs are easier to change than the industry\nprice levels, unless all parties change their prices together.\nInterviewer: That\u2019s called price fixing, and it\u2019s illegal.\nStudent: I was thinking more along the lines of airline and gas prices.\nRegardless, unless the industry prices go up, it will hurt everyone in the\nlong run.\nInterviewer: Fair enough.\nStudent: What were the revenues the year before?\nInterviewer: Motorcycles were up 67 percent, but overall revenues were"}
{"199": "Interviewer: I know what it means. Give me some solutions.\nStudent: Can I take a moment \u2026\nInterviewer: Knock yourself out.\n[Student takes 60 seconds to collect his thoughts and lay out his ideas as\nbullet points.]\nStudent: We were tasked to increase stock price and profits both in the\nshort and long terms. As far as increasing the stock price, most would\nadvise a stock buyback program. The stock is down over 40 percent and\nis probably a very good deal. But this would only increase the price in the\nshort run.\nI think the cash on hand would be better spent making acquisitions \u2013 not\njust a competitor, but suppliers as well. The company needs to\naggressively manage costs, and this is one way to do it. In addition, I\nwould continue to introduce more products, particularly accessories,\nwhich also builds brand. I\u2019d also continue spending ad dollars on the\nmotorcycles since that is the fastest growing segment.\nAnother thing we could do is to offer consumer financing, either at zero\nor 1 percent. Offer more for trade-ins as well. We determined that it was\nimportant not to drop prices, but we still have a lot of inventory on hand."}
{"200": "Student: We were tasked to figure out why our stock and profits were\nfalling and come up with some solutions to turn the company around. It\nwas quickly determined that external factors were hampering our profits.\nWhile much of that was out of our control, we decided to be proactive by\nusing our cash to make acquisitions in a depressed market and to\nexchange excess inventory for stock in oil companies, thus maintaining\nour prices and reducing inventory.\n( Case Takeaways )\n\u2022 The case had a nice conversational tone.\n\u2022 The student quantified related numbers as percentages.\n\u2022 He stated his hypothesis up front after learning that competitors\nwere having the same problem.\n\u2022 Many students don\u2019t look at external factors and answer the case in\na vacuum. This case was all about external factors; remember to\nalways at least touch on them.\n\u2022 His analysis of the chart was good. He didn\u2019t repeat the obvious; he\nquantified the numbers as percentages and based his analysis on\nthat. He also dug a little deeper wanting to know the breakdown of\nATVs and snowmobiles.\n\u2022 His idea to trade excess inventory for stock was different and\ninteresting \u2013 certainly something that no one else brought up.\n\u2022 His structure was good, his confidence level was high throughout,"}
{"201": "MUSIC TO MY EARS\nInterviewer: Our client is the leading manufacturer of musical\ninstruments and is best known for its grand piano. The piano sells for\n$200,000. Alicia Keys, Billy Joel, and Elton John all play and record on\nour client\u2019s piano. The company also manufactures a slew of other\nmusical instruments, everything from cellos and violins to saxophones\nand drums. These are also kind of pricey but nothing close to the piano \u2013\nthat\u2019s in the stratosphere by itself. The CEO called us in because he\u2019s\nthinking about entering the US high-end headphone market. He\u2019s talking\nabout over-the-ear headphones like Beats. He wants us to analyze the\nmarket and then make a recommendation on whether the company\nshould enter.\nStudent: So the leading manufacturer of musical instruments is\nconsidering entering the high-end US headphones market. We\u2019ve been\ntasked to analyze the market and then make a recommendation. Are there\nany other objectives?\nInterviewer: Yes. He\u2019s looking to get 5 percent market share within two\nyears after entering the market. It could be 5 percent of the number of\nunits sold or 5 percent of the industry revenues. Either one is fine with\nhim."}
{"202": "[Student takes 90 seconds, draws out his notes, and then turns the notes\ntoward the interviewer.]\nI\u2019d like to start off by looking at three different areas. First I\u2019d like to\nlook at the company; then, the high-end headphones market; and finally,\nthe different ways for the company to enter the market.\nInterviewer: Okay.\nStudent: Let\u2019s start with the company. I\u2019d like to know its revenues and\nprofits for the last three years. That will tell me how big it is and how\nwell the company it\u2019s doing. Next I\u2019d like to know more about it\nproducts and production facilities; specifically, does it make electric\nguitars and pianos, or are all their products acoustic? Does it have any\nexperience with electronics? I\u2019d like to investigate the brand. How well\nknown is it outside the music industry? When consumers hear the\ncompany\u2019s name, what type of music do they think of, thus what is its\ncurrent customer segmentation? Finally, I\u2019d like to look at the company\u2019s\ncurrent distribution channels \u2013 how and where its products are sold, and\ndoes this match up to distribution channels used by the major brand\nheadphones sellers?\nNext, I\u2019d like to look at the high-end headphones market. What\u2019s the\ncurrent size of the market and what\u2019s its growth rate? How many"}
{"203": "make an acquisition of an existing player, form a joint venture, or\noutsource to a third party.\nMy hypothesis is that the company should not enter this market because\nit\u2019s very competitive, brand-driven, and dominated by some major\nplayers. But we\u2019ll see.\nInterviewer: Okay. Let me answer some of your questions. Our client\u2019s\nrevenues were $850 million last year, with profits of $175 million.\nStudent: That\u2019s just a little more than 20 percent profit margin. Is that\nnormal in the musical instrument industry?\nInterviewer: It\u2019s toward the high end. Our client makes only acoustic\ninstruments.\nStudent: So no electronic experience.\nInterviewer: No.\nStudent: That means they don\u2019t have the knowledge, capacity, or\ncapability to make headphones.\nInterviewer: Correct, not yet. Addressing your other questions,\nconsumers think of classical music when they hear our client\u2019s name,"}
{"204": "Student: If we are going to get 5 percent of this market, we need to go\nway beyond our current distribution channels and customer base. What\nabout the market?\nInterviewer: I\u2019m going to do a little data dump here. I want you to\nanalyze the information and then give me at least five takeaways. We\nknow that the high-end headphones market is growing at 20 percent a\nyear. High-end headphones are any headphones that sell for $100 or\nmore. The market leader is Company A. They have 60 percent of the\nmarket, and we\u2019ll say they did $400 million in sales. Company B has 20\npercent, and the remaining 20 percent is made up of 24 other players\nfrom C to Z. And you have some big-name brands in there as well. The\nlast bits of information are some prices. We\u2019ll assume that each of these\nplayers only has one headphone product in the high-end category just to\nmake life easier. Company G sells its headphones for $100, Company C\nis $125, Company A is $225, and Company B is $425.\n[Student draws this chart and then takes two minutes to analyze the\ninformation.]\nCompany Price Market\nShare"}
{"205": "remaining 20 percent. We have neither the brand recognition nor the\n\u201cstreet cred\u201d to elbow out 5 percent. The client might still get 5 percent\nof the overall industry revenues; that\u2019s a possibility but it seems unlikely.\nMy second takeaway is that this market isn\u2019t that price sensitive.\nConsumers are willing to pay for brand and/or quality. It shows us that\n80 percent are going for the two most expensive players. Number three,\nI\u2019d like to figure out the size of the market. You said Company A has 60\npercent of the market and that they did $400 million in sales. So if I take\n400 and divide it by .6, I get \u2026 666. I\u2019m going to round down to a\nmarket size of $660 million. If the market size is $660 million and we\u2019re\ngoing after 5 percent, the client is looking for sales of around $32\nmillion.\nInterviewer: What else?\nStudent: I\u2019d like to compare that $32 million with the $850 million the\nclient did in revenues last year to see what kind of revenue bump this\nwould provide. So 10 percent is 85 million; 5 percent is around 42\nmillion, so it\u2019s got to be around 3 percent. Not much of a bump. We\ncould probably get that a variety of ways without the risk. But the rub is,\nthe headphones market is growing at 20 percent. It makes it intriguing.\nInterviewer: Anything else?"}
{"206": "Student: Acquisition.\nInterviewer: Good. Earlier, you laid out different ways for the client to\nenter the market. Go through them and give me the pros and cons of\neach.\nStudent: Can I take a minute to write down my thoughts?\nInterviewer: No, I want you to list them off the top of your head.\nStudent: The first way for us to enter is to produce it in-house. The pros\nare that we would have total control over the design, process, and\nquality. Another pro would probably be a high margin in the long run. If\nthe headphone was successful, it would be a good entry point for us to\nbring in other products like speakers. The cons would be that we don\u2019t\nhave the knowledge, talent, or capability to manufacture the headphones,\nnor do we have the correct distribution channels. I think the biggest con\nwould be time to market. This market\u2019s on fire and \u2026\nInterviewer: What\u2019s next?\nStudent: Acquisition. The pros are that it would get us to the market\nmore quickly than growing organically. The company we purchase has\neverything we don\u2019t have, an existing product, manufacturing, market"}
{"207": "nightmare. We\u2019re talking about an old-school company that crafts pianos\nby hand and a young electronic company working with plastics. I can\u2019t\nimagine two company cultures being further apart.\nInterviewer: Okay, good. Next?\nStudent: A joint venture or strategic alliance. The pros would be similar\nto an acquisition; in addition, it would be easy in and easy out. It\u2019s\nprobably the quickest, easiest, and least expensive way for the client to\nenter the market. And if for some reason it failed, we could exit the\nmarket gracefully. The cons would be loss of control \u2013 and we\u2019d have to\ndo $64 million in sales to reach our goal. And that\u2019s figuring on a fifty-\nfifty spilt, which I don\u2019t see happening because we don\u2019t bring much to\nthe table. Finally, outsourcing. The pros would be we\u2019d have an expert\nproducing the product for us, and the initial costs would be lower. The\ncon would be loss of control. We are basically turning over our brand to\na third party and we\u2019d still have to find new distribution channels and\nbuild our brand.\nInterviewer: We need to make a recommendation to the CEO. What do\nyou tell him?\nStudent: Can I take a minute \u2026"}
{"208": "Student: No, it shouldn\u2019t enter this market. 80 percent of the market is\ndominated by two major players. You have 24 other companies fighting\nfor the remaining 20 percent. There are going to be others entering this\nmarket as well. We\u2019d never get the 5 percent you want. The company has\nno experience with electronics. We have neither the brand recognition\nnor the customer base needed to make this successful. In addition, we\ndon\u2019t know how long the market will continue to grow at this rate. Plus,\nthis will be a tough sell to his board of directors. And even if we were\nsuccessful, it only represents a 3 percent increase in revenues while\ntaking on a lot of risk."}
{"209": "In the long term I\u2019d fix the branding problem. I\u2019d hire someone like\nAlicia Keyes as a spokesperson to expand the brand and bring in a\nyounger and more diverse customer base. But she\u2019ll need something to\nsell. So the company can come out with a line of electric pianos. I\u2019d also\nlook for another instrument maker to buy. Someone like Gibson guitars.\nWe can take advantage of the synergies like the raw material, distribution\nchannels, and the back of the house functions. We need to get rid of the\nclassical music label. I know some purist might object, but in 1965 Bob\nDylan walked out on the stage at the Newport Jazz Festival with an\nelectric guitar for the first time, and although he got booed, it worked out\npretty well for him in the long run.\nInterviewer: Let me tell you why you\u2019re wrong. I\u2019d enter the market. I\u2019d\noutsource it to company B, the one with the highest quality. The problem\nwith Company B is that we\u2019d have to charge $500 a pair, otherwise the\ncompany wouldn\u2019t manufacture for us because it would cannibalize its\nown brand. But I have no problem charging $500 a pair; in fact, I\u2019d\ncharge $800 a pair because we\u2019re the Ferrari of pianos. We need to be the\nFerrari of headphones, otherwise it would hurt our brand. If we sold\nthem for $800, we\u2019d only have to sell 40,000 pairs to hit that $32 million\ntarget. That\u2019s a little more than 3,000 units a month. And our existing\nclient base is high-end consumers who could drop $800 on a pair of\nheadphones and not think twice about it. You said the 3 percent or $32"}
{"210": "years, which is a lifetime to someone in their twenties. I don\u2019t think your\nmarket is sustainable, and it certainly won\u2019t grow at 20 percent like the\nlower end of the market.\nInterviewer: Okay, that was productive. Many thanks for coming in\ntoday.\n( Case Takeaways )\n\u2022 Ask about the company before the market in an entering a new\nmarket question so you can analyze it through the company\u2019s eyes\nand not just through your eyes.\n\u2022 Always ask why they want to enter. This is an important clarifying\nquestion.\n\u2022 Run out the numbers. The student not only figured out the size of\nthe total market, he also figured out the 5 percent and then\ncompared that with the company\u2019s revenues. Very few students\never run out these numbers. You need to do it to put things in\nperspective.\n\u2022 Look at the big picture. I\u2019ve given this case live over 200 times,\nand only two people mentioned that the instrument company is\nbigger than the entire high-end headphones industry.\n\u2022 The interviewer cut him off and wanted to move on, but he held\nhis ground to get a key point in."}
{"211": "IN THE RED\nInterviewer: Our client is the number-one player in the DVD rental\nkiosk market. They place kiosks in front of CVS, Walmart, 7-Eleven, and\nmultiple grocery stores and fast-food restaurants. Over the past seven\nyears, the client has installed more than 40,000 DVD rental kiosks, but\nthis year, for the first time, they are uninstalling more than 500 units.\nLast year, the company\u2019s revenues rose by 3 percent to $2 billion, and it\nis projected to stay flat this year and then decline for the next several\nyears. The board of directors wants to get ahead of this trend, so they\u2019ve\nhired you to increase revenue 10 percent in the short term and then 5\npercent a year for the next four years.\nStudent: Let me make sure I\u2019ve got this right. The market leader in the\nDVD rental kiosk industry has installed 40,000 kiosks over the past\nseven years. Recently they\u2019ve uninstalled 500 kiosks, or about 1 percent\nof their machines. Although the company\u2019s revenues were up 3 percent\nlast year to $2 billion, they\u2019ve seen their sales flatten and are now facing\ndeclining revenues for the next several years. They\u2019ve asked us to come\nup with a five-year plan that will increase revenues by 10 percent in the\nshort term; that\u2019s $200 million and 5 percent a year for the next four\nyears. Are there any other objectives I need to consider?"}
{"212": "Interviewer: They were unprofitable. The volume dropped. When the\nclient puts a kiosk in front of a store, they need to pay that store rent, and\nthey need to pay a licensing fee to the movie studios.\nStudent: Interesting. I\u2019d like to investigate that more, but first I\u2019d like to\ntake a moment to write out my thoughts.\nInterviewer: Fine.\n[Student takes 90 seconds, draws out her notes, and then turns the notes\ntoward the interviewer.]\nStudent: Let me show you what I have so far. I\u2019ve broken it down into\nthree buckets: the company, the movie rental industry, and ways to\nincrease revenues. The client wants to raise its revenues by 10 percent\nthis year. We know that revenue equals price times volume. Its revenues\nwent from growing 41 percent to flat, and then up 3 percent. I want to\nknow what happened. Next I\u2019d like to look at their product mix and their\ndistribution channels. Do they do any online streaming or downloading,\nor is it strictly kiosk-based? I\u2019d also want to look at their pricing; when\nwas the last time they raised their prices, and what was the result? I\u2019d\nlike to review their costs \u2026\nInterviewer: Why do you want to look at costs? This is a revenue case,"}
{"213": "In my third bucket I\u2019d like to explore ways to increase sales, not only\nthrough the traditional methods of increasing marketing, but maybe\nchanging the product mix.\nMy hypothesis is that sales have gone flat because the physical DVD\nindustry is in decline. I\u2019d like to start with the company. Why did\nrevenues go from climbing 41 percent one year to flat the next?\nInterviewer: That was the last year the client installed new kiosks.\nStudent: Then why did the client\u2019s revenue go up 3 percent this year?\nInterviewer: A change in product mix. We reduced the number of Blu-\nrays because they weren\u2019t selling, and increased the number of video\ngames. What\u2019s nice about video games is that customers usually hang\nonto them for multiple nights, compared with just one night for movies.\nStudent: Can you tell me about their current product mix?\nInterviewer: The client rents three items \u2013 DVD movies, Blu-ray\nmovies, and video games. The DVD movies make up 70 percent of our\nsales, and we charge $1.20 a night. Blu-rays make up 5 percent of our\nsales and we charge $1.50 a night. And video games make up 25 percent\nof our sales, and they rent for $2 a night."}
{"214": "Student: I\u2019d like to go for the low-hanging fruit first. When was the last\ntime we raised our prices, and did we see much fallout? Did we lose\nmany customers?\nInterviewer: It\u2019s been almost three years since we raised them from $1\nto $1.20, and we saw very little fallout. Say we decide to raise the price\nof a DVD from $1.20 to $1.50. How much additional revenue would that\nbring in? Assuming little customer fallout.\nStudent: Well, raising prices from $1.20 to $1.50 is a 25 percent\nincrease. If the DVDs make up 70 percent of our revenues, that means 70\npercent of $2 billion, which is $1.4 billion. We take the $1.4 billion and\nmultiply it by .25 and we get $350 million. Which far exceeds the $200\nmillion we need for a short-term gain of 10 percent. That buys us some\ntime.\nInterviewer: What\u2019s next?\nStudent: I\u2019d like to stay with the product mix for a moment. Maybe\nthere are other items we can sell through the kiosks. Are they retoolable?\nInterviewer: Yes, to a certain degree.\nStudent: Who\u2019s our client base?"}
{"215": "Student: If that\u2019s our demographic\u2026 Let me think. Low-to-middle-\nincome families. I\u2019m thinking young kids. I saw an ad for Fatheads the\nother day \u2013 you know, the wall decals of famous athletes. While those\nare far too expensive, I\u2019m thinking autographed color photos of Disney\ncharacters. Maybe even headphones or earbuds with Disney characters.\nWhen I was a kid, we had educational computer games like Freddie Fish\nand Spy Fox. We could add those as well, which would extend rental\ntimes. For the gamers, maybe we can rent a controller. That way they can\nplay against a friend. I\u2019d want to be careful not to sell anything that the\n7-Eleven we\u2019re sitting in front of might also sell, but that might prove\ntough. We want something that will raise the purchase price, maybe\nsome sort of electronics such as a phone charger, earbuds, and other\nsimilar electronics. Can we put in more video games as well?\nInterviewer: We\u2019ve hit a limit with the number of video games we can\nrent. There are only so many titles. And while people will rent an old\nmovie, they won\u2019t rent an old video game. Okay, in the short term we\u2019ll\nraise our price for DVD rentals and change the product mix. What\u2019s\nnext?\nStudent: What can you tell me about the industry? What are the industry\ndrivers? I assume our direct competition is having the same issues."}
{"216": "Student: So other kiosk players are facing the same problems. I might\nwant to go after the disenfranchised Netflix customers who want to stick\nwith DVDs but feel neglected. Maybe increase marketing overall.\nInterviewer: In the short term, your plan is to raise the price of the\nDVDs, change the product mix, and increase marketing. How about the\nlong term?\nStudent: We need to reinstall the 500 kiosks. They\u2019re not making any\nmoney just sitting around. You said we uninstalled them because of low\nvolume. Let\u2019s reinstall them in high-traffic areas. Subway or tube stops,\ntrain and bus stations, and domestic airports.\nInterviewer: Give me the pros and cons of putting them in airports.\nStudent: For the pros, we can charge airport prices, maybe $3 a movie.\nTravelers can rent in one city and return in another. While there is\ninternet on airplanes, it\u2019s not often strong enough to stream. People still\nneed to bring their content with them for the most part. The cons are that\nmany laptops don\u2019t have a CD drive any longer. So we might have to\nstart putting content on flash drives if we can figure out a way for\ncustomers not to copy it. Another con is that planes show movies; some\nlike JetBlue and Virgin even have DirecTV or a library of movies you\ncan view through the airlines entertainment app. There\u2019s a lot of"}
{"217": "Student: Okay then, are all the kiosks in the US?\nInterviewer: Yes.\nStudent: Then I\u2019d look at other countries, perhaps Canada and Mexico.\nTheir internet infrastructure probably isn\u2019t as good as in the US They\nlike American movies.\nInterviewer: Give me the pros and cons of placing our kiosks in Brazil.\nStudent: I\u2019ve never been to Brazil, but I would think that the pros would\nbe a large population, low-income compared with American standards,\nso they might not have computers but probably do have DVD players.\nWeak internet infrastructure would be another pro. It would also be a\ngood distribution channel for local filmmakers. But I see many more\ncons. First, they speak Portuguese, so movies would need to be dubbed. I\ndon\u2019t believe many Brazilians carry credit cards, which would be\nessential. Piracy is an issue. Why pay a dollar to rent a movie when you\ncan buy one for 50 cents? While servicing the kiosks in the cities might\nnot be a problem, the more rural locations would be.\nInterviewer: How would we get around the servicing issue?\nStudent: Some sort of service contract with a local company or \u2013 you"}
{"218": "bringing in $150 million more than was requested. We changed and\nexpanded the product mix. We looked at the industry, saw that our\ncompetitors are having the same issues, and we validated our hypothesis\nthat the DVD market has matured and is in decline. In the long run, we\nrecommended expanding internationally and forming a strategic alliance\nwith Coke. This will get the client through the next five years; however,\nthey need to diversify their business model to secure long-term growth\nand survival.\nInterviewer: Good. End of case. Thanks for coming by.\n( Case Takeaways )\n\u2022 The student\u2019s structure was good, but she should have divided into\nshort-term and long-term solutions right up front.\n\u2022 She asked about costs in a revenue case and was quickly corrected.\nShe rolled with the punches and just moved on as if nothing had\nhappened, which was the right thing to do. It\u2019s never bad to ask\nabout costs; just be prepared to move on.\n\u2022 Her clarifying questions were solid. Whenever you get a revenue\ncase, always ask what the revenues did the previous three years.\nConsultants like to put things in perspective, which is why they\nalways ask for trends and why they quantify related numbers as\npercentages."}
{"219": "\u2022 She didn\u2019t commingle or bounce back and forth between pros and\ncons.\n\u2022 The student was able to go beyond the expected answer by coming\nup with creative options for new products and the idea of a joint\nventure with Coke."}
{"220": "RED ROCKET SPORTS\nInterviewer: The Red Rocket Sports Company designs and markets\napparel and footwear products under many brand names. All products\nare produced using similar manufacturing processes. Additionally, these\nproducts share similar distribution channels and are marketed and sold to\nsimilar types of customers. Take a look at the numbers below and tell me\nwhat\u2019s going on with Red Rocket Sports and where they should be\nconcentrating their efforts. I\u2019ll be back in 30 minutes for your analysis."}
{"221": "These numbers are estimated but should be pretty accurate.\nY2/Y3 Y1/Y2\nProduct: Footwear 10% 10%\nApparel 10% 20%\nMarket: U.S. 2% 12%\nUK 7% 7%\nEurope 15% 15%\nOther 30% 20%\nThere are a number of things we can infer from this chart:\n\u2022 Footwear has grown consistently by about 10 percent over the last\ntwo years.\n\u2022 Apparel growth has slowed, from 20 percent in Y1/Y2 to just\nunder 10 percent in Y2/Y3.\n\u2022 The US market has had dramatic declining growth \u2013 from 12\npercent to 2 percent, although it is still by far our biggest market."}
{"222": "Next, I looked at what part of the business each product line and market\nrepresents.\nWhat this chart tells us:\n\u2022 Footwear represents two-thirds of our sales and has remained as\nsuch over the last few years.\n\u2022 The US is by far our biggest market, making up more than half of\nour sales, but that number is inching down.\n\u2022 The UK has inched down as well \u2013 declining from 15 percent to 12\npercent. Europe has hung in at 20 percent.\n\u2022 The \u201cother\u201d markets have inched up, now representing 11 percent\nof sales."}
{"223": "Y3. That\u2019s an increase of $685,300. Of that number, apparel accounted\nfor almost half, despite representing only 35 percent of sales.\nInterviewer: Okay, so what should Red Rocket do about this?\nStudent: Action 1: Concentrate efforts on growth areas, particularly in\n\u201cother\u201d markets:\n\u2022 Increase the product line, particularly in apparel.\n\u2022 Increase distribution channels.\n\u2022 Reinforce the sales force.\n\u2022 Launch a major marketing campaign.\nAction 2: Secure our traditional markets to maintain business:\n\u2022 Launch marketing campaigns to boost sales in mature markets.\n\u2022 Focus on best-performing distribution points and best-performing\nstores.\nAction 3: Investigate market trends to anticipate future changes:\n\u2022 Talk to industry analysts and get their opinion of the trends.\n\u2022 Elaborate our strategy of product/market effort, based on info from\nexperts.\n\u2022 It\u2019s easy to graph these recommendations in a 2x2 matrix."}
{"224": "Type of Case: Company analysis\nComments: The student used charts and bullet points to make her\npresentation. She eyeballed the numbers because of the time constraint\nand the absence of a calculator. Remember, consultants use charts,\ngraphs, and PowerPoint slides to get their points across with clients \u2013\nyou need to do the same."}
{"225": "COKE\nInterviewer: The Coca-Cola Company is trying to boost profitability\ndomestically by raising its prices. It\u2019s focusing on the grocery store\nmarket, where the volume is high but the margin is low. What are the\neconomics of raising the prices, and is this a good idea?\nStudent: So Coke plans to increase profitability by raising prices. It\nwants to know if that\u2019s a good idea.\nInterviewer: That\u2019s right.\nStudent: I know that raising profitability is its main objective. Are there\nother objectives that I should be aware of?\nInterviewer: It doesn\u2019t want to lose market share.\nStudent: Are we just focusing on Coke and not any of its other brands?\nInterviewer: You can think of all Coke products as one product \u2013 Coke.\nStudent: What\u2019s Coke\u2019s current market share?\nInterviewer: Not relevant to the question."}
{"226": "Student: Let\u2019s see. First I can multiply and then find the difference:\n100,000,000 cans x .27 x 1.01 = $27,270,000\n100,000,000 cans x .23 x 1.06 = $24,380,000\nDifference = $ 2,890,000\nSo even though Coke would be selling 5 million fewer cans, it\u2019d be\nmaking more of a profit, about $3 million more.\nInterviewer: Profitability would be boosted by what percent?\nStudent: I can take 27 minus 24 equals 3 divided by 24 equals\napproximately 12 percent. By raising prices and selling less, Coke can\nboost its sales by approximately 12 percent.\nInterviewer: To maintain market share, Coke needs to stir up consumer\ndemand with a major marketing campaign to raise brand awareness and\nfocus on lifestyle issues. Knowing that, if you were Pepsi, what would\nyou do?\nStudent: Pepsi has three choices. It can follow Coke\u2019s lead and raise its\nprices to match Coke\u2019s; it can leave prices the way they are; or it can\ntake advantage of the price change and lower its price. If Coke spends a\nfortune marketing its product and does its job and gets people into the"}
{"227": "In sailing, if you are behind, you\u2019re not going to catch up with or beat the\nopponent by sailing the same course. You have to take a different tack. If\nPepsi lowers its prices and cuts marketing costs, it can steal customers\naway from Coke through in-store promotions and point-of-contact\ndisplays.\nInterviewer: So, if you were Pepsi, what would you do?\nStudent: Let\u2019s run some numbers. How many cans does Pepsi sell to\ngrocery stores?\nInterviewer: Pepsi sells 80 million cans at 23 cents apiece. If Pepsi\nfollows Coke and raises its prices, its volume will drop from 6 percent to\n3 percent. If Pepsi keeps its price the same, its volume will increase from\n6 percent to 12 percent. If Pepsi lowers its prices to 21 cents, Pepsi\u2019s\nvolume will increase from 6 percent to 20 percent.\nStudent: Okay, let me run some numbers.\n80,000,000 x 1.03 = 82,400,000 x .27 = 22,248,000\n80,000,000 x 1.12 = 89,600,000 x .23 = 20,608,000\n80,000,000 x 1.20 = 96,000,000 x .21 = 20,160,000\nI\u2019d follow Coke\u2019s lead."}
{"228": "THE UP-IN-SMOKE CIGARETTE COMPANY\nInterviewer: The Up-in-Smoke Cigarette Company is considering\noutsourcing its distribution truck fleet to an independent company. To\ncontinue providing this service in-house, they would have to\nsignificantly upgrade their fleet to conform with new regulations\nrequiring commercial trucks to provide electronic records of their\nitineraries \u2013 including average and maximum speed, time driven\ncontinuously, and the length of driver breaks.\nThe COO has hired you to help her decide: Should they outsource, or\nshould they upgrade the fleet and continue doing distribution in-house?\nStudent: Our client is a cigarette company that is trying to decide\nwhether to outsource its delivery function. They\u2019ve asked us to\ndetermine whether it is best to outsource the work or keep it in-house.\nAre there any other objectives we need to focus on?\nInterviewer: No. Do you have a problem with having a cigarette\ncompany as your client?\nStudent: It wouldn\u2019t be my first choice, but no.\nInterviewer: We don\u2019t always get to pick our clients. If this might be a"}
{"229": "Under the risks, I\u2019ll look at both the internal and external risks.\nInternally, I\u2019ll look at the cultural impact and impacts on labor \u2013\nincluding the potential of a strike. Externally, I\u2019ll look at the bigger\nmacroeconomic risks \u2013 cost of gasoline, government regulations, and\nfuture flexibility versus competition.\nInterviewer: Okay, good. Where would you want to start?\nStudent: I would like to start with the economics of the problem. I\nwould like to figure out the costs for both outsourcing and keeping it in-\nhouse. Also, I\u2019d like to know the estimated investments and the\ncompany\u2019s required payback period.\nInterviewer: It would cost the company about $1 million to upgrade its\nfleet, including trade-ins on the old trucks. Essentially, it would have to\nbuy new trucks, as the current trucks are old and the COO figures they\nare due for replacement soon anyway. The company requires a four-year\npayback on all its investments. For detailed information on the costs\nassociated with both in-house deliveries and outsourcing, please take a\nlook at the following table.\nIn- Outsource\nhouse"}
{"230": "includes oil change, tires,\netc.)\nCost per Delivery ? ?\nStudent: To determine the difference between outsourcing and in-house\ndeliveries, I need to determine the labor cost per delivery. Can you tell\nme more about labor costs? How many full-time drivers do we have, and\nhow much do we pay them on average?\nInterviewer: We have 10 full-time drivers that we employ for $5,000 per\nmonth, including benefits.\nStudent: Great. That means the salary costs are 10 drivers times 12\nmonths times $5,000 per month, which is $600,000 per year. We divide\n$600,000 by 400 deliveries and we now know that the labor cost per\ndelivery is $1,500. When we add the additional costs, we have the total\nin-house cost per delivery of $2,000. In the case of outsourcing, the total\ncost is $2,500.\nOverall, our client would save $500 per delivery or $500 times 400\ndeliveries, which equals $200,000 per year with in-house distribution.\nHowever, when we compare that with the required investment of $1"}
{"231": "to analyze some of the risks involved.\nAs I mentioned initially, the internal risks are worth analyzing. For\nexample, I\u2019d like to consider the potential impacts of this move to our\ninternal culture \u2013 we\u2019d be firing ten drivers. Also, I\u2019d like to assess the\npotential for a strike should we decide to outsource this service.\nInterviewer: Those are good points, but our drivers aren\u2019t unionized and\nwe estimate the culture will not change significantly. Can you think of\nanything else?\nStudent: Yes, actually. Looking at external factors, I can see some\nbenefits in outsourcing. Especially if we sign longer-term contracts, by\noutsourcing we externalize the risk for the increase in the price of gas\nand the changes in government regulations. These are important risks to\na company like ours and are significant benefits to our company.\nAdditionally, outsourcing would maintain the flexibility of our\ndistribution if our company expands.\nInterviewer: Great answer. Now let\u2019s change hats and try to figure out\nhow to make the in-house deliveries work. What would be some of your\nsuggestions?\nStudent: Well, we have a few variables. For the in-house deliveries to"}
{"232": "4. We could increase the number of deliveries, either for our\ncompany or for smaller companies, in order to increase the\nutilization of our drivers.\n5. We could reduce maintenance costs by outsourcing that job\nalone.\n6. We could introduce new software to determine more efficient\nrouting for our trucks, thus saving on gas and maintenance.\nInterviewer: Excellent. Thank you.\nType of case: Reducing costs, in-sourcing versus outsourcing\nComments: You\u2019re not always going to get the clients you want, and you\nneed to think this through. Many consulting firms will try to reassign you\nif you have a legitimate reason for not wanting to be on the case, but they\ncan\u2019t always control it because it has to do with scheduling. The student\ndid well. He analyzed the simple chart and was able to fill in the blanks.\nHe looked at both external and internal factors and came up with good\nrecommendations on both sides of the issue."}
{"233": "CABANA FEET\nInterviewer: Cabana Feet, LLC makes gel-flow flip-flops. These are\nlike traditional flip-flops but have the comfort of a gel insole. They come\nin a variety of sizes and colors. Earlier this year, Brad Pitt wore them in\nhis latest film, and they have become all the rage. Now, Cabana Feet is\nstruggling to keep up with demand. It\u2019s the only flip-flop maker\nproducing its footwear entirely in the US This has been a selling point in\ntheir advertisements for the last 10 years. Thus, they can\u2019t outsource\nproduction to other flip-flop makers. Take a look at the chart below and\nlay out for me in broad strokes some short-term and long-term strategies\nthat it might review.\nStudent: Our client is a company called Cabana Feet. They make gel-\nflow flip-flops. It looks like the company was producing about 6,000\npairs a month. Demand suddenly shot up when Brad Pitt wore their flip-\nflops in a movie. Cabana can\u2019t outsource because they are the only flip-\nflop maker in the US and a big part of their advertising rides on the fact\nthat their product is American-made."}
{"234": "Interviewer: That\u2019s right.\nStudent: You want me to come up with some short-term and long-term\nstrategies to help the company meet demand. Are there any other\nobjectives I should be aware of?\nInterviewer: No.\nStudent: Can I ask a few questions?"}
{"235": "Student: This is September, so the 25,000 number is an estimate?\nInterviewer: That\u2019s right.\nStudent: Have we been able to fill the orders up until now, or are there\nthousands of pairs back-ordered?\nInterviewer: We\u2019ve been able to fill the orders because of inventory;\nhowever, we\u2019ll run out of inventory at the end of the month.\nStudent: Do we expect the trend to continue? When does the DVD come\nout?\nInterviewer: The DVD comes out in December. What are your thoughts\non the trend?\nStudent: I would expect the trend to level off after the DVD\u2019s release.\nBut our strategy must take into consideration two scenarios. First, what\nhappens if Brad Pitt wears construction boots in his next movie and sales\nfall back to the 6,000 pairs a month number? And what can we do if it\nlevels off at around 25,000 pairs?\nInterviewer: Okay, good. What are you thinking? How can we meet\ndemand?"}
{"236": "Interviewer: There is no one inside the US that can help us. So the only\noption is to go outside the US\nStudent: Then go outside the US\nInterviewer: And throw away our marketing campaign of the last 10\nyears? We\u2019d get crucified in the press.\nStudent: Not if we go to the business press and argue that this is a\ntemporary measure \u2013 that for us to meet demand, we need to go outside\nthe US Once we can meet demand with US manufacturing, then we\u2019ll\nbring everything back home. Look, our sales jumped from 6,000 pairs a\nmonth to 25,000 pairs a month. Assume that 5,000 of the original 6,000\ncustomers bought our shoes because they were made in the US I think\nthat number is high, but let\u2019s assume it to be true. That means that we\nhave 20,000 new customers who are more interested in looking like Brad\nPitt from the ankles down than whether the flip-flops are made in the US.\nWe can\u2019t miss this opportunity. If we don\u2019t get those flip-flops on the\nmarket, someone else will. We have to worry about knock-offs and new\ncompetition.\nInterviewer: All right. By what percentage did the company\u2019s demand\ngrow from March to April?"}
{"237": "Interviewer: What would be the labor costs of one shift if benefits equal\n30 percent of wages and there is a miscellaneous cost of $232 per shift?\nStudent: Workers\u2019 wages \u2013 $15 an hour \u2013 times eight hours times 10\nworkers equals $1,200. The supervisor costs are $160. I got that by\nmultiplying $20 by eight hours. Total wages are $1,360 times 30 percent,\nwhich equals 408. So $1,360 plus $408 equals $1,768, plus the\nmiscellaneous cost of $232, equals $2,000 per shift.\nInterviewer: Good. Now assume that the total costs per shift, fixed and\nvariable, equal $2,000. Cabana makes $8 a pair, up to production\ncapacity. What is the shift breakeven point? How many pairs of flip-flops\ndoes the company have to make each shift to break even?\nStudent: That would be $2,000 divided by $8 equals \u2026 250 pairs of\nflip-flops.\nInterviewer: Capacity is 12,000 pairs of flip-flops a month. Capacity\nequals two shifts a day, five days a week. Given a normal eight-hour\nshift, what is the maximum production per shift?\nStudent: For a single shift?\nInterviewer: Yes, a single shift. I want you to think out loud when you"}
{"238": "Interviewer: If we started a third shift using our current employees,\nwhat would be the cost of the third shift? How much would our profits\nper pair of flip-flops change? Keep in mind that wages would go to time-\nand-a-half. And you\u2019ll need to add in the miscellaneous cost as well.\nStudent: Workers\u2019 wages at time-and-a-half would jump to \u2026 $1,800\n($22.50 x 10 x 8) and the supervisor\u2019s costs would be $240 (30 x 8).\nThat totals to $2,040. Multiply 2,040 by 30 percent for benefits and that\nequals $612, so that\u2019s $2,652 plus the miscellaneous $232. So the total is\n$2,884. So $2,884 minus $2,000 equals $884 in additional costs. Divide\nthe $884 by 300 pairs and it equals \u2026 $2.95. Thus we have $8 minus\n$2.95 equals $5.05 per pair.\nInterviewer: What are some of the pros and cons of adding a triple shift\nusing our current workers?\nStudent: Can I take a minute to think this through?\nInterviewer: Sure.\n[The student draws a line down the center of his page. He writes out the\npros and cons and presents them in bulk \u2013 all the pros first, then all the\ncons.]"}
{"239": "Saves on hiring costs and Might lower quality of\ntraining new employees product: may have to throw\nout some pairs\nEasy to get back to normal Less time for maintenance\nproduction levels\nBetter utilization of equipment Suppliers might not be able to\nmeet our demand\nInterviewer: By adding a third shift, we\u2019ll be able to produce only\n18,000 pairs a month. You said earlier that you thought demand would\nlevel out at 25,000 pairs. If we add a new line that produces 800 pairs a\nday, this includes two shifts; is this enough?\nStudent: Well, 800 pairs a day equals 16,000 pairs a month, given 20\nworkdays in a month. Add to that the current capacity of 12,000 pairs\nand we total 28,000 pairs a month. If demand is greater than that, we can\ngo as high as 42,000 pairs if we put on triple shifts on the new line and\nthe old line.\nInterviewer: Summarize the case for me.\nStudent: Our client, Cabana Feet, has seen demand for its flip-flops"}
{"240": "We learned that our breakeven point is 250 flip-flops per shift and that\nour capacity is 300 pairs per shift. We talked about the pros and cons of\nputting on a third shift, including watching our profit per pair drop from\n$8 to about $5 a pair. And finally, we are considering adding a new line,\nwhich could increase our overall production to 42,000 pairs a month.\nThat\u2019s a 250 percent increase over our current double shift capacity.\nInterviewer: Good. Thanks for coming by.\nType of case: Production and strategy case\nComments: The student did very well. He was quick to come up with\nsome long-term and short-term strategies. He held his own when the\ninterviewer pushed him about the outsourcing question. His math skills\nwere solid, and he tried to quantify his answer when he could."}
{"241": "WIRELESS WORRIES\nInterviewer: In Q4, the number-three US wireless carrier slipped further\nbehind its rivals in its number of customers, even as profits rose 35\npercent. What do you think is going on?\nStudent: Profits are up, but the number of customers is down. We need\nto figure out why this is happening.\nInterviewer: That\u2019s right.\nStudent: How many customers did we lose in Q4?"}
{"242": "[The student writes E(P = R \u2013C)M.]\nStudent: I\u2019d like to start by looking at the industry. Are the other two\nmajor wireless companies losing customers as well?\nInterviewer: Yes, but not in such large numbers. Traditionally, wireless\ncompanies lose about 1 percent of their customer base each quarter.\nHowever, they gain much more than they lose.\nStudent: Did we gain more than we lost?\nInterviewer: Yes. We gained 2 million new customers that quarter.\nStudent: Next, I\u2019d like to know why we are losing so many customers?\nInterviewer: Why do you think?\nStudent: It could be any number of reasons: the other companies might\nhave a better and more reliable network, more effective advertising,\nbetter pricing, cooler phones, or better customer service.\nInterviewer: Assume all that is true, but it makes up only a small portion\nof our losses. Why else?\nStudent: Did we have a satellite problem, causing our network to go"}
{"243": "Interviewer: One of our major concerns is bad debt. We spend a lot of\ntime, effort, and money on bill collection. It\u2019s a runaway cost. We spend\nso much time trying to get new accounts that we don\u2019t do a proper job\nvetting our customers\u2019 credit. Here\u2019s a chart for you to study. What does\nit say to you?\nStudent: It tells me that people with credit ratings between 500 and 600\nare a problem. Is it worth keeping them as customers? Let\u2019s figure out\nhow many customers we\u2019re talking about. According to the first pie\nchart, we have 60 million customers. It looks as if 1 percent of our\ncustomers fall into the 500 \u2013 549 range. They pay late 50 percent of the"}
{"244": "put us at 54 million. In turn, we would be 10 million customers behind\nthe number-two wireless company.\nInterviewer: So?\nStudent: How much does it cost us to collect late payments from each\ncustomer?\nInterviewer: $50.\nStudent: How much is the average bill?\nInterviewer: Overall, it\u2019s $75.\nStudent: So we\u2019re paying $50 to collect $75. Are our margins so great\nthat we\u2019re still making money off those customers?\nInterviewer: No. Profit margins are 25 percent.\nStudent: Do we charge a late fee?\nInterviewer: No.\nStudent: What percentage of those customers abandon their accounts\naltogether, leaving us holding the bag?"}
{"245": "customers who pay late. We spend $50 trying to recover the money;\nthat\u2019s $135 million. So we\u2019re losing $20 million a month going after\nthose customers.\nInterviewer: So what do you suggest?\nStudent: We can do a couple of things. Shed the high-risk late payers.\nVet new customers better. Require them to have a 600 credit rating or\nbetter. Start charging a late fee and go after the customer collections\nevery two months instead of every month. If we do that, we\u2019re spending\n$50 to go after $150.\nInterviewer: That\u2019s right.\nType of case: Reducing costs\nComments: She just did all right. The student\u2019s math was solid. She\nfigured out the annual loss of customers even though the numbers given\nto her were quarterly. She got stuck and the interviewer came to the\nrescue, but the student didn\u2019t pick up on the hint. She came up with a\ndecent list of suggestions, which showed she listened. Also, she did a\ngood job analyzing the second chart. It\u2019s a hard chart. It is not well\ndesigned and it has a lot of information on it. Oftentimes, the\ninterviewers will give you badly designed charts to test your analytical\nskills."}
{"246": "THE DISCOUNT BROKERAGE RACE\nInterviewer: Look at this chart. Your client is a discount brokerage.\nMost of its revenue comes from online trading. It achieved a 10 percent\ngrowth rate last year (Y1) and was ranked number six in the industry. In\nY2, it fell to seventh. The company wants to get back its sixth-place\nranking. How much will it have to grow to maintain that sixth-place\nranking in Y3, given the rate of growth of its competitors?"}
{"247": "Student: Is it fair to assume that the growth rates of all the other firms\nwill remain the same?\nInterviewer: Yes.\nStudent: Do you mind if I write on the chart you gave me?\nInterviewer: No, go ahead.\nStudent: I\u2019m going to do part of this through a process of elimination.\nEveryone below Company F growing at a smaller or equal rate, or whose\nrevenues are significantly below ours, can be eliminated. So that\u2019s easily\nthe bottom four \u2013 Companies I through L.\nWe know that A and B will remain the top two. So I\u2019ll concentrate on C\nthrough H, including us \u2013 F. So first I\u2019m going to do the calculations for\neach of those and see where they stand."}
{"248": "If we stayed at a 10 percent growth rate, we\u2019d have revenues of $726\nmillion, which would put us in seventh place.\nInterviewer: So how fast do we need to grow?\nStudent: If we round Company E\u2019s sales off to $772 million then that\u2019s\nour target number. We need to beat 772. So we need an increase of over\n$46 million (772 \u2013 726). So 660X = 772. Divide each side by 660 and\nwe get X equals \u2026 about 1.17 or 17 percent. A minimum 17 percent\ngrowth in Y3 would put us in sixth place.\nCompany Year 3 Industry\nRanking\nA 1\nB 2\nC 800 5\nD 882 3\nE 771.75 7"}
{"249": "Type of case: Numbers\nComments: This case was a pure numbers case. The student did well by\neliminating what was obvious and using her time efficiently."}
{"250": "THE BEST SHOT\nInterviewer: The Gates Foundation is the largest private foundation in\nthe US, holding $38 billion in assets. The foundation has poured $218\nmillion into polio and measles immunization and research worldwide.\nFoundation leaders have asked you to reduce the supply chain costs\nwhile expanding the vaccination program. How can this be achieved?\nStudent: The Gates Foundation has asked us to reduce its supply chain\ncosts and expand its polio and measles vaccination program. Are there\nany other mission goals or objectives I should be concerned with?\nInterviewer: No.\nStudent: Can you tell me a little about the current vaccination program?\nInterviewer: Vaccines are among the most cost-effective health\ninterventions ever developed. Immunization has led to the eradication of\nsmallpox, a 74 percent reduction in childhood deaths from measles over\nthe past decade, and the near-eradication of polio.\nOne in five children worldwide is not fully protected with even the most\nbasic vaccines. As a result, an estimated 1.5 million children die each\nyear \u2013 that\u2019s one every 20 seconds. These are preventable diseases."}
{"251": "percent below what was requested, leaving about 50 countries without\nadequate supplies. So why do you think there are shortages?\nStudent: I would like to think that the companies are having a hard time\ngetting the ingredients. But the skeptic in me thinks they are producing\nless so there will always be a need. Because if they produce enough and\npolio is eradicated, then they are out of business.\nInterviewer: Interesting, but you are way off base. Both firms are very\nreputable.\nStudent: I\u2019d like to take a minute to structure my thoughts. [The student\ntakes about two minutes.] I\u2019d like to break this down into several buckets\n\u2013 the key stakeholders, their roles and responsibilities, ways to reduce\nthe supply chain costs, and ways to expand the vaccine program.\nFirst, we have the key stakeholders \u2013 the Gates Foundation, maybe the\nWorld Health Organization, the 50 countries and their decision-makers,\nmaybe other NGOs, the drug manufacturers, and the storage and\ntransportation companies or organizations.\nSecond, the supply chain. We need to analyze the current process and\nthen investigate whether there are options to consolidate, automate,\nsimplify, or even eliminate parts of the process that are either slowing"}
{"252": "Alliance, the private sector, academia, civil society organizations, faith-\nbased organizations, and local communities \u2013 just to name a few. They\nall want to and need to participate.\nStudent: This seems overwhelming. And I\u2019d assume there is a lot of\nredundancy. What exactly is the Gates Foundation\u2019s part? What are its\npriorities?\nInterviewer: Gates supports the collection, analysis, and use of high-\nquality vaccine-related data, improving the measurement and evaluation\nof vaccination efforts. Gates also supports new diagnostic tools to help\nhealth workers access need. They support strengthening supply chain\nlogistics. That includes storage, transportation, and distribution of\nvaccines.\nStudent: When you say support, you mean give money.\nInterviewer: While that\u2019s a big part of it, they also contribute other\nthings.\nStudent: Pardon me. I\u2019d like to look at this problem from a 30,000-foot\nlevel. You have millions of children each year who need to be\nvaccinated. You have scores of organizations and agencies wanting to\nhelp out, clamoring for their share of the process. Helping out, yes, but"}
{"253": "selling a division that is slowly becoming obsolete. By acquiring the\ndivision, the cost of the vaccine should go down. We could increase\nproduction, ensuring that there is enough vaccine to reach all those in\nneed. They could also start to produce vaccines for measles and other\nearly childhood diseases.\nInterviewer: Both of those companies are huge. Serum is privately held,\nand Sanofi Pasteur did around $40 billion in revenues last year. Even if\npolio and measles make up only 3 percent of its revenues, that\u2019s still\naround $1.2 billion. Gates would have to pay at least $3 billion for the\ndivision. Besides, Sanofi just opened a $430 million vaccine plant in\nCanada, so maybe production will increase.\nStudent: The other problem is logistics. What if Gates partnered with\nFedEx, or UPS or Amazon? Private-sector firms are more efficient and\nhave greater experience in the area of logistics and transportation than\nthe NGOs do.\nInterviewer: Clearly you don\u2019t have the patience or tolerance to work\nwith government and nonprofit clients. These are very complex problems\nwith many partners and stakeholders. Yes, there is redundancy, even\ncorruption to a certain degree, but \u2026\nStudent: That\u2019s not true. I have great respect for the work the NGOs of"}
{"254": "Student: FedEx has established distribution networks within most, if not\nall, of these countries. They already partner with reliable companies to\nensure the delivery of packages for the last mile or miles. Gates can\ncontribute by buying refrigeration units sited at these companies\u2019\nwarehouses. While these transportation companies already go to many of\nthe towns, they might not go to all of the remote villages. This means\ntwo things. One, if they do start going to the remote villages, it would\nallow other vendors to get their goods to the village, and for village\ngoods to be transported out to other markets. If they don\u2019t, then the\nNGO\u2019s distribution chain has an established central location where they\ncan pick up the vaccines for transport to the villages. So instead of the\nNGOs taking them across country, they would be responsible for only\nthat last leg.\nIn addition, FedEx won an award for its relief efforts in response to the\n2015 Nepal earthquake. If I remember correctly, they partnered with\nDirect Relief and other nonprofits to transport critically needed\nmedicines, medical supplies, food, shelter, and water purification\nequipment from the US to Kathmandu. Their dedication to disaster relief\nprograms makes them an excellent potential partner for the Gates\nFoundation.\nInterviewer: You have a meeting with the director of the Gates"}
{"255": "Why? FedEx already has experience in disaster relief in remote parts of\nthe world. This experience can help consolidate and streamline the\ndelivery of vaccines. It will also stimulate the economies of the villages\nas goods move in and out more efficiently. But there are risks: This\naction may irk certain NGOs and possibly some government officials\nwho will want to control the process. In the short-term, though, next\nsteps might include contacting the NGOs and government agencies to get\ntheir approval on these changes. Working closely with FedEx and the\nfoundation could help control costs and fine-tune logistics. Long-term,\nwe can work with the vaccine manufacturers to ship directly through\nFedEx to the closest airports and government agencies to ensure\nexpedited handling through customs.\nInterviewer: Great. Thanks for coming by.\n( Case Takeaways )\n\u2022 Talked of mission (the right language)\n\u2022 Asked good clarifying questions to get to the two main problems\n\u2022 Designed a good structure\n\u2022 Defended himself when the interviewer attacked his patience with\nNGOs\n\u2022 Developed a credible creative solution even after his first idea was\nshot down"}
{"256": "PARTNER CASES\nPartner cases are cases that you can do with your friends, regardless of\nwhether they know how to give a case. There are five medium (Level 2)\ncases, and five hard (Level 3) cases. Your partner first needs to read the\nRoommate\u2019s Guide found below and then read the case all the way\nthrough, twice. The case gives your partner plenty of advice and\ninformation to make it easy and fun to grill you.\nFor some of the cases, there are charts in the back of the book (see\nPartner Case Charts), some of which can be hand-drawn in a few\nminutes\u2019 time.\nThere is flexibility built into these answers, and there is usually no one\nright answer. Have fun with these. After you have answered the case,\nturn around and give it to another friend. You learn just as much giving a\ncase as you do answering one. The average student serious about a career\nin consulting will do at least 30 \u201clive\u201d cases. Nothing beats live case\npractice."}
{"257": "Bulletproof Auto Glass \u2013 Market-sizing, new market entry, and pricing\nStatin Blue \u2013 Market-sizing, strategy, and new market entry\nBottled Water \u2013 Production, strategy, and reducing costs\nTedEx \u2013 Numbers and strategy\nGassed Up \u2013 New product strategy and numbers"}
{"258": "THE ROOMMATE\u2019S GUIDE\nIf you have been begged, bribed, or blackmailed into helping your\nfriend(s) prepare for case questions, here are some suggestions.\nYour prep\n\u2022 Read the question and the answer all the way through before\ngiving the case.\n\u2022 Be aware that there are multiple \u201cright\u201d answers.\n\u2022 It\u2019s all right to give them help if they lose their way.\n\u2022 Don\u2019t cop a know-it-all attitude.\nThings to watch for at the beginning\n\u2022 Are they writing down the case information?\n\u2022 Is there a long silence between the end of the question and the\nbeginning of the answer?\n\u2022 Are they summarizing the question?\n\u2022 Are they asking about the client\u2019s objective(s)?\n\u2022 Are they asking clarifying questions about the company, the\nindustry, the competition, and the product?\n\u2022 Are they laying out a logical structure for their answer?\nThings to watch and listen for during the course of the question"}
{"259": "Review list\n\u2022 Was their answer well organized? Did they manage their time\nwell?\n\u2022 Did they get bogged down in details?\n\u2022 Did they seem to go off on a tangent?\n\u2022 Did they ask probing questions?\n\u2022 Did they use business terms and buzzwords correctly?\n\u2022 Did they have trouble with math, multiplication, or percentage\ncalculation?\n\u2022 Did they try to get you to answer the question for them?\n\u2022 Were they coachable? Did they pick up on your hints?\n\u2022 Did they speak without thinking?\n\u2022 Did they have a positive attitude?\n\u2022 Did they summarize their answer?\nFinal analysis\n\u2022 Did they take your criticism well?\n\u2022 Did they defend themselves without sounding defensive?\nAftermath\n\u2022 Do another!"}
{"260": "STUCK\nProblem Statement\nOur client is the third-largest peanut butter manufacturer in the US Its\nbrand, Mickey\u2019s, sells 120 million jars of peanut butter a year, but trails\nbehind Skippy and Jif. Peter Pan is in fourth place, but only two market\nshare percentage points behind us, so they are breathing down our necks.\nMickey\u2019s sells to supermarkets and convenience stores nationwide,\nwhich makes up 60 percent of the company\u2019s sales volume. Sales to big\nbox stores such as Costco and BJ\u2019s Wholesale make up 25 percent, but\nthe biggest customer is Walmart, and Walmart alone makes up the\nremaining 15 percent.\nOur client received some bad news. Walmart is replacing Mickey\u2019s with\nWalmart\u2019s private-label peanut butter; however, the good news is that\nWalmart wants Mickey\u2019s to produce the chain\u2019s private label. They want\nthe same peanut butter, the same jar and cap \u2013 everything exactly the\nsame except the label. I want to break this case down into two parts.\nFirst, I want you to list the key strategic issues and concerns that\nMickey\u2019s should be thinking about when deciding whether or not to\nmove ahead with this decision. Second, I want you to run the numbers to\nsee if it makes financial sense."}
{"261": "I\u2019m looking for the student to state the issue first, and then follow it up\nwith a question.\n\u2022 Profit: Will the new contract be profitable? Walmart is known for\nsqueezing margins.\n\u2022 Brand image / brand equity: Will our brand take a hit if consumers\nfind out that Mickey\u2019s and Walmart\u2019s are the same product? How\nmuch will it hurt Mickey\u2019s when consumers don\u2019t see it on\nWalmart\u2019s shelf?\n\u2022 Capacity: What is the size of the contract and do we have the\ncapacity?\n\u2022 Cannibalization: Will we be stealing sales from ourselves?\n\u2022 Market share: Regardless of whether we do this or not, Mickey\u2019s\nwill fall to fourth place because they will be losing 15 percent of\nour volume. Mickey\u2019s doesn\u2019t get credit for Walmart\u2019s label.\n\u2022 Dependency on Walmart: Currently, Walmart makes up 15 percent\nof our volume. If the volume increases, they could be responsible\nfor up to 20 percent of our volume. And whenever one client has\n20 percent of your volume, it\u2019s a concern, especially when it is\nWalmart.\n\u2022 Private labels for others: If we say yes, we can then make private-\nlabel product for other stores. Will Mickey\u2019s get replaced in other\nstores with a private label?\n\u2022 Project what happens if we don\u2019t accept the contract. Mickey\u2019s"}
{"262": "\u2022 Mickey\u2019s makes a per-jar profit in the supermarkets of $1.20, a\nper-jar profit in the big box stores of $1.00, and a per-jar profit in\nWalmart of $.50 (selling as Mickey\u2019s).\n\u2022 The new Walmart contract is a one-year deal, calling for 50 million\njars with a $0.25 per-jar profit.\nAsk: Is this a good deal? Will our client be making more or less money\nwith the new contract? Compare Walmart to Walmart.\nOld deal: 120 million x .15 = 18 million x $0.50 = $9 million\nNew deal: 50 million x $0.25 = $12.5 million\nThe company makes $3.5 million more in profits; however, the new deal\nputs our plant into an overcapacity situation by 2 million jars (120m \u2013\n18m = 102m +50m = 152m jars). What do we do? (How quickly did\nthe student realize that there was a capacity issue?)\nThese are some of the ideas the student will come up with (along with\nyour response). As he offers these ideas, keep telling him \u201cno.\u201d Even cut\nhim off on one or two to see how he reacts. Does he remain calm? Or\ndoes he get upset or try to argue with you? How students react is more\nimportant than coming up with the right answer.\n\u2022 Mickey\u2019s can outsource the 2 million jars. (No, because the\nWalmart contract calls for Mickey\u2019s to produce the product, so we"}
{"263": "\u2022 Mickey\u2019s can acquire a smaller player and run the extra\nproduction through the acquisition. (No, because an acquisition\nis a long-term solution and we don\u2019t know whether Walmart will\nextend the contract, plus it is expensive.)\n\u2022 Mickey\u2019s can just ship 48 million jars to Walmart. (No, the\ncontract calls for 50 million jars. If Mickey\u2019s can\u2019t do it, Walmart\nwill find someone else.)\n\u2022 Mickey\u2019s can pull 2 million jars from the big box stores. (No,\nbecause that move will cost us $2 million in lost profits, and we\nworked hard on the Walmart contract just to make $3.5 million\nextra profit. It will reduce our Mickey\u2019s market share even more.\nIn addition, we want to continue to supply the supermarkets and\nbig box stores with all the peanut butter they need, plus fill the\nWalmart contract.)\nSolution\n\u2022 Raise your price at supermarkets by $0.20.\n\u2022 Mickey\u2019s price will still be a little less than the competition\u2019s while\nMickey\u2019s will be making $1.40 per jar.\n\u2022 Higher prices will reduce volume.\n\u2022 Our analysts projected that volume will drop 10 percent \u2013 have the\nstudent run the numbers to see if this makes sense.\n\u2022 60 percent of 120m jars is 72m jars. 72m x 1.20 = $86.4m."}
{"264": "Mickey\u2019s received notice that they are being replaced in Walmart by a\nprivate label; however, Walmart has asked them to manufacture their\nprivate label. It is a one-year contract that would require Mickey\u2019s to\nproduce 50 million jars at a profit of $0.25 a jar, leading to a $3.5 million\nincrease in profits overall. After reviewing the larger strategic issues,\nincluding brand equity and market share, Mickey\u2019s decided to move\nahead. However, the new contract pushes demand 2 million jars over\nplant capacity. To correct this, Mickey\u2019s decided to raise the price of its\npeanut butter in supermarkets by $0.20, thus reducing demand by 10\npercent (or 7 million jars), while making almost $5 million more.\nA good interview consists of:\n\u2022 Repeating the question, verifying the objective, and asking if there\nare any other objectives.\n\u2022 Maintaining composure as the interviewer rejects the student\u2019s\nideas.\n\u2022 Coming up with the price increase solution without any help from\nthe interviewer.\n\u2022 Quickly recognizing that there is a capacity issue.\n\u2022 Developing a good short summary touching on the most critical\npoints, but also bringing up points from both parts of the question.\n\u2022 Keeping well-organized and easy-to-read notes. Big points if the\nstudent made a chart like the one below."}
{"265": "Mark of a Good Case Discussion: While this is a first-round case, it is\na difficult first-round case because there is only one right answer. I have\ngiven this case live more than 200 times and I can tell you that fewer\nthan 10 percent of the students got the right answer of raising our price in\nthe supermarkets. I look for two other things: (1) how quickly did they\npick up on the capacity issue? and (2) how did they react when I cut\nthem off and kept telling them, \u201cNo, what else do you have?\u201d To me, this\nlast issue is much more important than coming up with the right answer."}
{"266": "NERVES OF STEEL\nProblem Statement\nOur client is the second-largest maker of steel filing cabinets and desks\nin the US It is nearing the end of a four-year rolled-steel contract, which\nexpires at the end of Year 7. Our client signed its steel contract in Year 4.\nThe CFO wants to know if it makes sense to stockpile two years\u2019 worth\nof steel at the end of the contract at the Year 4 price or sign a new\ncontract at Year 8\u2019s price.\nYou can assume that the company uses 12,000 tons of steel a year and\nwill continue to do so over the next five years.\nGuidance for the Interviewer\nMake sure that the student summarizes the question and verifies the\nobjective. The student is looking at two options. Option 1 is to sign a\nnew contract at Year 8 prices. Option 2 is to stockpile steel for two years,\nthen sign a new contract in Year 10.\nAsk the student what he needs to know. He should ask about:\n\u2022 Cost of steel in Y4 contract: $600/ton\n\u2022 Current Y7 cost of steel: $810/ton"}
{"267": "Show the student this chart, or write it out for him. You are looking to\nsee what conclusions the student can derive from this chart \u2013 which is\nnot much. The numbers are all over the place and there is very little\nconsistency. The most it can do is anchor the Year 8 price for him.\nAsk him what he thinks the Year 8 price will be. He has a starting point\nof $810 (the Year 7 price). He should take into consideration the\neconomic conditions around the world, particularly in the top three\nindustries that use rolled steel: automotive vehicles, aircraft, and\nappliances (which are tied to housing starts).\nRemember that there is no right answer. Students are often reluctant to\ngo out on a limb and give a number, but you need to force your student\nto come up with a price. (Let\u2019s assume he comes up with a Year 8 price\nof $850 per ton.) Once he has a price for Year 8, look to see what he does\nwith it. Give him big points if he draws up the final slide.\nThe Final Slide\nThis is critical. If the student has the foresight to build the final slide, he\nwill stand out from all the other candidates. Whenever you have a case"}
{"268": "In this case, it would look like this (assuming he came up with a Year 8\nprice of $850 per ton):\nThe student should also conclude that he needs to come up with a price\nfor Year 10. If he stockpiles for two years, then the company will have to\nsign a new contract. Again, he should talk about where he thinks the\neconomy \u2013 and those three industries in particular \u2013 are headed. Let\u2019s say\nhe comes up with a Year 10 price of $900 per ton.\nThe only two spaces left open in the final slide are the stockpiling\nnumbers. They would stockpile 24,000 tons, enough for two years.\nBecause they are stockpiling, they would need to pay for all of it up\nfront."}
{"269": "Year 2 / (12,000 x 50) = 600,000\nFV = PV (1 + i)n (n = 1)\nFV = 600,000 x 1.05\nFV = 630,000 or $0.63m\nY1 = $1.26m\nY2 = $0.63m\nTotal Storage and Inventory Costs = $1.89m\nSteel Costs\n24,000 tons x $600 = $14,400,000\nInterest rates are at 5 percent\nFV = PV (1 + i)n\nFV = 14,400,000 (1.05)2\nFV = 14,400,000 x (1.10)\nFV = 15,840,000 or $15.84m\nStockpiling = $15,840,000 + $1,890,000 (inventory costs)\nStockpiling = $17,730,000, or round off to $18,000,000\n$18,000,000 / 24,000 tons = $750/ton (it always comes out to $750/ton)"}
{"270": "Did the interviewee \u2026\n\u2022 repeat the question, verifying the objective and asking about other\nobjectives?\n\u2022 make math mistakes?\n\u2022 design the final slide?\n\u2022 draw the interviewer into a discussion?\n\u2022 produce a good, short summary touching on the most critical\npoints?\n\u2022 have well-organized and easy-to-read notes?\nMark of Good Case Discussion: The key things to look for are the\nstudent\u2019s knowledge of the current economic conditions, the realization\nthat the company will have to sign another contract two years later if\nthey stockpile, and of course the use of the final slide. It is important not\nonly that the slide be filled in, but also that the student turn it toward the\ninterviewer and walk through his analysis and decision."}
{"271": "GPS APP\nProblem Statement\nOur client is a pair of college students who have built an app that will\nallow customers to download celebrity voices onto their smartphones\u2019\nGPS systems. The client wants us to determine: the domestic market\nsize, the breakeven in terms of the number of downloads, the price, and\nestimated profits for the first year.\nGuidance for the Interviewer\nThe student should repeat the question to make sure everyone is on the\nsame page, and he should verify objectives (market-size, breakeven,\nprice, and profits). The student should also ask whether there are any\nother objectives he needs to be concerned with \u2013 in this case, the answer\nis no.\nMost students will try to determine the market size first. In this case, you\nare looking to see whether the student first determines the price of the\napp. He doesn\u2019t need to answer the questions in the order they were\ngiven. An exceptional student will determine the price first, as well as\nmake a final slide almost immediately. See below for an example.\nData Bank"}
{"272": "Price: There are three major ways to determine price: competitive\nanalysis, cost-based pricing, and price-based costing. But the student\nshould first determine what is the company\u2019s objective \u2013 to gain as much\nmarket share as possible before others enter the market.\nCompetitive analysis: Because we are first to market, there is no real\ncompetition. However, given the platform (iTunes, etc.), it is fair to try to\ncompare this with other products being sold, such as songs, ringtones,\nand games. These items range in price from free to $9.99. It is a fair\nassumption that this product will fall somewhere in that range.\nCost-based pricing: The product\u2019s variable cost is one-third of the price\nplus $0.50 per download. Make sure that all costs are covered.\nPrice-based costing: What will customers be willing to pay? Will they\nequate this app to a ringtone? Or something a little more sophisticated?\nIt is good practice to use numbers that will make your calculations easier.\nApple is taking a one-third fee. Songs and ringtones sell for $1.29 each.\nThe student probably wants to use $3 as a sale price. One dollar goes to\nApple, and $.50 goes to the celebrity, which leaves a profit of $1.50 for\nour client.\nDomestic market size: To determine the market size, the student should"}
{"273": "He came up with 10 million apps. Also assume that the average person\nwould purchase 2 voices, making the market 20 million apps the first\nyear.\nBreakeven:\nBE = fixed costs/margin (price \u2013 1/3 price \u2013 royalty)\nBE = 500,000/3 \u2013 1 \u2013 0.5 = 1.5\nBE = 500,000/1.5 = 333,333 downloads\nProfits: If the client sells 20 million apps the first year, profits would be\n20 million times $1.50 = $30 million minus startup costs of $500,000\nFirst-year profits are $29.5 million.\nFinal slide: This simple final slide can be drawn up immediately.\nInterviewers will watch the order in which the student writes the\nrequests. Did he stick with the order given, or did he realize they were"}
{"274": "Est. profits $29.5 m\nSummary\nThe student needs to jump right into this without any downtime to collect\nhis thoughts. If he made the final slide, then that is his summary. He\nshould turn the slide toward you and walk you through it. If not, a good\nsummary is about a minute, minute-and-a-half long. It is not a rehash of\neverything you spoke about; it is a short recap of the problem and two or\nthree main points that he wants the interviewer to remember.\nWe were asked to determine four main factors concerning the GPS app:\nprice, which was determined to be $3; market size, 20 million apps;\nbreakeven in volume, 333,000; and profit the first year; $29.5 million. If\nthese numbers hold up, this promises to be a very profitable venture.\nA good interview consists of:\n\u2022 Repeating the question, verifying the objective, and asking about\nother objectives.\n\u2022 Drawing the final slide.\n\u2022 Making no math mistakes.\n\u2022 Using fair assumptions. keeping well-organized and easy-to-read\nnotes.\n\u2022 Developing a good, short summary touching on the most critical"}
{"275": "FINDING FULFILLMENT\nOur client is an online toy store. They ship toys all over the US\nCurrently, they have one warehouse outside Boston where they receive\nand then distribute the toys. They ship to the end-user \u2013 the parents or\nthe kids, through FedEx. The client wants to know if it makes sense to\noutsource product distribution through a third party such as Amazon.com\nor Danny\u2019s. I\u2019d like to break this question into two parts. What are the\npros and cons of outsourcing our distribution and in this case, does it\nmake financial sense to outsource?\nGuidance for the Interviewer\nThe student should repeat the question, verify the objective, and then ask\nwhether there are any other objectives. In this case there are no other\nobjectives. Some students will try to lay out a structure, which is not\nnecessary. The structure is basically a list of pros and cons and a running\nof the numbers. The candidate should ask for a minute to write down his\nthoughts and then give you all the pros first, then the cons. If he tries to\ndo it off the top of his head, cut him off in mid-answer and say, \u201cI get it.\nWhat else do you have?\u201d How quickly does he move on to the next pro\nor con? Also, the candidate should realize that because this is a toy store,\nalmost 80 percent of the business takes place in the last two months of\nthe year. So the warehouse sits three-quarters empty for nine months."}
{"276": "sits three-fourths empty for 9\nmonths out of the year.\nTakes worry and stress out of Loss of control: can\u2019t\nfulfillment. Allows the client to individualize packages. If the\nfocus on marketing and selling client chooses Amazon,\n\u2013 what it does best. everything is shipped in an\nAmazon box. Loss of brand\nrecognition.\nOutside distributors have What if this fails? The client\nmultiple warehouses around already closed their warehouse\nthe country for faster delivery and fired their employees so\n(can shave a day or two off they\u2019d need to find another\ndelivery time), lower shipping vendor and have their entire\ncosts, and use less capital if inventory shipped to another\nthey expand. distributor\u2019s warehouses. If this\noccurs during holiday season,\nthey might not be able to find\nanother distributor.\nThe client doesn\u2019t have to ramp Reduces profit (can\u2019t make\nup during the busy holiday money on shipping). We\nseason by hiring seasonal charge an extra dollar per\nemployees, then lay people off package shipped. This is a"}
{"277": "\u2022 Average toy sells for $20.\n\u2022 Warehouse rental costs, utilities, and equipment cost $75,000 a\nyear.\n\u2022 Warehouse insurance costs $9,000.\n\u2022 Yearly labor costs for four full-time employees and benefits totals\n$200,000.\n\u2022 Thirty additional holiday employees at $10 per hour, 8 hours a day,\nfor 20 days a month, for 2 months. No benefits.\n\u2022 Danny\u2019s fulfillment center storage costs are $0.45 per cubic foot\n(10,000 cubic feet for 10 months, 100,000 cubic feet for 2\nmonths).\n\u2022 Restocking fee of $2 per item for any toys that are returned. They\nhave a 3 percent return rate.\n\u2022 The client pays the distributor a $1 handling fee per package.\nWatch to see how well he organizes his notes. He should walk you\nthrough his calculations.\nNumber of packages; revenues and profits\n250 orders a day, 20 days a month = 5,000 packages x 10 months equals\n50,000 packages 2,000 orders a day times 20 days a month = 40,000\npackages x 2 months equals 80,000 packages"}
{"278": "Warehouse rent and equipment (yearly) = $75,000\nWarehouse insurance (yearly) = $9,000\nWarehouse Costs\n$200,000 + 96,000 + 75,000 + 9,000 = $380,000\nWarehouse Income ($1 handling fee per package) = $130,000 (it is\nimportant that the student take this into consideration and subtract it\nfrom the warehouse costs).\nNet Warehouse Costs: $380,000 \u2013 130,000 = $250,000\nDanny\u2019s Fulfillment Center\n$0.45 x 10,000 = 4,500 x 10 months = $45,000\n$0.45 x 100,000 = 45,000 x 2 months = $90,000\nExtra shipping credit = 130,000\n3 percent return rate = 130,000 x .03 = 3,900 x $2 = $7,800\n$45,000 + $90,000 + $130,000 + 7,800 = $272,800\nWhile the cost of shipping the toys in-house is $22,800 cheaper, the\nstudent needs to take other things into consideration and put it into\nperspective before making a decision.\nFORCE THE STUDENT TO MAKE A DECISION."}
{"279": "\u2022 The company could pass Danny\u2019s $1 handling fee on to the\ncustomer.\n\u2022 Shipping costs would be lower.\n\u2022 The industry is headed toward free shipping during the holidays,\nso they would not receive $80,000 out of the $130,000 handling\nfee.\n\u2022 If the company is worrying about a bad distributor, they could go\nwith two distributors the first year and then choose one. They\nmight lose some volume discounts, but this strategy would be far\nless expensive than shipping toys from ten of Danny\u2019s warehouses\nto ten warehouses owned by a new distributor.\nIf he chooses to outsource distribution, your argument is:\n\u2022 They would maintain control of their brand and delivery times.\n\u2022 They have a warehouse sitting three-quarters empty 10 months out\nof the year. That is a real opportunity. The company could bring\nseasonal balance to their warehouse, and maybe to their bottom\nline too, by generating additional revenues: Because the\nwarehouse sits empty most of the year, the client could rent part of\nit out, or could begin to distribute products for other companies.\nThey could expand their website to include items that ship in the\nspring \u2013 e.g., patio furniture, pool equipment, or toys."}
{"280": "\u2022 Defending his recommendation without getting defensive.\nA great interview consists of all of the above, plus:\n\u2022 Putting the $22,800 in perspective.\n\u2022 Realizing that the handling fee could be passed on to the customer.\n\u2022 Suggesting that the industry was headed toward free shipping\nduring the holidays and what that would mean for the client.\n\u2022 Exploring additional revenue opportunities for the warehouse.\nNote: I\u2019ve given this case at least 200 times. Only one person came up\nwith the dual vendors; only one person suggested that the customers pay\nthe handling fee; only four people came up with additional revenue\nstreams for the warehouse. And no one spoke about free shipping."}
{"281": "KBO APPLIANCES\nProblem Statement\nKBO Appliances, a US maker of kitchen appliances, saw its stock price\ndrop from $34 to $30 a share on news that the experts had called the new\nline of products \u201cpedestrian,\u201d meaning lacking imagination. While the\nCEO ordered the design team to redesign the major products, she also\nwants to enter the college market. She wants to know if it makes sense to\ndevelop and market a mini-fridge/microwave combo.\nWhat is the size of this market? How would you price the product and\nhow much profit could we expect?\nGuidance for the Interviewer\nThe student should summarize the case to make sure everyone is on the\nsame page. She should quantify the change in stock price by percentage\n(the stock dropped by about 12 percent), and should also inquire about\nwhether there are any other objectives to be concerned with \u2013 in this\ncase, no.\nWe are looking for several things. What does the student want to know in\norder to make a decision? Ideally, she would ask about everything listed\non the chart below, but that is unlikely. You can give her the detailed"}
{"282": "variables that will come into play, she should fill in the chart. At the end\nof the case, she should turn the chart toward the interviewer. Her\nsummary is just going through the chart \u2013 touching on all eight issues \u2013\nand then making her decision. If she doesn\u2019t draw a similar chart, show\nher this one at the end of the interview.\nStart off by asking her what she wants to know.\nMarket-sizing. This is a general population problem. Start with 320\nmillion Americans. Assume that the average life expectancy of an\nAmerican is 80 years and that there are even numbers of people in each\nage group. Thus, there are same number of 2-year-olds as 72-year-olds.\nThat means each group has 4 million people. The average age of college\nstudents is 18 to 22 \u2013 covering five years (my daughter tells me it takes\nfive years to graduate from college now), so that\u2019s 20 million Americans\nof college age. We care only about full-time students who live in dorms.\nAssume that 8 million go to college full-time; 2 million are freshmen, 2\nmillion are sophomores, etc. Assume that all freshmen live in dorms and\nthat the number of students living in dorms falls off as they go through\nschool. Also, most freshmen have a roommate, so cut the 2 million\nnumber in half. That\u2019s 1 million freshmen as the market. Assume that all\nof the others add up to another million, taking into account that students\nwho buy the fridge will hold onto it year after year."}
{"283": "Mini-Fridge & Microwave\nMarket size 2 million units per year\nCompetition and 3 current products priced between $300 and\nproduct $350. Our product will be the fourth and will\ndifferentiation have a cow design on the outside of the\nproduct.\nCost (our $150 per unit\nmanufacturing\ncost)\nPricing In determining the price, the student should\ntake several things into consideration: (1) our\ncosts, (2) competition, (3) what students are\nwilling to pay, and (4) the fact that the CEO\nwants to enter this market.2 She should also\ntake into consideration that there is a 20\npercent retail markup. At what price do we\nsell to the retailer? Can we sell direct to\nschools and bypass the retailer? If we do\nthat, then follow-up sales will be lower\nbecause the schools will hang onto them year\nafter year."}
{"284": "response A tiger design? Get the company\u2019s customers\nto sign a long-term contract?\nOther factors Will we cannibalize our other products? (no)\nWhat other markets (e.g., small business,\nsenior housing, hospitals) can we expect to\npull from?\nDecision Look for a \u201cgo/no go\u201d decision.\nGuidance for the Interviewer\nThe student should have touched on all the issues above in a similar\norder. That is, we can\u2019t determine the company\u2019s estimated market share\nuntil we know the price. And we can\u2019t calculate profits until we know\nthe estimated market share. Does the student see the big picture? Does\nshe understand how everything is tied together?\nSummary\nThe perfect summary is walking you through the final slide. Don\u2019t\nexpect that \u2013 very few students ever make the final slide. If the student\ndoes it, she is exceptional! But she needs to jump right into the summary\nwithout any downtime to collect her thoughts. A good summary is about\na minute, minute-and-a-half long. It is not a rehash of everything she"}
{"285": "\u2022 Developing a good short summary using the chart and touching on\nthe most critical points.\nMark of a Good Case Discussion: Besides drawing the final slide, did\nshe have everything in the right order? Did she pick up on the fact that\nthe universities are the biggest buyers and that having an energy-efficient\nmodel would be the most appealing to them? I also look to see if the\nstudent suggests a price war as a response. If she does, tell her to avoid\nthat option in future cases if possible. Most firms view it as a knee-jerk\nreaction.\n2 The CEO wants to enter the market so she can introduce the quality of her\nproducts and brand to students in hopes of having them make bigger purchases as\nthey go through life (think long-term greed). This discovery should lead to\npricing the unit on the lower end of the price range \u2013 probably the lowest."}
{"286": "BULLETPROOF AUTO GLASS\nProblem Statement\nOur client is an applied materials manufacturer who wants to enter the\nbulletproof auto glass industry. They\u2019ve developed a one-way bullet-\nresistant glass. Bullets can\u2019t be fired into the car, but you can fire out.\nThis new glass is lighter and cheaper to manufacture. Word has leaked\nout, and their stock has jumped from $15 to $18 a share. They want you\nto calculate the size of the worldwide bulletproof auto glass market,\ncome up with a pricing strategy, determine the plant location, and make a\ndecision on whether to enter this market.\nGuidance for the Interviewer\nMake sure that the student summarizes the case and verifies the\nobjective(s) (calculate market size, come up with a pricing strategy,\ndetermine plant location, and decide whether to enter the market). When\nsummarizing, he should state that the stock jumped 20 percent, not $3.\nHe should also ask whether there are any other objectives about which he\nshould be concerned. In this case the answer is no. After he verifies the\nobjectives he should take a moment and lay out his structure.\nIdeally, his notes should look something like this."}
{"287": "Because this is a new product not only for the company but also for the\nindustry, the student might want to solve the pricing issue before figuring\nout the market size.\nData Bank\nThe information below should help you answer any of the student\u2019s\nquestions. If he asks you something you don\u2019t know, just say that it\u2019s not\nrelevant. If he doesn\u2019t ask you about all these areas, turn the question\nback on him (i.e., \u201cWhat do you think are some of the barriers to\nentry?\u201d) I look carefully at the structure. Ideally, he should lay out his"}
{"288": "\u2022 What constitutes a success? 10 percent market share within 3 years\nThe market\n\u2022 Major players and market share. Four players, 25 percent each.\nTwo are divisions of larger companies; two are smaller players.\nGrowing by 10 percent a year last five years.\n\u2022 Product differentiation. Ours would be lighter, cheaper to\nmanufacture, and one-way. No other one-way on the market.\n(Disadvantage: Once you fire out, the integrity of the glass is\ncompromised.)\n\u2022 Barriers to entry. Contracts with original equipment manufacturers\n(OEM) usually take four years to get established. OEMs make up\n80 percent, aftermarket 20 percent\nNote: If this company wants 10 percent market share and an OEM\ncontract takes four years, then they need to take market share out of the\naftermarket, which means that they need to get 50 percent of the\naftermarket \u2013 very unlikely without an acquisition.\nPricing\n\u2022 Company objective: 10 percent market share within 3 years.\n\u2022 Competitive analysis: Four competitors between $400 and $450\n/sq. ft."}
{"289": "What is the size of the worldwide bulletproof auto glass market?\nClients that use bulletproof glass are militaries, corporations, and\ngovernment agencies such as the FBI, local police, and their counterparts\naround the globe. Royalty, celebrities, mob bosses, drug lords, and\narmored trucks also use bulletproof glass. Plus, the market needs to\nbroken down into the OEM market and the aftermarket.\nThe best way for the student to solve this is to make some assumptions\nabout the US and extrapolate it out to the world market. Start with\nassumptions:\n1 percent of US vehicles have bulletproof glass.\nUS vehicles make up 10 percent of the world market.\nNext, figure out the number of US cars/vehicles on the road. This is a\nhousehold problem.\nAssume US population is 320 million, 3.2 people per household, or 100\nmillion US households. Next, break this data down by income levels. He\nshould draw this chart and walk you through his thought process for each\nlevel."}
{"290": "has a life expectancy of 10 years, which means 20 million new cars sold\nevery year.\n20 million new cars x .01 = 200,000 new cars with bulletproof glass in\nthe US\n180 million used cars x .01 = 1.8 million used cars with bulletproof glass\nTotal 2 million cars in US have bulletproof glass\nThe US has 10 percent of market \u2013 total market is 20 million cars with\nbulletproof glass worldwide\nRemember, his numbers might be very different, which is okay, provided\nthat there were solid assumptions and logic behind his thoughts.\nPlant location\nHe should be asking several questions. Where are the company\u2019s other\nplants? What are the bulletproof glass sales by region? Where are his\ncustomers?\nCurrent plants are in Ireland, Germany, Israel, the US, and Brazil. The\ncompany manufactures bulletproof glass for buildings in their Ireland\nplant. Other issues to take into consideration are the value chain, raw\nmaterials, labor, transportation, and customer base. The OEMs are\nlocated in Detroit, Tokyo, Berlin, and South Carolina.\nAnother consideration is whether acquiring a smaller player is discussed."}
{"291": "Entry into the market\nThe question here is not only should the company enter this market, but\nhow? What you should be looking for is for the student to suggest\nacquiring one of the smaller players, particularly if it has an OEM\ncontract. Remember, it takes four years to get one. This will speed up the\nprocess and help the company reach the 10 percent market share in three\nyears because each player represents 25 percent of the market.\nHere are five ways to enter the market. The student should review these,\nstating the advantages and disadvantages of each:\n\u2022 Start producing on their own and try to grow it organically.\n\u2022 Make an acquisition.\n\u2022 Sell their technology to another company.\n\u2022 Produce the glass for all the other companies (private label).\n\u2022 Do a joint venture.\nFinal slide: When a student gets a question that asks for \u201cthese four\nthings,\u201d he can usually make a final slide on a separate sheet of paper at\nthe start of the case. This acts as a scorecard as well as his summary. He\nshould fill it out as he goes along and then turn it toward the interviewer\nand walk him through it.\nBelow are some answers; the student may have come up with different"}
{"292": "Summary\nThe student needs to jump right into this without any downtime to collect\nhis thoughts. A good summary is about 30 seconds to a minute, possibly\na minute-and-a-half long at the most. It is not a rehash of everything he\nspoke about; it is a very short recap of the problem and two or three main\ntakeaways that he wants the interviewer to remember. If he made the\nfinal slide, that will serve as his summary. He should turn it toward you\nand walk you through it.\nWhile most students will suggest that the company enter the market, one\nthing I look for is whether they suggest an acquisition and how they\nincorporate that into their overall market entry strategy and plant location\ndiscussion.\nA good interview consists of:\n\u2022 Repeating the question, verifying the objective, and asking about\nother objectives.\n\u2022 Laying out a clear structure, starting with the company.\n\u2022 Asking solid and poignant questions.\n\u2022 Making no math mistakes.\n\u2022 Keeping well-organized and easy-to-read notes.\n\u2022 Drawing a final slide.\n\u2022 Developing a good, short summary touching on the most critical"}
{"293": "STATIN BLUE\nProblem Statement\nOur client is a large pharma company similar to Johnson & Johnson. It\nhas developed a new cholesterol drug called Statin Blue. Because Statin\nBlue is on the lower end of the statin chain (meaning that it is weaker\nthan traditional prescription cholesterol drugs), the company has the\noption of releasing this drug either as an over-the-counter product or as a\nprescription drug.\nYou need to:\n\u2022 Determine which market the company should enter \u2013 as a\nprescription drug or as an over-the-counter (OTC) product.\n\u2022 Figure the breakeven in terms of the number of customers for both\nmarkets.\n\u2022 Calculate the estimated profits for both markets.\n\u2022 List the pros and cons of entering the prescription market.\nGuidance for the Interviewer\nMake sure that the student summarizes the question and verifies the\nobjectives (determining which market to enter, figuring the breakeven in\nterms of customers for both markets, calculating the estimated profits for"}
{"294": "pros first, then all the cons. Make sure she doesn\u2019t ping-pong between\npros and cons.\nPrescription:\n+Price \u2013 can charge more\n+Insurance coverage\n+Perceived higher value\n+Good established sales force\n+Push & pull marketing to both doctors and patients\n\u2013Highly competitive market\n\u2013One dominant player (Lipitor, with 49 percent of the market)\n\u2013Competitive response\nOver-the-Counter:\n+Easy access \u2013 don\u2019t need to see a doctor\n+High volume\n+Easier to market\n+Company brand name recognition\n+Potentially huge, untapped market\n\u2013Patients in the high-risk category already on prescription drug\n\u2013Viewed as less effective (which it is \u2013 10 percent)\n\u2013 Educating the public that they need this drug"}
{"295": "Special note \u2013 Our company does not produce any of the other statin\nproducts.\nData Bank\nOur Client\n\u2022 Large pharma with a large drug portfolio. Fifty percent of its\nproducts are OTC and 50 percent are prescription drugs. This\nshould tell the student that our distribution and marketing channels\nare well established in either market."}
{"296": "Statin Blue\n\u2022 Think of it as Lipitor Light \u2013 it\u2019s less effective.\n\u2022 Similar side-effects as with regular statin drugs, but less intense.\nSide-effects include muscle pain, diarrhea, sexual dysfunction,\ncognitive impairment, and possible liver damage.\nCosts\n\u2022 Costs are $40 million a year. This is the only cost number the\nstudent needs to know.\nPricing\n\u2022 The cost to the end-user will be the same regardless of which\nmarket we use. By prescription, the patient will pay a $15 a month\nco-pay. For OTC, the patient will pay $15 for a month\u2019s supply.\nProfits\n\u2022 The company makes a $20 profit per bottle sold in the prescription\nmarket. The patient would use one bottle a month, 12 bottles a\nyear. Yearly profit is $240 per patient per year.\n\u2022 The company makes a $5 profit per bottle sold in the OTC market.\nThe patient would use one bottle a month, 12 bottles a year. Yearly\nprofit is $60 per patient per year.\nRx Markets: Walk the student through the chart below. Explain that if"}
{"297": "Before there was Statin Blue, if your level was 210 or above, the doctor\nwould most likely prescribe one of the seven current drugs; however,\nyou would be overmedicating and at risk for the side-effects.\n250 Big Trouble\n(represents 10% of the overall prescription\ncholesterol market)\n220 Other Prescription Cholesterol Drugs\n(represents 80% of the overall prescription\ncholesterol market)\n200 Statin Blue 10%\n(We believe that the patients whose cholesterol\nlevels fall between 200 and 220 make up 10% of\nthe overall cholesterol market.\n180 Honey Nut Cheerios and Exercise\n<180 Healthy\nOTC Market: The market sizing of the OTC market is the hardest part"}
{"298": "percent have a cholesterol problem (14/240 = 5.8 percent, or round\noff to 5 percent).\n\u2022 That leaves an uninsured population of 60 million (300 million x\n.2 = 60 million). Thus we can assume that at least 5 percent of the\nuninsured, or 3 million people, have a cholesterol problem (60\nmillion times 20 percent = 3 million). I would add in an extra 1\nmillion because the uninsured don\u2019t receive constant health care\nand have a higher rate of obesity, heart attacks, and strokes.\n\u2022 Out of the 4 million uninsured who likely have a cholesterol\nproblem, we\u2019ll assume that 25 percent or 1 million people would\nbuy this drug. Many have higher priorities (like feeding their\nfamilies or paying rent) than spending $15 a month on a drug for a\nproblem that they can\u2019t see or feel.\n\u2022 To that 1 million I\u2019d add a group of people ages 20 to 40. These\nare people who don\u2019t have a cholesterol problem, but their parents\nor relatives do. Because cholesterol levels are tied into genetics,\nthey know that they are on the same path as their parents and may\nwant to take something now to prevent problems later.\n\u2022 Three hundred million Americans divided into four even\ngenerations of 75 million each, thus there are 75 million\nAmericans aged 20 to 40.\n\u2022 Assume that 5 percent of their parents have a cholesterol problem.\nFive percent of 75 million is 3.75 million, or 4 million. Out of that"}
{"299": "$40 million in costs divided by a $20 per-bottle profit equals 2 million\nbottles divided by 12 months equals 166,667 customers.\n\u2022 OTC Breakeven:\n$40 million in costs divided by a $5 per-bottle profit equals 8 million\nbottles, which divided by 12 months equals 667,000 customers.\nProfit Calculations\n\u2022 Prescription: 560,000 customers x $240 = $134.4 million\n\u2022 OTC: 2 million customers x $60 = $120 million\nWhat would you do?\nThe Final Slide\nThis is critical. If the student has the foresight to build the final slide, she\nwill stand out from all the other candidates. Whenever you have a case\nthat compares two or more strategies, options, or ideas, and you are\napplying the same criteria to both, you should build the final slide almost\nimmediately. As you calculate the numbers, fill them in on the final\nslide; it keeps all relevant information in one place and makes it easier\nfor the interviewer to follow. Once all the information is filled out, the\nstudent turns the final slide toward the interviewer and walks him\nthrough it. It is the best summary. It is also similar to the final slide of a"}
{"300": "Market Choice\nOther Concerns\nSummary\nThe interviewee needs to jump right into this without any downtime to\ncollect her thoughts. A good summary is about a minute, minute-and-a-\nhalf long. It is not a rehash of everything you spoke about; it is a short\nrecap of the problem and two or three main points that the student wants\nthe interviewer to remember.\nDid the interviewee \u2026\n\u2022 repeat the question, verifying the objective and asking about other\nobjectives?\n\u2022 make math mistakes?\n\u2022 design the final slide?\n\u2022 develop a logical and well-thought-out market-sizing process?\n\u2022 draw the interviewer into a discussion?\n\u2022 produce a good short summary touching on the most critical\npoints?\n\u2022 have well-organized and easy-to-read notes?\nMark of a Good Case Discussion: Besides the final slide, students"}
{"301": "Note: Make sure the student\u2019s notes are neat, well-organized, and easy-\nto-read. At the very least she should have divided her notes into\nprescription and OTC. Did she draw a line down the middle of her\npaper? Did she do her prescription and OTC calculations on separate\nsheets of paper?"}
{"302": "BOTTLED WATER\nProblem Statement\nLast year Americans bought more than 4 billion gallons of bottled\ndrinking water. Our client sold 1 billion bottles of spring water in the .5-\nliter size. In the past, our client has purchased the empty bottles from a\nguy named Ed for 5 cents each. These bottles are made from\npolyethylene terephthalate (PET), which is a combination of natural gas\nand petroleum.\nEd wants to raise his price by one penny, which would increase the\nclient\u2019s costs by $10 million. The client is considering in-house bottle\nproduction but currently does not have the resources to do it. The CFO\nwants you to determine whether this makes sense. The CFO requires a\ntwo-year payback (breakeven) on investments and wants to know if the\ncompany should in-source the bottle production.\nBesides determining whether to in-source bottle production, can you tell\nme what market share our client has of the US bottled water industry?\nGuidance for the Interviewer\nMake sure that the student summarizes the question and verifies the\nobjective (determining whether to in-source bottle production) before"}
{"303": "Easiest way is to turn everything into pints:\n4 billion gallons = 32 billion pints\n1 billion .5 liters = 1 billion pints\n1 billion pints / 32 billion pints = about 3 percent of the market\nAsk the student: Before we get into the numbers, just off the top of your\nhead, can you lay out the pros and cons of in-house production?\nPros Cons\nPossible lower price Added risk\nMore control over costs in the No experience\nfuture\nProduce and sell to competitors Big investment setup costs\nEconomic variables out of our\ncontrol\nIf the student misses any from the list above, don\u2019t tell him what he\nmissed. He\u2019ll figure it out later. Show the student this chart."}
{"304": "Maintenance\nTransportation N/A ?\nAdmin N/A $1m\nCost per bottle $0.06 ?\nExplain that the left-hand column represents the costs associated with\nbottle production. The middle column represents what the client would\npay Ed (1 billion bottles x 6 cents each = $60 million). The right-hand\ncolumn represents the client. Some of the numbers have been filled in;\nothers are still a mystery and need to be figured out.\nData Bank\nCOGS \u2013 The cost per gallon of PET pellets is $5. It takes 10 gallons to\nmake 1,000 bottles. We want to make a billion bottles.\nAnswer: $50 / 1,000 = $0.05 per bottle\n$0.05 x 1 billion bottles = $50 million\nLabor \u2013 Twenty people x $4,000 a month each = $80,000 a month.\nAnswer: $80,000 x 12 months = $960,000 per year (don\u2019t round up)."}
{"305": "Building N/A $6m\nEquipment N/A $6m\nLabor N/A $.96m\nUtilities & N/A $.04m\nMaintenance\nTransportation N/A $5m\nAdmin N/A $1m\nCost per bottle $0.06 $0.067\n$67 million / 1 billion bottles = $0.067 per bottle\nClient $0.067\nOutsource -$0.060\n$0.007 x 1 billion bottles = $7 million\nThe first year, we pay $7 million more than if we had purchased the"}
{"306": "COGS $63m $52.5m\nBuilding N/A N/A\nEquipment N/A N/A\nLabor N/A $.96m\nUtilities & N/A $.04m\nMaintenance\nTransportation N/A $5.25m\nAdmin N/A $1m\nCost per bottle $0.06 $0.056\nHave the student come up with the new dollar total, which is $59.75\nmillion.\n(52.5 + .96 + .04 + 5.25 +1 = $59.75)\nI never make the student divide 59,750,000 by 1,050,000,000; just tell"}
{"307": "Remember: The CFO wanted a two-year payback.\nTell the student that in Y3, the company will end up with a $3.31 million\nprofit, so we are $510,000 in the black.\nWhat do you tell the CFO?\nThe student will most likely say that we won\u2019t break even in two years;\nhowever, if we stay the course, we will become profitable in Year 3.\nTell the student that the CFO wants a two-year payback and that he\nneeds to figure out (1) a way to break even through increasing revenues;\nand (2) a way to break even by lowering expenses.\nRevenues side: Produce more bottles to sell to competitors.\nExpense side: Lease the building and equipment.\nSummary\nThe interviewee needs to jump right into this without any downtime to\ncollect his thoughts. A good summary is about a minute, minute-and-a-\nhalf long. It is not a rehash of everything you spoke about; it is a short\nrecap of the problem and two or three main points that the student wants\nthe interviewer to remember.\nDid the interviewee \u2026"}
{"308": "and say, \u201cIf I go in to the meeting now, I will have to tell the CFO that\nwe can\u2019t do it in two years. Before I go in, I want to figure out a way to\ndo it in two years. That way I can tell the CFO, yes we can!\u201d\nThis shows great forethought and a can-do attitude that any consultant\nwould love."}
{"309": "TEDEX\nProblem Statement\nYour client is TedEx (very similar to FedEx). It has annual revenues of\n$40 billion. It ships 2.5 billion packages a year. The average customer\nships five packages per year.\nIn a survey, its customer service ranked high in all areas except lost\npackage compensation. The complaints included:\n\u2022 TedEx never tells me when my package is lost.\n\u2022 Time-consuming, multi-step process that puts the responsibility on\nthe customer.\n\u2022 Four-week payout timetable (takes four weeks to get a check).\nWhat are the lost packages and poor service costing TedEx, and how can\nit improve its customer service?\nGuidance for the Interviewer\nThe student should repeat the question to make sure everyone is on the\nsame page, and she should verify the objective \u2013 how can we improve\ncustomer service and increase profits? She should also ask whether there\nare any other objectives she needs to be concerned with \u2013 in this case,"}
{"310": "The student should ask: How many packages does TedEx lose each year,\nand what does this cost them?\nTell her: TedEx loses 3 percent of its packages, and 80 percent of those\nlost packages were insured for the minimum $100. This is the group we\ncare about \u2013 this 80 percent. We are assuming that every package is\nautomatically insured for $100 regardless of the contents.\nOf the 80 percent of the lost packages with an automatic value of $100,\n20 percent of those customers give up on their claim because the process\nis too cumbersome. Of that 20 percent, 25 percent abandon TedEx and\nswitch to another carrier.\nHere are the numbers: TedEx loses 3 percent of its packages (75\nmillion: 2.5B x .03 = 75 million) and 80 percent of those (60 million: 75\nmillion x .8 = 60 million) packages were insured for the minimum $100.\nWe are assuming that every package is automatically insured for $100\nregardless of the contents.\nOf those 60 million lost packages with an automatic value of $100\n(remember, this is the group we care about), 20 percent (12 million: 60\nmillion x .20 = 12 million) of the customers give up on their claim\nbecause the process is too cumbersome. Assume that customers lose\nonly one package, so think of packages and customers as the same."}
{"311": "Note: How the student lays out her notes becomes very important. The\nbiggest mistakes I see in this case result simply because the student\u2019s\nnotes are a mess. If she uses a tree design, then her notes will be clean\nand easy to follow, and she\u2019ll be able to find the information she needs\nquickly. Ideally, her notes should look something like the above. If they\ndon\u2019t, then at the end of the case, show her this.\nHow much does TedEx lose in future sales?\nTwenty-five percent of the 12 million, or 3 million (3 million: 12 million\nx .25 = 3 million) switch to another carrier. If the average revenue per\npackage is $16 ($16: $40 billion/2.5 billion = $16) and if the average\ncustomer ships 5 packages a year, that\u2019s equal to 15 million (15 million:\n3 million x 5 number of average packages sent = $15 million packages a\nyear times the $16 average revenue per package). Thus, TedEx loses\n$240 million a year in future sales \u2013 with customers switching after a\nfrustrating experience ($16 x 15 million = $240 million).\nHow much does it cost TedEx to process the lost package claim?\nAssume that it costs TedEx $4 per claim considering labor, processing\ntime, and overhead; 60 million packages times $4 equals $240 million.\nHow much does TedEx pay out in insurance?\nAssume that TedEx pays its own insurance; 80 percent of the 60 million"}
{"312": "That totals $5.28 billion. Ask the student what percentage of the total\nrevenues that represents. Answer: About 13 percent (5.28/40 = 13\npercent).\nGuidance for the Interviewer\nTell the student that a team down the hall came up with options to\nstreamline the process. The outcome was:\n1. The $4 lost package processing fee was reduced to $1 per lost\npackage.\n2. The customers who were going to leave TedEx\n3. actually stay.\n4. TedEx received very high marks on its customer service.\n5. The number of successful claims rose from 80 percent to 100\npercent.\nAsk: What does this mean for TedEx?\n1. The processing fee was reduced to $60 million from $240\nmillion.\n2. We don\u2019t lose $240 million in defecting customers.\n3. TedEx\u2019s customer service received high marks (which was one\nof the objectives).\n4. Insurance payouts grew from $4.8 billion to $6 billion; TedEx is\nnow paying out $1.2 billion more in insurance claims."}
{"313": "c. It might take customers three to six months to use up the\ncredit, or they may never use it.\nd. If the customer does use it and TedEx\u2019s margins are 50\npercent, then it is paying out only $25 in actual services.\nSummary\nThe candidate needs to jump right into to this without any downtime to\ncollect her thoughts. A good summary is about a minute, minute-and-a-\nhalf long. It is not a rehash of everything you spoke about; it is a short\nrecap of the problem and two or three main points that she wants the\ninterviewer to remember.\nTedEx has received poor marks on customer service relating to lost\npackages. It is also paying out an extraordinarily high 13 percent of its\nrevenues in lost package expenses. We\u2019ve worked to streamline the\nclaims process, which resulted in better customer service marks, but this\nalso greatly increased the amount of money TedEx pays out in claims.\nWe\u2019ve come up with three ways to reduce those payouts: reduce the\nnumber of lost packages, reduce the dollar amount of an automatic\npayout from $100 to $50 and credit the claim money to the customer\u2019s\nTedEx account. These three steps should greatly reduce the amount of\nmoney TedEx pays out in lost package claims."}
{"314": "Mark of a Good Case Discussion: Did the student realize that\nincreasing successful claims from 80 percent to 100 percent was going to\nbe expensive? Did she recognize that maybe TedEx had made the\nprocess cumbersome on purpose so that fewer people would claim their\nmoney (think mail-in rebate)? In addition, the last idea (crediting the\ncustomer\u2019s TedEx account) is the one idea that very few students ever\nget. So if they get it, they deserve big points."}
{"315": "GASSED UP\nProblem Statement\nOur client is a major rental car company. It has seen its revenues stall\nover the last three years, partly because of the economy and partly\nbecause of ridesharing. It has come up with a new program to increase its\nrevenues and wants us to vet the program.\nThe new program focuses on their business clients and their refueling\noptions. As you know, when you rent a car in the US you have a choice\nof three refueling options:\n1. Prepay: The customer can prepay for a full tank of gas at market\nprices and return the car without stopping to fill up the tank.\n2. We Fill: The customer returns the car without a full tank and the\nrent-a-car company fills the tank for the customer but charges\nthem a high premium.\n3. You Fill: Customers fill the tank themselves before returning the\ncar so as not to incur extra gas charges.\nThe fourth option and the one the client wants us to vet, is an annual\nprepay. Business clients would have the option of paying $500 a year\nupfront in return for the ability to return any car with any level of gas for"}
{"316": "if there are any other objectives or goals that she should be concerned\nwith. In this case, the answer is YES: (1) Increase the company\u2019s\nrevenues by 10 percent cumulatively over two years, and (2) Make sure\nthat the program is profitable. No set profit goal \u2013 the company just\ndoesn\u2019t want to lose money.\nClarifying question. Two good clarifying questions in this case are (1)\n\u201cWhat were the revenues and profits last year?\u201d ($2.2 billion; thus the\nprogram\u2019s revenue goal is $220 million combined over two years), and,\n(2) \u201cDoes any other company offer a similar program or would we be\nfirst to market?\u201d We would be first to market.\nNote: This case is not about ridesharing. Many students will focus their\nstrategy on ridesharing. If they do, tell them that another team down the\nhall is working on that; we are working on this program.\nThe student should lay out the advantages and disadvantages as well as\nher structure. The interviewer is looking for the student to get a least\nthree on each side. A plus if they get the cannibalization disadvantage.\nAdvantages Disadvantages\n\u2022 Client convenience \u2022 Price of gas will fluctuate"}
{"317": "The structure that the student lays out should identify what you would\nneed to know in order to vet the program and make a recommendation.\nHopefully the student will bring up at least four of these.\n\u2022 Company revenues and profits for the last three years broken down\nby revenue stream\n\u2022 Number of business clients and what percent of the business\nclients would sign up for this program\n\u2022 Average number of times the typical business traveler travels per\nyear\n\u2022 Average rental charge per trip and the company\u2019s profit margin\n\u2022 Percentage of cars returned in each of the three refueling options\nData Bank\nGive this information to the interviewee.\nCorporate revenues last year were $2.2 billion, profits were $250 million\n(thus the program\u2019s revenue goal is $220 million over two years).\nThe average business rental is $250 per trip and the profit margin is 40\npercent.\nIn year 1, the company will open this program only to existing business\ncustomers. It will only count the $500 gas fee toward revenues, not the"}
{"318": "Answer 1. We need to determine the number of program members and\nmultiply that number by $500.\n1.5 million customers x .60 will give us the number of business clients x\n.20 will give us the number of program members.\n1.5 million x .60 = 900,000\n900,000 x .20 = 180,000\n180,000 x $500 = $90,000,000 (an easy way to do this calculation is to\ntake 180,000 and multiply it by 1,000 and cut that number in half.)\nProgram revenues Y1 = $90,000,000\nQuestion 2. What are Y2 costs and profits?\nFixed costs: Tell the student that there is a one-time implementation cost\nof $2 million. This is to upgrade software, train employees, and market it\nto current business clients. There is also a $5 million-a- year\ncannibalization cost for the first two years.\nVariable costs: The average business client travels 20 times a year. We\nestimate that the company will need to fill each returning car with 10\ngallons of gas \u2013 $2.50 a gallon.\nFixed cost equal $7 million; that includes the $2 million implementation\ncost plus the $5 million cannibalization cost."}
{"319": "Profit at the end of Y1 = -$7 million\nQuestion 3. What are the revenues for Y2?\nTell the student that all 180,000 customers will stay with the program. To\nthat we will also see a 5 percent increase of that 180,000. This increase\nrepresents new customers who moved over from our competitors. (See if\nthe student remembers to include the new customers\u2019 gas fees and rental\nfees to the total program revenues.)\nOld customers: 180,000 x 500 = $90 million\nNew customers: 180,000 x .05 = 9,000 new customers\n9,000 x $500 gas fee = $4,500,000 + 9,000 x $250 per trip x 20 trips\n250 x 20 = 5,000 x 9,000 = $45 million\nTotal revenues for Y2 = 90m + 4.5m + 45m = 139.5m, or $140m\nQuestion 4. What are the costs and profits for Y2?\nFixed costs: Cannibalization cost $5m\nVariable costs:"}
{"320": "costs are 60 percent.)\nTotal costs: $126.5, or $127m\n$5m cannibalization\n$90m old customers\u2019 gas costs\n$4.5m new customers\u2019 gas costs\n$27m new customers\u2019 rental costs\nProfit in Y2 = 13m\n140 \u2013 127 = $13m\nQuestion 5. What\u2019s your recommendation?\nIn an ideal situation, the student should lay out a recommendation on a\nseparate sheet of paper as shown below. The top part is the final slide. He\ncould have made that chart almost immediately once he knew that the\ninterviewer was looking for revenues and profits over a two-year period\nand that there was a set goal. He could turn the sheet toward the\ninterviewer and walk her through it."}
{"321": "RISKS: Gas prices will fluctuate. It will cannibalize sales from the We\nfill and Pre-pay programs. The program is easy to copy, so a competitor\nmay copy the program and start a price war. However, the client makes\nmore money from rental income than it will lose through cannibalization\nor running with a loss on the gas income. The other risk is that we won\u2019t\nget the 9,000 new customers, which is where we saw the revenue hockey\nstick."}
{"322": "NO, the company should not start this program.\nEven though our analysis suggests that the company will meet both\ngoals, the risk of getting 9,000 new customers and having them travel 20\ntimes a year is unlikely. The customers who will sign up for this program\nare the ones who will use it the most. Gas prices will mostly likely go up,\nand the competition will start a price war.\nRISKS: That the client will lose an opportunity to differentiate itself\nfrom its competitors. Revenues will remain flat and ride share will\ncontinue to eat into our profits.\nNEXT STEPS: Address the ridesharing problem head on by starting its\nown ridesharing service.\nSummary\nThe perfect recommendation is walking you through the final slide.\nDon\u2019t expect that \u2013 very few students ever make the final slide.\nA good interview consists of:\n\u2022 Repeating the question, verifying the objective, and asking about\nother objectives.\n\u2022 Drawing the pie charts, making the recommendation visual"}
{"323": "6: FINAL ANALYSIS\nMost of this is psychological. The biggest assets a candidate can bring to\nan interview are a measure of confidence, a perspective of self-worth,\nand a good night\u2019s sleep. The interview structure is daunting, the people\ngenerally intimidating, and the atmosphere tense, but you can slay all\nthese dragons immediately when you choose to arm yourself with a\npositive self-image. In the end, it\u2019s not whether you are right or wrong,\nit\u2019s how you present yourself, your information, and your thinking. This\nis the measure of marketability for the firm \u2013 and it\u2019s what they seek to\ndetermine through an imperfect process.\nPractice before your interview. No athlete you ever admired went out on\nthe playing field without warming up. You need to do the same thing.\nYou need to get into the right mindset. Log on to CQI and run through\nsome math problems, market-sizing cases, or maybe even a full case. Do\na few case starts with the Video Vault. It\u2019s difficult to walk in off the\nstreet and do a case cold. When you did two or three practice cases with\nyour friends, you probably did better on the second or third case than the"}
{"324": "It\u2019s easy to forget that the firms know you can do the work \u2013 they\nwouldn\u2019t be interviewing you if they didn\u2019t think you were smart enough\nto succeed. Now it\u2019s just time to prove them right.\nCase closed!"}
{"325": "7: PARTNER CASE CHARTS\nNERVES OF STEEL\nSTATIN BLUE"}
{"326": "Equipment N/A $4m\nLabor N/A ?\nUtilities & N/A $.04m\nMaintenance\nTransportation N/A ?\nAdmin N/A $1m\nCost per bottle $0.06 ?"}
{"327": "ABOUT THE AUTHOR\nMarc Cosentino is the CEO of CaseQuestions.com."}
{"328": "Cosentino travels internationally giving case workshops at 45 schools\nannually, training students on how to answer case questions, training\ncareer services professionals on how to give case interviews, and\nteaching PhDs in private-sector firms how to think like a businessperson.\nHe consults with and designs cases for private-sector firms, government\nagencies, and nonprofits. Cosentino is a graduate of Harvard\u2019s Kennedy\nSchool and the University of Denver.\nPhoto credit: Paul Wellman"}
{"329": "Additional Products by CaseQuestions.com\nCaseQuestions Interactive Online Training\nMath drills, market sizing questions, Ivy case drills, written and video\ncase starts, three day-of-interview-warmup sessions, and 10 interactive\ncases."}
{"330": "\u76ee\u5f55\nCopyright 1\nDedication 2\nAcknowledgments 2\nNote To The Reader 2\nContents 5\n1 : Introduction 9\n2 : The Interview 12\n+ Introduction 12\n+ Questions About You 12\n+ Why Consulting? 16\n+ Your Questions 18\n+ Why Should I Hire You? 21\n+ First-Round Telephone Interviews 22\n+ First-Round Skype/Zoom Interviews 22\n+ First-Round Group Case Interviews 23\n+ Getting Challenged 24\n+ Confidence 25\n+ Advice For International Students 25"}
{"331": "+ If You Get Stuck 48\n+ The Trouble With Math 49\n+ Notes Design 52\n+ Recommendations, Summaries, And The Final Slide 55\n+ Case Journal 59\n4 : Ivy Case System 61\n+ Best Case Thinking - Building Your Case Cred 61\n+ The First Five Steps 62\n+ Key Questions For Additional Scenarios 84\n+ \u201cIf\u201d Scenarios To Remember 94\n5 : Practice Cases 97\n+ Harley-Davidson 99\n+ Coors Brewing Company 112\n+ Case Starts 123\n+ Case Starts \u2013 Cases 124\n+ Case Practice 136\n+ Case List 136\n+ The Show\u2019s Over \u2013 Netflix 138\n+ Xhead 145\n+ Snow Job 153\n+ Flatline 158"}
{"332": "+ Cabana Feet 232\n+ Wireless Worries 240\n+ The Discount Brokerage Race 245\n+ The Best Shot 249\n+ Partner Cases 255\n+ Partner Cases List 255\n+ The Roommate\u2019s Guide 257\n+ Stuck 259\n+ Nerves Of Steel 265\n+ Gps App 270\n+ Finding Fulfillment 274\n+ Kbo Appliances 280\n+ Bulletproof Auto Glass 285\n+ Statin Blue 292\n+ Bottled Water 301\n+ Tedex 308\n+ Gassed Up 314\n6 : Final Analysis 322\n7 : Partner Case Charts 324\n+ Nerves Of Steel 324\n+ Statin Blue 324"}
{"45": "formance. In a scalable system, you can add processing capacity in order to remain\nreliable under high load.\nMaintainability has many facets, but in essence it\u2019s about making life better for the\nengineering and operations teams who need to work with the system. Good abstrac\u2010\ntions can help reduce complexity and make the system easier to modify and adapt for\nnew use cases. Good operability means having good visibility into the system\u2019s health,\nand having effective ways of managing it.\nThere is unfortunately no easy fix for making applications reliable, scalable, or main\u2010\ntainable. However, there are certain patterns and techniques that keep reappearing in\ndifferent kinds of applications. In the next few chapters we will take a look at some\nexamples of data systems and analyze how they work toward those goals.\nLater in the book, in Part III, we will look at patterns for systems that consist of sev\u2010\neral components working together, such as the one in Figure 1-1.\nReferences\n[1] Michael Stonebraker and U\u011fur \u00c7etintemel: \u201c\u2018One Size Fits All\u2019: An Idea Whose\nTime Has Come and Gone,\u201d at 21st International Conference on Data Engineering\n(ICDE), April 2005.\n[2] Walter L. Heimerdinger and Charles B. Weinstock: \u201cA Conceptual Framework\nfor System Fault Tolerance,\u201d Technical Report CMU/SEI-92-TR-033, Software Engi\u2010\nneering Institute, Carnegie Mellon University, October 1992.\n[3] Ding Yuan, Yu Luo, Xin Zhuang, et al.: \u201cSimple Testing Can Prevent Most Criti\u2010\ncal Failures: An Analysis of Production Failures in Distributed Data-Intensive Sys\u2010\ntems,\u201d at 11th USENIX Symposium on Operating Systems Design and Implementation\n(OSDI), October 2014.\n[4] Yury Izrailevsky and Ariel Tseitlin: \u201cThe Netflix Simian Army,\u201d techblog.net\u2010\nflix.com, July 19, 2011.\n[5] Daniel Ford, Fran\u00e7ois Labelle, Florentina I. Popovici, et al.: \u201cAvailability in Glob\u2010\nally Distributed Storage Systems,\u201d at 9th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI), October 2010.\n[6] Brian Beach: \u201cHard Drive Reliability Update \u2013 Sep 2014,\u201d backblaze.com, Septem\u2010\nber 23, 2014.\n[7] Laurie Voss: \u201cAWS: The Good, the Bad and the Ugly,\u201d blog.awe.sm, December 18,\n2012."}
{"46": "[8] Haryadi S. Gunawi, Mingzhe Hao, Tanakorn Leesatapornwongsa, et al.: \u201cWhat\nBugs Live in the Cloud?,\u201d at 5th ACM Symposium on Cloud Computing (SoCC),\nNovember 2014. doi:10.1145/2670979.2670986\n[9] Nelson Minar: \u201cLeap Second Crashes Half the Internet,\u201d somebits.com, July 3,\n2012.\n[10] Amazon Web Services: \u201cSummary of the Amazon EC2 and Amazon RDS Ser\u2010\nvice Disruption in the US East Region,\u201d aws.amazon.com, April 29, 2011.\n[11] Richard I. Cook: \u201cHow Complex Systems Fail,\u201d Cognitive Technologies Labora\u2010\ntory, April 2000.\n[12] Jay Kreps: \u201cGetting Real About Distributed System Reliability,\u201d blog.empathy\u2010\nbox.com, March 19, 2012.\n[13] David Oppenheimer, Archana Ganapathi, and David A. Patterson: \u201cWhy Do\nInternet Services Fail, and What Can Be Done About It?,\u201d at 4th USENIX Symposium\non Internet Technologies and Systems (USITS), March 2003.\n[14] Nathan Marz: \u201cPrinciples of Software Engineering, Part 1,\u201d nathanmarz.com,\nApril 2, 2013.\n[15] Michael Jurewitz: \u201cThe Human Impact of Bugs,\u201d jury.me, March 15, 2013.\n[16] Raffi Krikorian: \u201cTimelines at Scale,\u201d at QCon San Francisco, November 2012.\n[17] Martin Fowler: Patterns of Enterprise Application Architecture. Addison Wesley,\n2002. ISBN: 978-0-321-12742-6\n[18] Kelly Sommers: \u201cAfter all that run around, what caused 500ms disk latency even\nwhen we replaced physical server?\u201d twitter.com, November 13, 2014.\n[19] Giuseppe DeCandia, Deniz Hastorun, Madan Jampani, et al.: \u201cDynamo: Ama\u2010\nzon\u2019s Highly Available Key-Value Store,\u201d at 21st ACM Symposium on Operating Sys\u2010\ntems Principles (SOSP), October 2007.\n[20] Greg Linden: \u201cMake Data Useful,\u201d slides from presentation at Stanford Univer\u2010\nsity Data Mining class (CS345), December 2006.\n[21] Tammy Everts: \u201cThe Real Cost of Slow Time vs Downtime,\u201d webperformanceto\u2010\nday.com, November 12, 2014.\n[22] Jake Brutlag: \u201cSpeed Matters for Google Web Search,\u201d googleresearch.blog\u2010\nspot.co.uk, June 22, 2009.\n[23] Tyler Treat: \u201cEverything You Know About Latency Is Wrong,\u201d bravenew\u2010"}
{"47": "[24] Jeffrey Dean and Luiz Andr\u00e9 Barroso: \u201cThe Tail at Scale,\u201d Communications of the\nACM, volume 56, number 2, pages 74\u201380, February 2013. doi:\n10.1145/2408776.2408794\n[25] Graham Cormode, Vladislav Shkapenyuk, Divesh Srivastava, and Bojian Xu:\n\u201cForward Decay: A Practical Time Decay Model for Streaming Systems,\u201d at 25th\nIEEE International Conference on Data Engineering (ICDE), March 2009.\n[26] Ted Dunning and Otmar Ertl: \u201cComputing Extremely Accurate Quantiles Using\nt-Digests,\u201d github.com, March 2014.\n[27] Gil Tene: \u201cHdrHistogram,\u201d hdrhistogram.org.\n[28] Baron Schwartz: \u201cWhy Percentiles Don\u2019t Work the Way You Think,\u201d vividcor\u2010\ntex.com, December 7, 2015.\n[29] James Hamilton: \u201cOn Designing and Deploying Internet-Scale Services,\u201d at 21st\nLarge Installation System Administration Conference (LISA), November 2007.\n[30] Brian Foote and Joseph Yoder: \u201cBig Ball of Mud,\u201d at 4th Conference on Pattern\nLanguages of Programs (PLoP), September 1997.\n[31] Frederick P Brooks: \u201cNo Silver Bullet \u2013 Essence and Accident in Software Engi\u2010\nneering,\u201d in The Mythical Man-Month, Anniversary edition, Addison-Wesley, 1995.\nISBN: 978-0-201-83595-3\n[32] Ben Moseley and Peter Marks: \u201cOut of the Tar Pit,\u201d at BCS Software Practice\nAdvancement (SPA), 2006.\n[33] Rich Hickey: \u201cSimple Made Easy,\u201d at Strange Loop, September 2011.\n[34] Hongyu Pei Breivold, Ivica Crnkovic, and Peter J. Eriksson: \u201cAnalyzing Software\nEvolvability,\u201d at 32nd Annual IEEE International Computer Software and Applica\u2010\ntions Conference (COMPSAC), July 2008. doi:10.1109/COMPSAC.2008.50"}
{"48": ""}
{"49": "CHAPTER 2\nData Models and Query Languages\nThe limits of my language mean the limits of my world.\n\u2014Ludwig Wittgenstein, Tractatus Logico-Philosophicus (1922)\nData models are perhaps the most important part of developing software, because\nthey have such a profound effect: not only on how the software is written, but also on\nhow we think about the problem that we are solving.\nMost applications are built by layering one data model on top of another. For each\nlayer, the key question is: how is it represented in terms of the next-lower layer? For\nexample:\n1. As an application developer, you look at the real world (in which there are peo\u2010\nple, organizations, goods, actions, money flows, sensors, etc.) and model it in\nterms of objects or data structures, and APIs that manipulate those data struc\u2010\ntures. Those structures are often specific to your application.\n2. When you want to store those data structures, you express them in terms of a\ngeneral-purpose data model, such as JSON or XML documents, tables in a rela\u2010\ntional database, or a graph model.\n3. The engineers who built your database software decided on a way of representing\nthat JSON/XML/relational/graph data in terms of bytes in memory, on disk, or\non a network. The representation may allow the data to be queried, searched,\nmanipulated, and processed in various ways.\n4. On yet lower levels, hardware engineers have figured out how to represent bytes\nin terms of electrical currents, pulses of light, magnetic fields, and more."}
{"50": "groups of people\u2014for example, the engineers at the database vendor and the applica\u2010\ntion developers using their database\u2014to work together effectively.\nThere are many different kinds of data models, and every data model embodies\nassumptions about how it is going to be used. Some kinds of usage are easy and some\nare not supported; some operations are fast and some perform badly; some data\ntransformations feel natural and some are awkward.\nIt can take a lot of effort to master just one data model (think how many books there\nare on relational data modeling). Building software is hard enough, even when work\u2010\ning with just one data model and without worrying about its inner workings. But\nsince the data model has such a profound effect on what the software above it can\nand can\u2019t do, it\u2019s important to choose one that is appropriate to the application.\nIn this chapter we will look at a range of general-purpose data models for data stor\u2010\nage and querying (point 2 in the preceding list). In particular, we will compare the\nrelational model, the document model, and a few graph-based data models. We will\nalso look at various query languages and compare their use cases. In Chapter 3 we\nwill discuss how storage engines work; that is, how these data models are actually\nimplemented (point 3 in the list).\nRelational Model Versus Document Model\nThe best-known data model today is probably that of SQL, based on the relational\nmodel proposed by Edgar Codd in 1970 [1]: data is organized into relations (called\ntables in SQL), where each relation is an unordered collection of tuples (rows in SQL).\nThe relational model was a theoretical proposal, and many people at the time\ndoubted whether it could be implemented efficiently. However, by the mid-1980s,\nrelational database management systems (RDBMSes) and SQL had become the tools\nof choice for most people who needed to store and query data with some kind of reg\u2010\nular structure. The dominance of relational databases has lasted around 25\u201230 years\n\u2014an eternity in computing history.\nThe roots of relational databases lie in business data processing, which was performed\non mainframe computers in the 1960s and \u201970s. The use cases appear mundane from\ntoday\u2019s perspective: typically transaction processing (entering sales or banking trans\u2010\nactions, airline reservations, stock-keeping in warehouses) and batch processing (cus\u2010\ntomer invoicing, payroll, reporting).\nOther databases at that time forced application developers to think a lot about the\ninternal representation of the data in the database. The goal of the relational model"}
{"51": "were the main alternatives, but the relational model came to dominate them. Object\ndatabases came and went again in the late 1980s and early 1990s. XML databases\nappeared in the early 2000s, but have only seen niche adoption. Each competitor to\nthe relational model generated a lot of hype in its time, but it never lasted [2].\nAs computers became vastly more powerful and networked, they started being used\nfor increasingly diverse purposes. And remarkably, relational databases turned out to\ngeneralize very well, beyond their original scope of business data processing, to a\nbroad variety of use cases. Much of what you see on the web today is still powered by\nrelational databases, be it online publishing, discussion, social networking, ecom\u2010\nmerce, games, software-as-a-service productivity applications, or much more.\nThe Birth of NoSQL\nNow, in the 2010s, NoSQL is the latest attempt to overthrow the relational model\u2019s\ndominance. The name \u201cNoSQL\u201d is unfortunate, since it doesn\u2019t actually refer to any\nparticular technology\u2014it was originally intended simply as a catchy Twitter hashtag\nfor a meetup on open source, distributed, nonrelational databases in 2009 [3]. Never\u2010\ntheless, the term struck a nerve and quickly spread through the web startup commu\u2010\nnity and beyond. A number of interesting database systems are now associated with\nthe #NoSQL hashtag, and it has been retroactively reinterpreted as Not Only SQL [4].\nThere are several driving forces behind the adoption of NoSQL databases, including:\n\u2022 A need for greater scalability than relational databases can easily achieve, includ\u2010\ning very large datasets or very high write throughput\n\u2022 A widespread preference for free and open source software over commercial\ndatabase products\n\u2022 Specialized query operations that are not well supported by the relational model\n\u2022 Frustration with the restrictiveness of relational schemas, and a desire for a more\ndynamic and expressive data model [5]\nDifferent applications have different requirements, and the best choice of technology\nfor one use case may well be different from the best choice for another use case. It\ntherefore seems likely that in the foreseeable future, relational databases will continue\nto be used alongside a broad variety of nonrelational datastores\u2014an idea that is\nsometimes called polyglot persistence [3].\nThe Object-Relational Mismatch"}
{"52": "application code and the database model of tables, rows, and columns. The discon\u2010\nnect between the models is sometimes called an impedance mismatch.i\nObject-relational mapping (ORM) frameworks like ActiveRecord and Hibernate\nreduce the amount of boilerplate code required for this translation layer, but they\ncan\u2019t completely hide the differences between the two models.\nFor example, Figure 2-1 illustrates how a r\u00e9sum\u00e9 (a LinkedIn profile) could be\nexpressed in a relational schema. The profile as a whole can be identified by a unique\nidentifier, user_id. Fields like first_name and last_name appear exactly once per\nuser, so they can be modeled as columns on the users table. However, most people\nhave had more than one job in their career (positions), and people may have varying\nnumbers of periods of education and any number of pieces of contact information.\nThere is a one-to-many relationship from the user to these items, which can be repre\u2010\nsented in various ways:\n\u2022 In the traditional SQL model (prior to SQL:1999), the most common normalized\nrepresentation is to put positions, education, and contact information in separate\ntables, with a foreign key reference to the users table, as in Figure 2-1.\n\u2022 Later versions of the SQL standard added support for structured datatypes and\nXML data; this allowed multi-valued data to be stored within a single row, with\nsupport for querying and indexing inside those documents. These features are\nsupported to varying degrees by Oracle, IBM DB2, MS SQL Server, and Post\u2010\ngreSQL [6, 7]. A JSON datatype is also supported by several databases, including\nIBM DB2, MySQL, and PostgreSQL [8].\n\u2022 A third option is to encode jobs, education, and contact info as a JSON or XML\ndocument, store it on a text column in the database, and let the application inter\u2010\npret its structure and content. In this setup, you typically cannot use the database\nto query for values inside that encoded column."}
{"53": "Figure 2-1. Representing a LinkedIn profile using a relational schema. Photo of Bill\nGates courtesy of Wikimedia Commons, Ricardo Stuckert, Ag\u00eancia Brasil.\nFor a data structure like a r\u00e9sum\u00e9, which is mostly a self-contained document, a JSON\nrepresentation can be quite appropriate: see Example 2-1. JSON has the appeal of\nbeing much simpler than XML. Document-oriented databases like MongoDB [9],\nRethinkDB [10], CouchDB [11], and Espresso [12] support this data model.\nExample 2-1. Representing a LinkedIn profile as a JSON document\n{\n\"user_id\": 251,\n\"first_name\": \"Bill\",\n\"last_name\": \"Gates\","}
{"54": "\"positions\": [\n{\"job_title\": \"Co-chair\", \"organization\": \"Bill & Melinda Gates Foundation\"},\n{\"job_title\": \"Co-founder, Chairman\", \"organization\": \"Microsoft\"}\n],\n\"education\": [\n{\"school_name\": \"Harvard University\", \"start\": 1973, \"end\": 1975},\n{\"school_name\": \"Lakeside School, Seattle\", \"start\": null, \"end\": null}\n],\n\"contact_info\": {\n\"blog\": \"http://thegatesnotes.com\",\n\"twitter\": \"http://twitter.com/BillGates\"\n}\n}\nSome developers feel that the JSON model reduces the impedance mismatch between\nthe application code and the storage layer. However, as we shall see in Chapter 4,\nthere are also problems with JSON as a data encoding format. The lack of a schema is\noften cited as an advantage; we will discuss this in \u201cSchema flexibility in the docu\u2010\nment model\u201d on page 39.\nThe JSON representation has better locality than the multi-table schema in\nFigure 2-1. If you want to fetch a profile in the relational example, you need to either\nperform multiple queries (query each table by user_id) or perform a messy multi-\nway join between the users table and its subordinate tables. In the JSON representa\u2010\ntion, all the relevant information is in one place, and one query is sufficient.\nThe one-to-many relationships from the user profile to the user\u2019s positions, educa\u2010\ntional history, and contact information imply a tree structure in the data, and the\nJSON representation makes this tree structure explicit (see Figure 2-2)."}
{"55": "Many-to-One and Many-to-Many Relationships\nIn Example 2-1 in the preceding section, region_id and industry_id are given as\nIDs, not as plain-text strings \"Greater Seattle Area\" and \"Philanthropy\". Why?\nIf the user interface has free-text fields for entering the region and the industry, it\nmakes sense to store them as plain-text strings. But there are advantages to having\nstandardized lists of geographic regions and industries, and letting users choose from\na drop-down list or autocompleter:\n\u2022 Consistent style and spelling across profiles\n\u2022 Avoiding ambiguity (e.g., if there are several cities with the same name)\n\u2022 Ease of updating\u2014the name is stored in only one place, so it is easy to update\nacross the board if it ever needs to be changed (e.g., change of a city name due to\npolitical events)\n\u2022 Localization support\u2014when the site is translated into other languages, the stand\u2010\nardized lists can be localized, so the region and industry can be displayed in the\nviewer\u2019s language\n\u2022 Better search\u2014e.g., a search for philanthropists in the state of Washington can\nmatch this profile, because the list of regions can encode the fact that Seattle is in\nWashington (which is not apparent from the string \"Greater Seattle Area\")\nWhether you store an ID or a text string is a question of duplication. When you use\nan ID, the information that is meaningful to humans (such as the word Philanthropy)\nis stored in only one place, and everything that refers to it uses an ID (which only has\nmeaning within the database). When you store the text directly, you are duplicating\nthe human-meaningful information in every record that uses it.\nThe advantage of using an ID is that because it has no meaning to humans, it never\nneeds to change: the ID can remain the same, even if the information it identifies\nchanges. Anything that is meaningful to humans may need to change sometime in\nthe future\u2014and if that information is duplicated, all the redundant copies need to be\nupdated. That incurs write overheads, and risks inconsistencies (where some copies\nof the information are updated but others aren\u2019t). Removing such duplication is the\nkey idea behind normalization in databases.ii"}
{"56": "Database administrators and developers love to argue about nor\u2010\nmalization and denormalization, but we will suspend judgment for\nnow. In Part III of this book we will return to this topic and explore\nsystematic ways of dealing with caching, denormalization, and\nderived data.\nUnfortunately, normalizing this data requires many-to-one relationships (many peo\u2010\nple live in one particular region, many people work in one particular industry), which\ndon\u2019t fit nicely into the document model. In relational databases, it\u2019s normal to refer\nto rows in other tables by ID, because joins are easy. In document databases, joins are\nnot needed for one-to-many tree structures, and support for joins is often weak.iii\nIf the database itself does not support joins, you have to emulate a join in application\ncode by making multiple queries to the database. (In this case, the lists of regions and\nindustries are probably small and slow-changing enough that the application can\nsimply keep them in memory. But nevertheless, the work of making the join is shifted\nfrom the database to the application code.)\nMoreover, even if the initial version of an application fits well in a join-free docu\u2010\nment model, data has a tendency of becoming more interconnected as features are\nadded to applications. For example, consider some changes we could make to the\nr\u00e9sum\u00e9 example:\nOrganizations and schools as entities\nIn the previous description, organization (the company where the user worked)\nand school_name (where they studied) are just strings. Perhaps they should be\nreferences to entities instead? Then each organization, school, or university could\nhave its own web page (with logo, news feed, etc.); each r\u00e9sum\u00e9 could link to the\norganizations and schools that it mentions, and include their logos and other\ninformation (see Figure 2-3 for an example from LinkedIn).\nRecommendations\nSay you want to add a new feature: one user can write a recommendation for\nanother user. The recommendation is shown on the r\u00e9sum\u00e9 of the user who was\nrecommended, together with the name and photo of the user making the recom\u2010\nmendation. If the recommender updates their photo, any recommendations they\nhave written need to reflect the new photo. Therefore, the recommendation\nshould have a reference to the author\u2019s profile."}
{"57": "Figure 2-3. The company name is not just a string, but a link to a company entity.\nScreenshot of linkedin.com.\nFigure 2-4 illustrates how these new features require many-to-many relationships.\nThe data within each dotted rectangle can be grouped into one document, but the\nreferences to organizations, schools, and other users need to be represented as refer\u2010\nences, and require joins when queried."}
{"58": "Are Document Databases Repeating History?\nWhile many-to-many relationships and joins are routinely used in relational data\u2010\nbases, document databases and NoSQL reopened the debate on how best to represent\nsuch relationships in a database. This debate is much older than NoSQL\u2014in fact, it\ngoes back to the very earliest computerized database systems.\nThe most popular database for business data processing in the 1970s was IBM\u2019s Infor\u2010\nmation Management System (IMS), originally developed for stock-keeping in the\nApollo space program and first commercially released in 1968 [13]. It is still in use\nand maintained today, running on OS/390 on IBM mainframes [14].\nThe design of IMS used a fairly simple data model called the hierarchical model,\nwhich has some remarkable similarities to the JSON model used by document data\u2010\nbases [2]. It represented all data as a tree of records nested within records, much like\nthe JSON structure of Figure 2-2.\nLike document databases, IMS worked well for one-to-many relationships, but it\nmade many-to-many relationships difficult, and it didn\u2019t support joins. Developers\nhad to decide whether to duplicate (denormalize) data or to manually resolve refer\u2010\nences from one record to another. These problems of the 1960s and \u201970s were very\nmuch like the problems that developers are running into with document databases\ntoday [15].\nVarious solutions were proposed to solve the limitations of the hierarchical model.\nThe two most prominent were the relational model (which became SQL, and took\nover the world) and the network model (which initially had a large following but\neventually faded into obscurity). The \u201cgreat debate\u201d between these two camps lasted\nfor much of the 1970s [2].\nSince the problem that the two models were solving is still so relevant today, it\u2019s\nworth briefly revisiting this debate in today\u2019s light.\nThe network model\nThe network model was standardized by a committee called the Conference on Data\nSystems Languages (CODASYL) and implemented by several different database ven\u2010\ndors; it is also known as the CODASYL model [16].\nThe CODASYL model was a generalization of the hierarchical model. In the tree\nstructure of the hierarchical model, every record has exactly one parent; in the net\u2010"}
{"59": "The links between records in the network model were not foreign keys, but more like\npointers in a programming language (while still being stored on disk). The only way\nof accessing a record was to follow a path from a root record along these chains of\nlinks. This was called an access path.\nIn the simplest case, an access path could be like the traversal of a linked list: start at\nthe head of the list, and look at one record at a time until you find the one you want.\nBut in a world of many-to-many relationships, several different paths can lead to the\nsame record, and a programmer working with the network model had to keep track\nof these different access paths in their head.\nA query in CODASYL was performed by moving a cursor through the database by\niterating over lists of records and following access paths. If a record had multiple\nparents (i.e., multiple incoming pointers from other records), the application code\nhad to keep track of all the various relationships. Even CODASYL committee mem\u2010\nbers admitted that this was like navigating around an n-dimensional data space [17].\nAlthough manual access path selection was able to make the most efficient use of the\nvery limited hardware capabilities in the 1970s (such as tape drives, whose seeks are\nextremely slow), the problem was that they made the code for querying and updating\nthe database complicated and inflexible. With both the hierarchical and the network\nmodel, if you didn\u2019t have a path to the data you wanted, you were in a difficult situa\u2010\ntion. You could change the access paths, but then you had to go through a lot of\nhandwritten database query code and rewrite it to handle the new access paths. It was\ndifficult to make changes to an application\u2019s data model.\nThe relational model\nWhat the relational model did, by contrast, was to lay out all the data in the open: a\nrelation (table) is simply a collection of tuples (rows), and that\u2019s it. There are no laby\u2010\nrinthine nested structures, no complicated access paths to follow if you want to look\nat the data. You can read any or all of the rows in a table, selecting those that match\nan arbitrary condition. You can read a particular row by designating some columns\nas a key and matching on those. You can insert a new row into any table without\nworrying about foreign key relationships to and from other tables.iv\nIn a relational database, the query optimizer automatically decides which parts of the\nquery to execute in which order, and which indexes to use. Those choices are effec\u2010\ntively the \u201caccess path,\u201d but the big difference is that they are made automatically by"}
{"60": "the query optimizer, not by the application developer, so we rarely need to think\nabout them.\nIf you want to query your data in new ways, you can just declare a new index, and\nqueries will automatically use whichever indexes are most appropriate. You don\u2019t\nneed to change your queries to take advantage of a new index. (See also \u201cQuery Lan\u2010\nguages for Data\u201d on page 42.) The relational model thus made it much easier to add\nnew features to applications.\nQuery optimizers for relational databases are complicated beasts, and they have con\u2010\nsumed many years of research and development effort [18]. But a key insight of the\nrelational model was this: you only need to build a query optimizer once, and then all\napplications that use the database can benefit from it. If you don\u2019t have a query opti\u2010\nmizer, it\u2019s easier to handcode the access paths for a particular query than to write a\ngeneral-purpose optimizer\u2014but the general-purpose solution wins in the long run.\nComparison to document databases\nDocument databases reverted back to the hierarchical model in one aspect: storing\nnested records (one-to-many relationships, like positions, education, and\ncontact_info in Figure 2-1) within their parent record rather than in a separate\ntable.\nHowever, when it comes to representing many-to-one and many-to-many relation\u2010\nships, relational and document databases are not fundamentally different: in both\ncases, the related item is referenced by a unique identifier, which is called a foreign\nkey in the relational model and a document reference in the document model [9].\nThat identifier is resolved at read time by using a join or follow-up queries. To date,\ndocument databases have not followed the path of CODASYL.\nRelational Versus Document Databases Today\nThere are many differences to consider when comparing relational databases to\ndocument databases, including their fault-tolerance properties (see Chapter 5) and\nhandling of concurrency (see Chapter 7). In this chapter, we will concentrate only on\nthe differences in the data model.\nThe main arguments in favor of the document data model are schema flexibility, bet\u2010\nter performance due to locality, and that for some applications it is closer to the data\nstructures used by the application. The relational model counters by providing better\nsupport for joins, and many-to-one and many-to-many relationships."}
{"61": "bly a good idea to use a document model. The relational technique of shredding\u2014\nsplitting a document-like structure into multiple tables (like positions, education,\nand contact_info in Figure 2-1)\u2014can lead to cumbersome schemas and unnecessa\u2010\nrily complicated application code.\nThe document model has limitations: for example, you cannot refer directly to a nes\u2010\nted item within a document, but instead you need to say something like \u201cthe second\nitem in the list of positions for user 251\u201d (much like an access path in the hierarchical\nmodel). However, as long as documents are not too deeply nested, that is not usually\na problem.\nThe poor support for joins in document databases may or may not be a problem,\ndepending on the application. For example, many-to-many relationships may never\nbe needed in an analytics application that uses a document database to record which\nevents occurred at which time [19].\nHowever, if your application does use many-to-many relationships, the document\nmodel becomes less appealing. It\u2019s possible to reduce the need for joins by denormal\u2010\nizing, but then the application code needs to do additional work to keep the denor\u2010\nmalized data consistent. Joins can be emulated in application code by making\nmultiple requests to the database, but that also moves complexity into the application\nand is usually slower than a join performed by specialized code inside the database.\nIn such cases, using a document model can lead to significantly more complex appli\u2010\ncation code and worse performance [15].\nIt\u2019s not possible to say in general which data model leads to simpler application code;\nit depends on the kinds of relationships that exist between data items. For highly\ninterconnected data, the document model is awkward, the relational model is accept\u2010\nable, and graph models (see \u201cGraph-Like Data Models\u201d on page 49) are the most\nnatural.\nSchema flexibility in the document model\nMost document databases, and the JSON support in relational databases, do not\nenforce any schema on the data in documents. XML support in relational databases\nusually comes with optional schema validation. No schema means that arbitrary keys\nand values can be added to a document, and when reading, clients have no guaran\u2010\ntees as to what fields the documents may contain.\nDocument databases are sometimes called schemaless, but that\u2019s misleading, as the\ncode that reads the data usually assumes some kind of structure\u2014i.e., there is an\nimplicit schema, but it is not enforced by the database [20]. A more accurate term is"}
{"62": "databases, where the schema is explicit and the database ensures all written data con\u2010\nforms to it) [21].\nSchema-on-read is similar to dynamic (runtime) type checking in programming lan\u2010\nguages, whereas schema-on-write is similar to static (compile-time) type checking.\nJust as the advocates of static and dynamic type checking have big debates about their\nrelative merits [22], enforcement of schemas in database is a contentious topic, and in\ngeneral there\u2019s no right or wrong answer.\nThe difference between the approaches is particularly noticeable in situations where\nan application wants to change the format of its data. For example, say you are cur\u2010\nrently storing each user\u2019s full name in one field, and you instead want to store the\nfirst name and last name separately [23]. In a document database, you would just\nstart writing new documents with the new fields and have code in the application that\nhandles the case when old documents are read. For example:\nif (user && user.name && !user.first_name) {\n// Documents written before Dec 8, 2013 don't have first_name\nuser.first_name = user.name.split(\" \")[0];\n}\nOn the other hand, in a \u201cstatically typed\u201d database schema, you would typically per\u2010\nform a migration along the lines of:\nALTER TABLE users ADD COLUMN first_name text;\nUPDATE users SET first_name = split_part(name, ' ', 1); -- PostgreSQL\nUPDATE users SET first_name = substring_index(name, ' ', 1); -- MySQL\nSchema changes have a bad reputation of being slow and requiring downtime. This\nreputation is not entirely deserved: most relational database systems execute the\nALTER TABLE statement in a few milliseconds. MySQL is a notable exception\u2014it\ncopies the entire table on ALTER TABLE, which can mean minutes or even hours of\ndowntime when altering a large table\u2014although various tools exist to work around\nthis limitation [24, 25, 26].\nRunning the UPDATE statement on a large table is likely to be slow on any database,\nsince every row needs to be rewritten. If that is not acceptable, the application can\nleave first_name set to its default of NULL and fill it in at read time, like it would with\na document database.\nThe schema-on-read approach is advantageous if the items in the collection don\u2019t all\nhave the same structure for some reason (i.e., the data is heterogeneous)\u2014for exam\u2010\nple, because:"}
{"63": "\u2022 The structure of the data is determined by external systems over which you have\nno control and which may change at any time.\nIn situations like these, a schema may hurt more than it helps, and schemaless docu\u2010\nments can be a much more natural data model. But in cases where all records are\nexpected to have the same structure, schemas are a useful mechanism for document\u2010\ning and enforcing that structure. We will discuss schemas and schema evolution in\nmore detail in Chapter 4.\nData locality for queries\nA document is usually stored as a single continuous string, encoded as JSON, XML,\nor a binary variant thereof (such as MongoDB\u2019s BSON). If your application often\nneeds to access the entire document (for example, to render it on a web page), there is\na performance advantage to this storage locality. If data is split across multiple tables,\nlike in Figure 2-1, multiple index lookups are required to retrieve it all, which may\nrequire more disk seeks and take more time.\nThe locality advantage only applies if you need large parts of the document at the\nsame time. The database typically needs to load the entire document, even if you\naccess only a small portion of it, which can be wasteful on large documents. On\nupdates to a document, the entire document usually needs to be rewritten\u2014only\nmodifications that don\u2019t change the encoded size of a document can easily be per\u2010\nformed in place [19]. For these reasons, it is generally recommended that you keep\ndocuments fairly small and avoid writes that increase the size of a document [9].\nThese performance limitations significantly reduce the set of situations in which\ndocument databases are useful.\nIt\u2019s worth pointing out that the idea of grouping related data together for locality is\nnot limited to the document model. For example, Google\u2019s Spanner database offers\nthe same locality properties in a relational data model, by allowing the schema to\ndeclare that a table\u2019s rows should be interleaved (nested) within a parent table [27].\nOracle allows the same, using a feature called multi-table index cluster tables [28].\nThe column-family concept in the Bigtable data model (used in Cassandra and\nHBase) has a similar purpose of managing locality [29].\nWe will also see more on locality in Chapter 3.\nConvergence of document and relational databases\nMost relational database systems (other than MySQL) have supported XML since the\nmid-2000s. This includes functions to make local modifications to XML documents"}
{"64": "PostgreSQL since version 9.3 [8], MySQL since version 5.7, and IBM DB2 since ver\u2010\nsion 10.5 [30] also have a similar level of support for JSON documents. Given the\npopularity of JSON for web APIs, it is likely that other relational databases will follow\nin their footsteps and add JSON support.\nOn the document database side, RethinkDB supports relational-like joins in its query\nlanguage, and some MongoDB drivers automatically resolve database references\n(effectively performing a client-side join, although this is likely to be slower than a\njoin performed in the database since it requires additional network round-trips and is\nless optimized).\nIt seems that relational and document databases are becoming more similar over\ntime, and that is a good thing: the data models complement each other.v If a database\nis able to handle document-like data and also perform relational queries on it, appli\u2010\ncations can use the combination of features that best fits their needs.\nA hybrid of the relational and document models is a good route for databases to take\nin the future.\nQuery Languages for Data\nWhen the relational model was introduced, it included a new way of querying data:\nSQL is a declarative query language, whereas IMS and CODASYL queried the data\u2010\nbase using imperative code. What does that mean?\nMany commonly used programming languages are imperative. For example, if you\nhave a list of animal species, you might write something like this to return only the\nsharks in the list:\nfunction getSharks() {\nvar sharks = [];\nfor (var i = 0; i < animals.length; i++) {\nif (animals[i].family === \"Sharks\") {\nsharks.push(animals[i]);\n}\n}\nreturn sharks;\n}\nIn the relational algebra, you would instead write:\nsharks = \u03c3 (animals)\nfamily = \u201cSharks\u201d"}
{"65": "where \u03c3 (the Greek letter sigma) is the selection operator, returning only those ani\u2010\nmals that match the condition family = \u201cSharks\u201d.\nWhen SQL was defined, it followed the structure of the relational algebra fairly\nclosely:\nSELECT * FROM animals WHERE family = 'Sharks';\nAn imperative language tells the computer to perform certain operations in a certain\norder. You can imagine stepping through the code line by line, evaluating conditions,\nupdating variables, and deciding whether to go around the loop one more time.\nIn a declarative query language, like SQL or relational algebra, you just specify the\npattern of the data you want\u2014what conditions the results must meet, and how you\nwant the data to be transformed (e.g., sorted, grouped, and aggregated)\u2014but not how\nto achieve that goal. It is up to the database system\u2019s query optimizer to decide which\nindexes and which join methods to use, and in which order to execute various parts\nof the query.\nA declarative query language is attractive because it is typically more concise and eas\u2010\nier to work with than an imperative API. But more importantly, it also hides imple\u2010\nmentation details of the database engine, which makes it possible for the database\nsystem to introduce performance improvements without requiring any changes to\nqueries.\nFor example, in the imperative code shown at the beginning of this section, the list of\nanimals appears in a particular order. If the database wants to reclaim unused disk\nspace behind the scenes, it might need to move records around, changing the order in\nwhich the animals appear. Can the database do that safely, without breaking queries?\nThe SQL example doesn\u2019t guarantee any particular ordering, and so it doesn\u2019t mind if\nthe order changes. But if the query is written as imperative code, the database can\nnever be sure whether the code is relying on the ordering or not. The fact that SQL is\nmore limited in functionality gives the database much more room for automatic opti\u2010\nmizations.\nFinally, declarative languages often lend themselves to parallel execution. Today,\nCPUs are getting faster by adding more cores, not by running at significantly higher\nclock speeds than before [31]. Imperative code is very hard to parallelize across mul\u2010\ntiple cores and multiple machines, because it specifies instructions that must be per\u2010\nformed in a particular order. Declarative languages have a better chance of getting\nfaster in parallel execution because they specify only the pattern of the results, not the\nalgorithm that is used to determine the results. The database is free to use a parallel"}
{"66": "Declarative Queries on the Web\nThe advantages of declarative query languages are not limited to just databases. To\nillustrate the point, let\u2019s compare declarative and imperative approaches in a com\u2010\npletely different environment: a web browser.\nSay you have a website about animals in the ocean. The user is currently viewing the\npage on sharks, so you mark the navigation item \u201cSharks\u201d as currently selected, like\nthis:\n<ul>\n<li class=\"selected\">\n<p>Sharks</p>\n<ul>\n<li>Great White Shark</li>\n<li>Tiger Shark</li>\n<li>Hammerhead Shark</li>\n</ul>\n</li>\n<li>\n<p>Whales</p>\n<ul>\n<li>Blue Whale</li>\n<li>Humpback Whale</li>\n<li>Fin Whale</li>\n</ul>\n</li>\n</ul>\nThe selected item is marked with the CSS class \"selected\".\n<p>Sharks</p> is the title of the currently selected page.\nNow say you want the title of the currently selected page to have a blue background,\nso that it is visually highlighted. This is easy, using CSS:\nli.selected > p {\nbackground-color: blue;\n}\nHere the CSS selector li.selected > p declares the pattern of elements to which we\nwant to apply the blue style: namely, all <p> elements whose direct parent is an <li>\nelement with a CSS class of selected. The element <p>Sharks</p> in the example\nmatches this pattern, but <p>Whales</p> does not match because its <li> parent\nlacks class=\"selected\"."}
{"67": "If you were using XSL instead of CSS, you could do something similar:\n<xsl:template match=\"li[@class='selected']/p\">\n<fo:block background-color=\"blue\">\n<xsl:apply-templates/>\n</fo:block>\n</xsl:template>\nHere, the XPath expression li[@class='selected']/p is equivalent to the CSS selec\u2010\ntor li.selected > p in the previous example. What CSS and XSL have in common\nis that they are both declarative languages for specifying the styling of a document.\nImagine what life would be like if you had to use an imperative approach. In Java\u2010\nScript, using the core Document Object Model (DOM) API, the result might look\nsomething like this:\nvar liElements = document.getElementsByTagName(\"li\");\nfor (var i = 0; i < liElements.length; i++) {\nif (liElements[i].className === \"selected\") {\nvar children = liElements[i].childNodes;\nfor (var j = 0; j < children.length; j++) {\nvar child = children[j];\nif (child.nodeType === Node.ELEMENT_NODE && child.tagName === \"P\") {\nchild.setAttribute(\"style\", \"background-color: blue\");\n}\n}\n}\n}\nThis JavaScript imperatively sets the element <p>Sharks</p> to have a blue back\u2010\nground, but the code is awful. Not only is it much longer and harder to understand\nthan the CSS and XSL equivalents, but it also has some serious problems:\n\u2022 If the selected class is removed (e.g., because the user clicks a different page),\nthe blue color won\u2019t be removed, even if the code is rerun\u2014and so the item will\nremain highlighted until the entire page is reloaded. With CSS, the browser auto\u2010\nmatically detects when the li.selected > p rule no longer applies and removes\nthe blue background as soon as the selected class is removed.\n\u2022 If you want to take advantage of a new API, such as document.getElementsBy\nClassName(\"selected\") or even document.evaluate()\u2014which may improve\nperformance\u2014you have to rewrite the code. On the other hand, browser vendors\ncan improve the performance of CSS and XPath without breaking compatibility."}
{"68": "In a web browser, using declarative CSS styling is much better than manipulating\nstyles imperatively in JavaScript. Similarly, in databases, declarative query languages\nlike SQL turned out to be much better than imperative query APIs.vi\nMapReduce Querying\nMapReduce is a programming model for processing large amounts of data in bulk\nacross many machines, popularized by Google [33]. A limited form of MapReduce is\nsupported by some NoSQL datastores, including MongoDB and CouchDB, as a\nmechanism for performing read-only queries across many documents.\nMapReduce in general is described in more detail in Chapter 10. For now, we\u2019ll just\nbriefly discuss MongoDB\u2019s use of the model.\nMapReduce is neither a declarative query language nor a fully imperative query API,\nbut somewhere in between: the logic of the query is expressed with snippets of code,\nwhich are called repeatedly by the processing framework. It is based on the map (also\nknown as collect) and reduce (also known as fold or inject) functions that exist\nin many functional programming languages.\nTo give an example, imagine you are a marine biologist, and you add an observation\nrecord to your database every time you see animals in the ocean. Now you want to\ngenerate a report saying how many sharks you have sighted per month.\nIn PostgreSQL you might express that query like this:\nSELECT date_trunc('month', observation_timestamp) AS observation_month,\nsum(num_animals) AS total_animals\nFROM observations\nWHERE family = 'Sharks'\nGROUP BY observation_month;\nThe date_trunc('month', timestamp) function determines the calendar month\ncontaining timestamp, and returns another timestamp representing the begin\u2010\nning of that month. In other words, it rounds a timestamp down to the nearest\nmonth.\nThis query first filters the observations to only show species in the Sharks family,\nthen groups the observations by the calendar month in which they occurred, and\nfinally adds up the number of animals seen in all observations in that month.\nThe same can be expressed with MongoDB\u2019s MapReduce feature as follows:"}
{"69": "db.observations.mapReduce(\nfunction map() {\nvar year = this.observationTimestamp.getFullYear();\nvar month = this.observationTimestamp.getMonth() + 1;\nemit(year + \"-\" + month, this.numAnimals);\n},\nfunction reduce(key, values) {\nreturn Array.sum(values);\n},\n{\nquery: { family: \"Sharks\" },\nout: \"monthlySharkReport\"\n}\n);\nThe filter to consider only shark species can be specified declaratively (this is a\nMongoDB-specific extension to MapReduce).\nThe JavaScript function map is called once for every document that matches\nquery, with this set to the document object.\nThe map function emits a key (a string consisting of year and month, such as\n\"2013-12\" or \"2014-1\") and a value (the number of animals in that observation).\nThe key-value pairs emitted by map are grouped by key. For all key-value pairs\nwith the same key (i.e., the same month and year), the reduce function is called\nonce.\nThe reduce function adds up the number of animals from all observations in a\nparticular month.\nThe final output is written to the collection monthlySharkReport.\nFor example, say the observations collection contains these two documents:\n{\nobservationTimestamp: Date.parse(\"Mon, 25 Dec 1995 12:34:56 GMT\"),\nfamily: \"Sharks\",\nspecies: \"Carcharodon carcharias\",\nnumAnimals: 3\n}\n{\nobservationTimestamp: Date.parse(\"Tue, 12 Dec 1995 16:17:18 GMT\"),\nfamily: \"Sharks\",\nspecies: \"Carcharias taurus\","}
{"70": "The map function would be called once for each document, resulting in\nemit(\"1995-12\", 3) and emit(\"1995-12\", 4). Subsequently, the reduce function\nwould be called with reduce(\"1995-12\", [3, 4]), returning 7.\nThe map and reduce functions are somewhat restricted in what they are allowed to\ndo. They must be pure functions, which means they only use the data that is passed to\nthem as input, they cannot perform additional database queries, and they must not\nhave any side effects. These restrictions allow the database to run the functions any\u2010\nwhere, in any order, and rerun them on failure. However, they are nevertheless pow\u2010\nerful: they can parse strings, call library functions, perform calculations, and more.\nMapReduce is a fairly low-level programming model for distributed execution on a\ncluster of machines. Higher-level query languages like SQL can be implemented as a\npipeline of MapReduce operations (see Chapter 10), but there are also many dis\u2010\ntributed implementations of SQL that don\u2019t use MapReduce. Note there is nothing in\nSQL that constrains it to running on a single machine, and MapReduce doesn\u2019t have\na monopoly on distributed query execution.\nBeing able to use JavaScript code in the middle of a query is a great feature for\nadvanced queries, but it\u2019s not limited to MapReduce\u2014some SQL databases can be\nextended with JavaScript functions too [34].\nA usability problem with MapReduce is that you have to write two carefully coordi\u2010\nnated JavaScript functions, which is often harder than writing a single query. More\u2010\nover, a declarative query language offers more opportunities for a query optimizer to\nimprove the performance of a query. For these reasons, MongoDB 2.2 added support\nfor a declarative query language called the aggregation pipeline [9]. In this language,\nthe same shark-counting query looks like this:\ndb.observations.aggregate([\n{ $match: { family: \"Sharks\" } },\n{ $group: {\n_id: {\nyear: { $year: \"$observationTimestamp\" },\nmonth: { $month: \"$observationTimestamp\" }\n},\ntotalAnimals: { $sum: \"$numAnimals\" }\n} }\n]);\nThe aggregation pipeline language is similar in expressiveness to a subset of SQL, but\nit uses a JSON-based syntax rather than SQL\u2019s English-sentence-style syntax; the dif\u2010\nference is perhaps a matter of taste. The moral of the story is that a NoSQL system\nmay find itself accidentally reinventing SQL, albeit in disguise."}
{"71": "Graph-Like Data Models\nWe saw earlier that many-to-many relationships are an important distinguishing fea\u2010\nture between different data models. If your application has mostly one-to-many rela\u2010\ntionships (tree-structured data) or no relationships between records, the document\nmodel is appropriate.\nBut what if many-to-many relationships are very common in your data? The rela\u2010\ntional model can handle simple cases of many-to-many relationships, but as the con\u2010\nnections within your data become more complex, it becomes more natural to start\nmodeling your data as a graph.\nA graph consists of two kinds of objects: vertices (also known as nodes or entities) and\nedges (also known as relationships or arcs). Many kinds of data can be modeled as a\ngraph. Typical examples include:\nSocial graphs\nVertices are people, and edges indicate which people know each other.\nThe web graph\nVertices are web pages, and edges indicate HTML links to other pages.\nRoad or rail networks\nVertices are junctions, and edges represent the roads or railway lines between\nthem.\nWell-known algorithms can operate on these graphs: for example, car navigation sys\u2010\ntems search for the shortest path between two points in a road network, and\nPageRank can be used on the web graph to determine the popularity of a web page\nand thus its ranking in search results.\nIn the examples just given, all the vertices in a graph represent the same kind of thing\n(people, web pages, or road junctions, respectively). However, graphs are not limited\nto such homogeneous data: an equally powerful use of graphs is to provide a consis\u2010\ntent way of storing completely different types of objects in a single datastore. For\nexample, Facebook maintains a single graph with many different types of vertices and\nedges: vertices represent people, locations, events, checkins, and comments made by\nusers; edges indicate which people are friends with each other, which checkin hap\u2010\npened in which location, who commented on which post, who attended which event,\nand so on [35].\nIn this section we will use the example shown in Figure 2-5. It could be taken from a\nsocial network or a genealogical database: it shows two people, Lucy from Idaho and"}
{"72": "Figure 2-5. Example of graph-structured data (boxes represent vertices, arrows repre\u2010\nsent edges).\nThere are several different, but related, ways of structuring and querying data in\ngraphs. In this section we will discuss the property graph model (implemented by\nNeo4j, Titan, and InfiniteGraph) and the triple-store model (implemented by\nDatomic, AllegroGraph, and others). We will look at three declarative query lan\u2010\nguages for graphs: Cypher, SPARQL, and Datalog. Besides these, there are also\nimperative graph query languages such as Gremlin [36] and graph processing frame\u2010\nworks like Pregel (see Chapter 10).\nProperty Graphs\nIn the property graph model, each vertex consists of:\n\u2022 A unique identifier\n\u2022 A set of outgoing edges\n\u2022 A set of incoming edges\n\u2022 A collection of properties (key-value pairs)\nEach edge consists of:"}
{"73": "\u2022 The vertex at which the edge ends (the head vertex)\n\u2022 A label to describe the kind of relationship between the two vertices\n\u2022 A collection of properties (key-value pairs)\nYou can think of a graph store as consisting of two relational tables, one for vertices\nand one for edges, as shown in Example 2-2 (this schema uses the PostgreSQL json\ndatatype to store the properties of each vertex or edge). The head and tail vertex are\nstored for each edge; if you want the set of incoming or outgoing edges for a vertex,\nyou can query the edges table by head_vertex or tail_vertex, respectively.\nExample 2-2. Representing a property graph using a relational schema\nCREATE TABLE vertices (\nvertex_id integer PRIMARY KEY,\nproperties json\n);\nCREATE TABLE edges (\nedge_id integer PRIMARY KEY,\ntail_vertex integer REFERENCES vertices (vertex_id),\nhead_vertex integer REFERENCES vertices (vertex_id),\nlabel text,\nproperties json\n);\nCREATE INDEX edges_tails ON edges (tail_vertex);\nCREATE INDEX edges_heads ON edges (head_vertex);\nSome important aspects of this model are:\n1. Any vertex can have an edge connecting it with any other vertex. There is no\nschema that restricts which kinds of things can or cannot be associated.\n2. Given any vertex, you can efficiently find both its incoming and its outgoing\nedges, and thus traverse the graph\u2014i.e., follow a path through a chain of vertices\n\u2014both forward and backward. (That\u2019s why Example 2-2 has indexes on both the\ntail_vertex and head_vertex columns.)\n3. By using different labels for different kinds of relationships, you can store several\ndifferent kinds of information in a single graph, while still maintaining a clean\ndata model.\nThose features give graphs a great deal of flexibility for data modeling, as illustrated"}
{"74": "intricacies of sovereign states and nations), and varying granularity of data (Lucy\u2019s\ncurrent residence is specified as a city, whereas her place of birth is specified only at\nthe level of a state).\nYou could imagine extending the graph to also include many other facts about Lucy\nand Alain, or other people. For instance, you could use it to indicate any food aller\u2010\ngies they have (by introducing a vertex for each allergen, and an edge between a per\u2010\nson and an allergen to indicate an allergy), and link the allergens with a set of vertices\nthat show which foods contain which substances. Then you could write a query to\nfind out what is safe for each person to eat. Graphs are good for evolvability: as you\nadd features to your application, a graph can easily be extended to accommodate\nchanges in your application\u2019s data structures.\nThe Cypher Query Language\nCypher is a declarative query language for property graphs, created for the Neo4j\ngraph database [37]. (It is named after a character in the movie The Matrix and is not\nrelated to ciphers in cryptography [38].)\nExample 2-3 shows the Cypher query to insert the lefthand portion of Figure 2-5 into\na graph database. The rest of the graph can be added similarly and is omitted for\nreadability. Each vertex is given a symbolic name like USA or Idaho, and other parts of\nthe query can use those names to create edges between the vertices, using an arrow\nnotation: (Idaho) -[:WITHIN]-> (USA) creates an edge labeled WITHIN, with Idaho\nas the tail node and USA as the head node.\nExample 2-3. A subset of the data in Figure 2-5, represented as a Cypher query\nCREATE\n(NAmerica:Location {name:'North America', type:'continent'}),\n(USA:Location {name:'United States', type:'country' }),\n(Idaho:Location {name:'Idaho', type:'state' }),\n(Lucy:Person {name:'Lucy' }),\n(Idaho) -[:WITHIN]-> (USA) -[:WITHIN]-> (NAmerica),\n(Lucy) -[:BORN_IN]-> (Idaho)\nWhen all the vertices and edges of Figure 2-5 are added to the database, we can start\nasking interesting questions: for example, find the names of all the people who emigra\u2010\nted from the United States to Europe. To be more precise, here we want to find all the\nvertices that have a BORN_IN edge to a location within the US, and also a LIVING_IN\nedge to a location within Europe, and return the name property of each of those verti\u2010\nces."}
{"75": "matches any two vertices that are related by an edge labeled BORN_IN. The tail vertex\nof that edge is bound to the variable person, and the head vertex is left unnamed.\nExample 2-4. Cypher query to find people who emigrated from the US to Europe\nMATCH\n(person) -[:BORN_IN]-> () -[:WITHIN*0..]-> (us:Location {name:'United States'}),\n(person) -[:LIVES_IN]-> () -[:WITHIN*0..]-> (eu:Location {name:'Europe'})\nRETURN person.name\nThe query can be read as follows:\nFind any vertex (call it person) that meets both of the following conditions:\n1. person has an outgoing BORN_IN edge to some vertex. From that vertex, you can\nfollow a chain of outgoing WITHIN edges until eventually you reach a vertex of\ntype Location, whose name property is equal to \"United States\".\n2. That same person vertex also has an outgoing LIVES_IN edge. Following that\nedge, and then a chain of outgoing WITHIN edges, you eventually reach a vertex of\ntype Location, whose name property is equal to \"Europe\".\nFor each such person vertex, return the name property.\nThere are several possible ways of executing the query. The description given here\nsuggests that you start by scanning all the people in the database, examine each per\u2010\nson\u2019s birthplace and residence, and return only those people who meet the criteria.\nBut equivalently, you could start with the two Location vertices and work backward.\nIf there is an index on the name property, you can probably efficiently find the two\nvertices representing the US and Europe. Then you can proceed to find all locations\n(states, regions, cities, etc.) in the US and Europe respectively by following all incom\u2010\ning WITHIN edges. Finally, you can look for people who can be found through an\nincoming BORN_IN or LIVES_IN edge at one of the location vertices.\nAs is typical for a declarative query language, you don\u2019t need to specify such execu\u2010\ntion details when writing the query: the query optimizer automatically chooses the\nstrategy that is predicted to be the most efficient, so you can get on with writing the\nrest of your application.\nGraph Queries in SQL\nExample 2-2 suggested that graph data can be represented in a relational database.\nBut if we put graph data in a relational structure, can we also query it using SQL?"}
{"76": "traverse a variable number of edges before you find the vertex you\u2019re looking for\u2014\nthat is, the number of joins is not fixed in advance.\nIn our example, that happens in the () -[:WITHIN*0..]-> () rule in the Cypher\nquery. A person\u2019s LIVES_IN edge may point at any kind of location: a street, a city, a\ndistrict, a region, a state, etc. A city may be WITHIN a region, a region WITHIN a state, a\nstate WITHIN a country, etc. The LIVES_IN edge may point directly at the location ver\u2010\ntex you\u2019re looking for, or it may be several levels removed in the location hierarchy.\nIn Cypher, :WITHIN*0.. expresses that fact very concisely: it means \u201cfollow a WITHIN\nedge, zero or more times.\u201d It is like the * operator in a regular expression.\nSince SQL:1999, this idea of variable-length traversal paths in a query can be\nexpressed using something called recursive common table expressions (the WITH\nRECURSIVE syntax). Example 2-5 shows the same query\u2014finding the names of people\nwho emigrated from the US to Europe\u2014expressed in SQL using this technique (sup\u2010\nported in PostgreSQL, IBM DB2, Oracle, and SQL Server). However, the syntax is\nvery clumsy in comparison to Cypher.\nExample 2-5. The same query as Example 2-4, expressed in SQL using recursive\ncommon table expressions\nWITH RECURSIVE\n-- in_usa is the set of vertex IDs of all locations within the United States\nin_usa(vertex_id) AS (\nSELECT vertex_id FROM vertices WHERE properties->>'name' = 'United States'\nUNION\nSELECT edges.tail_vertex FROM edges\nJOIN in_usa ON edges.head_vertex = in_usa.vertex_id\nWHERE edges.label = 'within'\n),\n-- in_europe is the set of vertex IDs of all locations within Europe\nin_europe(vertex_id) AS (\nSELECT vertex_id FROM vertices WHERE properties->>'name' = 'Europe'\nUNION\nSELECT edges.tail_vertex FROM edges\nJOIN in_europe ON edges.head_vertex = in_europe.vertex_id\nWHERE edges.label = 'within'\n),\n-- born_in_usa is the set of vertex IDs of all people born in the US\nborn_in_usa(vertex_id) AS (\nSELECT edges.tail_vertex FROM edges"}
{"77": "-- lives_in_europe is the set of vertex IDs of all people living in Europe\nlives_in_europe(vertex_id) AS (\nSELECT edges.tail_vertex FROM edges\nJOIN in_europe ON edges.head_vertex = in_europe.vertex_id\nWHERE edges.label = 'lives_in'\n)\nSELECT vertices.properties->>'name'\nFROM vertices\n-- join to find those people who were both born in the US *and* live in Europe\nJOIN born_in_usa ON vertices.vertex_id = born_in_usa.vertex_id\nJOIN lives_in_europe ON vertices.vertex_id = lives_in_europe.vertex_id;\nFirst find the vertex whose name property has the value \"United States\", and\nmake it the first element of the set of vertices in_usa.\nFollow all incoming within edges from vertices in the set in_usa, and add them\nto the same set, until all incoming within edges have been visited.\nDo the same starting with the vertex whose name property has the value\n\"Europe\", and build up the set of vertices in_europe.\nFor each of the vertices in the set in_usa, follow incoming born_in edges to find\npeople who were born in some place within the United States.\nSimilarly, for each of the vertices in the set in_europe, follow incoming lives_in\nedges to find people who live in Europe.\nFinally, intersect the set of people born in the USA with the set of people living in\nEurope, by joining them.\nIf the same query can be written in 4 lines in one query language but requires 29 lines\nin another, that just shows that different data models are designed to satisfy different\nuse cases. It\u2019s important to pick a data model that is suitable for your application.\nTriple-Stores and SPARQL\nThe triple-store model is mostly equivalent to the property graph model, using differ\u2010\nent words to describe the same ideas. It is nevertheless worth discussing, because\nthere are various tools and languages for triple-stores that can be valuable additions\nto your toolbox for building applications.\nIn a triple-store, all information is stored in the form of very simple three-part state\u2010"}
{"78": "The subject of a triple is equivalent to a vertex in a graph. The object is one of two\nthings:\n1. A value in a primitive datatype, such as a string or a number. In that case, the\npredicate and object of the triple are equivalent to the key and value of a property\non the subject vertex. For example, (lucy, age, 33) is like a vertex lucy with prop\u2010\nerties {\"age\":33}.\n2. Another vertex in the graph. In that case, the predicate is an edge in the graph,\nthe subject is the tail vertex, and the object is the head vertex. For example, in\n(lucy, marriedTo, alain) the subject and object lucy and alain are both vertices,\nand the predicate marriedTo is the label of the edge that connects them.\nExample 2-6 shows the same data as in Example 2-3, written as triples in a format\ncalled Turtle, a subset of Notation3 (N3) [39].\nExample 2-6. A subset of the data in Figure 2-5, represented as Turtle triples\n@prefix : <urn:example:>.\n_:lucy a :Person.\n_:lucy :name \"Lucy\".\n_:lucy :bornIn _:idaho.\n_:idaho a :Location.\n_:idaho :name \"Idaho\".\n_:idaho :type \"state\".\n_:idaho :within _:usa.\n_:usa a :Location.\n_:usa :name \"United States\".\n_:usa :type \"country\".\n_:usa :within _:namerica.\n_:namerica a :Location.\n_:namerica :name \"North America\".\n_:namerica :type \"continent\".\nIn this example, vertices of the graph are written as _:someName. The name doesn\u2019t\nmean anything outside of this file; it exists only because we otherwise wouldn\u2019t know\nwhich triples refer to the same vertex. When the predicate represents an edge, the\nobject is a vertex, as in _:idaho :within _:usa. When the predicate is a property,\nthe object is a string literal, as in _:usa :name \"United States\".\nIt\u2019s quite repetitive to repeat the same subject over and over again, but fortunately\nyou can use semicolons to say multiple things about the same subject. This makes the\nTurtle format quite nice and readable: see Example 2-7."}
{"79": "Example 2-7. A more concise way of writing the data in Example 2-6\n@prefix : <urn:example:>.\n_:lucy a :Person; :name \"Lucy\"; :bornIn _:idaho.\n_:idaho a :Location; :name \"Idaho\"; :type \"state\"; :within _:usa.\n_:usa a :Location; :name \"United States\"; :type \"country\"; :within _:namerica.\n_:namerica a :Location; :name \"North America\"; :type \"continent\".\nThe semantic web\nIf you read more about triple-stores, you may get sucked into a maelstrom of articles\nwritten about the semantic web. The triple-store data model is completely independ\u2010\nent of the semantic web\u2014for example, Datomic [40] is a triple-store that does not\nclaim to have anything to do with it.vii But since the two are so closely linked in many\npeople\u2019s minds, we should discuss them briefly.\nThe semantic web is fundamentally a simple and reasonable idea: websites already\npublish information as text and pictures for humans to read, so why don\u2019t they also\npublish information as machine-readable data for computers to read? The Resource\nDescription Framework (RDF) [41] was intended as a mechanism for different web\u2010\nsites to publish data in a consistent format, allowing data from different websites to\nbe automatically combined into a web of data\u2014a kind of internet-wide \u201cdatabase of\neverything.\u201d\nUnfortunately, the semantic web was overhyped in the early 2000s but so far hasn\u2019t\nshown any sign of being realized in practice, which has made many people cynical\nabout it. It has also suffered from a dizzying plethora of acronyms, overly complex\nstandards proposals, and hubris.\nHowever, if you look past those failings, there is also a lot of good work that has come\nout of the semantic web project. Triples can be a good internal data model for appli\u2010\ncations, even if you have no interest in publishing RDF data on the semantic web.\nThe RDF data model\nThe Turtle language we used in Example 2-7 is a human-readable format for RDF\ndata. Sometimes RDF is also written in an XML format, which does the same thing\nmuch more verbosely\u2014see Example 2-8. Turtle/N3 is preferable as it is much easier\non the eyes, and tools like Apache Jena [42] can automatically convert between differ\u2010\nent RDF formats if necessary."}
{"80": "Example 2-8. The data of Example 2-7, expressed using RDF/XML syntax\n<rdf:RDF xmlns=\"urn:example:\"\nxmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n<Location rdf:nodeID=\"idaho\">\n<name>Idaho</name>\n<type>state</type>\n<within>\n<Location rdf:nodeID=\"usa\">\n<name>United States</name>\n<type>country</type>\n<within>\n<Location rdf:nodeID=\"namerica\">\n<name>North America</name>\n<type>continent</type>\n</Location>\n</within>\n</Location>\n</within>\n</Location>\n<Person rdf:nodeID=\"lucy\">\n<name>Lucy</name>\n<bornIn rdf:nodeID=\"idaho\"/>\n</Person>\n</rdf:RDF>\nRDF has a few quirks due to the fact that it is designed for internet-wide data\nexchange. The subject, predicate, and object of a triple are often URIs. For example, a\npredicate might be an URI such as <http://my-company.com/namespace#within> or\n<http://my-company.com/namespace#lives_in>, rather than just WITHIN or\nLIVES_IN. The reasoning behind this design is that you should be able to combine\nyour data with someone else\u2019s data, and if they attach a different meaning to the word\nwithin or lives_in, you won\u2019t get a conflict because their predicates are actually\n<http://other.org/foo#within> and <http://other.org/foo#lives_in>.\nThe URL <http://my-company.com/namespace> doesn\u2019t necessarily need to resolve\nto anything\u2014from RDF\u2019s point of view, it is simply a namespace. To avoid potential\nconfusion with http:// URLs, the examples in this section use non-resolvable URIs\nsuch as urn:example:within. Fortunately, you can just specify this prefix once at the\ntop of the file, and then forget about it."}
{"81": "The SPARQL query language\nSPARQL is a query language for triple-stores using the RDF data model [43]. (It is an\nacronym for SPARQL Protocol and RDF Query Language, pronounced \u201csparkle.\u201d) It\npredates Cypher, and since Cypher\u2019s pattern matching is borrowed from SPARQL,\nthey look quite similar [37].\nThe same query as before\u2014finding people who have moved from the US to Europe\u2014\nis even more concise in SPARQL than it is in Cypher (see Example 2-9).\nExample 2-9. The same query as Example 2-4, expressed in SPARQL\nPREFIX : <urn:example:>\nSELECT ?personName WHERE {\n?person :name ?personName.\n?person :bornIn / :within* / :name \"United States\".\n?person :livesIn / :within* / :name \"Europe\".\n}\nThe structure is very similar. The following two expressions are equivalent (variables\nstart with a question mark in SPARQL):\n(person) -[:BORN_IN]-> () -[:WITHIN*0..]-> (location) # Cypher\n?person :bornIn / :within* ?location. # SPARQL\nBecause RDF doesn\u2019t distinguish between properties and edges but just uses predi\u2010\ncates for both, you can use the same syntax for matching properties. In the following\nexpression, the variable usa is bound to any vertex that has a name property whose\nvalue is the string \"United States\":\n(usa {name:'United States'}) # Cypher\n?usa :name \"United States\". # SPARQL\nSPARQL is a nice query language\u2014even if the semantic web never happens, it can be\na powerful tool for applications to use internally."}
{"82": "Graph Databases Compared to the Network Model\nIn \u201cAre Document Databases Repeating History?\u201d on page 36 we discussed how\nCODASYL and the relational model competed to solve the problem of many-to-\nmany relationships in IMS. At first glance, CODASYL\u2019s network model looks similar\nto the graph model. Are graph databases the second coming of CODASYL in\ndisguise?\nNo. They differ in several important ways:\n\u2022 In CODASYL, a database had a schema that specified which record type could be\nnested within which other record type. In a graph database, there is no such\nrestriction: any vertex can have an edge to any other vertex. This gives much\ngreater flexibility for applications to adapt to changing requirements.\n\u2022 In CODASYL, the only way to reach a particular record was to traverse one of\nthe access paths to it. In a graph database, you can refer directly to any vertex by\nits unique ID, or you can use an index to find vertices with a particular value.\n\u2022 In CODASYL, the children of a record were an ordered set, so the database had\nto maintain that ordering (which had consequences for the storage layout) and\napplications that inserted new records into the database had to worry about the\npositions of the new records in these sets. In a graph database, vertices and edges\nare not ordered (you can only sort the results when making a query).\n\u2022 In CODASYL, all queries were imperative, difficult to write and easily broken by\nchanges in the schema. In a graph database, you can write your traversal in\nimperative code if you want to, but most graph databases also support high-level,\ndeclarative query languages such as Cypher or SPARQL.\nThe Foundation: Datalog\nDatalog is a much older language than SPARQL or Cypher, having been studied\nextensively by academics in the 1980s [44, 45, 46]. It is less well known among soft\u2010\nware engineers, but it is nevertheless important, because it provides the foundation\nthat later query languages build upon.\nIn practice, Datalog is used in a few data systems: for example, it is the query lan\u2010\nguage of Datomic [40], and Cascalog [47] is a Datalog implementation for querying\nlarge datasets in Hadoop.viii"}
{"83": "Datalog\u2019s data model is similar to the triple-store model, generalized a bit. Instead of\nwriting a triple as (subject, predicate, object), we write it as predicate(subject, object).\nExample 2-10 shows how to write the data from our example in Datalog.\nExample 2-10. A subset of the data in Figure 2-5, represented as Datalog facts\nname(namerica, 'North America').\ntype(namerica, continent).\nname(usa, 'United States').\ntype(usa, country).\nwithin(usa, namerica).\nname(idaho, 'Idaho').\ntype(idaho, state).\nwithin(idaho, usa).\nname(lucy, 'Lucy').\nborn_in(lucy, idaho).\nNow that we have defined the data, we can write the same query as before, as shown\nin Example 2-11. It looks a bit different from the equivalent in Cypher or SPARQL,\nbut don\u2019t let that put you off. Datalog is a subset of Prolog, which you might have\nseen before if you\u2019ve studied computer science.\nExample 2-11. The same query as Example 2-4, expressed in Datalog\nwithin_recursive(Location, Name) :- name(Location, Name). /* Rule 1 */\nwithin_recursive(Location, Name) :- within(Location, Via), /* Rule 2 */\nwithin_recursive(Via, Name).\nmigrated(Name, BornIn, LivingIn) :- name(Person, Name), /* Rule 3 */\nborn_in(Person, BornLoc),\nwithin_recursive(BornLoc, BornIn),\nlives_in(Person, LivingLoc),\nwithin_recursive(LivingLoc, LivingIn).\n?- migrated(Who, 'United States', 'Europe').\n/* Who = 'Lucy'. */\nCypher and SPARQL jump in right away with SELECT, but Datalog takes a small step\nat a time. We define rules that tell the database about new predicates: here, we define\ntwo new predicates, within_recursive and migrated. These predicates aren\u2019t triples\nstored in the database, but instead they are derived from data or from other rules."}
{"84": "In rules, words that start with an uppercase letter are variables, and predicates are\nmatched like in Cypher and SPARQL. For example, name(Location, Name) matches\nthe triple name(namerica, 'North America') with variable bindings Location =\nnamerica and Name = 'North America'.\nA rule applies if the system can find a match for all predicates on the righthand side\nof the :- operator. When the rule applies, it\u2019s as though the lefthand side of the :-\nwas added to the database (with variables replaced by the values they matched).\nOne possible way of applying the rules is thus:\n1. name(namerica, 'North America') exists in the database, so rule 1 applies. It\ngenerates within_recursive(namerica, 'North America').\n2. within(usa, namerica) exists in the database and the previous step generated\nwithin_recursive(namerica, 'North America'), so rule 2 applies. It generates\nwithin_recursive(usa, 'North America').\n3. within(idaho, usa) exists in the database and the previous step generated\nwithin_recursive(usa, 'North America'), so rule 2 applies. It generates\nwithin_recursive(idaho, 'North America').\nBy repeated application of rules 1 and 2, the within_recursive predicate can tell us\nall the locations in North America (or any other location name) contained in our\ndatabase. This process is illustrated in Figure 2-6.\nFigure 2-6. Determining that Idaho is in North America, using the Datalog rules from\nExample 2-11.\nNow rule 3 can find people who were born in some location BornIn and live in some\nlocation LivingIn. By querying with BornIn = 'United States' and LivingIn =\n'Europe', and leaving the person as a variable Who, we ask the Datalog system to find"}
{"85": "The Datalog approach requires a different kind of thinking to the other query lan\u2010\nguages discussed in this chapter, but it\u2019s a very powerful approach, because rules can\nbe combined and reused in different queries. It\u2019s less convenient for simple one-off\nqueries, but it can cope better if your data is complex.\nSummary\nData models are a huge subject, and in this chapter we have taken a quick look at a\nbroad variety of different models. We didn\u2019t have space to go into all the details of\neach model, but hopefully the overview has been enough to whet your appetite to\nfind out more about the model that best fits your application\u2019s requirements.\nHistorically, data started out being represented as one big tree (the hierarchical\nmodel), but that wasn\u2019t good for representing many-to-many relationships, so the\nrelational model was invented to solve that problem. More recently, developers found\nthat some applications don\u2019t fit well in the relational model either. New nonrelational\n\u201cNoSQL\u201d datastores have diverged in two main directions:\n1. Document databases target use cases where data comes in self-contained docu\u2010\nments and relationships between one document and another are rare.\n2. Graph databases go in the opposite direction, targeting use cases where anything\nis potentially related to everything.\nAll three models (document, relational, and graph) are widely used today, and each is\ngood in its respective domain. One model can be emulated in terms of another model\n\u2014for example, graph data can be represented in a relational database\u2014but the result\nis often awkward. That\u2019s why we have different systems for different purposes, not a\nsingle one-size-fits-all solution.\nOne thing that document and graph databases have in common is that they typically\ndon\u2019t enforce a schema for the data they store, which can make it easier to adapt\napplications to changing requirements. However, your application most likely still\nassumes that data has a certain structure; it\u2019s just a question of whether the schema is\nexplicit (enforced on write) or implicit (handled on read).\nEach data model comes with its own query language or framework, and we discussed\nseveral examples: SQL, MapReduce, MongoDB\u2019s aggregation pipeline, Cypher,\nSPARQL, and Datalog. We also touched on CSS and XSL/XPath, which aren\u2019t data\u2010\nbase query languages but have interesting parallels.\nAlthough we have covered a lot of ground, there are still many data models left"}
{"86": "DNA molecule) and matching it against a large database of strings that are simi\u2010\nlar, but not identical. None of the databases described here can handle this kind\nof usage, which is why researchers have written specialized genome database\nsoftware like GenBank [48].\n\u2022 Particle physicists have been doing Big Data\u2013style large-scale data analysis for\ndecades, and projects like the Large Hadron Collider (LHC) now work with hun\u2010\ndreds of petabytes! At such a scale custom solutions are required to stop the\nhardware cost from spiraling out of control [49].\n\u2022 Full-text search is arguably a kind of data model that is frequently used alongside\ndatabases. Information retrieval is a large specialist subject that we won\u2019t cover in\ngreat detail in this book, but we\u2019ll touch on search indexes in Chapter 3 and\nPart III.\nWe have to leave it there for now. In the next chapter we will discuss some of the\ntrade-offs that come into play when implementing the data models described in this\nchapter.\nReferences\n[1] Edgar F. Codd: \u201cA Relational Model of Data for Large Shared Data Banks,\u201d Com\u2010\nmunications of the ACM, volume 13, number 6, pages 377\u2013387, June 1970. doi:\n10.1145/362384.362685\n[2] Michael Stonebraker and Joseph M. Hellerstein: \u201cWhat Goes Around Comes\nAround,\u201d in Readings in Database Systems, 4th edition, MIT Press, pages 2\u201341, 2005.\nISBN: 978-0-262-69314-1\n[3] Pramod J. Sadalage and Martin Fowler: NoSQL Distilled. Addison-Wesley, August\n2012. ISBN: 978-0-321-82662-6\n[4] Eric Evans: \u201cNoSQL: What\u2019s in a Name?,\u201d blog.sym-link.com, October 30, 2009.\n[5] James Phillips: \u201cSurprises in Our NoSQL Adoption Survey,\u201d blog.couchbase.com,\nFebruary 8, 2012.\n[6] Michael Wagner: SQL/XML:2006 \u2013 Evaluierung der Standardkonformit\u00e4t ausge\u2010\nw\u00e4hlter Datenbanksysteme. Diplomica Verlag, Hamburg, 2010. ISBN:\n978-3-836-64609-3\n[7] \u201cXML Data in SQL Server,\u201d SQL Server 2012 documentation, technet.micro\u2010\nsoft.com, 2013."}
{"87": "[10] \u201cRethinkDB 1.11 Documentation,\u201d rethinkdb.com, 2013.\n[11] \u201cApache CouchDB 1.6 Documentation,\u201d docs.couchdb.org, 2014.\n[12] Lin Qiao, Kapil Surlaker, Shirshanka Das, et al.: \u201cOn Brewing Fresh Espresso:\nLinkedIn\u2019s Distributed Data Serving Platform,\u201d at ACM International Conference on\nManagement of Data (SIGMOD), June 2013.\n[13] Rick Long, Mark Harrington, Robert Hain, and Geoff Nicholls: IMS Primer.\nIBM Redbook SG24-5352-00, IBM International Technical Support Organization,\nJanuary 2000.\n[14] Stephen D. Bartlett: \u201cIBM\u2019s IMS\u2014Myths, Realities, and Opportunities,\u201d The\nClipper Group Navigator, TCG2013015LI, July 2013.\n[15] Sarah Mei: \u201cWhy You Should Never Use MongoDB,\u201d sarahmei.com, November\n11, 2013.\n[16] J. S. Knowles and D. M. R. Bell: \u201cThe CODASYL Model,\u201d in Databases\u2014Role\nand Structure: An Advanced Course, edited by P. M. Stocker, P. M. D. Gray, and M. P.\nAtkinson, pages 19\u201356, Cambridge University Press, 1984. ISBN: 978-0-521-25430-4\n[17] Charles W. Bachman: \u201cThe Programmer as Navigator,\u201d Communications of the\nACM, volume 16, number 11, pages 653\u2013658, November 1973. doi:\n10.1145/355611.362534\n[18] Joseph M. Hellerstein, Michael Stonebraker, and James Hamilton: \u201cArchitecture\nof a Database System,\u201d Foundations and Trends in Databases, volume 1, number 2,\npages 141\u2013259, November 2007. doi:10.1561/1900000002\n[19] Sandeep Parikh and Kelly Stirman: \u201cSchema Design for Time Series Data in\nMongoDB,\u201d blog.mongodb.org, October 30, 2013.\n[20] Martin Fowler: \u201cSchemaless Data Structures,\u201d martinfowler.com, January 7,\n2013.\n[21] Amr Awadallah: \u201cSchema-on-Read vs. Schema-on-Write,\u201d at Berkeley EECS\nRAD Lab Retreat, Santa Cruz, CA, May 2009.\n[22] Martin Odersky: \u201cThe Trouble with Types,\u201d at Strange Loop, September 2013.\n[23] Conrad Irwin: \u201cMongoDB\u2014Confessions of a PostgreSQL Lover,\u201d at\nHTML5DevConf, October 2013.\n[24] \u201cPercona Toolkit Documentation: pt-online-schema-change,\u201d Percona Ireland\nLtd., 2013."}
{"88": "[26] Shlomi Noach: \u201cgh-ost: GitHub\u2019s Online Schema Migration Tool for MySQL,\u201d\ngithubengineering.com, August 1, 2016.\n[27] James C. Corbett, Jeffrey Dean, Michael Epstein, et al.: \u201cSpanner: Google\u2019s\nGlobally-Distributed Database,\u201d at 10th USENIX Symposium on Operating System\nDesign and Implementation (OSDI), October 2012.\n[28] Donald K. Burleson: \u201cReduce I/O with Oracle Cluster Tables,\u201d dba-oracle.com.\n[29] Fay Chang, Jeffrey Dean, Sanjay Ghemawat, et al.: \u201cBigtable: A Distributed Stor\u2010\nage System for Structured Data,\u201d at 7th USENIX Symposium on Operating System\nDesign and Implementation (OSDI), November 2006.\n[30] Bobbie J. Cochrane and Kathy A. McKnight: \u201cDB2 JSON Capabilities, Part 1:\nIntroduction to DB2 JSON,\u201d IBM developerWorks, June 20, 2013.\n[31] Herb Sutter: \u201cThe Free Lunch Is Over: A Fundamental Turn Toward Concur\u2010\nrency in Software,\u201d Dr. Dobb\u2019s Journal, volume 30, number 3, pages 202-210, March\n2005.\n[32] Joseph M. Hellerstein: \u201cThe Declarative Imperative: Experiences and Conjec\u2010\ntures in Distributed Logic,\u201d Electrical Engineering and Computer Sciences, Univer\u2010\nsity of California at Berkeley, Tech report UCB/EECS-2010-90, June 2010.\n[33] Jeffrey Dean and Sanjay Ghemawat: \u201cMapReduce: Simplified Data Processing on\nLarge Clusters,\u201d at 6th USENIX Symposium on Operating System Design and Imple\u2010\nmentation (OSDI), December 2004.\n[34] Craig Kerstiens: \u201cJavaScript in Your Postgres,\u201d blog.heroku.com, June 5, 2013.\n[35] Nathan Bronson, Zach Amsden, George Cabrera, et al.: \u201cTAO: Facebook\u2019s Dis\u2010\ntributed Data Store for the Social Graph,\u201d at USENIX Annual Technical Conference\n(USENIX ATC), June 2013.\n[36] \u201cApache TinkerPop3.2.3 Documentation,\u201d tinkerpop.apache.org, October 2016.\n[37] \u201cThe Neo4j Manual v2.0.0,\u201d Neo Technology, 2013.\n[38] Emil Eifrem: Twitter correspondence, January 3, 2014.\n[39] David Beckett and Tim Berners-Lee: \u201cTurtle \u2013 Terse RDF Triple Language,\u201d\nW3C Team Submission, March 28, 2011.\n[40] \u201cDatomic Development Resources,\u201d Metadata Partners, LLC, 2013.\n[41] W3C RDF Working Group: \u201cResource Description Framework (RDF),\u201d w3.org,\n10 February 2004."}
{"89": "[43] Steve Harris, Andy Seaborne, and Eric Prud\u2019hommeaux: \u201cSPARQL 1.1 Query\nLanguage,\u201d W3C Recommendation, March 2013.\n[44] Todd J. Green, Shan Shan Huang, Boon Thau Loo, and Wenchao Zhou: \u201cData\u2010\nlog and Recursive Query Processing,\u201d Foundations and Trends in Databases, volume\n5, number 2, pages 105\u2013195, November 2013. doi:10.1561/1900000017\n[45] Stefano Ceri, Georg Gottlob, and Letizia Tanca: \u201cWhat You Always Wanted to\nKnow About Datalog (And Never Dared to Ask),\u201d IEEE Transactions on Knowledge\nand Data Engineering, volume 1, number 1, pages 146\u2013166, March 1989. doi:\n10.1109/69.43410\n[46] Serge Abiteboul, Richard Hull, and Victor Vianu: Foundations of Databases.\nAddison-Wesley, 1995. ISBN: 978-0-201-53771-0, available online at web\u2010\ndam.inria.fr/Alice\n[47] Nathan Marz: \u201cCascalog,\u201d cascalog.org.\n[48] Dennis A. Benson, Ilene Karsch-Mizrachi, David J. Lipman, et al.: \u201cGenBank,\u201d\nNucleic Acids Research, volume 36, Database issue, pages D25\u2013D30, December 2007.\ndoi:10.1093/nar/gkm929\n[49] Fons Rademakers: \u201cROOT for Big Data Analysis,\u201d at Workshop on the Future of\nBig Data Management, London, UK, June 2013."}
{"90": ""}
{"91": "CHAPTER 3\nStorage and Retrieval\nWer Ordnung h\u00e4lt, ist nur zu faul zum Suchen.\n(If you keep things tidily ordered, you\u2019re just too lazy to go searching.)\n\u2014German proverb\nOn the most fundamental level, a database needs to do two things: when you give it\nsome data, it should store the data, and when you ask it again later, it should give the\ndata back to you.\nIn Chapter 2 we discussed data models and query languages\u2014i.e., the format in\nwhich you (the application developer) give the database your data, and the mecha\u2010\nnism by which you can ask for it again later. In this chapter we discuss the same from\nthe database\u2019s point of view: how we can store the data that we\u2019re given, and how we\ncan find it again when we\u2019re asked for it.\nWhy should you, as an application developer, care how the database handles storage\nand retrieval internally? You\u2019re probably not going to implement your own storage\nengine from scratch, but you do need to select a storage engine that is appropriate for\nyour application, from the many that are available. In order to tune a storage engine\nto perform well on your kind of workload, you need to have a rough idea of what the\nstorage engine is doing under the hood.\nIn particular, there is a big difference between storage engines that are optimized for\ntransactional workloads and those that are optimized for analytics. We will explore\nthat distinction later in \u201cTransaction Processing or Analytics?\u201d on page 90, and in\n\u201cColumn-Oriented Storage\u201d on page 95 we\u2019ll discuss a family of storage engines that\nis optimized for analytics."}
{"92": "storage engines: log-structured storage engines, and page-oriented storage engines\nsuch as B-trees.\nData Structures That Power Your Database\nConsider the world\u2019s simplest database, implemented as two Bash functions:\n#!/bin/bash\ndb_set () {\necho \"$1,$2\" >> database\n}\ndb_get () {\ngrep \"^$1,\" database | sed -e \"s/^$1,//\" | tail -n 1\n}\nThese two functions implement a key-value store. You can call db_set key value,\nwhich will store key and value in the database. The key and value can be (almost)\nanything you like\u2014for example, the value could be a JSON document. You can then\ncall db_get key, which looks up the most recent value associated with that particular\nkey and returns it.\nAnd it works:\n$ db_set 123456 '{\"name\":\"London\",\"attractions\":[\"Big Ben\",\"London Eye\"]}'\n$ db_set 42 '{\"name\":\"San Francisco\",\"attractions\":[\"Golden Gate Bridge\"]}'\n$ db_get 42\n{\"name\":\"San Francisco\",\"attractions\":[\"Golden Gate Bridge\"]}\nThe underlying storage format is very simple: a text file where each line contains a\nkey-value pair, separated by a comma (roughly like a CSV file, ignoring escaping\nissues). Every call to db_set appends to the end of the file, so if you update a key sev\u2010\neral times, the old versions of the value are not overwritten\u2014you need to look at the\nlast occurrence of a key in a file to find the latest value (hence the tail -n 1 in\ndb_get):\n$ db_set 42 '{\"name\":\"San Francisco\",\"attractions\":[\"Exploratorium\"]}'\n$ db_get 42\n{\"name\":\"San Francisco\",\"attractions\":[\"Exploratorium\"]}\n$ cat database\n123456,{\"name\":\"London\",\"attractions\":[\"Big Ben\",\"London Eye\"]}"}
{"93": "Our db_set function actually has pretty good performance for something that is so\nsimple, because appending to a file is generally very efficient. Similarly to what\ndb_set does, many databases internally use a log, which is an append-only data file.\nReal databases have more issues to deal with (such as concurrency control, reclaim\u2010\ning disk space so that the log doesn\u2019t grow forever, and handling errors and partially\nwritten records), but the basic principle is the same. Logs are incredibly useful, and\nwe will encounter them several times in the rest of this book.\nThe word log is often used to refer to application logs, where an\napplication outputs text that describes what\u2019s happening. In this\nbook, log is used in the more general sense: an append-only\nsequence of records. It doesn\u2019t have to be human-readable; it might\nbe binary and intended only for other programs to read.\nOn the other hand, our db_get function has terrible performance if you have a large\nnumber of records in your database. Every time you want to look up a key, db_get\nhas to scan the entire database file from beginning to end, looking for occurrences of\nthe key. In algorithmic terms, the cost of a lookup is O(n): if you double the number\nof records n in your database, a lookup takes twice as long. That\u2019s not good.\nIn order to efficiently find the value for a particular key in the database, we need a\ndifferent data structure: an index. In this chapter we will look at a range of indexing\nstructures and see how they compare; the general idea behind them is to keep some\nadditional metadata on the side, which acts as a signpost and helps you to locate the\ndata you want. If you want to search the same data in several different ways, you may\nneed several different indexes on different parts of the data.\nAn index is an additional structure that is derived from the primary data. Many data\u2010\nbases allow you to add and remove indexes, and this doesn\u2019t affect the contents of the\ndatabase; it only affects the performance of queries. Maintaining additional structures\nincurs overhead, especially on writes. For writes, it\u2019s hard to beat the performance of\nsimply appending to a file, because that\u2019s the simplest possible write operation. Any\nkind of index usually slows down writes, because the index also needs to be updated\nevery time data is written.\nThis is an important trade-off in storage systems: well-chosen indexes speed up read\nqueries, but every index slows down writes. For this reason, databases don\u2019t usually\nindex everything by default, but require you\u2014the application developer or database\nadministrator\u2014to choose indexes manually, using your knowledge of the applica\u2010\ntion\u2019s typical query patterns. You can then choose the indexes that give your applica\u2010"}
{"94": "Hash Indexes\nLet\u2019s start with indexes for key-value data. This is not the only kind of data you can\nindex, but it\u2019s very common, and it\u2019s a useful building block for more complex\nindexes.\nKey-value stores are quite similar to the dictionary type that you can find in most\nprogramming languages, and which is usually implemented as a hash map (hash\ntable). Hash maps are described in many algorithms textbooks [1, 2], so we won\u2019t go\ninto detail of how they work here. Since we already have hash maps for our in-\nmemory data structures, why not use them to index our data on disk?\nLet\u2019s say our data storage consists only of appending to a file, as in the preceding\nexample. Then the simplest possible indexing strategy is this: keep an in-memory\nhash map where every key is mapped to a byte offset in the data file\u2014the location at\nwhich the value can be found, as illustrated in Figure 3-1. Whenever you append a\nnew key-value pair to the file, you also update the hash map to reflect the offset of the\ndata you just wrote (this works both for inserting new keys and for updating existing\nkeys). When you want to look up a value, use the hash map to find the offset in the\ndata file, seek to that location, and read the value.\nFigure 3-1. Storing a log of key-value pairs in a CSV-like format, indexed with an in-\nmemory hash map.\nThis may sound simplistic, but it is a viable approach. In fact, this is essentially what\nBitcask (the default storage engine in Riak) does [3]. Bitcask offers high-performance"}
{"95": "seek. If that part of the data file is already in the filesystem cache, a read doesn\u2019t\nrequire any disk I/O at all.\nA storage engine like Bitcask is well suited to situations where the value for each key\nis updated frequently. For example, the key might be the URL of a cat video, and the\nvalue might be the number of times it has been played (incremented every time\nsomeone hits the play button). In this kind of workload, there are a lot of writes, but\nthere are not too many distinct keys\u2014you have a large number of writes per key, but\nit\u2019s feasible to keep all keys in memory.\nAs described so far, we only ever append to a file\u2014so how do we avoid eventually\nrunning out of disk space? A good solution is to break the log into segments of a cer\u2010\ntain size by closing a segment file when it reaches a certain size, and making subse\u2010\nquent writes to a new segment file. We can then perform compaction on these\nsegments, as illustrated in Figure 3-2. Compaction means throwing away duplicate\nkeys in the log, and keeping only the most recent update for each key.\nFigure 3-2. Compaction of a key-value update log (counting the number of times each\ncat video was played), retaining only the most recent value for each key.\nMoreover, since compaction often makes segments much smaller (assuming that a\nkey is overwritten several times on average within one segment), we can also merge\nseveral segments together at the same time as performing the compaction, as shown\nin Figure 3-3. Segments are never modified after they have been written, so the\nmerged segment is written to a new file. The merging and compaction of frozen seg\u2010\nments can be done in a background thread, and while it is going on, we can still con\u2010\ntinue to serve read and write requests as normal, using the old segment files. After the\nmerging process is complete, we switch read requests to using the new merged seg\u2010\nment instead of the old segments\u2014and then the old segment files can simply be"}
{"96": "Figure 3-3. Performing compaction and segment merging simultaneously.\nEach segment now has its own in-memory hash table, mapping keys to file offsets. In\norder to find the value for a key, we first check the most recent segment\u2019s hash map;\nif the key is not present we check the second-most-recent segment, and so on. The\nmerging process keeps the number of segments small, so lookups don\u2019t need to check\nmany hash maps.\nLots of detail goes into making this simple idea work in practice. Briefly, some of the\nissues that are important in a real implementation are:\nFile format\nCSV is not the best format for a log. It\u2019s faster and simpler to use a binary format\nthat first encodes the length of a string in bytes, followed by the raw string\n(without need for escaping).\nDeleting records\nIf you want to delete a key and its associated value, you have to append a special\ndeletion record to the data file (sometimes called a tombstone). When log seg\u2010\nments are merged, the tombstone tells the merging process to discard any previ\u2010\nous values for the deleted key.\nCrash recovery\nIf the database is restarted, the in-memory hash maps are lost. In principle, you\ncan restore each segment\u2019s hash map by reading the entire segment file from\nbeginning to end and noting the offset of the most recent value for every key as"}
{"97": "a snapshot of each segment\u2019s hash map on disk, which can be loaded into mem\u2010\nory more quickly.\nPartially written records\nThe database may crash at any time, including halfway through appending a\nrecord to the log. Bitcask files include checksums, allowing such corrupted parts\nof the log to be detected and ignored.\nConcurrency control\nAs writes are appended to the log in a strictly sequential order, a common imple\u2010\nmentation choice is to have only one writer thread. Data file segments are\nappend-only and otherwise immutable, so they can be read concurrently by mul\u2010\ntiple threads.\nAn append-only log seems wasteful at first glance: why don\u2019t you update the file in\nplace, overwriting the old value with the new value? But an append-only design turns\nout to be good for several reasons:\n\u2022 Appending and segment merging are sequential write operations, which are gen\u2010\nerally much faster than random writes, especially on magnetic spinning-disk\nhard drives. To some extent sequential writes are also preferable on flash-based\nsolid state drives (SSDs) [4]. We will discuss this issue further in \u201cComparing B-\nTrees and LSM-Trees\u201d on page 83.\n\u2022 Concurrency and crash recovery are much simpler if segment files are append-\nonly or immutable. For example, you don\u2019t have to worry about the case where a\ncrash happened while a value was being overwritten, leaving you with a file con\u2010\ntaining part of the old and part of the new value spliced together.\n\u2022 Merging old segments avoids the problem of data files getting fragmented over\ntime.\nHowever, the hash table index also has limitations:\n\u2022 The hash table must fit in memory, so if you have a very large number of keys,\nyou\u2019re out of luck. In principle, you could maintain a hash map on disk, but\nunfortunately it is difficult to make an on-disk hash map perform well. It\nrequires a lot of random access I/O, it is expensive to grow when it becomes full,\nand hash collisions require fiddly logic [5].\n\u2022 Range queries are not efficient. For example, you cannot easily scan over all keys\nbetween kitty00000 and kitty99999\u2014you\u2019d have to look up each key individu\u2010\nally in the hash maps."}
{"98": "SSTables and LSM-Trees\nIn Figure 3-3, each log-structured storage segment is a sequence of key-value pairs.\nThese pairs appear in the order that they were written, and values later in the log take\nprecedence over values for the same key earlier in the log. Apart from that, the order\nof key-value pairs in the file does not matter.\nNow we can make a simple change to the format of our segment files: we require that\nthe sequence of key-value pairs is sorted by key. At first glance, that requirement\nseems to break our ability to use sequential writes, but we\u2019ll get to that in a moment.\nWe call this format Sorted String Table, or SSTable for short. We also require that\neach key only appears once within each merged segment file (the compaction process\nalready ensures that). SSTables have several big advantages over log segments with\nhash indexes:\n1. Merging segments is simple and efficient, even if the files are bigger than the\navailable memory. The approach is like the one used in the mergesort algorithm\nand is illustrated in Figure 3-4: you start reading the input files side by side, look\nat the first key in each file, copy the lowest key (according to the sort order) to\nthe output file, and repeat. This produces a new merged segment file, also sorted\nby key."}
{"99": "What if the same key appears in several input segments? Remember that each\nsegment contains all the values written to the database during some period of\ntime. This means that all the values in one input segment must be more recent\nthan all the values in the other segment (assuming that we always merge adjacent\nsegments). When multiple segments contain the same key, we can keep the value\nfrom the most recent segment and discard the values in older segments.\n2. In order to find a particular key in the file, you no longer need to keep an index\nof all the keys in memory. See Figure 3-5 for an example: say you\u2019re looking for\nthe key handiwork, but you don\u2019t know the exact offset of that key in the segment\nfile. However, you do know the offsets for the keys handbag and handsome, and\nbecause of the sorting you know that handiwork must appear between those two.\nThis means you can jump to the offset for handbag and scan from there until you\nfind handiwork (or not, if the key is not present in the file).\nFigure 3-5. An SSTable with an in-memory index.\nYou still need an in-memory index to tell you the offsets for some of the keys, but\nit can be sparse: one key for every few kilobytes of segment file is sufficient,\nbecause a few kilobytes can be scanned very quickly.i\n3. Since read requests need to scan over several key-value pairs in the requested\nrange anyway, it is possible to group those records into a block and compress it\nbefore writing it to disk (indicated by the shaded area in Figure 3-5). Each entry\nof the sparse in-memory index then points at the start of a compressed block.\nBesides saving disk space, compression also reduces the I/O bandwidth use."}
{"100": "Constructing and maintaining SSTables\nFine so far\u2014but how do you get your data to be sorted by key in the first place? Our\nincoming writes can occur in any order.\nMaintaining a sorted structure on disk is possible (see \u201cB-Trees\u201d on page 79), but\nmaintaining it in memory is much easier. There are plenty of well-known tree data\nstructures that you can use, such as red-black trees or AVL trees [2]. With these data\nstructures, you can insert keys in any order and read them back in sorted order.\nWe can now make our storage engine work as follows:\n\u2022 When a write comes in, add it to an in-memory balanced tree data structure (for\nexample, a red-black tree). This in-memory tree is sometimes called a memtable.\n\u2022 When the memtable gets bigger than some threshold\u2014typically a few megabytes\n\u2014write it out to disk as an SSTable file. This can be done efficiently because the\ntree already maintains the key-value pairs sorted by key. The new SSTable file\nbecomes the most recent segment of the database. While the SSTable is being\nwritten out to disk, writes can continue to a new memtable instance.\n\u2022 In order to serve a read request, first try to find the key in the memtable, then in\nthe most recent on-disk segment, then in the next-older segment, etc.\n\u2022 From time to time, run a merging and compaction process in the background to\ncombine segment files and to discard overwritten or deleted values.\nThis scheme works very well. It only suffers from one problem: if the database\ncrashes, the most recent writes (which are in the memtable but not yet written out to\ndisk) are lost. In order to avoid that problem, we can keep a separate log on disk to\nwhich every write is immediately appended, just like in the previous section. That log\nis not in sorted order, but that doesn\u2019t matter, because its only purpose is to restore\nthe memtable after a crash. Every time the memtable is written out to an SSTable, the\ncorresponding log can be discarded.\nMaking an LSM-tree out of SSTables\nThe algorithm described here is essentially what is used in LevelDB [6] and RocksDB\n[7], key-value storage engine libraries that are designed to be embedded into other\napplications. Among other things, LevelDB can be used in Riak as an alternative to\nBitcask. Similar storage engines are used in Cassandra and HBase [8], both of which\nwere inspired by Google\u2019s Bigtable paper [9] (which introduced the terms SSTable\nand memtable)."}
{"101": "log-structured filesystems [11]. Storage engines that are based on this principle of\nmerging and compacting sorted files are often called LSM storage engines.\nLucene, an indexing engine for full-text search used by Elasticsearch and Solr, uses a\nsimilar method for storing its term dictionary [12, 13]. A full-text index is much more\ncomplex than a key-value index but is based on a similar idea: given a word in a\nsearch query, find all the documents (web pages, product descriptions, etc.) that\nmention the word. This is implemented with a key-value structure where the key is a\nword (a term) and the value is the list of IDs of all the documents that contain the\nword (the postings list). In Lucene, this mapping from term to postings list is kept in\nSSTable-like sorted files, which are merged in the background as needed [14].\nPerformance optimizations\nAs always, a lot of detail goes into making a storage engine perform well in practice.\nFor example, the LSM-tree algorithm can be slow when looking up keys that do not\nexist in the database: you have to check the memtable, then the segments all the way\nback to the oldest (possibly having to read from disk for each one) before you can be\nsure that the key does not exist. In order to optimize this kind of access, storage\nengines often use additional Bloom filters [15]. (A Bloom filter is a memory-efficient\ndata structure for approximating the contents of a set. It can tell you if a key does not\nappear in the database, and thus saves many unnecessary disk reads for nonexistent\nkeys.)\nThere are also different strategies to determine the order and timing of how SSTables\nare compacted and merged. The most common options are size-tiered and leveled\ncompaction. LevelDB and RocksDB use leveled compaction (hence the name of Lev\u2010\nelDB), HBase uses size-tiered, and Cassandra supports both [16]. In size-tiered com\u2010\npaction, newer and smaller SSTables are successively merged into older and larger\nSSTables. In leveled compaction, the key range is split up into smaller SSTables and\nolder data is moved into separate \u201clevels,\u201d which allows the compaction to proceed\nmore incrementally and use less disk space.\nEven though there are many subtleties, the basic idea of LSM-trees\u2014keeping a cas\u2010\ncade of SSTables that are merged in the background\u2014is simple and effective. Even\nwhen the dataset is much bigger than the available memory it continues to work well.\nSince data is stored in sorted order, you can efficiently perform range queries (scan\u2010\nning all keys above some minimum and up to some maximum), and because the disk\nwrites are sequential the LSM-tree can support remarkably high write throughput.\nB-Trees"}
{"102": "Introduced in 1970 [17] and called \u201cubiquitous\u201d less than 10 years later [18], B-trees\nhave stood the test of time very well. They remain the standard index implementation\nin almost all relational databases, and many nonrelational databases use them too.\nLike SSTables, B-trees keep key-value pairs sorted by key, which allows efficient key-\nvalue lookups and range queries. But that\u2019s where the similarity ends: B-trees have a\nvery different design philosophy.\nThe log-structured indexes we saw earlier break the database down into variable-size\nsegments, typically several megabytes or more in size, and always write a segment\nsequentially. By contrast, B-trees break the database down into fixed-size blocks or\npages, traditionally 4 KB in size (sometimes bigger), and read or write one page at a\ntime. This design corresponds more closely to the underlying hardware, as disks are\nalso arranged in fixed-size blocks.\nEach page can be identified using an address or location, which allows one page to\nrefer to another\u2014similar to a pointer, but on disk instead of in memory. We can use\nthese page references to construct a tree of pages, as illustrated in Figure 3-6.\nFigure 3-6. Looking up a key using a B-tree index.\nOne page is designated as the root of the B-tree; whenever you want to look up a key\nin the index, you start here. The page contains several keys and references to child\npages. Each child is responsible for a continuous range of keys, and the keys between\nthe references indicate where the boundaries between those ranges lie."}
{"103": "Eventually we get down to a page containing individual keys (a leaf page), which\neither contains the value for each key inline or contains references to the pages where\nthe values can be found.\nThe number of references to child pages in one page of the B-tree is called the\nbranching factor. For example, in Figure 3-6 the branching factor is six. In practice,\nthe branching factor depends on the amount of space required to store the page refer\u2010\nences and the range boundaries, but typically it is several hundred.\nIf you want to update the value for an existing key in a B-tree, you search for the leaf\npage containing that key, change the value in that page, and write the page back to\ndisk (any references to that page remain valid). If you want to add a new key, you\nneed to find the page whose range encompasses the new key and add it to that page.\nIf there isn\u2019t enough free space in the page to accommodate the new key, it is split\ninto two half-full pages, and the parent page is updated to account for the new subdi\u2010\nvision of key ranges\u2014see Figure 3-7.ii\nFigure 3-7. Growing a B-tree by splitting a page.\nThis algorithm ensures that the tree remains balanced: a B-tree with n keys always\nhas a depth of O(log n). Most databases can fit into a B-tree that is three or four levels\ndeep, so you don\u2019t need to follow many page references to find the page you are look\u2010\ning for. (A four-level tree of 4 KB pages with a branching factor of 500 can store up to\n256 TB.)"}
{"104": "Making B-trees reliable\nThe basic underlying write operation of a B-tree is to overwrite a page on disk with\nnew data. It is assumed that the overwrite does not change the location of the page;\ni.e., all references to that page remain intact when the page is overwritten. This is in\nstark contrast to log-structured indexes such as LSM-trees, which only append to files\n(and eventually delete obsolete files) but never modify files in place.\nYou can think of overwriting a page on disk as an actual hardware operation. On a\nmagnetic hard drive, this means moving the disk head to the right place, waiting for\nthe right position on the spinning platter to come around, and then overwriting the\nappropriate sector with new data. On SSDs, what happens is somewhat more compli\u2010\ncated, due to the fact that an SSD must erase and rewrite fairly large blocks of a stor\u2010\nage chip at a time [19].\nMoreover, some operations require several different pages to be overwritten. For\nexample, if you split a page because an insertion caused it to be overfull, you need to\nwrite the two pages that were split, and also overwrite their parent page to update the\nreferences to the two child pages. This is a dangerous operation, because if the data\u2010\nbase crashes after only some of the pages have been written, you end up with a cor\u2010\nrupted index (e.g., there may be an orphan page that is not a child of any parent).\nIn order to make the database resilient to crashes, it is common for B-tree implemen\u2010\ntations to include an additional data structure on disk: a write-ahead log (WAL, also\nknown as a redo log). This is an append-only file to which every B-tree modification\nmust be written before it can be applied to the pages of the tree itself. When the data\u2010\nbase comes back up after a crash, this log is used to restore the B-tree back to a con\u2010\nsistent state [5, 20].\nAn additional complication of updating pages in place is that careful concurrency\ncontrol is required if multiple threads are going to access the B-tree at the same time\n\u2014otherwise a thread may see the tree in an inconsistent state. This is typically done\nby protecting the tree\u2019s data structures with latches (lightweight locks). Log-\nstructured approaches are simpler in this regard, because they do all the merging in\nthe background without interfering with incoming queries and atomically swap old\nsegments for new segments from time to time.\nB-tree optimizations\nAs B-trees have been around for so long, it\u2019s not surprising that many optimizations\nhave been developed over the years. To mention just a few:\n\u2022 Instead of overwriting pages and maintaining a WAL for crash recovery, some"}
{"105": "rency control, as we shall see in \u201cSnapshot Isolation and Repeatable Read\u201d on\npage 237.\n\u2022 We can save space in pages by not storing the entire key, but abbreviating it.\nEspecially in pages on the interior of the tree, keys only need to provide enough\ninformation to act as boundaries between key ranges. Packing more keys into a\npage allows the tree to have a higher branching factor, and thus fewer levels.iii\n\u2022 In general, pages can be positioned anywhere on disk; there is nothing requiring\npages with nearby key ranges to be nearby on disk. If a query needs to scan over a\nlarge part of the key range in sorted order, that page-by-page layout can be ineffi\u2010\ncient, because a disk seek may be required for every page that is read. Many B-\ntree implementations therefore try to lay out the tree so that leaf pages appear in\nsequential order on disk. However, it\u2019s difficult to maintain that order as the tree\ngrows. By contrast, since LSM-trees rewrite large segments of the storage in one\ngo during merging, it\u2019s easier for them to keep sequential keys close to each other\non disk.\n\u2022 Additional pointers have been added to the tree. For example, each leaf page may\nhave references to its sibling pages to the left and right, which allows scanning\nkeys in order without jumping back to parent pages.\n\u2022 B-tree variants such as fractal trees [22] borrow some log-structured ideas to\nreduce disk seeks (and they have nothing to do with fractals).\nComparing B-Trees and LSM-Trees\nEven though B-tree implementations are generally more mature than LSM-tree\nimplementations, LSM-trees are also interesting due to their performance character\u2010\nistics. As a rule of thumb, LSM-trees are typically faster for writes, whereas B-trees\nare thought to be faster for reads [23]. Reads are typically slower on LSM-trees\nbecause they have to check several different data structures and SSTables at different\nstages of compaction.\nHowever, benchmarks are often inconclusive and sensitive to details of the workload.\nYou need to test systems with your particular workload in order to make a valid com\u2010\nparison. In this section we will briefly discuss a few things that are worth considering\nwhen measuring the performance of a storage engine."}
{"106": "Advantages of LSM-trees\nA B-tree index must write every piece of data at least twice: once to the write-ahead\nlog, and once to the tree page itself (and perhaps again as pages are split). There is\nalso overhead from having to write an entire page at a time, even if only a few bytes in\nthat page changed. Some storage engines even overwrite the same page twice in order\nto avoid ending up with a partially updated page in the event of a power failure [24,\n25].\nLog-structured indexes also rewrite data multiple times due to repeated compaction\nand merging of SSTables. This effect\u2014one write to the database resulting in multiple\nwrites to the disk over the course of the database\u2019s lifetime\u2014is known as write ampli\u2010\nfication. It is of particular concern on SSDs, which can only overwrite blocks a limi\u2010\nted number of times before wearing out.\nIn write-heavy applications, the performance bottleneck might be the rate at which\nthe database can write to disk. In this case, write amplification has a direct perfor\u2010\nmance cost: the more that a storage engine writes to disk, the fewer writes per second\nit can handle within the available disk bandwidth.\nMoreover, LSM-trees are typically able to sustain higher write throughput than B-\ntrees, partly because they sometimes have lower write amplification (although this\ndepends on the storage engine configuration and workload), and partly because they\nsequentially write compact SSTable files rather than having to overwrite several pages\nin the tree [26]. This difference is particularly important on magnetic hard drives,\nwhere sequential writes are much faster than random writes.\nLSM-trees can be compressed better, and thus often produce smaller files on disk\nthan B-trees. B-tree storage engines leave some disk space unused due to fragmenta\u2010\ntion: when a page is split or when a row cannot fit into an existing page, some space\nin a page remains unused. Since LSM-trees are not page-oriented and periodically\nrewrite SSTables to remove fragmentation, they have lower storage overheads, espe\u2010\ncially when using leveled compaction [27].\nOn many SSDs, the firmware internally uses a log-structured algorithm to turn ran\u2010\ndom writes into sequential writes on the underlying storage chips, so the impact of\nthe storage engine\u2019s write pattern is less pronounced [19]. However, lower write\namplification and reduced fragmentation are still advantageous on SSDs: represent\u2010\ning data more compactly allows more read and write requests within the available I/O\nbandwidth.\nDownsides of LSM-trees"}
{"107": "access, disks have limited resources, so it can easily happen that a request needs to\nwait while the disk finishes an expensive compaction operation. The impact on\nthroughput and average response time is usually small, but at higher percentiles (see\n\u201cDescribing Performance\u201d on page 13) the response time of queries to log-structured\nstorage engines can sometimes be quite high, and B-trees can be more predictable\n[28].\nAnother issue with compaction arises at high write throughput: the disk\u2019s finite write\nbandwidth needs to be shared between the initial write (logging and flushing a\nmemtable to disk) and the compaction threads running in the background. When\nwriting to an empty database, the full disk bandwidth can be used for the initial write,\nbut the bigger the database gets, the more disk bandwidth is required for compaction.\nIf write throughput is high and compaction is not configured carefully, it can happen\nthat compaction cannot keep up with the rate of incoming writes. In this case, the\nnumber of unmerged segments on disk keeps growing until you run out of disk\nspace, and reads also slow down because they need to check more segment files. Typ\u2010\nically, SSTable-based storage engines do not throttle the rate of incoming writes, even\nif compaction cannot keep up, so you need explicit monitoring to detect this situa\u2010\ntion [29, 30].\nAn advantage of B-trees is that each key exists in exactly one place in the index,\nwhereas a log-structured storage engine may have multiple copies of the same key in\ndifferent segments. This aspect makes B-trees attractive in databases that want to\noffer strong transactional semantics: in many relational databases, transaction isola\u2010\ntion is implemented using locks on ranges of keys, and in a B-tree index, those locks\ncan be directly attached to the tree [5]. In Chapter 7 we will discuss this point in more\ndetail.\nB-trees are very ingrained in the architecture of databases and provide consistently\ngood performance for many workloads, so it\u2019s unlikely that they will go away anytime\nsoon. In new datastores, log-structured indexes are becoming increasingly popular.\nThere is no quick and easy rule for determining which type of storage engine is better\nfor your use case, so it is worth testing empirically.\nOther Indexing Structures\nSo far we have only discussed key-value indexes, which are like a primary key index in\nthe relational model. A primary key uniquely identifies one row in a relational table,\nor one document in a document database, or one vertex in a graph database. Other\nrecords in the database can refer to that row/document/vertex by its primary key (or"}
{"108": "mand, and they are often crucial for performing joins efficiently. For example, in\nFigure 2-1 in Chapter 2 you would most likely have a secondary index on the\nuser_id columns so that you can find all the rows belonging to the same user in each\nof the tables.\nA secondary index can easily be constructed from a key-value index. The main differ\u2010\nence is that keys are not unique; i.e., there might be many rows (documents, vertices)\nwith the same key. This can be solved in two ways: either by making each value in the\nindex a list of matching row identifiers (like a postings list in a full-text index) or by\nmaking each key unique by appending a row identifier to it. Either way, both B-trees\nand log-structured indexes can be used as secondary indexes.\nStoring values within the index\nThe key in an index is the thing that queries search for, but the value can be one of\ntwo things: it could be the actual row (document, vertex) in question, or it could be a\nreference to the row stored elsewhere. In the latter case, the place where rows are\nstored is known as a heap file, and it stores data in no particular order (it may be\nappend-only, or it may keep track of deleted rows in order to overwrite them with\nnew data later). The heap file approach is common because it avoids duplicating data\nwhen multiple secondary indexes are present: each index just references a location in\nthe heap file, and the actual data is kept in one place.\nWhen updating a value without changing the key, the heap file approach can be quite\nefficient: the record can be overwritten in place, provided that the new value is not\nlarger than the old value. The situation is more complicated if the new value is larger,\nas it probably needs to be moved to a new location in the heap where there is enough\nspace. In that case, either all indexes need to be updated to point at the new heap\nlocation of the record, or a forwarding pointer is left behind in the old heap location\n[5].\nIn some situations, the extra hop from the index to the heap file is too much of a per\u2010\nformance penalty for reads, so it can be desirable to store the indexed row directly\nwithin an index. This is known as a clustered index. For example, in MySQL\u2019s\nInnoDB storage engine, the primary key of a table is always a clustered index, and\nsecondary indexes refer to the primary key (rather than a heap file location) [31]. In\nSQL Server, you can specify one clustered index per table [32].\nA compromise between a clustered index (storing all row data within the index) and\na nonclustered index (storing only references to the data within the index) is known\nas a covering index or index with included columns, which stores some of a table\u2019s col\u2010\numns within the index [33]. This allows some queries to be answered by using the"}
{"109": "As with any kind of duplication of data, clustered and covering indexes can speed up\nreads, but they require additional storage and can add overhead on writes. Databases\nalso need to go to additional effort to enforce transactional guarantees, because appli\u2010\ncations should not see inconsistencies due to the duplication.\nMulti-column indexes\nThe indexes discussed so far only map a single key to a value. That is not sufficient if\nwe need to query multiple columns of a table (or multiple fields in a document)\nsimultaneously.\nThe most common type of multi-column index is called a concatenated index, which\nsimply combines several fields into one key by appending one column to another (the\nindex definition specifies in which order the fields are concatenated). This is like an\nold-fashioned paper phone book, which provides an index from (lastname, first\u2010\nname) to phone number. Due to the sort order, the index can be used to find all the\npeople with a particular last name, or all the people with a particular lastname-\nfirstname combination. However, the index is useless if you want to find all the peo\u2010\nple with a particular first name.\nMulti-dimensional indexes are a more general way of querying several columns at\nonce, which is particularly important for geospatial data. For example, a restaurant-\nsearch website may have a database containing the latitude and longitude of each res\u2010\ntaurant. When a user is looking at the restaurants on a map, the website needs to\nsearch for all the restaurants within the rectangular map area that the user is cur\u2010\nrently viewing. This requires a two-dimensional range query like the following:\nSELECT * FROM restaurants WHERE latitude > 51.4946 AND latitude < 51.5079\nAND longitude > -0.1162 AND longitude < -0.1004;\nA standard B-tree or LSM-tree index is not able to answer that kind of query effi\u2010\nciently: it can give you either all the restaurants in a range of latitudes (but at any lon\u2010\ngitude), or all the restaurants in a range of longitudes (but anywhere between the\nNorth and South poles), but not both simultaneously.\nOne option is to translate a two-dimensional location into a single number using a\nspace-filling curve, and then to use a regular B-tree index [34]. More commonly, spe\u2010\ncialized spatial indexes such as R-trees are used. For example, PostGIS implements\ngeospatial indexes as R-trees using PostgreSQL\u2019s Generalized Search Tree indexing\nfacility [35]. We don\u2019t have space to describe R-trees in detail here, but there is plenty\nof literature on them.\nAn interesting idea is that multi-dimensional indexes are not just for geographic"}
{"110": "index on (date, temperature) in order to efficiently search for all the observations\nduring the year 2013 where the temperature was between 25 and 30\u2103. With a one-\ndimensional index, you would have to either scan over all the records from 2013\n(regardless of temperature) and then filter them by temperature, or vice versa. A 2D\nindex could narrow down by timestamp and temperature simultaneously. This tech\u2010\nnique is used by HyperDex [36].\nFull-text search and fuzzy indexes\nAll the indexes discussed so far assume that you have exact data and allow you to\nquery for exact values of a key, or a range of values of a key with a sort order. What\nthey don\u2019t allow you to do is search for similar keys, such as misspelled words. Such\nfuzzy querying requires different techniques.\nFor example, full-text search engines commonly allow a search for one word to be\nexpanded to include synonyms of the word, to ignore grammatical variations of\nwords, and to search for occurrences of words near each other in the same document,\nand support various other features that depend on linguistic analysis of the text. To\ncope with typos in documents or queries, Lucene is able to search text for words\nwithin a certain edit distance (an edit distance of 1 means that one letter has been\nadded, removed, or replaced) [37].\nAs mentioned in \u201cMaking an LSM-tree out of SSTables\u201d on page 78, Lucene uses a\nSSTable-like structure for its term dictionary. This structure requires a small in-\nmemory index that tells queries at which offset in the sorted file they need to look for\na key. In LevelDB, this in-memory index is a sparse collection of some of the keys,\nbut in Lucene, the in-memory index is a finite state automaton over the characters in\nthe keys, similar to a trie [38]. This automaton can be transformed into a Levenshtein\nautomaton, which supports efficient search for words within a given edit distance\n[39].\nOther fuzzy search techniques go in the direction of document classification and\nmachine learning. See an information retrieval textbook for more detail [e.g., 40].\nKeeping everything in memory\nThe data structures discussed so far in this chapter have all been answers to the limi\u2010\ntations of disks. Compared to main memory, disks are awkward to deal with. With\nboth magnetic disks and SSDs, data on disk needs to be laid out carefully if you want\ngood performance on reads and writes. However, we tolerate this awkwardness\nbecause disks have two significant advantages: they are durable (their contents are\nnot lost if the power is turned off), and they have a lower cost per gigabyte than"}
{"111": "tially distributed across several machines. This has led to the development of in-\nmemory databases.\nSome in-memory key-value stores, such as Memcached, are intended for caching use\nonly, where it\u2019s acceptable for data to be lost if a machine is restarted. But other in-\nmemory databases aim for durability, which can be achieved with special hardware\n(such as battery-powered RAM), by writing a log of changes to disk, by writing peri\u2010\nodic snapshots to disk, or by replicating the in-memory state to other machines.\nWhen an in-memory database is restarted, it needs to reload its state, either from disk\nor over the network from a replica (unless special hardware is used). Despite writing\nto disk, it\u2019s still an in-memory database, because the disk is merely used as an\nappend-only log for durability, and reads are served entirely from memory. Writing\nto disk also has operational advantages: files on disk can easily be backed up,\ninspected, and analyzed by external utilities.\nProducts such as VoltDB, MemSQL, and Oracle TimesTen are in-memory databases\nwith a relational model, and the vendors claim that they can offer big performance\nimprovements by removing all the overheads associated with managing on-disk data\nstructures [41, 42]. RAMCloud is an open source, in-memory key-value store with\ndurability (using a log-structured approach for the data in memory as well as the data\non disk) [43]. Redis and Couchbase provide weak durability by writing to disk asyn\u2010\nchronously.\nCounterintuitively, the performance advantage of in-memory databases is not due to\nthe fact that they don\u2019t need to read from disk. Even a disk-based storage engine may\nnever need to read from disk if you have enough memory, because the operating sys\u2010\ntem caches recently used disk blocks in memory anyway. Rather, they can be faster\nbecause they can avoid the overheads of encoding in-memory data structures in a\nform that can be written to disk [44].\nBesides performance, another interesting area for in-memory databases is providing\ndata models that are difficult to implement with disk-based indexes. For example,\nRedis offers a database-like interface to various data structures such as priority\nqueues and sets. Because it keeps all data in memory, its implementation is compara\u2010\ntively simple.\nRecent research indicates that an in-memory database architecture could be extended\nto support datasets larger than the available memory, without bringing back the over\u2010\nheads of a disk-centric architecture [45]. The so-called anti-caching approach works\nby evicting the least recently used data from memory to disk when there is not\nenough memory, and loading it back into memory when it is accessed again in the"}
{"112": "approach still requires indexes to fit entirely in memory, though (like the Bitcask\nexample at the beginning of the chapter).\nFurther changes to storage engine design will probably be needed if non-volatile\nmemory (NVM) technologies become more widely adopted [46]. At present, this is a\nnew area of research, but it is worth keeping an eye on in the future.\nTransaction Processing or Analytics?\nIn the early days of business data processing, a write to the database typically corre\u2010\nsponded to a commercial transaction taking place: making a sale, placing an order\nwith a supplier, paying an employee\u2019s salary, etc. As databases expanded into areas\nthat didn\u2019t involve money changing hands, the term transaction nevertheless stuck,\nreferring to a group of reads and writes that form a logical unit.\nA transaction needn\u2019t necessarily have ACID (atomicity, consis\u2010\ntency, isolation, and durability) properties. Transaction processing\njust means allowing clients to make low-latency reads and writes\u2014\nas opposed to batch processing jobs, which only run periodically\n(for example, once per day). We discuss the ACID properties in\nChapter 7 and batch processing in Chapter 10.\nEven though databases started being used for many different kinds of data\u2014com\u2010\nments on blog posts, actions in a game, contacts in an address book, etc.\u2014the basic\naccess pattern remained similar to processing business transactions. An application\ntypically looks up a small number of records by some key, using an index. Records\nare inserted or updated based on the user\u2019s input. Because these applications are\ninteractive, the access pattern became known as online transaction processing\n(OLTP).\nHowever, databases also started being increasingly used for data analytics, which has\nvery different access patterns. Usually an analytic query needs to scan over a huge\nnumber of records, only reading a few columns per record, and calculates aggregate\nstatistics (such as count, sum, or average) rather than returning the raw data to the\nuser. For example, if your data is a table of sales transactions, then analytic queries\nmight be:\n\u2022 What was the total revenue of each of our stores in January?\n\u2022 How many more bananas than usual did we sell during our latest promotion?\n\u2022 Which brand of baby food is most often purchased together with brand X"}
{"113": "These queries are often written by business analysts, and feed into reports that help\nthe management of a company make better decisions (business intelligence). In order\nto differentiate this pattern of using databases from transaction processing, it has\nbeen called online analytic processing (OLAP) [47].iv The difference between OLTP\nand OLAP is not always clear-cut, but some typical characteristics are listed in\nTable 3-1.\nTable 3-1. Comparing characteristics of transaction processing versus analytic systems\nProperty Transaction processing systems (OLTP) Analytic systems (OLAP)\nMain read pattern Small number of records per query, fetched by key Aggregate over large number of records\nMain write pattern Random-access, low-latency writes from user input Bulk import (ETL) or event stream\nPrimarily used by End user/customer, via web application Internal analyst, for decision support\nWhat data represents Latest state of data (current point in time) History of events that happened over time\nDataset size Gigabytes to terabytes Terabytes to petabytes\nAt first, the same databases were used for both transaction processing and analytic\nqueries. SQL turned out to be quite flexible in this regard: it works well for OLTP-\ntype queries as well as OLAP-type queries. Nevertheless, in the late 1980s and early\n1990s, there was a trend for companies to stop using their OLTP systems for analytics\npurposes, and to run the analytics on a separate database instead. This separate data\u2010\nbase was called a data warehouse.\nData Warehousing\nAn enterprise may have dozens of different transaction processing systems: systems\npowering the customer-facing website, controlling point of sale (checkout) systems in\nphysical stores, tracking inventory in warehouses, planning routes for vehicles, man\u2010\naging suppliers, administering employees, etc. Each of these systems is complex and\nneeds a team of people to maintain it, so the systems end up operating mostly auton\u2010\nomously from each other.\nThese OLTP systems are usually expected to be highly available and to process trans\u2010\nactions with low latency, since they are often critical to the operation of the business.\nDatabase administrators therefore closely guard their OLTP databases. They are usu\u2010\nally reluctant to let business analysts run ad hoc analytic queries on an OLTP data\u2010\nbase, since those queries are often expensive, scanning large parts of the dataset,\nwhich can harm the performance of concurrently executing transactions."}
{"114": "A data warehouse, by contrast, is a separate database that analysts can query to their\nhearts\u2019 content, without affecting OLTP operations [48]. The data warehouse con\u2010\ntains a read-only copy of the data in all the various OLTP systems in the company.\nData is extracted from OLTP databases (using either a periodic data dump or a con\u2010\ntinuous stream of updates), transformed into an analysis-friendly schema, cleaned\nup, and then loaded into the data warehouse. This process of getting data into the\nwarehouse is known as Extract\u2013Transform\u2013Load (ETL) and is illustrated in\nFigure 3-8.\nFigure 3-8. Simplified outline of ETL into a data warehouse.\nData warehouses now exist in almost all large enterprises, but in small companies\nthey are almost unheard of. This is probably because most small companies don\u2019t\nhave so many different OLTP systems, and most small companies have a small\namount of data\u2014small enough that it can be queried in a conventional SQL database,\nor even analyzed in a spreadsheet. In a large company, a lot of heavy lifting is\nrequired to do something that is simple in a small company.\nA big advantage of using a separate data warehouse, rather than querying OLTP sys\u2010"}
{"115": "In the rest of this chapter we will look at storage engines that are optimized for ana\u2010\nlytics instead.\nThe divergence between OLTP databases and data warehouses\nThe data model of a data warehouse is most commonly relational, because SQL is\ngenerally a good fit for analytic queries. There are many graphical data analysis tools\nthat generate SQL queries, visualize the results, and allow analysts to explore the data\n(through operations such as drill-down and slicing and dicing).\nOn the surface, a data warehouse and a relational OLTP database look similar,\nbecause they both have a SQL query interface. However, the internals of the systems\ncan look quite different, because they are optimized for very different query patterns.\nMany database vendors now focus on supporting either transaction processing or\nanalytics workloads, but not both.\nSome databases, such as Microsoft SQL Server and SAP HANA, have support for\ntransaction processing and data warehousing in the same product. However, they are\nincreasingly becoming two separate storage and query engines, which happen to be\naccessible through a common SQL interface [49, 50, 51].\nData warehouse vendors such as Teradata, Vertica, SAP HANA, and ParAccel typi\u2010\ncally sell their systems under expensive commercial licenses. Amazon RedShift is a\nhosted version of ParAccel. More recently, a plethora of open source SQL-on-\nHadoop projects have emerged; they are young but aiming to compete with commer\u2010\ncial data warehouse systems. These include Apache Hive, Spark SQL, Cloudera\nImpala, Facebook Presto, Apache Tajo, and Apache Drill [52, 53]. Some of them are\nbased on ideas from Google\u2019s Dremel [54].\nStars and Snowflakes: Schemas for Analytics\nAs explored in Chapter 2, a wide range of different data models are used in the realm\nof transaction processing, depending on the needs of the application. On the other\nhand, in analytics, there is much less diversity of data models. Many data warehouses\nare used in a fairly formulaic style, known as a star schema (also known as dimen\u2010\nsional modeling [55]).\nThe example schema in Figure 3-9 shows a data warehouse that might be found at a\ngrocery retailer. At the center of the schema is a so-called fact table (in this example,\nit is called fact_sales). Each row of the fact table represents an event that occurred\nat a particular time (here, each row represents a customer\u2019s purchase of a product). If\nwe were analyzing website traffic rather than retail sales, each row might represent a"}
{"116": "Figure 3-9. Example of a star schema for use in a data warehouse.\nUsually, facts are captured as individual events, because this allows maximum flexi\u2010\nbility of analysis later. However, this means that the fact table can become extremely\nlarge. A big enterprise like Apple, Walmart, or eBay may have tens of petabytes of\ntransaction history in its data warehouse, most of which is in fact tables [56].\nSome of the columns in the fact table are attributes, such as the price at which the\nproduct was sold and the cost of buying it from the supplier (allowing the profit mar\u2010\ngin to be calculated). Other columns in the fact table are foreign key references to\nother tables, called dimension tables. As each row in the fact table represents an event,"}
{"117": "its stock-keeping unit (SKU), description, brand name, category, fat content, package\nsize, etc. Each row in the fact_sales table uses a foreign key to indicate which prod\u2010\nuct was sold in that particular transaction. (For simplicity, if the customer buys sev\u2010\neral different products at once, they are represented as separate rows in the fact\ntable.)\nEven date and time are often represented using dimension tables, because this allows\nadditional information about dates (such as public holidays) to be encoded, allowing\nqueries to differentiate between sales on holidays and non-holidays.\nThe name \u201cstar schema\u201d comes from the fact that when the table relationships are\nvisualized, the fact table is in the middle, surrounded by its dimension tables; the\nconnections to these tables are like the rays of a star.\nA variation of this template is known as the snowflake schema, where dimensions are\nfurther broken down into subdimensions. For example, there could be separate tables\nfor brands and product categories, and each row in the dim_product table could ref\u2010\nerence the brand and category as foreign keys, rather than storing them as strings in\nthe dim_product table. Snowflake schemas are more normalized than star schemas,\nbut star schemas are often preferred because they are simpler for analysts to work\nwith [55].\nIn a typical data warehouse, tables are often very wide: fact tables often have over 100\ncolumns, sometimes several hundred [51]. Dimension tables can also be very wide, as\nthey include all the metadata that may be relevant for analysis\u2014for example, the\ndim_store table may include details of which services are offered at each store,\nwhether it has an in-store bakery, the square footage, the date when the store was first\nopened, when it was last remodeled, how far it is from the nearest highway, etc.\nColumn-Oriented Storage\nIf you have trillions of rows and petabytes of data in your fact tables, storing and\nquerying them efficiently becomes a challenging problem. Dimension tables are usu\u2010\nally much smaller (millions of rows), so in this section we will concentrate primarily\non storage of facts.\nAlthough fact tables are often over 100 columns wide, a typical data warehouse query\nonly accesses 4 or 5 of them at one time (\"SELECT *\" queries are rarely needed for\nanalytics) [51]. Take the query in Example 3-1: it accesses a large number of rows\n(every occurrence of someone buying fruit or candy during the 2013 calendar year),\nbut it only needs to access three columns of the fact_sales table: date_key,"}
{"118": "Example 3-1. Analyzing whether people are more inclined to buy fresh fruit or candy,\ndepending on the day of the week\nSELECT\ndim_date.weekday, dim_product.category,\nSUM(fact_sales.quantity) AS quantity_sold\nFROM fact_sales\nJOIN dim_date ON fact_sales.date_key = dim_date.date_key\nJOIN dim_product ON fact_sales.product_sk = dim_product.product_sk\nWHERE\ndim_date.year = 2013 AND\ndim_product.category IN ('Fresh fruit', 'Candy')\nGROUP BY\ndim_date.weekday, dim_product.category;\nHow can we execute this query efficiently?\nIn most OLTP databases, storage is laid out in a row-oriented fashion: all the values\nfrom one row of a table are stored next to each other. Document databases are simi\u2010\nlar: an entire document is typically stored as one contiguous sequence of bytes. You\ncan see this in the CSV example of Figure 3-1.\nIn order to process a query like Example 3-1, you may have indexes on\nfact_sales.date_key and/or fact_sales.product_sk that tell the storage engine\nwhere to find all the sales for a particular date or for a particular product. But then, a\nrow-oriented storage engine still needs to load all of those rows (each consisting of\nover 100 attributes) from disk into memory, parse them, and filter out those that\ndon\u2019t meet the required conditions. That can take a long time.\nThe idea behind column-oriented storage is simple: don\u2019t store all the values from one\nrow together, but store all the values from each column together instead. If each col\u2010\numn is stored in a separate file, a query only needs to read and parse those columns\nthat are used in that query, which can save a lot of work. This principle is illustrated\nin Figure 3-10.\nColumn storage is easiest to understand in a relational data model,\nbut it applies equally to nonrelational data. For example, Parquet\n[57] is a columnar storage format that supports a document data\nmodel, based on Google\u2019s Dremel [54]."}
{"119": "Figure 3-10. Storing relational data by column, rather than by row.\nThe column-oriented storage layout relies on each column file containing the rows in\nthe same order. Thus, if you need to reassemble an entire row, you can take the 23rd\nentry from each of the individual column files and put them together to form the\n23rd row of the table.\nColumn Compression\nBesides only loading those columns from disk that are required for a query, we can\nfurther reduce the demands on disk throughput by compressing data. Fortunately,\ncolumn-oriented storage often lends itself very well to compression.\nTake a look at the sequences of values for each column in Figure 3-10: they often look\nquite repetitive, which is a good sign for compression. Depending on the data in the\ncolumn, different compression techniques can be used. One technique that is particu\u2010\nlarly effective in data warehouses is bitmap encoding, illustrated in Figure 3-11."}
{"120": "Figure 3-11. Compressed, bitmap-indexed storage of a single column.\nOften, the number of distinct values in a column is small compared to the number of\nrows (for example, a retailer may have billions of sales transactions, but only 100,000\ndistinct products). We can now take a column with n distinct values and turn it into\nn separate bitmaps: one bitmap for each distinct value, with one bit for each row. The\nbit is 1 if the row has that value, and 0 if not.\nIf n is very small (for example, a country column may have approximately 200 dis\u2010\ntinct values), those bitmaps can be stored with one bit per row. But if n is bigger,\nthere will be a lot of zeros in most of the bitmaps (we say that they are sparse). In that\ncase, the bitmaps can additionally be run-length encoded, as shown at the bottom of\nFigure 3-11. This can make the encoding of a column remarkably compact.\nBitmap indexes such as these are very well suited for the kinds of queries that are\ncommon in a data warehouse. For example:\nWHERE product_sk IN (30, 68, 69):\nLoad the three bitmaps for product_sk = 30, product_sk = 68, and product_sk"}
{"121": "WHERE product_sk = 31 AND store_sk = 3:\nLoad the bitmaps for product_sk = 31 and store_sk = 3, and calculate the bit\u2010\nwise AND. This works because the columns contain the rows in the same order,\nso the kth bit in one column\u2019s bitmap corresponds to the same row as the kth bit\nin another column\u2019s bitmap.\nThere are also various other compression schemes for different kinds of data, but we\nwon\u2019t go into them in detail\u2014see [58] for an overview.\nColumn-oriented storage and column families\nCassandra and HBase have a concept of column families, which\nthey inherited from Bigtable [9]. However, it is very misleading to\ncall them column-oriented: within each column family, they store\nall columns from a row together, along with a row key, and they do\nnot use column compression. Thus, the Bigtable model is still\nmostly row-oriented.\nMemory bandwidth and vectorized processing\nFor data warehouse queries that need to scan over millions of rows, a big bottleneck\nis the bandwidth for getting data from disk into memory. However, that is not the\nonly bottleneck. Developers of analytical databases also worry about efficiently using\nthe bandwidth from main memory into the CPU cache, avoiding branch mispredic\u2010\ntions and bubbles in the CPU instruction processing pipeline, and making use of\nsingle-instruction-multi-data (SIMD) instructions in modern CPUs [59, 60].\nBesides reducing the volume of data that needs to be loaded from disk, column-\noriented storage layouts are also good for making efficient use of CPU cycles. For\nexample, the query engine can take a chunk of compressed column data that fits\ncomfortably in the CPU\u2019s L1 cache and iterate through it in a tight loop (that is, with\nno function calls). A CPU can execute such a loop much faster than code that\nrequires a lot of function calls and conditions for each record that is processed. Col\u2010\numn compression allows more rows from a column to fit in the same amount of L1\ncache. Operators, such as the bitwise AND and OR described previously, can be\ndesigned to operate on such chunks of compressed column data directly. This techni\u2010\nque is known as vectorized processing [58, 49].\nSort Order in Column Storage\nIn a column store, it doesn\u2019t necessarily matter in which order the rows are stored.\nIt\u2019s easiest to store them in the order in which they were inserted, since then inserting"}
{"122": "Note that it wouldn\u2019t make sense to sort each column independently, because then\nwe would no longer know which items in the columns belong to the same row. We\ncan only reconstruct a row because we know that the kth item in one column belongs\nto the same row as the kth item in another column.\nRather, the data needs to be sorted an entire row at a time, even though it is stored by\ncolumn. The administrator of the database can choose the columns by which the\ntable should be sorted, using their knowledge of common queries. For example, if\nqueries often target date ranges, such as the last month, it might make sense to make\ndate_key the first sort key. Then the query optimizer can scan only the rows from the\nlast month, which will be much faster than scanning all rows.\nA second column can determine the sort order of any rows that have the same value\nin the first column. For example, if date_key is the first sort key in Figure 3-10, it\nmight make sense for product_sk to be the second sort key so that all sales for the\nsame product on the same day are grouped together in storage. That will help queries\nthat need to group or filter sales by product within a certain date range.\nAnother advantage of sorted order is that it can help with compression of columns. If\nthe primary sort column does not have many distinct values, then after sorting, it will\nhave long sequences where the same value is repeated many times in a row. A simple\nrun-length encoding, like we used for the bitmaps in Figure 3-11, could compress\nthat column down to a few kilobytes\u2014even if the table has billions of rows.\nThat compression effect is strongest on the first sort key. The second and third sort\nkeys will be more jumbled up, and thus not have such long runs of repeated values.\nColumns further down the sorting priority appear in essentially random order, so\nthey probably won\u2019t compress as well. But having the first few columns sorted is still\na win overall.\nSeveral different sort orders\nA clever extension of this idea was introduced in C-Store and adopted in the com\u2010\nmercial data warehouse Vertica [61, 62]. Different queries benefit from different sort\norders, so why not store the same data sorted in several different ways? Data needs to\nbe replicated to multiple machines anyway, so that you don\u2019t lose data if one machine\nfails. You might as well store that redundant data sorted in different ways so that\nwhen you\u2019re processing a query, you can use the version that best fits the query\npattern.\nHaving multiple sort orders in a column-oriented store is a bit similar to having mul\u2010\ntiple secondary indexes in a row-oriented store. But the big difference is that the row-"}
{"123": "Writing to Column-Oriented Storage\nThese optimizations make sense in data warehouses, because most of the load con\u2010\nsists of large read-only queries run by analysts. Column-oriented storage, compres\u2010\nsion, and sorting all help to make those read queries faster. However, they have the\ndownside of making writes more difficult.\nAn update-in-place approach, like B-trees use, is not possible with compressed col\u2010\numns. If you wanted to insert a row in the middle of a sorted table, you would most\nlikely have to rewrite all the column files. As rows are identified by their position\nwithin a column, the insertion has to update all columns consistently.\nFortunately, we have already seen a good solution earlier in this chapter: LSM-trees.\nAll writes first go to an in-memory store, where they are added to a sorted structure\nand prepared for writing to disk. It doesn\u2019t matter whether the in-memory store is\nrow-oriented or column-oriented. When enough writes have accumulated, they are\nmerged with the column files on disk and written to new files in bulk. This is essen\u2010\ntially what Vertica does [62].\nQueries need to examine both the column data on disk and the recent writes in mem\u2010\nory, and combine the two. However, the query optimizer hides this distinction from\nthe user. From an analyst\u2019s point of view, data that has been modified with inserts,\nupdates, or deletes is immediately reflected in subsequent queries.\nAggregation: Data Cubes and Materialized Views\nNot every data warehouse is necessarily a column store: traditional row-oriented\ndatabases and a few other architectures are also used. However, columnar storage can\nbe significantly faster for ad hoc analytical queries, so it is rapidly gaining popularity\n[51, 63].\nAnother aspect of data warehouses that is worth mentioning briefly is materialized\naggregates. As discussed earlier, data warehouse queries often involve an aggregate\nfunction, such as COUNT, SUM, AVG, MIN, or MAX in SQL. If the same aggregates are used\nby many different queries, it can be wasteful to crunch through the raw data every\ntime. Why not cache some of the counts or sums that queries use most often?\nOne way of creating such a cache is a materialized view. In a relational data model, it\nis often defined like a standard (virtual) view: a table-like object whose contents are\nthe results of some query. The difference is that a materialized view is an actual copy\nof the query results, written to disk, whereas a virtual view is just a shortcut for writ\u2010\ning queries. When you read from a virtual view, the SQL engine expands it into the"}
{"124": "such updates make writes more expensive, which is why materialized views are not\noften used in OLTP databases. In read-heavy data warehouses they can make more\nsense (whether or not they actually improve read performance depends on the indi\u2010\nvidual case).\nA common special case of a materialized view is known as a data cube or OLAP cube\n[64]. It is a grid of aggregates grouped by different dimensions. Figure 3-12 shows an\nexample.\nFigure 3-12. Two dimensions of a data cube, aggregating data by summing.\nImagine for now that each fact has foreign keys to only two dimension tables\u2014in\nFigure 3-12, these are date and product. You can now draw a two-dimensional table,\nwith dates along one axis and products along the other. Each cell contains the aggre\u2010\ngate (e.g., SUM) of an attribute (e.g., net_price) of all facts with that date-product\ncombination. Then you can apply the same aggregate along each row or column and\nget a summary that has been reduced by one dimension (the sales by product regard\u2010\nless of date, or the sales by date regardless of product).\nIn general, facts often have more than two dimensions. In Figure 3-9 there are five\ndimensions: date, product, store, promotion, and customer. It\u2019s a lot harder to imag\u2010\nine what a five-dimensional hypercube would look like, but the principle remains the\nsame: each cell contains the sales for a particular date-product-store-promotion-\ncustomer combination. These values can then repeatedly be summarized along each\nof the dimensions."}
{"125": "the total sales per store yesterday, you just need to look at the totals along the appro\u2010\npriate dimension\u2014no need to scan millions of rows.\nThe disadvantage is that a data cube doesn\u2019t have the same flexibility as querying the\nraw data. For example, there is no way of calculating which proportion of sales comes\nfrom items that cost more than $100, because the price isn\u2019t one of the dimensions.\nMost data warehouses therefore try to keep as much raw data as possible, and use\naggregates such as data cubes only as a performance boost for certain queries.\nSummary\nIn this chapter we tried to get to the bottom of how databases handle storage and\nretrieval. What happens when you store data in a database, and what does the data\u2010\nbase do when you query for the data again later?\nOn a high level, we saw that storage engines fall into two broad categories: those opti\u2010\nmized for transaction processing (OLTP), and those optimized for analytics (OLAP).\nThere are big differences between the access patterns in those use cases:\n\u2022 OLTP systems are typically user-facing, which means that they may see a huge\nvolume of requests. In order to handle the load, applications usually only touch a\nsmall number of records in each query. The application requests records using\nsome kind of key, and the storage engine uses an index to find the data for the\nrequested key. Disk seek time is often the bottleneck here.\n\u2022 Data warehouses and similar analytic systems are less well known, because they\nare primarily used by business analysts, not by end users. They handle a much\nlower volume of queries than OLTP systems, but each query is typically very\ndemanding, requiring many millions of records to be scanned in a short time.\nDisk bandwidth (not seek time) is often the bottleneck here, and column-\noriented storage is an increasingly popular solution for this kind of workload.\nOn the OLTP side, we saw storage engines from two main schools of thought:\n\u2022 The log-structured school, which only permits appending to files and deleting\nobsolete files, but never updates a file that has been written. Bitcask, SSTables,\nLSM-trees, LevelDB, Cassandra, HBase, Lucene, and others belong to this group.\n\u2022 The update-in-place school, which treats the disk as a set of fixed-size pages that\ncan be overwritten. B-trees are the biggest example of this philosophy, being used\nin all major relational databases and also many nonrelational ones."}
{"126": "Finishing off the OLTP side, we did a brief tour through some more complicated\nindexing structures, and databases that are optimized for keeping all data in memory.\nWe then took a detour from the internals of storage engines to look at the high-level\narchitecture of a typical data warehouse. This background illustrated why analytic\nworkloads are so different from OLTP: when your queries require sequentially scan\u2010\nning across a large number of rows, indexes are much less relevant. Instead it\nbecomes important to encode data very compactly, to minimize the amount of data\nthat the query needs to read from disk. We discussed how column-oriented storage\nhelps achieve this goal.\nAs an application developer, if you\u2019re armed with this knowledge about the internals\nof storage engines, you are in a much better position to know which tool is best suited\nfor your particular application. If you need to adjust a database\u2019s tuning parameters,\nthis understanding allows you to imagine what effect a higher or a lower value may\nhave.\nAlthough this chapter couldn\u2019t make you an expert in tuning any one particular stor\u2010\nage engine, it has hopefully equipped you with enough vocabulary and ideas that you\ncan make sense of the documentation for the database of your choice.\nReferences\n[1] Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman: Data Structures and\nAlgorithms. Addison-Wesley, 1983. ISBN: 978-0-201-00023-8\n[2] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein:\nIntroduction to Algorithms, 3rd edition. MIT Press, 2009. ISBN: 978-0-262-53305-8\n[3] Justin Sheehy and David Smith: \u201cBitcask: A Log-Structured Hash Table for Fast\nKey/Value Data,\u201d Basho Technologies, April 2010.\n[4] Yinan Li, Bingsheng He, Robin Jun Yang, et al.: \u201cTree Indexing on Solid State\nDrives,\u201d Proceedings of the VLDB Endowment, volume 3, number 1, pages 1195\u20131206,\nSeptember 2010.\n[5] Goetz Graefe: \u201cModern B-Tree Techniques,\u201d Foundations and Trends in Data\u2010\nbases, volume 3, number 4, pages 203\u2013402, August 2011. doi:10.1561/1900000028\n[6] Jeffrey Dean and Sanjay Ghemawat: \u201cLevelDB Implementation Notes,\u201d lev\u2010\neldb.googlecode.com.\n[7] Dhruba Borthakur: \u201cThe History of RocksDB,\u201d rocksdb.blogspot.com, November\n24, 2013."}
{"127": "[9] Fay Chang, Jeffrey Dean, Sanjay Ghemawat, et al.: \u201cBigtable: A Distributed Stor\u2010\nage System for Structured Data,\u201d at 7th USENIX Symposium on Operating System\nDesign and Implementation (OSDI), November 2006.\n[10] Patrick O\u2019Neil, Edward Cheng, Dieter Gawlick, and Elizabeth O\u2019Neil: \u201cThe Log-\nStructured Merge-Tree (LSM-Tree),\u201d Acta Informatica, volume 33, number 4, pages\n351\u2013385, June 1996. doi:10.1007/s002360050048\n[11] Mendel Rosenblum and John K. Ousterhout: \u201cThe Design and Implementation\nof a Log-Structured File System,\u201d ACM Transactions on Computer Systems, volume\n10, number 1, pages 26\u201352, February 1992. doi:10.1145/146941.146943\n[12] Adrien Grand: \u201cWhat Is in a Lucene Index?,\u201d at Lucene/Solr Revolution, Novem\u2010\nber 14, 2013.\n[13] Deepak Kandepet: \u201cHacking Lucene\u2014The Index Format,\u201d hackerlabs.org, Octo\u2010\nber 1, 2011.\n[14] Michael McCandless: \u201cVisualizing Lucene\u2019s Segment Merges,\u201d blog.mikemccand\u2010\nless.com, February 11, 2011.\n[15] Burton H. Bloom: \u201cSpace/Time Trade-offs in Hash Coding with Allowable\nErrors,\u201d Communications of the ACM, volume 13, number 7, pages 422\u2013426, July\n1970. doi:10.1145/362686.362692\n[16] \u201cOperating Cassandra: Compaction,\u201d Apache Cassandra Documentation v4.0,\n2016.\n[17] Rudolf Bayer and Edward M. McCreight: \u201cOrganization and Maintenance of\nLarge Ordered Indices,\u201d Boeing Scientific Research Laboratories, Mathematical and\nInformation Sciences Laboratory, report no. 20, July 1970.\n[18] Douglas Comer: \u201cThe Ubiquitous B-Tree,\u201d ACM Computing Surveys, volume 11,\nnumber 2, pages 121\u2013137, June 1979. doi:10.1145/356770.356776\n[19] Emmanuel Goossaert: \u201cCoding for SSDs,\u201d codecapsule.com, February 12, 2014.\n[20] C. Mohan and Frank Levine: \u201cARIES/IM: An Efficient and High Concurrency\nIndex Management Method Using Write-Ahead Logging,\u201d at ACM International\nConference on Management of Data (SIGMOD), June 1992. doi:\n10.1145/130283.130338\n[21] Howard Chu: \u201cLDAP at Lightning Speed,\u201d at Build Stuff \u201914, November 2014.\n[22] Bradley C. Kuszmaul: \u201cA Comparison of Fractal Trees to Log-Structured Merge\n(LSM) Trees,\u201d tokutek.com, April 22, 2014."}
{"128": "[24] Peter Zaitsev: \u201cInnodb Double Write,\u201d percona.com, August 4, 2006.\n[25] Tomas Vondra: \u201cOn the Impact of Full-Page Writes,\u201d blog.2ndquadrant.com,\nNovember 23, 2016.\n[26] Mark Callaghan: \u201cThe Advantages of an LSM vs a B-Tree,\u201d smalldatum.blog\u2010\nspot.co.uk, January 19, 2016.\n[27] Mark Callaghan: \u201cChoosing Between Efficiency and Performance with\nRocksDB,\u201d at Code Mesh, November 4, 2016.\n[28] Michi Mutsuzaki: \u201cMySQL vs. LevelDB,\u201d github.com, August 2011.\n[29] Benjamin Coverston, Jonathan Ellis, et al.: \u201cCASSANDRA-1608: Redesigned\nCompaction, issues.apache.org, July 2011.\n[30] Igor Canadi, Siying Dong, and Mark Callaghan: \u201cRocksDB Tuning Guide,\u201d git\u2010\nhub.com, 2016.\n[31] MySQL 5.7 Reference Manual. Oracle, 2014.\n[32] Books Online for SQL Server 2012. Microsoft, 2012.\n[33] Joe Webb: \u201cUsing Covering Indexes to Improve Query Performance,\u201d simple-\ntalk.com, 29 September 2008.\n[34] Frank Ramsak, Volker Markl, Robert Fenk, et al.: \u201cIntegrating the UB-Tree into\na Database System Kernel,\u201d at 26th International Conference on Very Large Data\nBases (VLDB), September 2000.\n[35] The PostGIS Development Group: \u201cPostGIS 2.1.2dev Manual,\u201d postgis.net, 2014.\n[36] Robert Escriva, Bernard Wong, and Emin G\u00fcn Sirer: \u201cHyperDex: A Distributed,\nSearchable Key-Value Store,\u201d at ACM SIGCOMM Conference, August 2012. doi:\n10.1145/2377677.2377681\n[37] Michael McCandless: \u201cLucene\u2019s FuzzyQuery Is 100 Times Faster in 4.0,\u201d\nblog.mikemccandless.com, March 24, 2011.\n[38] Steffen Heinz, Justin Zobel, and Hugh E. Williams: \u201cBurst Tries: A Fast, Efficient\nData Structure for String Keys,\u201d ACM Transactions on Information Systems, volume\n20, number 2, pages 192\u2013223, April 2002. doi:10.1145/506309.506312\n[39] Klaus U. Schulz and Stoyan Mihov: \u201cFast String Correction with Levenshtein\nAutomata,\u201d International Journal on Document Analysis and Recognition, volume 5,\nnumber 1, pages 67\u201385, November 2002. doi:10.1007/s10032-002-0082-8"}
{"129": "[41] Michael Stonebraker, Samuel Madden, Daniel J. Abadi, et al.: \u201cThe End of an\nArchitectural Era (It\u2019s Time for a Complete Rewrite),\u201d at 33rd International Confer\u2010\nence on Very Large Data Bases (VLDB), September 2007.\n[42] \u201cVoltDB Technical Overview White Paper,\u201d VoltDB, 2014.\n[43] Stephen M. Rumble, Ankita Kejriwal, and John K. Ousterhout: \u201cLog-Structured\nMemory for DRAM-Based Storage,\u201d at 12th USENIX Conference on File and Storage\nTechnologies (FAST), February 2014.\n[44] Stavros Harizopoulos, Daniel J. Abadi, Samuel Madden, and Michael Stone\u2010\nbraker: \u201cOLTP Through the Looking Glass, and What We Found There,\u201d at ACM\nInternational Conference on Management of Data (SIGMOD), June 2008. doi:\n10.1145/1376616.1376713\n[45] Justin DeBrabant, Andrew Pavlo, Stephen Tu, et al.: \u201cAnti-Caching: A New\nApproach to Database Management System Architecture,\u201d Proceedings of the VLDB\nEndowment, volume 6, number 14, pages 1942\u20131953, September 2013.\n[46] Joy Arulraj, Andrew Pavlo, and Subramanya R. Dulloor: \u201cLet\u2019s Talk About Stor\u2010\nage & Recovery Methods for Non-Volatile Memory Database Systems,\u201d at ACM\nInternational Conference on Management of Data (SIGMOD), June 2015. doi:\n10.1145/2723372.2749441\n[47] Edgar F. Codd, S. B. Codd, and C. T. Salley: \u201cProviding OLAP to User-Analysts:\nAn IT Mandate,\u201d E. F. Codd Associates, 1993.\n[48] Surajit Chaudhuri and Umeshwar Dayal: \u201cAn Overview of Data Warehousing\nand OLAP Technology,\u201d ACM SIGMOD Record, volume 26, number 1, pages 65\u201374,\nMarch 1997. doi:10.1145/248603.248616\n[49] Per-\u00c5ke Larson, Cipri Clinciu, Campbell Fraser, et al.: \u201cEnhancements to SQL\nServer Column Stores,\u201d at ACM International Conference on Management of Data\n(SIGMOD), June 2013.\n[50] Franz F\u00e4rber, Norman May, Wolfgang Lehner, et al.: \u201cThe SAP HANA Database\n\u2013 An Architecture Overview,\u201d IEEE Data Engineering Bulletin, volume 35, number 1,\npages 28\u201333, March 2012.\n[51] Michael Stonebraker: \u201cThe Traditional RDBMS Wisdom Is (Almost Certainly)\nAll Wrong,\u201d presentation at EPFL, May 2013.\n[52] Daniel J. Abadi: \u201cClassifying the SQL-on-Hadoop Solutions,\u201d hadapt.com, Octo\u2010\nber 2, 2013."}
{"130": "[54] Sergey Melnik, Andrey Gubarev, Jing Jing Long, et al.: \u201cDremel: Interactive\nAnalysis of Web-Scale Datasets,\u201d at 36th International Conference on Very Large Data\nBases (VLDB), pages 330\u2013339, September 2010.\n[55] Ralph Kimball and Margy Ross: The Data Warehouse Toolkit: The Definitive\nGuide to Dimensional Modeling, 3rd edition. John Wiley & Sons, July 2013. ISBN:\n978-1-118-53080-1\n[56] Derrick Harris: \u201cWhy Apple, eBay, and Walmart Have Some of the Biggest Data\nWarehouses You\u2019ve Ever Seen,\u201d gigaom.com, March 27, 2013.\n[57] Julien Le Dem: \u201cDremel Made Simple with Parquet,\u201d blog.twitter.com, Septem\u2010\nber 11, 2013.\n[58] Daniel J. Abadi, Peter Boncz, Stavros Harizopoulos, et al.: \u201cThe Design and\nImplementation of Modern Column-Oriented Database Systems,\u201d Foundations and\nTrends in Databases, volume 5, number 3, pages 197\u2013280, December 2013. doi:\n10.1561/1900000024\n[59] Peter Boncz, Marcin Zukowski, and Niels Nes: \u201cMonetDB/X100: Hyper-\nPipelining Query Execution,\u201d at 2nd Biennial Conference on Innovative Data Systems\nResearch (CIDR), January 2005.\n[60] Jingren Zhou and Kenneth A. Ross: \u201cImplementing Database Operations Using\nSIMD Instructions,\u201d at ACM International Conference on Management of Data (SIG\u2010\nMOD), pages 145\u2013156, June 2002. doi:10.1145/564691.564709\n[61] Michael Stonebraker, Daniel J. Abadi, Adam Batkin, et al.: \u201cC-Store: A Column-\noriented DBMS,\u201d at 31st International Conference on Very Large Data Bases (VLDB),\npages 553\u2013564, September 2005.\n[62] Andrew Lamb, Matt Fuller, Ramakrishna Varadarajan, et al.: \u201cThe Vertica Ana\u2010\nlytic Database: C-Store 7 Years Later,\u201d Proceedings of the VLDB Endowment, volume\n5, number 12, pages 1790\u20131801, August 2012.\n[63] Julien Le Dem and Nong Li: \u201cEfficient Data Storage for Analytics with Apache\nParquet 2.0,\u201d at Hadoop Summit, San Jose, June 2014.\n[64] Jim Gray, Surajit Chaudhuri, Adam Bosworth, et al.: \u201cData Cube: A Relational\nAggregation Operator Generalizing Group-By, Cross-Tab, and Sub-Totals,\u201d Data\nMining and Knowledge Discovery, volume 1, number 1, pages 29\u201353, March 2007.\ndoi:10.1023/A:1009726021843"}
{"131": ""}
{"132": ""}
{"133": "CHAPTER 4\nEncoding and Evolution\nEverything changes and nothing stands still.\n\u2014Heraclitus of Ephesus, as quoted by Plato in Cratylus (360 BCE)\nApplications inevitably change over time. Features are added or modified as new\nproducts are launched, user requirements become better understood, or business cir\u2010\ncumstances change. In Chapter 1 we introduced the idea of evolvability: we should\naim to build systems that make it easy to adapt to change (see \u201cEvolvability: Making\nChange Easy\u201d on page 21).\nIn most cases, a change to an application\u2019s features also requires a change to data that\nit stores: perhaps a new field or record type needs to be captured, or perhaps existing\ndata needs to be presented in a new way.\nThe data models we discussed in Chapter 2 have different ways of coping with such\nchange. Relational databases generally assume that all data in the database conforms\nto one schema: although that schema can be changed (through schema migrations;\ni.e., ALTER statements), there is exactly one schema in force at any one point in time.\nBy contrast, schema-on-read (\u201cschemaless\u201d) databases don\u2019t enforce a schema, so the\ndatabase can contain a mixture of older and newer data formats written at different\ntimes (see \u201cSchema flexibility in the document model\u201d on page 39).\nWhen a data format or schema changes, a corresponding change to application code\noften needs to happen (for example, you add a new field to a record, and the applica\u2010\ntion code starts reading and writing that field). However, in a large application, code\nchanges often cannot happen instantaneously:"}
{"134": "\u2022 With server-side applications you may want to perform a rolling upgrade (also\nknown as a staged rollout), deploying the new version to a few nodes at a time,\nchecking whether the new version is running smoothly, and gradually working\nyour way through all the nodes. This allows new versions to be deployed without\nservice downtime, and thus encourages more frequent releases and better evolva\u2010\nbility.\n\u2022 With client-side applications you\u2019re at the mercy of the user, who may not install\nthe update for some time.\nThis means that old and new versions of the code, and old and new data formats,\nmay potentially all coexist in the system at the same time. In order for the system to\ncontinue running smoothly, we need to maintain compatibility in both directions:\nBackward compatibility\nNewer code can read data that was written by older code.\nForward compatibility\nOlder code can read data that was written by newer code.\nBackward compatibility is normally not hard to achieve: as author of the newer code,\nyou know the format of data written by older code, and so you can explicitly handle it\n(if necessary by simply keeping the old code to read the old data). Forward compati\u2010\nbility can be trickier, because it requires older code to ignore additions made by a\nnewer version of the code.\nIn this chapter we will look at several formats for encoding data, including JSON,\nXML, Protocol Buffers, Thrift, and Avro. In particular, we will look at how they han\u2010\ndle schema changes and how they support systems where old and new data and code\nneed to coexist. We will then discuss how those formats are used for data storage and\nfor communication: in web services, Representational State Transfer (REST), and\nremote procedure calls (RPC), as well as message-passing systems such as actors and\nmessage queues.\nFormats for Encoding Data\nPrograms usually work with data in (at least) two different representations:\n1. In memory, data is kept in objects, structs, lists, arrays, hash tables, trees, and so\non. These data structures are optimized for efficient access and manipulation by\nthe CPU (typically using pointers)."}
{"135": "sequence-of-bytes representation looks quite different from the data structures\nthat are normally used in memory.i\nThus, we need some kind of translation between the two representations. The trans\u2010\nlation from the in-memory representation to a byte sequence is called encoding (also\nknown as serialization or marshalling), and the reverse is called decoding (parsing,\ndeserialization, unmarshalling).ii\nTerminology clash\nSerialization is unfortunately also used in the context of transac\u2010\ntions (see Chapter 7), with a completely different meaning. To\navoid overloading the word we\u2019ll stick with encoding in this book,\neven though serialization is perhaps a more common term.\nAs this is such a common problem, there are a myriad different libraries and encod\u2010\ning formats to choose from. Let\u2019s do a brief overview.\nLanguage-Specific Formats\nMany programming languages come with built-in support for encoding in-memory\nobjects into byte sequences. For example, Java has java.io.Serializable [1], Ruby\nhas Marshal [2], Python has pickle [3], and so on. Many third-party libraries also\nexist, such as Kryo for Java [4].\nThese encoding libraries are very convenient, because they allow in-memory objects\nto be saved and restored with minimal additional code. However, they also have a\nnumber of deep problems:\n\u2022 The encoding is often tied to a particular programming language, and reading\nthe data in another language is very difficult. If you store or transmit data in such\nan encoding, you are committing yourself to your current programming lan\u2010\nguage for potentially a very long time, and precluding integrating your systems\nwith those of other organizations (which may use different languages).\n\u2022 In order to restore data in the same object types, the decoding process needs to\nbe able to instantiate arbitrary classes. This is frequently a source of security\nproblems [5]: if an attacker can get your application to decode an arbitrary byte\nsequence, they can instantiate arbitrary classes, which in turn often allows them\nto do terrible things such as remotely executing arbitrary code [6, 7]."}
{"136": "\u2022 Versioning data is often an afterthought in these libraries: as they are intended\nfor quick and easy encoding of data, they often neglect the inconvenient prob\u2010\nlems of forward and backward compatibility.\n\u2022 Efficiency (CPU time taken to encode or decode, and the size of the encoded\nstructure) is also often an afterthought. For example, Java\u2019s built-in serialization\nis notorious for its bad performance and bloated encoding [8].\nFor these reasons it\u2019s generally a bad idea to use your language\u2019s built-in encoding for\nanything other than very transient purposes.\nJSON, XML, and Binary Variants\nMoving to standardized encodings that can be written and read by many program\u2010\nming languages, JSON and XML are the obvious contenders. They are widely known,\nwidely supported, and almost as widely disliked. XML is often criticized for being too\nverbose and unnecessarily complicated [9]. JSON\u2019s popularity is mainly due to its\nbuilt-in support in web browsers (by virtue of being a subset of JavaScript) and sim\u2010\nplicity relative to XML. CSV is another popular language-independent format, albeit\nless powerful.\nJSON, XML, and CSV are textual formats, and thus somewhat human-readable\n(although the syntax is a popular topic of debate). Besides the superficial syntactic\nissues, they also have some subtle problems:\n\u2022 There is a lot of ambiguity around the encoding of numbers. In XML and CSV,\nyou cannot distinguish between a number and a string that happens to consist of\ndigits (except by referring to an external schema). JSON distinguishes strings and\nnumbers, but it doesn\u2019t distinguish integers and floating-point numbers, and it\ndoesn\u2019t specify a precision.\nThis is a problem when dealing with large numbers; for example, integers greater\nthan 253 cannot be exactly represented in an IEEE 754 double-precision floating-\npoint number, so such numbers become inaccurate when parsed in a language\nthat uses floating-point numbers (such as JavaScript). An example of numbers\nlarger than 253 occurs on Twitter, which uses a 64-bit number to identify each\ntweet. The JSON returned by Twitter\u2019s API includes tweet IDs twice, once as a\nJSON number and once as a decimal string, to work around the fact that the\nnumbers are not correctly parsed by JavaScript applications [10].\n\u2022 JSON and XML have good support for Unicode character strings (i.e., human-\nreadable text), but they don\u2019t support binary strings (sequences of bytes without"}
{"137": "\u2022 There is optional schema support for both XML [11] and JSON [12]. These\nschema languages are quite powerful, and thus quite complicated to learn and\nimplement. Use of XML schemas is fairly widespread, but many JSON-based\ntools don\u2019t bother using schemas. Since the correct interpretation of data (such\nas numbers and binary strings) depends on information in the schema, applica\u2010\ntions that don\u2019t use XML/JSON schemas need to potentially hardcode the appro\u2010\npriate encoding/decoding logic instead.\n\u2022 CSV does not have any schema, so it is up to the application to define the mean\u2010\ning of each row and column. If an application change adds a new row or column,\nyou have to handle that change manually. CSV is also a quite vague format (what\nhappens if a value contains a comma or a newline character?). Although its\nescaping rules have been formally specified [13], not all parsers implement them\ncorrectly.\nDespite these flaws, JSON, XML, and CSV are good enough for many purposes. It\u2019s\nlikely that they will remain popular, especially as data interchange formats (i.e., for\nsending data from one organization to another). In these situations, as long as people\nagree on what the format is, it often doesn\u2019t matter how pretty or efficient the format\nis. The difficulty of getting different organizations to agree on anything outweighs\nmost other concerns.\nBinary encoding\nFor data that is used only internally within your organization, there is less pressure to\nuse a lowest-common-denominator encoding format. For example, you could choose\na format that is more compact or faster to parse. For a small dataset, the gains are\nnegligible, but once you get into the terabytes, the choice of data format can have a\nbig impact.\nJSON is less verbose than XML, but both still use a lot of space compared to binary\nformats. This observation led to the development of a profusion of binary encodings\nfor JSON (MessagePack, BSON, BJSON, UBJSON, BISON, and Smile, to name a few)\nand for XML (WBXML and Fast Infoset, for example). These formats have been\nadopted in various niches, but none of them are as widely adopted as the textual ver\u2010\nsions of JSON and XML.\nSome of these formats extend the set of datatypes (e.g., distinguishing integers and\nfloating-point numbers, or adding support for binary strings), but otherwise they\nkeep the JSON/XML data model unchanged. In particular, since they don\u2019t prescribe\na schema, they need to include all the object field names within the encoded data."}
{"138": "Example 4-1. Example record which we will encode in several binary formats in this\nchapter\n{\n\"userName\": \"Martin\",\n\"favoriteNumber\": 1337,\n\"interests\": [\"daydreaming\", \"hacking\"]\n}\nLet\u2019s look at an example of MessagePack, a binary encoding for JSON. Figure 4-1\nshows the byte sequence that you get if you encode the JSON document in\nExample 4-1 with MessagePack [14]. The first few bytes are as follows:\n1. The first byte, 0x83, indicates that what follows is an object (top four bits = 0x80)\nwith three fields (bottom four bits = 0x03). (In case you\u2019re wondering what hap\u2010\npens if an object has more than 15 fields, so that the number of fields doesn\u2019t fit\nin four bits, it then gets a different type indicator, and the number of fields is\nencoded in two or four bytes.)\n2. The second byte, 0xa8, indicates that what follows is a string (top four bits =\n0xa0) that is eight bytes long (bottom four bits = 0x08).\n3. The next eight bytes are the field name userName in ASCII. Since the length was\nindicated previously, there\u2019s no need for any marker to tell us where the string\nends (or any escaping).\n4. The next seven bytes encode the six-letter string value Martin with a prefix 0xa6,\nand so on.\nThe binary encoding is 66 bytes long, which is only a little less than the 81 bytes taken\nby the textual JSON encoding (with whitespace removed). All the binary encodings of\nJSON are similar in this regard. It\u2019s not clear whether such a small space reduction\n(and perhaps a speedup in parsing) is worth the loss of human-readability.\nIn the following sections we will see how we can do much better, and encode the\nsame record in just 32 bytes."}
{"139": "Figure 4-1. Example record (Example 4-1) encoded using MessagePack.\nThrift and Protocol Buffers\nApache Thrift [15] and Protocol Buffers (protobuf) [16] are binary encoding libraries\nthat are based on the same principle. Protocol Buffers was originally developed at\nGoogle, Thrift was originally developed at Facebook, and both were made open\nsource in 2007\u201308 [17].\nBoth Thrift and Protocol Buffers require a schema for any data that is encoded. To\nencode the data in Example 4-1 in Thrift, you would describe the schema in the\nThrift interface definition language (IDL) like this:\nstruct Person {\n1: required string userName,"}
{"140": "The equivalent schema definition for Protocol Buffers looks very similar:\nmessage Person {\nrequired string user_name = 1;\noptional int64 favorite_number = 2;\nrepeated string interests = 3;\n}\nThrift and Protocol Buffers each come with a code generation tool that takes a\nschema definition like the ones shown here, and produces classes that implement the\nschema in various programming languages [18]. Your application code can call this\ngenerated code to encode or decode records of the schema.\nWhat does data encoded with this schema look like? Confusingly, Thrift has two dif\u2010\nferent binary encoding formats,iii called BinaryProtocol and CompactProtocol, respec\u2010\ntively. Let\u2019s look at BinaryProtocol first. Encoding Example 4-1 in that format takes\n59 bytes, as shown in Figure 4-2 [19].\nFigure 4-2. Example record encoded using Thrift\u2019s BinaryProtocol."}
{"141": "Similarly to Figure 4-1, each field has a type annotation (to indicate whether it is a\nstring, integer, list, etc.) and, where required, a length indication (length of a string,\nnumber of items in a list). The strings that appear in the data (\u201cMartin\u201d, \u201cdaydream\u2010\ning\u201d, \u201chacking\u201d) are also encoded as ASCII (or rather, UTF-8), similar to before.\nThe big difference compared to Figure 4-1 is that there are no field names (userName,\nfavoriteNumber, interests). Instead, the encoded data contains field tags, which are\nnumbers (1, 2, and 3). Those are the numbers that appear in the schema definition.\nField tags are like aliases for fields\u2014they are a compact way of saying what field we\u2019re\ntalking about, without having to spell out the field name.\nThe Thrift CompactProtocol encoding is semantically equivalent to BinaryProtocol,\nbut as you can see in Figure 4-3, it packs the same information into only 34 bytes. It\ndoes this by packing the field type and tag number into a single byte, and by using\nvariable-length integers. Rather than using a full eight bytes for the number 1337, it is\nencoded in two bytes, with the top bit of each byte used to indicate whether there are\nstill more bytes to come. This means numbers between \u201364 and 63 are encoded in\none byte, numbers between \u20138192 and 8191 are encoded in two bytes, etc. Bigger\nnumbers use more bytes."}
{"142": "otherwise very similar to Thrift\u2019s CompactProtocol. Protocol Buffers fits the same\nrecord in 33 bytes.\nFigure 4-4. Example record encoded using Protocol Buffers.\nOne detail to note: in the schemas shown earlier, each field was marked either\nrequired or optional, but this makes no difference to how the field is encoded\n(nothing in the binary data indicates whether a field was required). The difference is\nsimply that required enables a runtime check that fails if the field is not set, which\ncan be useful for catching bugs.\nField tags and schema evolution\nWe said previously that schemas inevitably need to change over time. We call this\nschema evolution. How do Thrift and Protocol Buffers handle schema changes while\nkeeping backward and forward compatibility?\nAs you can see from the examples, an encoded record is just the concatenation of its\nencoded fields. Each field is identified by its tag number (the numbers 1, 2, 3 in the\nsample schemas) and annotated with a datatype (e.g., string or integer). If a field\nvalue is not set, it is simply omitted from the encoded record. From this you can see\nthat field tags are critical to the meaning of the encoded data. You can change the"}
{"143": "You can add new fields to the schema, provided that you give each field a new tag\nnumber. If old code (which doesn\u2019t know about the new tag numbers you added)\ntries to read data written by new code, including a new field with a tag number it\ndoesn\u2019t recognize, it can simply ignore that field. The datatype annotation allows the\nparser to determine how many bytes it needs to skip. This maintains forward com\u2010\npatibility: old code can read records that were written by new code.\nWhat about backward compatibility? As long as each field has a unique tag number,\nnew code can always read old data, because the tag numbers still have the same\nmeaning. The only detail is that if you add a new field, you cannot make it required.\nIf you were to add a field and make it required, that check would fail if new code read\ndata written by old code, because the old code will not have written the new field that\nyou added. Therefore, to maintain backward compatibility, every field you add after\nthe initial deployment of the schema must be optional or have a default value.\nRemoving a field is just like adding a field, with backward and forward compatibility\nconcerns reversed. That means you can only remove a field that is optional (a\nrequired field can never be removed), and you can never use the same tag number\nagain (because you may still have data written somewhere that includes the old tag\nnumber, and that field must be ignored by new code).\nDatatypes and schema evolution\nWhat about changing the datatype of a field? That may be possible\u2014check the docu\u2010\nmentation for details\u2014but there is a risk that values will lose precision or get trunca\u2010\nted. For example, say you change a 32-bit integer into a 64-bit integer. New code can\neasily read data written by old code, because the parser can fill in any missing bits\nwith zeros. However, if old code reads data written by new code, the old code is still\nusing a 32-bit variable to hold the value. If the decoded 64-bit value won\u2019t fit in 32\nbits, it will be truncated.\nA curious detail of Protocol Buffers is that it does not have a list or array datatype,\nbut instead has a repeated marker for fields (which is a third option alongside\nrequired and optional). As you can see in Figure 4-4, the encoding of a repeated\nfield is just what it says on the tin: the same field tag simply appears multiple times in\nthe record. This has the nice effect that it\u2019s okay to change an optional (single-\nvalued) field into a repeated (multi-valued) field. New code reading old data sees a\nlist with zero or one elements (depending on whether the field was present); old code\nreading new data sees only the last element of the list.\nThrift has a dedicated list datatype, which is parameterized with the datatype of the"}
{"144": "Avro\nApache Avro [20] is another binary encoding format that is interestingly different\nfrom Protocol Buffers and Thrift. It was started in 2009 as a subproject of Hadoop, as\na result of Thrift not being a good fit for Hadoop\u2019s use cases [21].\nAvro also uses a schema to specify the structure of the data being encoded. It has two\nschema languages: one (Avro IDL) intended for human editing, and one (based on\nJSON) that is more easily machine-readable.\nOur example schema, written in Avro IDL, might look like this:\nrecord Person {\nstring userName;\nunion { null, long } favoriteNumber = null;\narray<string> interests;\n}\nThe equivalent JSON representation of that schema is as follows:\n{\n\"type\": \"record\",\n\"name\": \"Person\",\n\"fields\": [\n{\"name\": \"userName\", \"type\": \"string\"},\n{\"name\": \"favoriteNumber\", \"type\": [\"null\", \"long\"], \"default\": null},\n{\"name\": \"interests\", \"type\": {\"type\": \"array\", \"items\": \"string\"}}\n]\n}\nFirst of all, notice that there are no tag numbers in the schema. If we encode our\nexample record (Example 4-1) using this schema, the Avro binary encoding is just 32\nbytes long\u2014the most compact of all the encodings we have seen. The breakdown of\nthe encoded byte sequence is shown in Figure 4-5.\nIf you examine the byte sequence, you can see that there is nothing to identify fields\nor their datatypes. The encoding simply consists of values concatenated together. A\nstring is just a length prefix followed by UTF-8 bytes, but there\u2019s nothing in the enco\u2010\nded data that tells you that it is a string. It could just as well be an integer, or some\u2010\nthing else entirely. An integer is encoded using a variable-length encoding (the same\nas Thrift\u2019s CompactProtocol)."}
{"145": "Figure 4-5. Example record encoded using Avro.\nTo parse the binary data, you go through the fields in the order that they appear in\nthe schema and use the schema to tell you the datatype of each field. This means that\nthe binary data can only be decoded correctly if the code reading the data is using the\nexact same schema as the code that wrote the data. Any mismatch in the schema\nbetween the reader and the writer would mean incorrectly decoded data.\nSo, how does Avro support schema evolution?\nThe writer\u2019s schema and the reader\u2019s schema\nWith Avro, when an application wants to encode some data (to write it to a file or\ndatabase, to send it over the network, etc.), it encodes the data using whatever version\nof the schema it knows about\u2014for example, that schema may be compiled into the\napplication. This is known as the writer\u2019s schema.\nWhen an application wants to decode some data (read it from a file or database,\nreceive it from the network, etc.), it is expecting the data to be in some schema, which\nis known as the reader\u2019s schema. That is the schema the application code is relying on\n\u2014code may have been generated from that schema during the application\u2019s build"}
{"146": "Avro library resolves the differences by looking at the writer\u2019s schema and the\nreader\u2019s schema side by side and translating the data from the writer\u2019s schema into\nthe reader\u2019s schema. The Avro specification [20] defines exactly how this resolution\nworks, and it is illustrated in Figure 4-6.\nFor example, it\u2019s no problem if the writer\u2019s schema and the reader\u2019s schema have\ntheir fields in a different order, because the schema resolution matches up the fields\nby field name. If the code reading the data encounters a field that appears in the\nwriter\u2019s schema but not in the reader\u2019s schema, it is ignored. If the code reading the\ndata expects some field, but the writer\u2019s schema does not contain a field of that name,\nit is filled in with a default value declared in the reader\u2019s schema.\nFigure 4-6. An Avro reader resolves differences between the writer\u2019s schema and the\nreader\u2019s schema.\nSchema evolution rules\nWith Avro, forward compatibility means that you can have a new version of the\nschema as writer and an old version of the schema as reader. Conversely, backward\ncompatibility means that you can have a new version of the schema as reader and an\nold version as writer.\nTo maintain compatibility, you may only add or remove a field that has a default\nvalue. (The field favoriteNumber in our Avro schema has a default value of null.)\nFor example, say you add a field with a default value, so this new field exists in the\nnew schema but not the old one. When a reader using the new schema reads a record\nwritten with the old schema, the default value is filled in for the missing field.\nIf you were to add a field that has no default value, new readers wouldn\u2019t be able to\nread data written by old writers, so you would break backward compatibility. If you\nwere to remove a field that has no default value, old readers wouldn\u2019t be able to read"}
{"147": "In some programming languages, null is an acceptable default for any variable, but\nthis is not the case in Avro: if you want to allow a field to be null, you have to use a\nunion type. For example, union { null, long, string } field; indicates that\nfield can be a number, or a string, or null. You can only use null as a default value if\nit is one of the branches of the union.iv This is a little more verbose than having every\u2010\nthing nullable by default, but it helps prevent bugs by being explicit about what can\nand cannot be null [22].\nConsequently, Avro doesn\u2019t have optional and required markers in the same way as\nProtocol Buffers and Thrift do (it has union types and default values instead).\nChanging the datatype of a field is possible, provided that Avro can convert the type.\nChanging the name of a field is possible but a little tricky: the reader\u2019s schema can\ncontain aliases for field names, so it can match an old writer\u2019s schema field names\nagainst the aliases. This means that changing a field name is backward compatible but\nnot forward compatible. Similarly, adding a branch to a union type is backward com\u2010\npatible but not forward compatible.\nBut what is the writer\u2019s schema?\nThere is an important question that we\u2019ve glossed over so far: how does the reader\nknow the writer\u2019s schema with which a particular piece of data was encoded? We\ncan\u2019t just include the entire schema with every record, because the schema would\nlikely be much bigger than the encoded data, making all the space savings from the\nbinary encoding futile.\nThe answer depends on the context in which Avro is being used. To give a few exam\u2010\nples:\nLarge file with lots of records\nA common use for Avro\u2014especially in the context of Hadoop\u2014is for storing a\nlarge file containing millions of records, all encoded with the same schema. (We\nwill discuss this kind of situation in Chapter 10.) In this case, the writer of that\nfile can just include the writer\u2019s schema once at the beginning of the file. Avro\nspecifies a file format (object container files) to do this.\nDatabase with individually written records\nIn a database, different records may be written at different points in time using\ndifferent writer\u2019s schemas\u2014you cannot assume that all the records will have the\nsame schema. The simplest solution is to include a version number at the begin\u2010\nning of every encoded record, and to keep a list of schema versions in your data\u2010"}
{"148": "base. A reader can fetch a record, extract the version number, and then fetch the\nwriter\u2019s schema for that version number from the database. Using that writer\u2019s\nschema, it can decode the rest of the record. (Espresso [23] works this way, for\nexample.)\nSending records over a network connection\nWhen two processes are communicating over a bidirectional network connec\u2010\ntion, they can negotiate the schema version on connection setup and then use\nthat schema for the lifetime of the connection. The Avro RPC protocol (see\n\u201cDataflow Through Services: REST and RPC\u201d on page 131) works like this.\nA database of schema versions is a useful thing to have in any case, since it acts as\ndocumentation and gives you a chance to check schema compatibility [24]. As the\nversion number, you could use a simple incrementing integer, or you could use a\nhash of the schema.\nDynamically generated schemas\nOne advantage of Avro\u2019s approach, compared to Protocol Buffers and Thrift, is that\nthe schema doesn\u2019t contain any tag numbers. But why is this important? What\u2019s the\nproblem with keeping a couple of numbers in the schema?\nThe difference is that Avro is friendlier to dynamically generated schemas. For exam\u2010\nple, say you have a relational database whose contents you want to dump to a file, and\nyou want to use a binary format to avoid the aforementioned problems with textual\nformats (JSON, CSV, SQL). If you use Avro, you can fairly easily generate an Avro\nschema (in the JSON representation we saw earlier) from the relational schema and\nencode the database contents using that schema, dumping it all to an Avro object\ncontainer file [25]. You generate a record schema for each database table, and each\ncolumn becomes a field in that record. The column name in the database maps to the\nfield name in Avro.\nNow, if the database schema changes (for example, a table has one column added and\none column removed), you can just generate a new Avro schema from the updated\ndatabase schema and export data in the new Avro schema. The data export process\ndoes not need to pay any attention to the schema change\u2014it can simply do the\nschema conversion every time it runs. Anyone who reads the new data files will see\nthat the fields of the record have changed, but since the fields are identified by name,\nthe updated writer\u2019s schema can still be matched up with the old reader\u2019s schema.\nBy contrast, if you were using Thrift or Protocol Buffers for this purpose, the field\ntags would likely have to be assigned by hand: every time the database schema"}
{"149": "tags.) This kind of dynamically generated schema simply wasn\u2019t a design goal of\nThrift or Protocol Buffers, whereas it was for Avro.\nCode generation and dynamically typed languages\nThrift and Protocol Buffers rely on code generation: after a schema has been defined,\nyou can generate code that implements this schema in a programming language of\nyour choice. This is useful in statically typed languages such as Java, C++, or C#,\nbecause it allows efficient in-memory structures to be used for decoded data, and it\nallows type checking and autocompletion in IDEs when writing programs that access\nthe data structures.\nIn dynamically typed programming languages such as JavaScript, Ruby, or Python,\nthere is not much point in generating code, since there is no compile-time type\nchecker to satisfy. Code generation is often frowned upon in these languages, since\nthey otherwise avoid an explicit compilation step. Moreover, in the case of a dynami\u2010\ncally generated schema (such as an Avro schema generated from a database table),\ncode generation is an unnecessarily obstacle to getting to the data.\nAvro provides optional code generation for statically typed programming languages,\nbut it can be used just as well without any code generation. If you have an object con\u2010\ntainer file (which embeds the writer\u2019s schema), you can simply open it using the Avro\nlibrary and look at the data in the same way as you could look at a JSON file. The file\nis self-describing since it includes all the necessary metadata.\nThis property is especially useful in conjunction with dynamically typed data pro\u2010\ncessing languages like Apache Pig [26]. In Pig, you can just open some Avro files,\nstart analyzing them, and write derived datasets to output files in Avro format\nwithout even thinking about schemas.\nThe Merits of Schemas\nAs we saw, Protocol Buffers, Thrift, and Avro all use a schema to describe a binary\nencoding format. Their schema languages are much simpler than XML Schema or\nJSON Schema, which support much more detailed validation rules (e.g., \u201cthe string\nvalue of this field must match this regular expression\u201d or \u201cthe integer value of this\nfield must be between 0 and 100\u201d). As Protocol Buffers, Thrift, and Avro are simpler\nto implement and simpler to use, they have grown to support a fairly wide range of\nprogramming languages.\nThe ideas on which these encodings are based are by no means new. For example,\nthey have a lot in common with ASN.1, a schema definition language that was first"}
{"150": "fers and Thrift [29]. However, it\u2019s also very complex and badly documented, so\nASN.1 is probably not a good choice for new applications.\nMany data systems also implement some kind of proprietary binary encoding for\ntheir data. For example, most relational databases have a network protocol over\nwhich you can send queries to the database and get back responses. Those protocols\nare generally specific to a particular database, and the database vendor provides a\ndriver (e.g., using the ODBC or JDBC APIs) that decodes responses from the data\u2010\nbase\u2019s network protocol into in-memory data structures.\nSo, we can see that although textual data formats such as JSON, XML, and CSV are\nwidespread, binary encodings based on schemas are also a viable option. They have a\nnumber of nice properties:\n\u2022 They can be much more compact than the various \u201cbinary JSON\u201d variants, since\nthey can omit field names from the encoded data.\n\u2022 The schema is a valuable form of documentation, and because the schema is\nrequired for decoding, you can be sure that it is up to date (whereas manually\nmaintained documentation may easily diverge from reality).\n\u2022 Keeping a database of schemas allows you to check forward and backward com\u2010\npatibility of schema changes, before anything is deployed.\n\u2022 For users of statically typed programming languages, the ability to generate code\nfrom the schema is useful, since it enables type checking at compile time.\nIn summary, schema evolution allows the same kind of flexibility as schemaless/\nschema-on-read JSON databases provide (see \u201cSchema flexibility in the document\nmodel\u201d on page 39), while also providing better guarantees about your data and bet\u2010\nter tooling.\nModes of Dataflow\nAt the beginning of this chapter we said that whenever you want to send some data to\nanother process with which you don\u2019t share memory\u2014for example, whenever you\nwant to send data over the network or write it to a file\u2014you need to encode it as a\nsequence of bytes. We then discussed a variety of different encodings for doing this.\nWe talked about forward and backward compatibility, which are important for evolv\u2010\nability (making change easy by allowing you to upgrade different parts of your system\nindependently, and not having to change everything at once). Compatibility is a rela\u2010\ntionship between one process that encodes the data, and another process that decodes"}
{"151": "That\u2019s a fairly abstract idea\u2014there are many ways data can flow from one process to\nanother. Who encodes the data, and who decodes it? In the rest of this chapter we\nwill explore some of the most common ways how data flows between processes:\n\u2022 Via databases (see \u201cDataflow Through Databases\u201d on page 129)\n\u2022 Via service calls (see \u201cDataflow Through Services: REST and RPC\u201d on page 131)\n\u2022 Via asynchronous message passing (see \u201cMessage-Passing Dataflow\u201d on page 136)\nDataflow Through Databases\nIn a database, the process that writes to the database encodes the data, and the pro\u2010\ncess that reads from the database decodes it. There may just be a single process\naccessing the database, in which case the reader is simply a later version of the same\nprocess\u2014in that case you can think of storing something in the database as sending a\nmessage to your future self.\nBackward compatibility is clearly necessary here; otherwise your future self won\u2019t be\nable to decode what you previously wrote.\nIn general, it\u2019s common for several different processes to be accessing a database at\nthe same time. Those processes might be several different applications or services, or\nthey may simply be several instances of the same service (running in parallel for scal\u2010\nability or fault tolerance). Either way, in an environment where the application is\nchanging, it is likely that some processes accessing the database will be running newer\ncode and some will be running older code\u2014for example because a new version is cur\u2010\nrently being deployed in a rolling upgrade, so some instances have been updated\nwhile others haven\u2019t yet.\nThis means that a value in the database may be written by a newer version of the\ncode, and subsequently read by an older version of the code that is still running.\nThus, forward compatibility is also often required for databases.\nHowever, there is an additional snag. Say you add a field to a record schema, and the\nnewer code writes a value for that new field to the database. Subsequently, an older\nversion of the code (which doesn\u2019t yet know about the new field) reads the record,\nupdates it, and writes it back. In this situation, the desirable behavior is usually for\nthe old code to keep the new field intact, even though it couldn\u2019t be interpreted.\nThe encoding formats discussed previously support such preservation of unknown\nfields, but sometimes you need to take care at an application level, as illustrated in\nFigure 4-7. For example, if you decode a database value into model objects in the"}
{"152": "Figure 4-7. When an older version of the application updates data previously written\nby a newer version of the application, data may be lost if you\u2019re not careful.\nDifferent values written at different times\nA database generally allows any value to be updated at any time. This means that\nwithin a single database you may have some values that were written five milli\u2010\nseconds ago, and some values that were written five years ago.\nWhen you deploy a new version of your application (of a server-side application, at\nleast), you may entirely replace the old version with the new version within a few\nminutes. The same is not true of database contents: the five-year-old data will still be\nthere, in the original encoding, unless you have explicitly rewritten it since then. This\nobservation is sometimes summed up as data outlives code.\nRewriting (migrating) data into a new schema is certainly possible, but it\u2019s an expen\u2010\nsive thing to do on a large dataset, so most databases avoid it if possible. Most rela\u2010\ntional databases allow simple schema changes, such as adding a new column with a\nnull default value, without rewriting existing data.v When an old row is read, the\ndatabase fills in nulls for any columns that are missing from the encoded data on\ndisk. LinkedIn\u2019s document database Espresso uses Avro for storage, allowing it to use\nAvro\u2019s schema evolution rules [23]."}
{"153": "Schema evolution thus allows the entire database to appear as if it was encoded with a\nsingle schema, even though the underlying storage may contain records encoded with\nvarious historical versions of the schema.\nArchival storage\nPerhaps you take a snapshot of your database from time to time, say for backup pur\u2010\nposes or for loading into a data warehouse (see \u201cData Warehousing\u201d on page 91). In\nthis case, the data dump will typically be encoded using the latest schema, even if the\noriginal encoding in the source database contained a mixture of schema versions\nfrom different eras. Since you\u2019re copying the data anyway, you might as well encode\nthe copy of the data consistently.\nAs the data dump is written in one go and is thereafter immutable, formats like Avro\nobject container files are a good fit. This is also a good opportunity to encode the data\nin an analytics-friendly column-oriented format such as Parquet (see \u201cColumn Com\u2010\npression\u201d on page 97).\nIn Chapter 10 we will talk more about using data in archival storage.\nDataflow Through Services: REST and RPC\nWhen you have processes that need to communicate over a network, there are a few\ndifferent ways of arranging that communication. The most common arrangement is\nto have two roles: clients and servers. The servers expose an API over the network,\nand the clients can connect to the servers to make requests to that API. The API\nexposed by the server is known as a service.\nThe web works this way: clients (web browsers) make requests to web servers, mak\u2010\ning GET requests to download HTML, CSS, JavaScript, images, etc., and making POST\nrequests to submit data to the server. The API consists of a standardized set of proto\u2010\ncols and data formats (HTTP, URLs, SSL/TLS, HTML, etc.). Because web browsers,\nweb servers, and website authors mostly agree on these standards, you can use any\nweb browser to access any website (at least in theory!).\nWeb browsers are not the only type of client. For example, a native app running on a\nmobile device or a desktop computer can also make network requests to a server, and\na client-side JavaScript application running inside a web browser can use\nXMLHttpRequest to become an HTTP client (this technique is known as Ajax [30]).\nIn this case, the server\u2019s response is typically not HTML for displaying to a human,\nbut rather data in an encoding that is convenient for further processing by the client-\nside application code (such as JSON). Although HTTP may be used as the transport"}
{"154": "Moreover, a server can itself be a client to another service (for example, a typical web\napp server acts as client to a database). This approach is often used to decompose a\nlarge application into smaller services by area of functionality, such that one service\nmakes a request to another when it requires some functionality or data from that\nother service. This way of building applications has traditionally been called a service-\noriented architecture (SOA), more recently refined and rebranded as microservices\narchitecture [31, 32].\nIn some ways, services are similar to databases: they typically allow clients to submit\nand query data. However, while databases allow arbitrary queries using the query lan\u2010\nguages we discussed in Chapter 2, services expose an application-specific API that\nonly allows inputs and outputs that are predetermined by the business logic (applica\u2010\ntion code) of the service [33]. This restriction provides a degree of encapsulation:\nservices can impose fine-grained restrictions on what clients can and cannot do.\nA key design goal of a service-oriented/microservices architecture is to make the\napplication easier to change and maintain by making services independently deploya\u2010\nble and evolvable. For example, each service should be owned by one team, and that\nteam should be able to release new versions of the service frequently, without having\nto coordinate with other teams. In other words, we should expect old and new ver\u2010\nsions of servers and clients to be running at the same time, and so the data encoding\nused by servers and clients must be compatible across versions of the service API\u2014\nprecisely what we\u2019ve been talking about in this chapter.\nWeb services\nWhen HTTP is used as the underlying protocol for talking to the service, it is called a\nweb service. This is perhaps a slight misnomer, because web services are not only used\non the web, but in several different contexts. For example:\n1. A client application running on a user\u2019s device (e.g., a native app on a mobile\ndevice, or JavaScript web app using Ajax) making requests to a service over\nHTTP. These requests typically go over the public internet.\n2. One service making requests to another service owned by the same organization,\noften located within the same datacenter, as part of a service-oriented/microser\u2010\nvices architecture. (Software that supports this kind of use case is sometimes\ncalled middleware.)\n3. One service making requests to a service owned by a different organization, usu\u2010\nally via the internet. This is used for data exchange between different organiza\u2010\ntions\u2019 backend systems. This category includes public APIs provided by online"}
{"155": "There are two popular approaches to web services: REST and SOAP. They are almost\ndiametrically opposed in terms of philosophy, and often the subject of heated debate\namong their respective proponents.vi\nREST is not a protocol, but rather a design philosophy that builds upon the principles\nof HTTP [34, 35]. It emphasizes simple data formats, using URLs for identifying\nresources and using HTTP features for cache control, authentication, and content\ntype negotiation. REST has been gaining popularity compared to SOAP, at least in\nthe context of cross-organizational service integration [36], and is often associated\nwith microservices [31]. An API designed according to the principles of REST is\ncalled RESTful.\nBy contrast, SOAP is an XML-based protocol for making network API requests.vii\nAlthough it is most commonly used over HTTP, it aims to be independent from\nHTTP and avoids using most HTTP features. Instead, it comes with a sprawling and\ncomplex multitude of related standards (the web service framework, known as WS-*)\nthat add various features [37].\nThe API of a SOAP web service is described using an XML-based language called the\nWeb Services Description Language, or WSDL. WSDL enables code generation so\nthat a client can access a remote service using local classes and method calls (which\nare encoded to XML messages and decoded again by the framework). This is useful in\nstatically typed programming languages, but less so in dynamically typed ones (see\n\u201cCode generation and dynamically typed languages\u201d on page 127).\nAs WSDL is not designed to be human-readable, and as SOAP messages are often too\ncomplex to construct manually, users of SOAP rely heavily on tool support, code\ngeneration, and IDEs [38]. For users of programming languages that are not sup\u2010\nported by SOAP vendors, integration with SOAP services is difficult.\nEven though SOAP and its various extensions are ostensibly standardized, interoper\u2010\nability between different vendors\u2019 implementations often causes problems [39]. For\nall of these reasons, although SOAP is still used in many large enterprises, it has fallen\nout of favor in most smaller companies.\nRESTful APIs tend to favor simpler approaches, typically involving less code genera\u2010\ntion and automated tooling. A definition format such as OpenAPI, also known as\nSwagger [40], can be used to describe RESTful APIs and produce documentation."}
{"156": "The problems with remote procedure calls (RPCs)\nWeb services are merely the latest incarnation of a long line of technologies for mak\u2010\ning API requests over a network, many of which received a lot of hype but have seri\u2010\nous problems. Enterprise JavaBeans (EJB) and Java\u2019s Remote Method Invocation\n(RMI) are limited to Java. The Distributed Component Object Model (DCOM) is\nlimited to Microsoft platforms. The Common Object Request Broker Architecture\n(CORBA) is excessively complex, and does not provide backward or forward compat\u2010\nibility [41].\nAll of these are based on the idea of a remote procedure call (RPC), which has been\naround since the 1970s [42]. The RPC model tries to make a request to a remote net\u2010\nwork service look the same as calling a function or method in your programming lan\u2010\nguage, within the same process (this abstraction is called location transparency).\nAlthough RPC seems convenient at first, the approach is fundamentally flawed [43,\n44]. A network request is very different from a local function call:\n\u2022 A local function call is predictable and either succeeds or fails, depending only on\nparameters that are under your control. A network request is unpredictable: the\nrequest or response may be lost due to a network problem, or the remote\nmachine may be slow or unavailable, and such problems are entirely outside of\nyour control. Network problems are common, so you have to anticipate them,\nfor example by retrying a failed request.\n\u2022 A local function call either returns a result, or throws an exception, or never\nreturns (because it goes into an infinite loop or the process crashes). A network\nrequest has another possible outcome: it may return without a result, due to a\ntimeout. In that case, you simply don\u2019t know what happened: if you don\u2019t get a\nresponse from the remote service, you have no way of knowing whether the\nrequest got through or not. (We discuss this issue in more detail in Chapter 8.)\n\u2022 If you retry a failed network request, it could happen that the requests are\nactually getting through, and only the responses are getting lost. In that case,\nretrying will cause the action to be performed multiple times, unless you build a\nmechanism for deduplication (idempotence) into the protocol. Local function\ncalls don\u2019t have this problem. (We discuss idempotence in more detail in Chap\u2010\nter 11.)\n\u2022 Every time you call a local function, it normally takes about the same time to exe\u2010\ncute. A network request is much slower than a function call, and its latency is\nalso wildly variable: at good times it may complete in less than a millisecond, but\nwhen the network is congested or the remote service is overloaded it may take"}
{"157": "need to be encoded into a sequence of bytes that can be sent over the network.\nThat\u2019s okay if the parameters are primitives like numbers or strings, but quickly\nbecomes problematic with larger objects.\n\u2022 The client and the service may be implemented in different programming lan\u2010\nguages, so the RPC framework must translate datatypes from one language into\nanother. This can end up ugly, since not all languages have the same types\u2014\nrecall JavaScript\u2019s problems with numbers greater than 253, for example (see\n\u201cJSON, XML, and Binary Variants\u201d on page 114). This problem doesn\u2019t exist in a\nsingle process written in a single language.\nAll of these factors mean that there\u2019s no point trying to make a remote service look\ntoo much like a local object in your programming language, because it\u2019s a fundamen\u2010\ntally different thing. Part of the appeal of REST is that it doesn\u2019t try to hide the fact\nthat it\u2019s a network protocol (although this doesn\u2019t seem to stop people from building\nRPC libraries on top of REST).\nCurrent directions for RPC\nDespite all these problems, RPC isn\u2019t going away. Various RPC frameworks have\nbeen built on top of all the encodings mentioned in this chapter: for example, Thrift\nand Avro come with RPC support included, gRPC is an RPC implementation using\nProtocol Buffers, Finagle also uses Thrift, and Rest.li uses JSON over HTTP.\nThis new generation of RPC frameworks is more explicit about the fact that a remote\nrequest is different from a local function call. For example, Finagle and Rest.li use\nfutures (promises) to encapsulate asynchronous actions that may fail. Futures also\nsimplify situations where you need to make requests to multiple services in parallel,\nand combine their results [45]. gRPC supports streams, where a call consists of not\njust one request and one response, but a series of requests and responses over time\n[46].\nSome of these frameworks also provide service discovery\u2014that is, allowing a client to\nfind out at which IP address and port number it can find a particular service. We will\nreturn to this topic in \u201cRequest Routing\u201d on page 214.\nCustom RPC protocols with a binary encoding format can achieve better perfor\u2010\nmance than something generic like JSON over REST. However, a RESTful API has\nother significant advantages: it is good for experimentation and debugging (you can\nsimply make requests to it using a web browser or the command-line tool curl,\nwithout any code generation or software installation), it is supported by all main\u2010\nstream programming languages and platforms, and there is a vast ecosystem of tools"}
{"158": "For these reasons, REST seems to be the predominant style for public APIs. The main\nfocus of RPC frameworks is on requests between services owned by the same organi\u2010\nzation, typically within the same datacenter.\nData encoding and evolution for RPC\nFor evolvability, it is important that RPC clients and servers can be changed and\ndeployed independently. Compared to data flowing through databases (as described\nin the last section), we can make a simplifying assumption in the case of dataflow\nthrough services: it is reasonable to assume that all the servers will be updated first,\nand all the clients second. Thus, you only need backward compatibility on requests,\nand forward compatibility on responses.\nThe backward and forward compatibility properties of an RPC scheme are inherited\nfrom whatever encoding it uses:\n\u2022 Thrift, gRPC (Protocol Buffers), and Avro RPC can be evolved according to the\ncompatibility rules of the respective encoding format.\n\u2022 In SOAP, requests and responses are specified with XML schemas. These can be\nevolved, but there are some subtle pitfalls [47].\n\u2022 RESTful APIs most commonly use JSON (without a formally specified schema)\nfor responses, and JSON or URI-encoded/form-encoded request parameters for\nrequests. Adding optional request parameters and adding new fields to response\nobjects are usually considered changes that maintain compatibility.\nService compatibility is made harder by the fact that RPC is often used for communi\u2010\ncation across organizational boundaries, so the provider of a service often has no\ncontrol over its clients and cannot force them to upgrade. Thus, compatibility needs\nto be maintained for a long time, perhaps indefinitely. If a compatibility-breaking\nchange is required, the service provider often ends up maintaining multiple versions\nof the service API side by side.\nThere is no agreement on how API versioning should work (i.e., how a client can\nindicate which version of the API it wants to use [48]). For RESTful APIs, common\napproaches are to use a version number in the URL or in the HTTP Accept header.\nFor services that use API keys to identify a particular client, another option is to store\na client\u2019s requested API version on the server and to allow this version selection to be\nupdated through a separate administrative interface [49].\nMessage-Passing Dataflow"}
{"159": "and databases (where one process writes encoded data, and another process reads it\nagain sometime in the future).\nIn this final section, we will briefly look at asynchronous message-passing systems,\nwhich are somewhere between RPC and databases. They are similar to RPC in that a\nclient\u2019s request (usually called a message) is delivered to another process with low\nlatency. They are similar to databases in that the message is not sent via a direct net\u2010\nwork connection, but goes via an intermediary called a message broker (also called a\nmessage queue or message-oriented middleware), which stores the message temporar\u2010\nily.\nUsing a message broker has several advantages compared to direct RPC:\n\u2022 It can act as a buffer if the recipient is unavailable or overloaded, and thus\nimprove system reliability.\n\u2022 It can automatically redeliver messages to a process that has crashed, and thus\nprevent messages from being lost.\n\u2022 It avoids the sender needing to know the IP address and port number of the\nrecipient (which is particularly useful in a cloud deployment where virtual\nmachines often come and go).\n\u2022 It allows one message to be sent to several recipients.\n\u2022 It logically decouples the sender from the recipient (the sender just publishes\nmessages and doesn\u2019t care who consumes them).\nHowever, a difference compared to RPC is that message-passing communication is\nusually one-way: a sender normally doesn\u2019t expect to receive a reply to its messages. It\nis possible for a process to send a response, but this would usually be done on a sepa\u2010\nrate channel. This communication pattern is asynchronous: the sender doesn\u2019t wait\nfor the message to be delivered, but simply sends it and then forgets about it.\nMessage brokers\nIn the past, the landscape of message brokers was dominated by commercial enter\u2010\nprise software from companies such as TIBCO, IBM WebSphere, and webMethods.\nMore recently, open source implementations such as RabbitMQ, ActiveMQ, Hor\u2010\nnetQ, NATS, and Apache Kafka have become popular. We will compare them in\nmore detail in Chapter 11.\nThe detailed delivery semantics vary by implementation and configuration, but in\ngeneral, message brokers are used as follows: one process sends a message to a named"}
{"160": "A topic provides only one-way dataflow. However, a consumer may itself publish\nmessages to another topic (so you can chain them together, as we shall see in Chap\u2010\nter 11), or to a reply queue that is consumed by the sender of the original message\n(allowing a request/response dataflow, similar to RPC).\nMessage brokers typically don\u2019t enforce any particular data model\u2014a message is just\na sequence of bytes with some metadata, so you can use any encoding format. If the\nencoding is backward and forward compatible, you have the greatest flexibility to\nchange publishers and consumers independently and deploy them in any order.\nIf a consumer republishes messages to another topic, you may need to be careful to\npreserve unknown fields, to prevent the issue described previously in the context of\ndatabases (Figure 4-7).\nDistributed actor frameworks\nThe actor model is a programming model for concurrency in a single process. Rather\nthan dealing directly with threads (and the associated problems of race conditions,\nlocking, and deadlock), logic is encapsulated in actors. Each actor typically represents\none client or entity, it may have some local state (which is not shared with any other\nactor), and it communicates with other actors by sending and receiving asynchro\u2010\nnous messages. Message delivery is not guaranteed: in certain error scenarios, mes\u2010\nsages will be lost. Since each actor processes only one message at a time, it doesn\u2019t\nneed to worry about threads, and each actor can be scheduled independently by the\nframework.\nIn distributed actor frameworks, this programming model is used to scale an applica\u2010\ntion across multiple nodes. The same message-passing mechanism is used, no matter\nwhether the sender and recipient are on the same node or different nodes. If they are\non different nodes, the message is transparently encoded into a byte sequence, sent\nover the network, and decoded on the other side.\nLocation transparency works better in the actor model than in RPC, because the actor\nmodel already assumes that messages may be lost, even within a single process.\nAlthough latency over the network is likely higher than within the same process,\nthere is less of a fundamental mismatch between local and remote communication\nwhen using the actor model.\nA distributed actor framework essentially integrates a message broker and the actor\nprogramming model into a single framework. However, if you want to perform roll\u2010\ning upgrades of your actor-based application, you still have to worry about forward\nand backward compatibility, as messages may be sent from a node running the new"}
{"161": "\u2022 Akka uses Java\u2019s built-in serialization by default, which does not provide forward\nor backward compatibility. However, you can replace it with something like Pro\u2010\ntocol Buffers, and thus gain the ability to do rolling upgrades [50].\n\u2022 Orleans by default uses a custom data encoding format that does not support\nrolling upgrade deployments; to deploy a new version of your application, you\nneed to set up a new cluster, move traffic from the old cluster to the new one, and\nshut down the old one [51, 52]. Like with Akka, custom serialization plug-ins can\nbe used.\n\u2022 In Erlang OTP it is surprisingly hard to make changes to record schemas (despite\nthe system having many features designed for high availability); rolling upgrades\nare possible but need to be planned carefully [53]. An experimental new maps\ndatatype (a JSON-like structure, introduced in Erlang R17 in 2014) may make\nthis easier in the future [54].\nSummary\nIn this chapter we looked at several ways of turning data structures into bytes on the\nnetwork or bytes on disk. We saw how the details of these encodings affect not only\ntheir efficiency, but more importantly also the architecture of applications and your\noptions for deploying them.\nIn particular, many services need to support rolling upgrades, where a new version of\na service is gradually deployed to a few nodes at a time, rather than deploying to all\nnodes simultaneously. Rolling upgrades allow new versions of a service to be released\nwithout downtime (thus encouraging frequent small releases over rare big releases)\nand make deployments less risky (allowing faulty releases to be detected and rolled\nback before they affect a large number of users). These properties are hugely benefi\u2010\ncial for evolvability, the ease of making changes to an application.\nDuring rolling upgrades, or for various other reasons, we must assume that different\nnodes are running the different versions of our application\u2019s code. Thus, it is impor\u2010\ntant that all data flowing around the system is encoded in a way that provides back\u2010\nward compatibility (new code can read old data) and forward compatibility (old code\ncan read new data).\nWe discussed several data encoding formats and their compatibility properties:\n\u2022 Programming language\u2013specific encodings are restricted to a single program\u2010\nming language and often fail to provide forward and backward compatibility."}
{"162": "vague about datatypes, so you have to be careful with things like numbers and\nbinary strings.\n\u2022 Binary schema\u2013driven formats like Thrift, Protocol Buffers, and Avro allow\ncompact, efficient encoding with clearly defined forward and backward compati\u2010\nbility semantics. The schemas can be useful for documentation and code genera\u2010\ntion in statically typed languages. However, they have the downside that data\nneeds to be decoded before it is human-readable.\nWe also discussed several modes of dataflow, illustrating different scenarios in which\ndata encodings are important:\n\u2022 Databases, where the process writing to the database encodes the data and the\nprocess reading from the database decodes it\n\u2022 RPC and REST APIs, where the client encodes a request, the server decodes the\nrequest and encodes a response, and the client finally decodes the response\n\u2022 Asynchronous message passing (using message brokers or actors), where nodes\ncommunicate by sending each other messages that are encoded by the sender\nand decoded by the recipient\nWe can conclude that with a bit of care, backward/forward compatibility and rolling\nupgrades are quite achievable. May your application\u2019s evolution be rapid and your\ndeployments be frequent.\nReferences\n[1] \u201cJava Object Serialization Specification,\u201d docs.oracle.com, 2010.\n[2] \u201cRuby 2.2.0 API Documentation,\u201d ruby-doc.org, Dec 2014.\n[3] \u201cThe Python 3.4.3 Standard Library Reference Manual,\u201d docs.python.org, Febru\u2010\nary 2015.\n[4] \u201cEsotericSoftware/kryo,\u201d github.com, October 2014.\n[5] \u201cCWE-502: Deserialization of Untrusted Data,\u201d Common Weakness Enumera\u2010\ntion, cwe.mitre.org, July 30, 2014.\n[6] Steve Breen: \u201cWhat Do WebLogic, WebSphere, JBoss, Jenkins, OpenNMS, and\nYour Application Have in Common? This Vulnerability,\u201d foxglovesecurity.com,\nNovember 6, 2015.\n[7] Patrick McKenzie: \u201cWhat the Rails Security Issue Means for Your Startup,\u201d kalzu\u2010"}
{"163": "[9] \u201cXML Is a Poor Copy of S-Expressions,\u201d c2.com wiki.\n[10] Matt Harris: \u201cSnowflake: An Update and Some Very Important Information,\u201d\nemail to Twitter Development Talk mailing list, October 19, 2010.\n[11] Shudi (Sandy) Gao, C. M. Sperberg-McQueen, and Henry S. Thompson: \u201cXML\nSchema 1.1,\u201d W3C Recommendation, May 2001.\n[12] Francis Galiegue, Kris Zyp, and Gary Court: \u201cJSON Schema,\u201d IETF Internet-\nDraft, February 2013.\n[13] Yakov Shafranovich: \u201cRFC 4180: Common Format and MIME Type for\nComma-Separated Values (CSV) Files,\u201d October 2005.\n[14] \u201cMessagePack Specification,\u201d msgpack.org.\n[15] Mark Slee, Aditya Agarwal, and Marc Kwiatkowski: \u201cThrift: Scalable Cross-\nLanguage Services Implementation,\u201d Facebook technical report, April 2007.\n[16] \u201cProtocol Buffers Developer Guide,\u201d Google, Inc., developers.google.com.\n[17] Igor Anishchenko: \u201cThrift vs Protocol Buffers vs Avro - Biased Comparison,\u201d\nslideshare.net, September 17, 2012.\n[18] \u201cA Matrix of the Features Each Individual Language Library Supports,\u201d\nwiki.apache.org.\n[19] Martin Kleppmann: \u201cSchema Evolution in Avro, Protocol Buffers and Thrift,\u201d\nmartin.kleppmann.com, December 5, 2012.\n[20] \u201cApache Avro 1.7.7 Documentation,\u201d avro.apache.org, July 2014.\n[21] Doug Cutting, Chad Walters, Jim Kellerman, et al.: \u201c[PROPOSAL] New Subpro\u2010\nject: Avro,\u201d email thread on hadoop-general mailing list, mail-archives.apache.org,\nApril 2009.\n[22] Tony Hoare: \u201cNull References: The Billion Dollar Mistake,\u201d at QCon London,\nMarch 2009.\n[23] Aditya Auradkar and Tom Quiggle: \u201cIntroducing Espresso\u2014LinkedIn\u2019s Hot\nNew Distributed Document Store,\u201d engineering.linkedin.com, January 21, 2015.\n[24] Jay Kreps: \u201cPutting Apache Kafka to Use: A Practical Guide to Building a Stream\nData Platform (Part 2),\u201d blog.confluent.io, February 25, 2015.\n[25] Gwen Shapira: \u201cThe Problem of Managing Schemas,\u201d radar.oreilly.com, Novem\u2010\nber 4, 2014."}
{"164": "[28] Russell Housley, Warwick Ford, Tim Polk, and David Solo: \u201cRFC 2459: Internet\nX.509 Public Key Infrastructure: Certificate and CRL Profile,\u201d IETF Network Work\u2010\ning Group, Standards Track, January 1999.\n[29] Lev Walkin: \u201cQuestion: Extensibility and Dropping Fields,\u201d lionet.info, Septem\u2010\nber 21, 2010.\n[30] Jesse James Garrett: \u201cAjax: A New Approach to Web Applications,\u201d adaptive\u2010\npath.com, February 18, 2005.\n[31] Sam Newman: Building Microservices. O\u2019Reilly Media, 2015. ISBN:\n978-1-491-95035-7\n[32] Chris Richardson: \u201cMicroservices: Decomposing Applications for Deployability\nand Scalability,\u201d infoq.com, May 25, 2014.\n[33] Pat Helland: \u201cData on the Outside Versus Data on the Inside,\u201d at 2nd Biennial\nConference on Innovative Data Systems Research (CIDR), January 2005.\n[34] Roy Thomas Fielding: \u201cArchitectural Styles and the Design of Network-Based\nSoftware Architectures,\u201d PhD Thesis, University of California, Irvine, 2000.\n[35] Roy Thomas Fielding: \u201cREST APIs Must Be Hypertext-Driven,\u201d roy.gbiv.com,\nOctober 20 2008.\n[36] \u201cREST in Peace, SOAP,\u201d royal.pingdom.com, October 15, 2010.\n[37] \u201cWeb Services Standards as of Q1 2007,\u201d innoq.com, February 2007.\n[38] Pete Lacey: \u201cThe S Stands for Simple,\u201d harmful.cat-v.org, November 15, 2006.\n[39] Stefan Tilkov: \u201cInterview: Pete Lacey Criticizes Web Services,\u201d infoq.com,\nDecember 12, 2006.\n[40] \u201cOpenAPI Specification (fka Swagger RESTful API Documentation Specifica\u2010\ntion) Version 2.0,\u201d swagger.io, September 8, 2014.\n[41] Michi Henning: \u201cThe Rise and Fall of CORBA,\u201d ACM Queue, volume 4, number\n5, pages 28\u201334, June 2006. doi:10.1145/1142031.1142044\n[42] Andrew D. Birrell and Bruce Jay Nelson: \u201cImplementing Remote Procedure\nCalls,\u201d ACM Transactions on Computer Systems (TOCS), volume 2, number 1, pages\n39\u201359, February 1984. doi:10.1145/2080.357392\n[43] Jim Waldo, Geoff Wyant, Ann Wollrath, and Sam Kendall: \u201cA Note on Dis\u2010\ntributed Computing,\u201d Sun Microsystems Laboratories, Inc., Technical Report\nTR-94-29, November 1994."}
{"165": "[45] Marius Eriksen: \u201cYour Server as a Function,\u201d at 7th Workshop on Programming\nLanguages and Operating Systems (PLOS), November 2013. doi:\n10.1145/2525528.2525538\n[46] \u201cgrpc-common Documentation,\u201d Google, Inc., github.com, February 2015.\n[47] Aditya Narayan and Irina Singh: \u201cDesigning and Versioning Compatible Web\nServices,\u201d ibm.com, March 28, 2007.\n[48] Troy Hunt: \u201cYour API Versioning Is Wrong, Which Is Why I Decided to Do It 3\nDifferent Wrong Ways,\u201d troyhunt.com, February 10, 2014.\n[49] \u201cAPI Upgrades,\u201d Stripe, Inc., April 2015.\n[50] Jonas Bon\u00e9r: \u201cUpgrade in an Akka Cluster,\u201d email to akka-user mailing list, grok\u2010\nbase.com, August 28, 2013.\n[51] Philip A. Bernstein, Sergey Bykov, Alan Geller, et al.: \u201cOrleans: Distributed Vir\u2010\ntual Actors for Programmability and Scalability,\u201d Microsoft Research Technical\nReport MSR-TR-2014-41, March 2014.\n[52] \u201cMicrosoft Project Orleans Documentation,\u201d Microsoft Research, dotnet.git\u2010\nhub.io, 2015.\n[53] David Mercer, Sean Hinde, Yinso Chen, and Richard A O\u2019Keefe: \u201cbeginner:\nUpdating Data Structures,\u201d email thread on erlang-questions mailing list, erlang.com,\nOctober 29, 2007.\n[54] Fred Hebert: \u201cPostscript: Maps,\u201d learnyousomeerlang.com, April 9, 2014."}
{"166": ""}
{"167": "PART II\nDistributed Data\nFor a successful technology, reality must take precedence over public relations, for nature\ncannot be fooled.\n\u2014Richard Feynman, Rogers Commission Report (1986)\nIn Part I of this book, we discussed aspects of data systems that apply when data is\nstored on a single machine. Now, in Part II, we move up a level and ask: what hap\u2010\npens if multiple machines are involved in storage and retrieval of data?\nThere are various reasons why you might want to distribute a database across multi\u2010\nple machines:\nScalability\nIf your data volume, read load, or write load grows bigger than a single machine\ncan handle, you can potentially spread the load across multiple machines.\nFault tolerance/high availability\nIf your application needs to continue working even if one machine (or several\nmachines, or the network, or an entire datacenter) goes down, you can use multi\u2010\nple machines to give you redundancy. When one fails, another one can take over.\nLatency\nIf you have users around the world, you might want to have servers at various\nlocations worldwide so that each user can be served from a datacenter that is geo\u2010\ngraphically close to them. That avoids the users having to wait for network pack\u2010"}
{"168": "Scaling to Higher Load\nIf all you need is to scale to higher load, the simplest approach is to buy a more pow\u2010\nerful machine (sometimes called vertical scaling or scaling up). Many CPUs, many\nRAM chips, and many disks can be joined together under one operating system, and\na fast interconnect allows any CPU to access any part of the memory or disk. In this\nkind of shared-memory architecture, all the components can be treated as a single\nmachine [1].i\nThe problem with a shared-memory approach is that the cost grows faster than line\u2010\narly: a machine with twice as many CPUs, twice as much RAM, and twice as much\ndisk capacity as another typically costs significantly more than twice as much. And\ndue to bottlenecks, a machine twice the size cannot necessarily handle twice the load.\nA shared-memory architecture may offer limited fault tolerance\u2014high-end machines\nhave hot-swappable components (you can replace disks, memory modules, and even\nCPUs without shutting down the machines)\u2014but it is definitely limited to a single\ngeographic location.\nAnother approach is the shared-disk architecture, which uses several machines with\nindependent CPUs and RAM, but stores data on an array of disks that is shared\nbetween the machines, which are connected via a fast network.ii This architecture is\nused for some data warehousing workloads, but contention and the overhead of lock\u2010\ning limit the scalability of the shared-disk approach [2].\nShared-Nothing Architectures\nBy contrast, shared-nothing architectures [3] (sometimes called horizontal scaling or\nscaling out) have gained a lot of popularity. In this approach, each machine or virtual\nmachine running the database software is called a node. Each node uses its CPUs,\nRAM, and disks independently. Any coordination between nodes is done at the soft\u2010\nware level, using a conventional network.\nNo special hardware is required by a shared-nothing system, so you can use whatever\nmachines have the best price/performance ratio. You can potentially distribute data\nacross multiple geographic regions, and thus reduce latency for users and potentially\nbe able to survive the loss of an entire datacenter. With cloud deployments of virtual\ni. In a large machine, although any CPU can access any part of memory, some banks of memory are closer to"}
{"169": "machines, you don\u2019t need to be operating at Google scale: even for small companies,\na multi-region distributed architecture is now feasible.\nIn this part of the book, we focus on shared-nothing architectures\u2014not because they\nare necessarily the best choice for every use case, but rather because they require the\nmost caution from you, the application developer. If your data is distributed across\nmultiple nodes, you need to be aware of the constraints and trade-offs that occur in\nsuch a distributed system\u2014the database cannot magically hide these from you.\nWhile a distributed shared-nothing architecture has many advantages, it usually also\nincurs additional complexity for applications and sometimes limits the expressive\u2010\nness of the data models you can use. In some cases, a simple single-threaded program\ncan perform significantly better than a cluster with over 100 CPU cores [4]. On the\nother hand, shared-nothing systems can be very powerful. The next few chapters go\ninto details on the issues that arise when data is distributed.\nReplication Versus Partitioning\nThere are two common ways data is distributed across multiple nodes:\nReplication\nKeeping a copy of the same data on several different nodes, potentially in differ\u2010\nent locations. Replication provides redundancy: if some nodes are unavailable,\nthe data can still be served from the remaining nodes. Replication can also help\nimprove performance. We discuss replication in Chapter 5.\nPartitioning\nSplitting a big database into smaller subsets called partitions so that different par\u2010\ntitions can be assigned to different nodes (also known as sharding). We discuss\npartitioning in Chapter 6.\nThese are separate mechanisms, but they often go hand in hand, as illustrated in\nFigure II-1."}
{"170": "Figure II-1. A database split into two partitions, with two replicas per partition.\nWith an understanding of those concepts, we can discuss the difficult trade-offs that\nyou need to make in a distributed system. We\u2019ll discuss transactions in Chapter 7, as\nthat will help you understand all the many things that can go wrong in a data system,\nand what you can do about them. We\u2019ll conclude this part of the book by discussing\nthe fundamental limitations of distributed systems in Chapters 8 and 9.\nLater, in Part III of this book, we will discuss how you can take several (potentially\ndistributed) datastores and integrate them into a larger system, satisfying the needs of\na complex application. But first, let\u2019s talk about distributed data.\nReferences\n[1] Ulrich Drepper: \u201cWhat Every Programmer Should Know About Memory,\u201d akka\u2010\ndia.org, November 21, 2007.\n[2] Ben Stopford: \u201cShared Nothing vs. Shared Disk Architectures: An Independent\nView,\u201d benstopford.com, November 24, 2009.\n[3] Michael Stonebraker: \u201cThe Case for Shared Nothing,\u201d IEEE Database Engineering\nBulletin, volume 9, number 1, pages 4\u20139, March 1986.\n[4] Frank McSherry, Michael Isard, and Derek G. Murray: \u201cScalability! But at What\nCOST?,\u201d at 15th USENIX Workshop on Hot Topics in Operating Systems (HotOS),"}
{"171": ""}
{"172": ""}
{"173": "CHAPTER 5\nReplication\nThe major difference between a thing that might go wrong and a thing that cannot possibly\ngo wrong is that when a thing that cannot possibly go wrong goes wrong it usually turns out\nto be impossible to get at or repair.\n\u2014Douglas Adams, Mostly Harmless (1992)\nReplication means keeping a copy of the same data on multiple machines that are\nconnected via a network. As discussed in the introduction to Part II, there are several\nreasons why you might want to replicate data:\n\u2022 To keep data geographically close to your users (and thus reduce latency)\n\u2022 To allow the system to continue working even if some of its parts have failed\n(and thus increase availability)\n\u2022 To scale out the number of machines that can serve read queries (and thus\nincrease read throughput)\nIn this chapter we will assume that your dataset is so small that each machine can\nhold a copy of the entire dataset. In Chapter 6 we will relax that assumption and dis\u2010\ncuss partitioning (sharding) of datasets that are too big for a single machine. In later\nchapters we will discuss various kinds of faults that can occur in a replicated data sys\u2010\ntem, and how to deal with them.\nIf the data that you\u2019re replicating does not change over time, then replication is easy:\nyou just need to copy the data to every node once, and you\u2019re done. All of the diffi\u2010\nculty in replication lies in handling changes to replicated data, and that\u2019s what this\nchapter is about. We will discuss three popular algorithms for replicating changes"}
{"174": "There are many trade-offs to consider with replication: for example, whether to use\nsynchronous or asynchronous replication, and how to handle failed replicas. Those\nare often configuration options in databases, and although the details vary by data\u2010\nbase, the general principles are similar across many different implementations. We\nwill discuss the consequences of such choices in this chapter.\nReplication of databases is an old topic\u2014the principles haven\u2019t changed much since\nthey were studied in the 1970s [1], because the fundamental constraints of networks\nhave remained the same. However, outside of research, many developers continued\nto assume for a long time that a database consisted of just one node. Mainstream use\nof distributed databases is more recent. Since many application developers are new to\nthis area, there has been a lot of misunderstanding around issues such as eventual\nconsistency. In \u201cProblems with Replication Lag\u201d on page 161 we will get more precise\nabout eventual consistency and discuss things like the read-your-writes and mono\u2010\ntonic reads guarantees.\nLeaders and Followers\nEach node that stores a copy of the database is called a replica. With multiple replicas,\na question inevitably arises: how do we ensure that all the data ends up on all the rep\u2010\nlicas?\nEvery write to the database needs to be processed by every replica; otherwise, the rep\u2010\nlicas would no longer contain the same data. The most common solution for this is\ncalled leader-based replication (also known as active/passive or master\u2013slave replica\u2010\ntion) and is illustrated in Figure 5-1. It works as follows:\n1. One of the replicas is designated the leader (also known as master or primary).\nWhen clients want to write to the database, they must send their requests to the\nleader, which first writes the new data to its local storage.\n2. The other replicas are known as followers (read replicas, slaves, secondaries, or hot\nstandbys).i Whenever the leader writes new data to its local storage, it also sends\nthe data change to all of its followers as part of a replication log or change stream.\nEach follower takes the log from the leader and updates its local copy of the data\u2010\nbase accordingly, by applying all writes in the same order as they were processed\non the leader."}
{"175": "3. When a client wants to read from the database, it can query either the leader or\nany of the followers. However, writes are only accepted on the leader (the follow\u2010\ners are read-only from the client\u2019s point of view).\nFigure 5-1. Leader-based (master\u2013slave) replication.\nThis mode of replication is a built-in feature of many relational databases, such as\nPostgreSQL (since version 9.0), MySQL, Oracle Data Guard [2], and SQL Server\u2019s\nAlwaysOn Availability Groups [3]. It is also used in some nonrelational databases,\nincluding MongoDB, RethinkDB, and Espresso [4]. Finally, leader-based replication\nis not restricted to only databases: distributed message brokers such as Kafka [5] and\nRabbitMQ highly available queues [6] also use it. Some network filesystems and\nreplicated block devices such as DRBD are similar.\nSynchronous Versus Asynchronous Replication\nAn important detail of a replicated system is whether the replication happens syn\u2010\nchronously or asynchronously. (In relational databases, this is often a configurable\noption; other systems are often hardcoded to be either one or the other.)\nThink about what happens in Figure 5-1, where the user of a website updates their\nprofile image. At some point in time, the client sends the update request to the leader;\nshortly afterward, it is received by the leader. At some point, the leader forwards the\ndata change to the followers. Eventually, the leader notifies the client that the update\nwas successful.\nFigure 5-2 shows the communication between various components of the system: the\nuser\u2019s client, the leader, and two followers. Time flows from left to right. A request or\nresponse message is shown as a thick arrow."}
{"176": "Figure 5-2. Leader-based replication with one synchronous and one asynchronous fol\u2010\nlower.\nIn the example of Figure 5-2, the replication to follower 1 is synchronous: the leader\nwaits until follower 1 has confirmed that it received the write before reporting success\nto the user, and before making the write visible to other clients. The replication to\nfollower 2 is asynchronous: the leader sends the message, but doesn\u2019t wait for a\nresponse from the follower.\nThe diagram shows that there is a substantial delay before follower 2 processes the\nmessage. Normally, replication is quite fast: most database systems apply changes to\nfollowers in less than a second. However, there is no guarantee of how long it might\ntake. There are circumstances when followers might fall behind the leader by several\nminutes or more; for example, if a follower is recovering from a failure, if the system\nis operating near maximum capacity, or if there are network problems between the\nnodes.\nThe advantage of synchronous replication is that the follower is guaranteed to have\nan up-to-date copy of the data that is consistent with the leader. If the leader sud\u2010\ndenly fails, we can be sure that the data is still available on the follower. The disad\u2010\nvantage is that if the synchronous follower doesn\u2019t respond (because it has crashed,\nor there is a network fault, or for any other reason), the write cannot be processed.\nThe leader must block all writes and wait until the synchronous replica is available\nagain.\nFor that reason, it is impractical for all followers to be synchronous: any one node\noutage would cause the whole system to grind to a halt. In practice, if you enable syn\u2010\nchronous replication on a database, it usually means that one of the followers is syn\u2010\nchronous, and the others are asynchronous. If the synchronous follower becomes"}
{"177": "leader and one synchronous follower. This configuration is sometimes also called\nsemi-synchronous [7].\nOften, leader-based replication is configured to be completely asynchronous. In this\ncase, if the leader fails and is not recoverable, any writes that have not yet been repli\u2010\ncated to followers are lost. This means that a write is not guaranteed to be durable,\neven if it has been confirmed to the client. However, a fully asynchronous configura\u2010\ntion has the advantage that the leader can continue processing writes, even if all of its\nfollowers have fallen behind.\nWeakening durability may sound like a bad trade-off, but asynchronous replication is\nnevertheless widely used, especially if there are many followers or if they are geo\u2010\ngraphically distributed. We will return to this issue in \u201cProblems with Replication\nLag\u201d on page 161.\nResearch on Replication\nIt can be a serious problem for asynchronously replicated systems to lose data if the\nleader fails, so researchers have continued investigating replication methods that do\nnot lose data but still provide good performance and availability. For example, chain\nreplication [8, 9] is a variant of synchronous replication that has been successfully\nimplemented in a few systems such as Microsoft Azure Storage [10, 11].\nThere is a strong connection between consistency of replication and consensus (get\u2010\nting several nodes to agree on a value), and we will explore this area of theory in more\ndetail in Chapter 9. In this chapter we will concentrate on the simpler forms of repli\u2010\ncation that are most commonly used in databases in practice.\nSetting Up New Followers\nFrom time to time, you need to set up new followers\u2014perhaps to increase the num\u2010\nber of replicas, or to replace failed nodes. How do you ensure that the new follower\nhas an accurate copy of the leader\u2019s data?\nSimply copying data files from one node to another is typically not sufficient: clients\nare constantly writing to the database, and the data is always in flux, so a standard file\ncopy would see different parts of the database at different points in time. The result\nmight not make any sense.\nYou could make the files on disk consistent by locking the database (making it\nunavailable for writes), but that would go against our goal of high availability. Fortu\u2010"}
{"178": "1. Take a consistent snapshot of the leader\u2019s database at some point in time\u2014if pos\u2010\nsible, without taking a lock on the entire database. Most databases have this fea\u2010\nture, as it is also required for backups. In some cases, third-party tools are\nneeded, such as innobackupex for MySQL [12].\n2. Copy the snapshot to the new follower node.\n3. The follower connects to the leader and requests all the data changes that have\nhappened since the snapshot was taken. This requires that the snapshot is associ\u2010\nated with an exact position in the leader\u2019s replication log. That position has vari\u2010\nous names: for example, PostgreSQL calls it the log sequence number, and\nMySQL calls it the binlog coordinates.\n4. When the follower has processed the backlog of data changes since the snapshot,\nwe say it has caught up. It can now continue to process data changes from the\nleader as they happen.\nThe practical steps of setting up a follower vary significantly by database. In some\nsystems the process is fully automated, whereas in others it can be a somewhat arcane\nmulti-step workflow that needs to be manually performed by an administrator.\nHandling Node Outages\nAny node in the system can go down, perhaps unexpectedly due to a fault, but just as\nlikely due to planned maintenance (for example, rebooting a machine to install a ker\u2010\nnel security patch). Being able to reboot individual nodes without downtime is a big\nadvantage for operations and maintenance. Thus, our goal is to keep the system as a\nwhole running despite individual node failures, and to keep the impact of a node out\u2010\nage as small as possible.\nHow do you achieve high availability with leader-based replication?\nFollower failure: Catch-up recovery\nOn its local disk, each follower keeps a log of the data changes it has received from\nthe leader. If a follower crashes and is restarted, or if the network between the leader\nand the follower is temporarily interrupted, the follower can recover quite easily:\nfrom its log, it knows the last transaction that was processed before the fault occur\u2010\nred. Thus, the follower can connect to the leader and request all the data changes that\noccurred during the time when the follower was disconnected. When it has applied\nthese changes, it has caught up to the leader and can continue receiving a stream of\ndata changes as before."}
{"179": "Leader failure: Failover\nHandling a failure of the leader is trickier: one of the followers needs to be promoted\nto be the new leader, clients need to be reconfigured to send their writes to the new\nleader, and the other followers need to start consuming data changes from the new\nleader. This process is called failover.\nFailover can happen manually (an administrator is notified that the leader has failed\nand takes the necessary steps to make a new leader) or automatically. An automatic\nfailover process usually consists of the following steps:\n1. Determining that the leader has failed. There are many things that could poten\u2010\ntially go wrong: crashes, power outages, network issues, and more. There is no\nfoolproof way of detecting what has gone wrong, so most systems simply use a\ntimeout: nodes frequently bounce messages back and forth between each other,\nand if a node doesn\u2019t respond for some period of time\u2014say, 30 seconds\u2014it is\nassumed to be dead. (If the leader is deliberately taken down for planned mainte\u2010\nnance, this doesn\u2019t apply.)\n2. Choosing a new leader. This could be done through an election process (where\nthe leader is chosen by a majority of the remaining replicas), or a new leader\ncould be appointed by a previously elected controller node. The best candidate for\nleadership is usually the replica with the most up-to-date data changes from the\nold leader (to minimize any data loss). Getting all the nodes to agree on a new\nleader is a consensus problem, discussed in detail in Chapter 9.\n3. Reconfiguring the system to use the new leader. Clients now need to send\ntheir write requests to the new leader (we discuss this in \u201cRequest Routing\u201d on\npage 214). If the old leader comes back, it might still believe that it is the leader,\nnot realizing that the other replicas have forced it to step down. The system\nneeds to ensure that the old leader becomes a follower and recognizes the new\nleader.\nFailover is fraught with things that can go wrong:\n\u2022 If asynchronous replication is used, the new leader may not have received all the\nwrites from the old leader before it failed. If the former leader rejoins the cluster\nafter a new leader has been chosen, what should happen to those writes? The new\nleader may have received conflicting writes in the meantime. The most common\nsolution is for the old leader\u2019s unreplicated writes to simply be discarded, which\nmay violate clients\u2019 durability expectations.\n\u2022 Discarding writes is especially dangerous if other storage systems outside of the"}
{"180": "rows, but because the new leader\u2019s counter lagged behind the old leader\u2019s, it\nreused some primary keys that were previously assigned by the old leader. These\nprimary keys were also used in a Redis store, so the reuse of primary keys resul\u2010\nted in inconsistency between MySQL and Redis, which caused some private data\nto be disclosed to the wrong users.\n\u2022 In certain fault scenarios (see Chapter 8), it could happen that two nodes both\nbelieve that they are the leader. This situation is called split brain, and it is dan\u2010\ngerous: if both leaders accept writes, and there is no process for resolving con\u2010\nflicts (see \u201cMulti-Leader Replication\u201d on page 168), data is likely to be lost or\ncorrupted. As a safety catch, some systems have a mechanism to shut down one\nnode if two leaders are detected.ii However, if this mechanism is not carefully\ndesigned, you can end up with both nodes being shut down [14].\n\u2022 What is the right timeout before the leader is declared dead? A longer timeout\nmeans a longer time to recovery in the case where the leader fails. However, if the\ntimeout is too short, there could be unnecessary failovers. For example, a tempo\u2010\nrary load spike could cause a node\u2019s response time to increase above the timeout,\nor a network glitch could cause delayed packets. If the system is already strug\u2010\ngling with high load or network problems, an unnecessary failover is likely to\nmake the situation worse, not better.\nThere are no easy solutions to these problems. For this reason, some operations\nteams prefer to perform failovers manually, even if the software supports automatic\nfailover.\nThese issues\u2014node failures; unreliable networks; and trade-offs around replica con\u2010\nsistency, durability, availability, and latency\u2014are in fact fundamental problems in\ndistributed systems. In Chapter 8 and Chapter 9 we will discuss them in greater\ndepth.\nImplementation of Replication Logs\nHow does leader-based replication work under the hood? Several different replica\u2010\ntion methods are used in practice, so let\u2019s look at each one briefly.\nStatement-based replication\nIn the simplest case, the leader logs every write request (statement) that it executes\nand sends that statement log to its followers. For a relational database, this means\nthat every INSERT, UPDATE, or DELETE statement is forwarded to followers, and each"}
{"181": "follower parses and executes that SQL statement as if it had been received from a\nclient.\nAlthough this may sound reasonable, there are various ways in which this approach\nto replication can break down:\n\u2022 Any statement that calls a nondeterministic function, such as NOW() to get the\ncurrent date and time or RAND() to get a random number, is likely to generate a\ndifferent value on each replica.\n\u2022 If statements use an autoincrementing column, or if they depend on the existing\ndata in the database (e.g., UPDATE \u2026 WHERE <some condition>), they must be\nexecuted in exactly the same order on each replica, or else they may have a differ\u2010\nent effect. This can be limiting when there are multiple concurrently executing\ntransactions.\n\u2022 Statements that have side effects (e.g., triggers, stored procedures, user-defined\nfunctions) may result in different side effects occurring on each replica, unless\nthe side effects are absolutely deterministic.\nIt is possible to work around those issues\u2014for example, the leader can replace any\nnondeterministic function calls with a fixed return value when the statement is log\u2010\nged so that the followers all get the same value. However, because there are so many\nedge cases, other replication methods are now generally preferred.\nStatement-based replication was used in MySQL before version 5.1. It is still some\u2010\ntimes used today, as it is quite compact, but by default MySQL now switches to row-\nbased replication (discussed shortly) if there is any nondeterminism in a statement.\nVoltDB uses statement-based replication, and makes it safe by requiring transactions\nto be deterministic [15].\nWrite-ahead log (WAL) shipping\nIn Chapter 3 we discussed how storage engines represent data on disk, and we found\nthat usually every write is appended to a log:\n\u2022 In the case of a log-structured storage engine (see \u201cSSTables and LSM-Trees\u201d on\npage 76), this log is the main place for storage. Log segments are compacted and\ngarbage-collected in the background.\n\u2022 In the case of a B-tree (see \u201cB-Trees\u201d on page 79), which overwrites individual\ndisk blocks, every modification is first written to a write-ahead log so that the\nindex can be restored to a consistent state after a crash."}
{"182": "When the follower processes this log, it builds a copy of the exact same data struc\u2010\ntures as found on the leader.\nThis method of replication is used in PostgreSQL and Oracle, among others [16]. The\nmain disadvantage is that the log describes the data on a very low level: a WAL con\u2010\ntains details of which bytes were changed in which disk blocks. This makes replica\u2010\ntion closely coupled to the storage engine. If the database changes its storage format\nfrom one version to another, it is typically not possible to run different versions of\nthe database software on the leader and the followers.\nThat may seem like a minor implementation detail, but it can have a big operational\nimpact. If the replication protocol allows the follower to use a newer software version\nthan the leader, you can perform a zero-downtime upgrade of the database software\nby first upgrading the followers and then performing a failover to make one of the\nupgraded nodes the new leader. If the replication protocol does not allow this version\nmismatch, as is often the case with WAL shipping, such upgrades require downtime.\nLogical (row-based) log replication\nAn alternative is to use different log formats for replication and for the storage\nengine, which allows the replication log to be decoupled from the storage engine\ninternals. This kind of replication log is called a logical log, to distinguish it from the\nstorage engine\u2019s (physical) data representation.\nA logical log for a relational database is usually a sequence of records describing\nwrites to database tables at the granularity of a row:\n\u2022 For an inserted row, the log contains the new values of all columns.\n\u2022 For a deleted row, the log contains enough information to uniquely identify the\nrow that was deleted. Typically this would be the primary key, but if there is no\nprimary key on the table, the old values of all columns need to be logged.\n\u2022 For an updated row, the log contains enough information to uniquely identify\nthe updated row, and the new values of all columns (or at least the new values of\nall columns that changed).\nA transaction that modifies several rows generates several such log records, followed\nby a record indicating that the transaction was committed. MySQL\u2019s binlog (when\nconfigured to use row-based replication) uses this approach [17].\nSince a logical log is decoupled from the storage engine internals, it can more easily\nbe kept backward compatible, allowing the leader and the follower to run different"}
{"183": "warehouse for offline analysis, or for building custom indexes and caches [18]. This\ntechnique is called change data capture, and we will return to it in Chapter 11.\nTrigger-based replication\nThe replication approaches described so far are implemented by the database system,\nwithout involving any application code. In many cases, that\u2019s what you want\u2014but\nthere are some circumstances where more flexibility is needed. For example, if you\nwant to only replicate a subset of the data, or want to replicate from one kind of\ndatabase to another, or if you need conflict resolution logic (see \u201cHandling Write\nConflicts\u201d on page 171), then you may need to move replication up to the application\nlayer.\nSome tools, such as Oracle GoldenGate [19], can make data changes available to an\napplication by reading the database log. An alternative is to use features that are\navailable in many relational databases: triggers and stored procedures.\nA trigger lets you register custom application code that is automatically executed\nwhen a data change (write transaction) occurs in a database system. The trigger has\nthe opportunity to log this change into a separate table, from which it can be read by\nan external process. That external process can then apply any necessary application\nlogic and replicate the data change to another system. Databus for Oracle [20] and\nBucardo for Postgres [21] work like this, for example.\nTrigger-based replication typically has greater overheads than other replication\nmethods, and is more prone to bugs and limitations than the database\u2019s built-in repli\u2010\ncation. However, it can nevertheless be useful due to its flexibility.\nProblems with Replication Lag\nBeing able to tolerate node failures is just one reason for wanting replication. As\nmentioned in the introduction to Part II, other reasons are scalability (processing\nmore requests than a single machine can handle) and latency (placing replicas geo\u2010\ngraphically closer to users).\nLeader-based replication requires all writes to go through a single node, but read-\nonly queries can go to any replica. For workloads that consist of mostly reads and\nonly a small percentage of writes (a common pattern on the web), there is an attrac\u2010\ntive option: create many followers, and distribute the read requests across those fol\u2010\nlowers. This removes load from the leader and allows read requests to be served by\nnearby replicas."}
{"184": "unavailable for writing. And the more nodes you have, the likelier it is that one will\nbe down, so a fully synchronous configuration would be very unreliable.\nUnfortunately, if an application reads from an asynchronous follower, it may see out\u2010\ndated information if the follower has fallen behind. This leads to apparent inconsis\u2010\ntencies in the database: if you run the same query on the leader and a follower at the\nsame time, you may get different results, because not all writes have been reflected in\nthe follower. This inconsistency is just a temporary state\u2014if you stop writing to the\ndatabase and wait a while, the followers will eventually catch up and become consis\u2010\ntent with the leader. For that reason, this effect is known as eventual consistency [22,\n23].iii\nThe term \u201ceventually\u201d is deliberately vague: in general, there is no limit to how far a\nreplica can fall behind. In normal operation, the delay between a write happening on\nthe leader and being reflected on a follower\u2014the replication lag\u2014may be only a frac\u2010\ntion of a second, and not noticeable in practice. However, if the system is operating\nnear capacity or if there is a problem in the network, the lag can easily increase to\nseveral seconds or even minutes.\nWhen the lag is so large, the inconsistencies it introduces are not just a theoretical\nissue but a real problem for applications. In this section we will highlight three exam\u2010\nples of problems that are likely to occur when there is replication lag and outline\nsome approaches to solving them.\nReading Your Own Writes\nMany applications let the user submit some data and then view what they have sub\u2010\nmitted. This might be a record in a customer database, or a comment on a discussion\nthread, or something else of that sort. When new data is submitted, it must be sent to\nthe leader, but when the user views the data, it can be read from a follower. This is\nespecially appropriate if data is frequently viewed but only occasionally written.\nWith asynchronous replication, there is a problem, illustrated in Figure 5-3: if the\nuser views the data shortly after making a write, the new data may not yet have\nreached the replica. To the user, it looks as though the data they submitted was lost,\nso they will be understandably unhappy."}
{"185": "Figure 5-3. A user makes a write, followed by a read from a stale replica. To prevent\nthis anomaly, we need read-after-write consistency.\nIn this situation, we need read-after-write consistency, also known as read-your-writes\nconsistency [24]. This is a guarantee that if the user reloads the page, they will always\nsee any updates they submitted themselves. It makes no promises about other users:\nother users\u2019 updates may not be visible until some later time. However, it reassures\nthe user that their own input has been saved correctly.\nHow can we implement read-after-write consistency in a system with leader-based\nreplication? There are various possible techniques. To mention a few:\n\u2022 When reading something that the user may have modified, read it from the\nleader; otherwise, read it from a follower. This requires that you have some way\nof knowing whether something might have been modified, without actually\nquerying it. For example, user profile information on a social network is nor\u2010\nmally only editable by the owner of the profile, not by anybody else. Thus, a sim\u2010\nple rule is: always read the user\u2019s own profile from the leader, and any other\nusers\u2019 profiles from a follower.\n\u2022 If most things in the application are potentially editable by the user, that\napproach won\u2019t be effective, as most things would have to be read from the\nleader (negating the benefit of read scaling). In that case, other criteria may be\nused to decide whether to read from the leader. For example, you could track the\ntime of the last update and, for one minute after the last update, make all reads\nfrom the leader. You could also monitor the replication lag on followers and pre\u2010\nvent queries on any follower that is more than one minute behind the leader.\n\u2022 The client can remember the timestamp of its most recent write\u2014then the sys\u2010"}
{"186": "caught up. The timestamp could be a logical timestamp (something that indicates\nordering of writes, such as the log sequence number) or the actual system clock\n(in which case clock synchronization becomes critical; see \u201cUnreliable Clocks\u201d\non page 287).\n\u2022 If your replicas are distributed across multiple datacenters (for geographical\nproximity to users or for availability), there is additional complexity. Any request\nthat needs to be served by the leader must be routed to the datacenter that con\u2010\ntains the leader.\nAnother complication arises when the same user is accessing your service from mul\u2010\ntiple devices, for example a desktop web browser and a mobile app. In this case you\nmay want to provide cross-device read-after-write consistency: if the user enters some\ninformation on one device and then views it on another device, they should see the\ninformation they just entered.\nIn this case, there are some additional issues to consider:\n\u2022 Approaches that require remembering the timestamp of the user\u2019s last update\nbecome more difficult, because the code running on one device doesn\u2019t know\nwhat updates have happened on the other device. This metadata will need to be\ncentralized.\n\u2022 If your replicas are distributed across different datacenters, there is no guarantee\nthat connections from different devices will be routed to the same datacenter.\n(For example, if the user\u2019s desktop computer uses the home broadband connec\u2010\ntion and their mobile device uses the cellular data network, the devices\u2019 network\nroutes may be completely different.) If your approach requires reading from the\nleader, you may first need to route requests from all of a user\u2019s devices to the\nsame datacenter.\nMonotonic Reads\nOur second example of an anomaly that can occur when reading from asynchronous\nfollowers is that it\u2019s possible for a user to see things moving backward in time.\nThis can happen if a user makes several reads from different replicas. For example,\nFigure 5-4 shows user 2345 making the same query twice, first to a follower with little\nlag, then to a follower with greater lag. (This scenario is quite likely if the user\nrefreshes a web page, and each request is routed to a random server.) The first query\nreturns a comment that was recently added by user 1234, but the second query"}
{"187": "ment. However, it\u2019s very confusing for user 2345 if they first see user 1234\u2019s comment\nappear, and then see it disappear again.\nFigure 5-4. A user first reads from a fresh replica, then from a stale replica. Time\nappears to go backward. To prevent this anomaly, we need monotonic reads.\nMonotonic reads [23] is a guarantee that this kind of anomaly does not happen. It\u2019s a\nlesser guarantee than strong consistency, but a stronger guarantee than eventual con\u2010\nsistency. When you read data, you may see an old value; monotonic reads only means\nthat if one user makes several reads in sequence, they will not see time go backward\u2014\ni.e., they will not read older data after having previously read newer data.\nOne way of achieving monotonic reads is to make sure that each user always makes\ntheir reads from the same replica (different users can read from different replicas).\nFor example, the replica can be chosen based on a hash of the user ID, rather than\nrandomly. However, if that replica fails, the user\u2019s queries will need to be rerouted to\nanother replica.\nConsistent Prefix Reads\nOur third example of replication lag anomalies concerns violation of causality. Imag\u2010\nine the following short dialog between Mr. Poons and Mrs. Cake:\nMr. Poons\nHow far into the future can you see, Mrs. Cake?"}
{"188": "There is a causal dependency between those two sentences: Mrs. Cake heard Mr.\nPoons\u2019s question and answered it.\nNow, imagine a third person is listening to this conversation through followers. The\nthings said by Mrs. Cake go through a follower with little lag, but the things said by\nMr. Poons have a longer replication lag (see Figure 5-5). This observer would hear\nthe following:\nMrs. Cake\nAbout ten seconds usually, Mr. Poons.\nMr. Poons\nHow far into the future can you see, Mrs. Cake?\nTo the observer it looks as though Mrs. Cake is answering the question before Mr.\nPoons has even asked it. Such psychic powers are impressive, but very confusing [25].\nFigure 5-5. If some partitions are replicated slower than others, an observer may see the\nanswer before they see the question.\nPreventing this kind of anomaly requires another type of guarantee: consistent prefix\nreads [23]. This guarantee says that if a sequence of writes happens in a certain order,\nthen anyone reading those writes will see them appear in the same order."}
{"189": "databases, different partitions operate independently, so there is no global ordering of\nwrites: when a user reads from the database, they may see some parts of the database\nin an older state and some in a newer state.\nOne solution is to make sure that any writes that are causally related to each other are\nwritten to the same partition\u2014but in some applications that cannot be done effi\u2010\nciently. There are also algorithms that explicitly keep track of causal dependencies, a\ntopic that we will return to in \u201cThe \u201chappens-before\u201d relationship and concurrency\u201d\non page 186.\nSolutions for Replication Lag\nWhen working with an eventually consistent system, it is worth thinking about how\nthe application behaves if the replication lag increases to several minutes or even\nhours. If the answer is \u201cno problem,\u201d that\u2019s great. However, if the result is a bad expe\u2010\nrience for users, it\u2019s important to design the system to provide a stronger guarantee,\nsuch as read-after-write. Pretending that replication is synchronous when in fact it is\nasynchronous is a recipe for problems down the line.\nAs discussed earlier, there are ways in which an application can provide a stronger\nguarantee than the underlying database\u2014for example, by performing certain kinds of\nreads on the leader. However, dealing with these issues in application code is com\u2010\nplex and easy to get wrong.\nIt would be better if application developers didn\u2019t have to worry about subtle replica\u2010\ntion issues and could just trust their databases to \u201cdo the right thing.\u201d This is why\ntransactions exist: they are a way for a database to provide stronger guarantees so that\nthe application can be simpler.\nSingle-node transactions have existed for a long time. However, in the move to dis\u2010\ntributed (replicated and partitioned) databases, many systems have abandoned them,\nclaiming that transactions are too expensive in terms of performance and availability,\nand asserting that eventual consistency is inevitable in a scalable system. There is\nsome truth in that statement, but it is overly simplistic, and we will develop a more\nnuanced view over the course of the rest of this book. We will return to the topic of\ntransactions in Chapters 7 and 9, and we will discuss some alternative mechanisms in\nPart III."}
{"190": "Multi-Leader Replication\nSo far in this chapter we have only considered replication architectures using a single\nleader. Although that is a common approach, there are interesting alternatives.\nLeader-based replication has one major downside: there is only one leader, and all\nwrites must go through it.iv If you can\u2019t connect to the leader for any reason, for\nexample due to a network interruption between you and the leader, you can\u2019t write to\nthe database.\nA natural extension of the leader-based replication model is to allow more than one\nnode to accept writes. Replication still happens in the same way: each node that pro\u2010\ncesses a write must forward that data change to all the other nodes. We call this a\nmulti-leader configuration (also known as master\u2013master or active/active replication).\nIn this setup, each leader simultaneously acts as a follower to the other leaders.\nUse Cases for Multi-Leader Replication\nIt rarely makes sense to use a multi-leader setup within a single datacenter, because\nthe benefits rarely outweigh the added complexity. However, there are some situa\u2010\ntions in which this configuration is reasonable.\nMulti-datacenter operation\nImagine you have a database with replicas in several different datacenters (perhaps so\nthat you can tolerate failure of an entire datacenter, or perhaps in order to be closer\nto your users). With a normal leader-based replication setup, the leader has to be in\none of the datacenters, and all writes must go through that datacenter.\nIn a multi-leader configuration, you can have a leader in each datacenter. Figure 5-6\nshows what this architecture might look like. Within each datacenter, regular leader\u2013\nfollower replication is used; between datacenters, each datacenter\u2019s leader replicates\nits changes to the leaders in other datacenters."}
{"191": "Figure 5-6. Multi-leader replication across multiple datacenters.\nLet\u2019s compare how the single-leader and multi-leader configurations fare in a multi-\ndatacenter deployment:\nPerformance\nIn a single-leader configuration, every write must go over the internet to the\ndatacenter with the leader. This can add significant latency to writes and might\ncontravene the purpose of having multiple datacenters in the first place. In a\nmulti-leader configuration, every write can be processed in the local datacenter\nand is replicated asynchronously to the other datacenters. Thus, the inter-\ndatacenter network delay is hidden from users, which means the perceived per\u2010\nformance may be better.\nTolerance of datacenter outages\nIn a single-leader configuration, if the datacenter with the leader fails, failover\ncan promote a follower in another datacenter to be leader. In a multi-leader con\u2010\nfiguration, each datacenter can continue operating independently of the others,\nand replication catches up when the failed datacenter comes back online.\nTolerance of network problems\nTraffic between datacenters usually goes over the public internet, which may be\nless reliable than the local network within a datacenter. A single-leader configu\u2010\nration is very sensitive to problems in this inter-datacenter link, because writes\nare made synchronously over this link. A multi-leader configuration with asyn\u2010\nchronous replication can usually tolerate network problems better: a temporary"}
{"192": "Some databases support multi-leader configurations by default, but it is also often\nimplemented with external tools, such as Tungsten Replicator for MySQL [26], BDR\nfor PostgreSQL [27], and GoldenGate for Oracle [19].\nAlthough multi-leader replication has advantages, it also has a big downside: the\nsame data may be concurrently modified in two different datacenters, and those write\nconflicts must be resolved (indicated as \u201cconflict resolution\u201d in Figure 5-6). We will\ndiscuss this issue in \u201cHandling Write Conflicts\u201d on page 171.\nAs multi-leader replication is a somewhat retrofitted feature in many databases, there\nare often subtle configuration pitfalls and surprising interactions with other database\nfeatures. For example, autoincrementing keys, triggers, and integrity constraints can\nbe problematic. For this reason, multi-leader replication is often considered danger\u2010\nous territory that should be avoided if possible [28].\nClients with offline operation\nAnother situation in which multi-leader replication is appropriate is if you have an\napplication that needs to continue to work while it is disconnected from the internet.\nFor example, consider the calendar apps on your mobile phone, your laptop, and\nother devices. You need to be able to see your meetings (make read requests) and\nenter new meetings (make write requests) at any time, regardless of whether your\ndevice currently has an internet connection. If you make any changes while you are\noffline, they need to be synced with a server and your other devices when the device\nis next online.\nIn this case, every device has a local database that acts as a leader (it accepts write\nrequests), and there is an asynchronous multi-leader replication process (sync)\nbetween the replicas of your calendar on all of your devices. The replication lag may\nbe hours or even days, depending on when you have internet access available.\nFrom an architectural point of view, this setup is essentially the same as multi-leader\nreplication between datacenters, taken to the extreme: each device is a \u201cdatacenter,\u201d\nand the network connection between them is extremely unreliable. As the rich his\u2010\ntory of broken calendar sync implementations demonstrates, multi-leader replication\nis a tricky thing to get right.\nThere are tools that aim to make this kind of multi-leader configuration easier. For\nexample, CouchDB is designed for this mode of operation [29].\nCollaborative editing"}
{"193": "We don\u2019t usually think of collaborative editing as a database replication problem, but\nit has a lot in common with the previously mentioned offline editing use case. When\none user edits a document, the changes are instantly applied to their local replica (the\nstate of the document in their web browser or client application) and asynchronously\nreplicated to the server and any other users who are editing the same document.\nIf you want to guarantee that there will be no editing conflicts, the application must\nobtain a lock on the document before a user can edit it. If another user wants to edit\nthe same document, they first have to wait until the first user has committed their\nchanges and released the lock. This collaboration model is equivalent to single-leader\nreplication with transactions on the leader.\nHowever, for faster collaboration, you may want to make the unit of change very\nsmall (e.g., a single keystroke) and avoid locking. This approach allows multiple users\nto edit simultaneously, but it also brings all the challenges of multi-leader replication,\nincluding requiring conflict resolution [32].\nHandling Write Conflicts\nThe biggest problem with multi-leader replication is that write conflicts can occur,\nwhich means that conflict resolution is required.\nFor example, consider a wiki page that is simultaneously being edited by two users, as\nshown in Figure 5-7. User 1 changes the title of the page from A to B, and user 2\nchanges the title from A to C at the same time. Each user\u2019s change is successfully\napplied to their local leader. However, when the changes are asynchronously replica\u2010\nted, a conflict is detected [33]. This problem does not occur in a single-leader data\u2010\nbase."}
{"194": "Synchronous versus asynchronous conflict detection\nIn a single-leader database, the second writer will either block and wait for the first\nwrite to complete, or abort the second write transaction, forcing the user to retry the\nwrite. On the other hand, in a multi-leader setup, both writes are successful, and the\nconflict is only detected asynchronously at some later point in time. At that time, it\nmay be too late to ask the user to resolve the conflict.\nIn principle, you could make the conflict detection synchronous\u2014i.e., wait for the\nwrite to be replicated to all replicas before telling the user that the write was success\u2010\nful. However, by doing so, you would lose the main advantage of multi-leader repli\u2010\ncation: allowing each replica to accept writes independently. If you want synchronous\nconflict detection, you might as well just use single-leader replication.\nConflict avoidance\nThe simplest strategy for dealing with conflicts is to avoid them: if the application can\nensure that all writes for a particular record go through the same leader, then con\u2010\nflicts cannot occur. Since many implementations of multi-leader replication handle\nconflicts quite poorly, avoiding conflicts is a frequently recommended approach [34].\nFor example, in an application where a user can edit their own data, you can ensure\nthat requests from a particular user are always routed to the same datacenter and use\nthe leader in that datacenter for reading and writing. Different users may have differ\u2010\nent \u201chome\u201d datacenters (perhaps picked based on geographic proximity to the user),\nbut from any one user\u2019s point of view the configuration is essentially single-leader.\nHowever, sometimes you might want to change the designated leader for a record\u2014\nperhaps because one datacenter has failed and you need to reroute traffic to another\ndatacenter, or perhaps because a user has moved to a different location and is now\ncloser to a different datacenter. In this situation, conflict avoidance breaks down, and\nyou have to deal with the possibility of concurrent writes on different leaders.\nConverging toward a consistent state\nA single-leader database applies writes in a sequential order: if there are several\nupdates to the same field, the last write determines the final value of the field.\nIn a multi-leader configuration, there is no defined ordering of writes, so it\u2019s not clear\nwhat the final value should be. In Figure 5-7, at leader 1 the title is first updated to B\nand then to C; at leader 2 it is first updated to C and then to B. Neither order is \u201cmore\ncorrect\u201d than the other."}
{"195": "convergent way, which means that all replicas must arrive at the same final value\nwhen all changes have been replicated.\nThere are various ways of achieving convergent conflict resolution:\n\u2022 Give each write a unique ID (e.g., a timestamp, a long random number, a UUID,\nor a hash of the key and value), pick the write with the highest ID as the winner,\nand throw away the other writes. If a timestamp is used, this technique is known\nas last write wins (LWW). Although this approach is popular, it is dangerously\nprone to data loss [35]. We will discuss LWW in more detail at the end of this\nchapter (\u201cDetecting Concurrent Writes\u201d on page 184).\n\u2022 Give each replica a unique ID, and let writes that originated at a higher-\nnumbered replica always take precedence over writes that originated at a lower-\nnumbered replica. This approach also implies data loss.\n\u2022 Somehow merge the values together\u2014e.g., order them alphabetically and then\nconcatenate them (in Figure 5-7, the merged title might be something like\n\u201cB/C\u201d).\n\u2022 Record the conflict in an explicit data structure that preserves all information,\nand write application code that resolves the conflict at some later time (perhaps\nby prompting the user).\nCustom conflict resolution logic\nAs the most appropriate way of resolving a conflict may depend on the application,\nmost multi-leader replication tools let you write conflict resolution logic using appli\u2010\ncation code. That code may be executed on write or on read:\nOn write\nAs soon as the database system detects a conflict in the log of replicated changes,\nit calls the conflict handler. For example, Bucardo allows you to write a snippet of\nPerl for this purpose. This handler typically cannot prompt a user\u2014it runs in a\nbackground process and it must execute quickly.\nOn read\nWhen a conflict is detected, all the conflicting writes are stored. The next time\nthe data is read, these multiple versions of the data are returned to the applica\u2010\ntion. The application may prompt the user or automatically resolve the conflict,\nand write the result back to the database. CouchDB works this way, for example.\nNote that conflict resolution usually applies at the level of an individual row or docu\u2010"}
{"196": "Automatic Conflict Resolution\nConflict resolution rules can quickly become complicated, and custom code can be\nerror-prone. Amazon is a frequently cited example of surprising effects due to a con\u2010\nflict resolution handler: for some time, the conflict resolution logic on the shopping\ncart would preserve items added to the cart, but not items removed from the cart.\nThus, customers would sometimes see items reappearing in their carts even though\nthey had previously been removed [37].\nThere has been some interesting research into automatically resolving conflicts\ncaused by concurrent data modifications. A few lines of research are worth mention\u2010\ning:\n\u2022 Conflict-free replicated datatypes (CRDTs) [32, 38] are a family of data structures\nfor sets, maps, ordered lists, counters, etc. that can be concurrently edited by\nmultiple users, and which automatically resolve conflicts in sensible ways. Some\nCRDTs have been implemented in Riak 2.0 [39, 40].\n\u2022 Mergeable persistent data structures [41] track history explicitly, similarly to the\nGit version control system, and use a three-way merge function (whereas CRDTs\nuse two-way merges).\n\u2022 Operational transformation [42] is the conflict resolution algorithm behind col\u2010\nlaborative editing applications such as Etherpad [30] and Google Docs [31]. It\nwas designed particularly for concurrent editing of an ordered list of items, such\nas the list of characters that constitute a text document.\nImplementations of these algorithms in databases are still young, but it\u2019s likely that\nthey will be integrated into more replicated data systems in the future. Automatic\nconflict resolution could make multi-leader data synchronization much simpler for\napplications to deal with.\nWhat is a conflict?\nSome kinds of conflict are obvious. In the example in Figure 5-7, two writes concur\u2010\nrently modified the same field in the same record, setting it to two different values.\nThere is little doubt that this is a conflict.\nOther kinds of conflict can be more subtle to detect. For example, consider a meeting\nroom booking system: it tracks which room is booked by which group of people at\nwhich time. This application needs to ensure that each room is only booked by one"}
{"197": "allowing a user to make a booking, there can be a conflict if the two bookings are\nmade on two different leaders.\nThere isn\u2019t a quick ready-made answer, but in the following chapters we will trace a\npath toward a good understanding of this problem. We will see some more examples\nof conflicts in Chapter 7, and in Chapter 12 we will discuss scalable approaches for\ndetecting and resolving conflicts in a replicated system.\nMulti-Leader Replication Topologies\nA replication topology describes the communication paths along which writes are\npropagated from one node to another. If you have two leaders, like in Figure 5-7,\nthere is only one plausible topology: leader 1 must send all of its writes to leader 2,\nand vice versa. With more than two leaders, various different topologies are possible.\nSome examples are illustrated in Figure 5-8.\nFigure 5-8. Three example topologies in which multi-leader replication can be set up.\nThe most general topology is all-to-all (Figure 5-8 [c]), in which every leader sends its\nwrites to every other leader. However, more restricted topologies are also used: for\nexample, MySQL by default supports only a circular topology [34], in which each\nnode receives writes from one node and forwards those writes (plus any writes of its\nown) to one other node. Another popular topology has the shape of a star:v one desig\u2010\nnated root node forwards writes to all of the other nodes. The star topology can be\ngeneralized to a tree.\nIn circular and star topologies, a write may need to pass through several nodes before\nit reaches all replicas. Therefore, nodes need to forward data changes they receive\nfrom other nodes. To prevent infinite replication loops, each node is given a unique\nidentifier, and in the replication log, each write is tagged with the identifiers of all the\nnodes it has passed through [43]. When a node receives a data change that is tagged"}
{"198": "with its own identifier, that data change is ignored, because the node knows that it\nhas already been processed.\nA problem with circular and star topologies is that if just one node fails, it can inter\u2010\nrupt the flow of replication messages between other nodes, causing them to be unable\nto communicate until the node is fixed. The topology could be reconfigured to work\naround the failed node, but in most deployments such reconfiguration would have to\nbe done manually. The fault tolerance of a more densely connected topology (such as\nall-to-all) is better because it allows messages to travel along different paths, avoiding\na single point of failure.\nOn the other hand, all-to-all topologies can have issues too. In particular, some net\u2010\nwork links may be faster than others (e.g., due to network congestion), with the result\nthat some replication messages may \u201covertake\u201d others, as illustrated in Figure 5-9.\nFigure 5-9. With multi-leader replication, writes may arrive in the wrong order at some\nreplicas.\nIn Figure 5-9, client A inserts a row into a table on leader 1, and client B updates that\nrow on leader 3. However, leader 2 may receive the writes in a different order: it may\nfirst receive the update (which, from its point of view, is an update to a row that does\nnot exist in the database) and only later receive the corresponding insert (which\nshould have preceded the update).\nThis is a problem of causality, similar to the one we saw in \u201cConsistent Prefix Reads\u201d"}
{"199": "every write is not sufficient, because clocks cannot be trusted to be sufficiently in sync\nto correctly order these events at leader 2 (see Chapter 8).\nTo order these events correctly, a technique called version vectors can be used, which\nwe will discuss later in this chapter (see \u201cDetecting Concurrent Writes\u201d on page 184).\nHowever, conflict detection techniques are poorly implemented in many multi-leader\nreplication systems. For example, at the time of writing, PostgreSQL BDR does not\nprovide causal ordering of writes [27], and Tungsten Replicator for MySQL doesn\u2019t\neven try to detect conflicts [34].\nIf you are using a system with multi-leader replication, it is worth being aware of\nthese issues, carefully reading the documentation, and thoroughly testing your data\u2010\nbase to ensure that it really does provide the guarantees you believe it to have.\nLeaderless Replication\nThe replication approaches we have discussed so far in this chapter\u2014single-leader\nand multi-leader replication\u2014are based on the idea that a client sends a write request\nto one node (the leader), and the database system takes care of copying that write to\nthe other replicas. A leader determines the order in which writes should be processed,\nand followers apply the leader\u2019s writes in the same order.\nSome data storage systems take a different approach, abandoning the concept of a\nleader and allowing any replica to directly accept writes from clients. Some of the ear\u2010\nliest replicated data systems were leaderless [1, 44], but the idea was mostly forgotten\nduring the era of dominance of relational databases. It once again became a fashiona\u2010\nble architecture for databases after Amazon used it for its in-house Dynamo system\n[37].vi Riak, Cassandra, and Voldemort are open source datastores with leaderless\nreplication models inspired by Dynamo, so this kind of database is also known as\nDynamo-style.\nIn some leaderless implementations, the client directly sends its writes to several rep\u2010\nlicas, while in others, a coordinator node does this on behalf of the client. However,\nunlike a leader database, that coordinator does not enforce a particular ordering of\nwrites. As we shall see, this difference in design has profound consequences for the\nway the database is used.\nWriting to the Database When a Node Is Down\nImagine you have a database with three replicas, and one of the replicas is currently\nunavailable\u2014perhaps it is being rebooted to install a system update. In a leader-based"}
{"200": "configuration, if you want to continue processing writes, you may need to perform a\nfailover (see \u201cHandling Node Outages\u201d on page 156).\nOn the other hand, in a leaderless configuration, failover does not exist. Figure 5-10\nshows what happens: the client (user 1234) sends the write to all three replicas in par\u2010\nallel, and the two available replicas accept the write but the unavailable replica misses\nit. Let\u2019s say that it\u2019s sufficient for two out of three replicas to acknowledge the write:\nafter user 1234 has received two ok responses, we consider the write to be successful.\nThe client simply ignores the fact that one of the replicas missed the write.\nFigure 5-10. A quorum write, quorum read, and read repair after a node outage.\nNow imagine that the unavailable node comes back online, and clients start reading\nfrom it. Any writes that happened while the node was down are missing from that\nnode. Thus, if you read from that node, you may get stale (outdated) values as\nresponses.\nTo solve that problem, when a client reads from the database, it doesn\u2019t just send its\nrequest to one replica: read requests are also sent to several nodes in parallel. The cli\u2010\nent may get different responses from different nodes; i.e., the up-to-date value from\none node and a stale value from another. Version numbers are used to determine\nwhich value is newer (see \u201cDetecting Concurrent Writes\u201d on page 184).\nRead repair and anti-entropy"}
{"201": "Two mechanisms are often used in Dynamo-style datastores:\nRead repair\nWhen a client makes a read from several nodes in parallel, it can detect any stale\nresponses. For example, in Figure 5-10, user 2345 gets a version 6 value from rep\u2010\nlica 3 and a version 7 value from replicas 1 and 2. The client sees that replica 3\nhas a stale value and writes the newer value back to that replica. This approach\nworks well for values that are frequently read.\nAnti-entropy process\nIn addition, some datastores have a background process that constantly looks for\ndifferences in the data between replicas and copies any missing data from one\nreplica to another. Unlike the replication log in leader-based replication, this\nanti-entropy process does not copy writes in any particular order, and there may\nbe a significant delay before data is copied.\nNot all systems implement both of these; for example, Voldemort currently does not\nhave an anti-entropy process. Note that without an anti-entropy process, values that\nare rarely read may be missing from some replicas and thus have reduced durability,\nbecause read repair is only performed when a value is read by the application.\nQuorums for reading and writing\nIn the example of Figure 5-10, we considered the write to be successful even though it\nwas only processed on two out of three replicas. What if only one out of three replicas\naccepted the write? How far can we push this?\nIf we know that every successful write is guaranteed to be present on at least two out\nof three replicas, that means at most one replica can be stale. Thus, if we read from at\nleast two replicas, we can be sure that at least one of the two is up to date. If the third\nreplica is down or slow to respond, reads can nevertheless continue returning an up-\nto-date value.\nMore generally, if there are n replicas, every write must be confirmed by w nodes to\nbe considered successful, and we must query at least r nodes for each read. (In our\nexample, n = 3, w = 2, r = 2.) As long as w + r > n, we expect to get an up-to-date\nvalue when reading, because at least one of the r nodes we\u2019re reading from must be\nup to date. Reads and writes that obey these r and w values are called quorum reads\nand writes [44].vii You can think of r and w as the minimum number of votes required\nfor the read or write to be valid."}
{"202": "In Dynamo-style databases, the parameters n, w, and r are typically configurable. A\ncommon choice is to make n an odd number (typically 3 or 5) and to set w = r =\n(n + 1) / 2 (rounded up). However, you can vary the numbers as you see fit. For\nexample, a workload with few writes and many reads may benefit from setting w = n\nand r = 1. This makes reads faster, but has the disadvantage that just one failed node\ncauses all database writes to fail.\nThere may be more than n nodes in the cluster, but any given value\nis stored only on n nodes. This allows the dataset to be partitioned,\nsupporting datasets that are larger than you can fit on one node.\nWe will return to partitioning in Chapter 6.\nThe quorum condition, w + r > n, allows the system to tolerate unavailable nodes as\nfollows:\n\u2022 If w < n, we can still process writes if a node is unavailable.\n\u2022 If r < n, we can still process reads if a node is unavailable.\n\u2022 With n = 3, w = 2, r = 2 we can tolerate one unavailable node.\n\u2022 With n = 5, w = 3, r = 3 we can tolerate two unavailable nodes. This case is illus\u2010\ntrated in Figure 5-11.\n\u2022 Normally, reads and writes are always sent to all n replicas in parallel. The\nparameters w and r determine how many nodes we wait for\u2014i.e., how many of\nthe n nodes need to report success before we consider the read or write to be suc\u2010\ncessful."}
{"203": "If fewer than the required w or r nodes are available, writes or reads return an error.\nA node could be unavailable for many reasons: because the node is down (crashed,\npowered down), due to an error executing the operation (can\u2019t write because the disk\nis full), due to a network interruption between the client and the node, or for any\nnumber of other reasons. We only care whether the node returned a successful\nresponse and don\u2019t need to distinguish between different kinds of fault.\nLimitations of Quorum Consistency\nIf you have n replicas, and you choose w and r such that w + r > n, you can generally\nexpect every read to return the most recent value written for a key. This is the case\nbecause the set of nodes to which you\u2019ve written and the set of nodes from which\nyou\u2019ve read must overlap. That is, among the nodes you read there must be at least\none node with the latest value (illustrated in Figure 5-11).\nOften, r and w are chosen to be a majority (more than n/2) of nodes, because that\nensures w + r > n while still tolerating up to n/2 node failures. But quorums are not\nnecessarily majorities\u2014it only matters that the sets of nodes used by the read and\nwrite operations overlap in at least one node. Other quorum assignments are possi\u2010\nble, which allows some flexibility in the design of distributed algorithms [45].\nYou may also set w and r to smaller numbers, so that w + r \u2264 n (i.e., the quorum con\u2010\ndition is not satisfied). In this case, reads and writes will still be sent to n nodes, but a\nsmaller number of successful responses is required for the operation to succeed.\nWith a smaller w and r you are more likely to read stale values, because it\u2019s more\nlikely that your read didn\u2019t include the node with the latest value. On the upside, this\nconfiguration allows lower latency and higher availability: if there is a network inter\u2010\nruption and many replicas become unreachable, there\u2019s a higher chance that you can\ncontinue processing reads and writes. Only after the number of reachable replicas\nfalls below w or r does the database become unavailable for writing or reading,\nrespectively.\nHowever, even with w + r > n, there are likely to be edge cases where stale values are\nreturned. These depend on the implementation, but possible scenarios include:\n\u2022 If a sloppy quorum is used (see \u201cSloppy Quorums and Hinted Handoff\u201d on page\n183), the w writes may end up on different nodes than the r reads, so there is no\nlonger a guaranteed overlap between the r nodes and the w nodes [46].\n\u2022 If two writes occur concurrently, it is not clear which one happened first. In this\ncase, the only safe solution is to merge the concurrent writes (see \u201cHandling"}
{"204": "\u2022 If a write happens concurrently with a read, the write may be reflected on only\nsome of the replicas. In this case, it\u2019s undetermined whether the read returns the\nold or the new value.\n\u2022 If a write succeeded on some replicas but failed on others (for example because\nthe disks on some nodes are full), and overall succeeded on fewer than w replicas,\nit is not rolled back on the replicas where it succeeded. This means that if a write\nwas reported as failed, subsequent reads may or may not return the value from\nthat write [47].\n\u2022 If a node carrying a new value fails, and its data is restored from a replica carry\u2010\ning an old value, the number of replicas storing the new value may fall below w,\nbreaking the quorum condition.\n\u2022 Even if everything is working correctly, there are edge cases in which you can get\nunlucky with the timing, as we shall see in \u201cLinearizability and quorums\u201d on\npage 334.\nThus, although quorums appear to guarantee that a read returns the latest written\nvalue, in practice it is not so simple. Dynamo-style databases are generally optimized\nfor use cases that can tolerate eventual consistency. The parameters w and r allow you\nto adjust the probability of stale values being read, but it\u2019s wise to not take them as\nabsolute guarantees.\nIn particular, you usually do not get the guarantees discussed in \u201cProblems with Rep\u2010\nlication Lag\u201d on page 161 (reading your writes, monotonic reads, or consistent prefix\nreads), so the previously mentioned anomalies can occur in applications. Stronger\nguarantees generally require transactions or consensus. We will return to these topics\nin Chapter 7 and Chapter 9.\nMonitoring staleness\nFrom an operational perspective, it\u2019s important to monitor whether your databases\nare returning up-to-date results. Even if your application can tolerate stale reads, you\nneed to be aware of the health of your replication. If it falls behind significantly, it\nshould alert you so that you can investigate the cause (for example, a problem in the\nnetwork or an overloaded node).\nFor leader-based replication, the database typically exposes metrics for the replication\nlag, which you can feed into a monitoring system. This is possible because writes are\napplied to the leader and to followers in the same order, and each node has a position\nin the replication log (the number of writes it has applied locally). By subtracting a\nfollower\u2019s current position from the leader\u2019s current position, you can measure the"}
{"205": "only uses read repair (no anti-entropy), there is no limit to how old a value might be\n\u2014if a value is only infrequently read, the value returned by a stale replica may be\nancient.\nThere has been some research on measuring replica staleness in databases with lead\u2010\nerless replication and predicting the expected percentage of stale reads depending on\nthe parameters n, w, and r [48]. This is unfortunately not yet common practice, but it\nwould be good to include staleness measurements in the standard set of metrics for\ndatabases. Eventual consistency is a deliberately vague guarantee, but for operability\nit\u2019s important to be able to quantify \u201ceventual.\u201d\nSloppy Quorums and Hinted Handoff\nDatabases with appropriately configured quorums can tolerate the failure of individ\u2010\nual nodes without the need for failover. They can also tolerate individual nodes going\nslow, because requests don\u2019t have to wait for all n nodes to respond\u2014they can return\nwhen w or r nodes have responded. These characteristics make databases with leader\u2010\nless replication appealing for use cases that require high availability and low latency,\nand that can tolerate occasional stale reads.\nHowever, quorums (as described so far) are not as fault-tolerant as they could be. A\nnetwork interruption can easily cut off a client from a large number of database\nnodes. Although those nodes are alive, and other clients may be able to connect to\nthem, to a client that is cut off from the database nodes, they might as well be dead. In\nthis situation, it\u2019s likely that fewer than w or r reachable nodes remain, so the client\ncan no longer reach a quorum.\nIn a large cluster (with significantly more than n nodes) it\u2019s likely that the client can\nconnect to some database nodes during the network interruption, just not to the\nnodes that it needs to assemble a quorum for a particular value. In that case, database\ndesigners face a trade-off:\n\u2022 Is it better to return errors to all requests for which we cannot reach a quorum of\nw or r nodes?\n\u2022 Or should we accept writes anyway, and write them to some nodes that are\nreachable but aren\u2019t among the n nodes on which the value usually lives?\nThe latter is known as a sloppy quorum [37]: writes and reads still require w and r\nsuccessful responses, but those may include nodes that are not among the designated\nn \u201chome\u201d nodes for a value. By analogy, if you lock yourself out of your house, you\nmay knock on the neighbor\u2019s door and ask whether you may stay on their couch tem\u2010"}
{"206": "called hinted handoff. (Once you find the keys to your house again, your neighbor\npolitely asks you to get off their couch and go home.)\nSloppy quorums are particularly useful for increasing write availability: as long as any\nw nodes are available, the database can accept writes. However, this means that even\nwhen w + r > n, you cannot be sure to read the latest value for a key, because the\nlatest value may have been temporarily written to some nodes outside of n [47].\nThus, a sloppy quorum actually isn\u2019t a quorum at all in the traditional sense. It\u2019s only\nan assurance of durability, namely that the data is stored on w nodes somewhere.\nThere is no guarantee that a read of r nodes will see it until the hinted handoff has\ncompleted.\nSloppy quorums are optional in all common Dynamo implementations. In Riak they\nare enabled by default, and in Cassandra and Voldemort they are disabled by default\n[46, 49, 50].\nMulti-datacenter operation\nWe previously discussed cross-datacenter replication as a use case for multi-leader\nreplication (see \u201cMulti-Leader Replication\u201d on page 168). Leaderless replication is\nalso suitable for multi-datacenter operation, since it is designed to tolerate conflicting\nconcurrent writes, network interruptions, and latency spikes.\nCassandra and Voldemort implement their multi-datacenter support within the nor\u2010\nmal leaderless model: the number of replicas n includes nodes in all datacenters, and\nin the configuration you can specify how many of the n replicas you want to have in\neach datacenter. Each write from a client is sent to all replicas, regardless of datacen\u2010\nter, but the client usually only waits for acknowledgment from a quorum of nodes\nwithin its local datacenter so that it is unaffected by delays and interruptions on the\ncross-datacenter link. The higher-latency writes to other datacenters are often config\u2010\nured to happen asynchronously, although there is some flexibility in the configura\u2010\ntion [50, 51].\nRiak keeps all communication between clients and database nodes local to one data\u2010\ncenter, so n describes the number of replicas within one datacenter. Cross-datacenter\nreplication between database clusters happens asynchronously in the background, in\na style that is similar to multi-leader replication [52].\nDetecting Concurrent Writes\nDynamo-style databases allow several clients to concurrently write to the same key,\nwhich means that conflicts will occur even if strict quorums are used. The situation is"}
{"207": "The problem is that events may arrive in a different order at different nodes, due to\nvariable network delays and partial failures. For example, Figure 5-12 shows two cli\u2010\nents, A and B, simultaneously writing to a key X in a three-node datastore:\n\u2022 Node 1 receives the write from A, but never receives the write from B due to a\ntransient outage.\n\u2022 Node 2 first receives the write from A, then the write from B.\n\u2022 Node 3 first receives the write from B, then the write from A.\nFigure 5-12. Concurrent writes in a Dynamo-style datastore: there is no well-defined\nordering.\nIf each node simply overwrote the value for a key whenever it received a write request\nfrom a client, the nodes would become permanently inconsistent, as shown by the\nfinal get request in Figure 5-12: node 2 thinks that the final value of X is B, whereas\nthe other nodes think that the value is A.\nIn order to become eventually consistent, the replicas should converge toward the\nsame value. How do they do that? One might hope that replicated databases would\nhandle this automatically, but unfortunately most implementations are quite poor: if\nyou want to avoid losing data, you\u2014the application developer\u2014need to know a lot\nabout the internals of your database\u2019s conflict handling.\nWe briefly touched on some techniques for conflict resolution in \u201cHandling Write\nConflicts\u201d on page 171. Before we wrap up this chapter, let\u2019s explore the issue in a bit"}
{"208": "Last write wins (discarding concurrent writes)\nOne approach for achieving eventual convergence is to declare that each replica need\nonly store the most \u201crecent\u201d value and allow \u201colder\u201d values to be overwritten and dis\u2010\ncarded. Then, as long as we have some way of unambiguously determining which\nwrite is more \u201crecent,\u201d and every write is eventually copied to every replica, the repli\u2010\ncas will eventually converge to the same value.\nAs indicated by the quotes around \u201crecent,\u201d this idea is actually quite misleading. In\nthe example of Figure 5-12, neither client knew about the other one when it sent its\nwrite requests to the database nodes, so it\u2019s not clear which one happened first. In\nfact, it doesn\u2019t really make sense to say that either happened \u201cfirst\u201d: we say the writes\nare concurrent, so their order is undefined.\nEven though the writes don\u2019t have a natural ordering, we can force an arbitrary order\non them. For example, we can attach a timestamp to each write, pick the biggest\ntimestamp as the most \u201crecent,\u201d and discard any writes with an earlier timestamp.\nThis conflict resolution algorithm, called last write wins (LWW), is the only sup\u2010\nported conflict resolution method in Cassandra [53], and an optional feature in Riak\n[35].\nLWW achieves the goal of eventual convergence, but at the cost of durability: if there\nare several concurrent writes to the same key, even if they were all reported as suc\u2010\ncessful to the client (because they were written to w replicas), only one of the writes\nwill survive and the others will be silently discarded. Moreover, LWW may even drop\nwrites that are not concurrent, as we shall discuss in \u201cTimestamps for ordering\nevents\u201d on page 291.\nThere are some situations, such as caching, in which lost writes are perhaps accepta\u2010\nble. If losing data is not acceptable, LWW is a poor choice for conflict resolution.\nThe only safe way of using a database with LWW is to ensure that a key is only writ\u2010\nten once and thereafter treated as immutable, thus avoiding any concurrent updates\nto the same key. For example, a recommended way of using Cassandra is to use a\nUUID as the key, thus giving each write operation a unique key [53].\nThe \u201chappens-before\u201d relationship and concurrency\nHow do we decide whether two operations are concurrent or not? To develop an\nintuition, let\u2019s look at some examples:\n\u2022 In Figure 5-9, the two writes are not concurrent: A\u2019s insert happens before B\u2019s\nincrement, because the value incremented by B is the value inserted by A. In"}
{"209": "\u2022 On the other hand, the two writes in Figure 5-12 are concurrent: when each cli\u2010\nent starts the operation, it does not know that another client is also performing\nan operation on the same key. Thus, there is no causal dependency between the\noperations.\nAn operation A happens before another operation B if B knows about A, or depends\non A, or builds upon A in some way. Whether one operation happens before another\noperation is the key to defining what concurrency means. In fact, we can simply say\nthat two operations are concurrent if neither happens before the other (i.e., neither\nknows about the other) [54].\nThus, whenever you have two operations A and B, there are three possibilities: either\nA happened before B, or B happened before A, or A and B are concurrent. What we\nneed is an algorithm to tell us whether two operations are concurrent or not. If one\noperation happened before another, the later operation should overwrite the earlier\noperation, but if the operations are concurrent, we have a conflict that needs to be\nresolved.\nConcurrency, Time, and Relativity\nIt may seem that two operations should be called concurrent if they occur \u201cat the\nsame time\u201d\u2014but in fact, it is not important whether they literally overlap in time.\nBecause of problems with clocks in distributed systems, it is actually quite difficult to\ntell whether two things happened at exactly the same time\u2014an issue we will discuss\nin more detail in Chapter 8.\nFor defining concurrency, exact time doesn\u2019t matter: we simply call two operations\nconcurrent if they are both unaware of each other, regardless of the physical time at\nwhich they occurred. People sometimes make a connection between this principle\nand the special theory of relativity in physics [54], which introduced the idea that\ninformation cannot travel faster than the speed of light. Consequently, two events\nthat occur some distance apart cannot possibly affect each other if the time between\nthe events is shorter than the time it takes light to travel the distance between them.\nIn computer systems, two operations might be concurrent even though the speed of\nlight would in principle have allowed one operation to affect the other. For example,\nif the network was slow or interrupted at the time, two operations can occur some\ntime apart and still be concurrent, because the network problems prevented one\noperation from being able to know about the other.\nCapturing the happens-before relationship"}
{"210": "base that has only one replica. Once we have worked out how to do this on a single\nreplica, we can generalize the approach to a leaderless database with multiple replicas.\nFigure 5-13 shows two clients concurrently adding items to the same shopping cart.\n(If that example strikes you as too inane, imagine instead two air traffic controllers\nconcurrently adding aircraft to the sector they are tracking.) Initially, the cart is\nempty. Between them, the clients make five writes to the database:\n1. Client 1 adds milk to the cart. This is the first write to that key, so the server suc\u2010\ncessfully stores it and assigns it version 1. The server also echoes the value back\nto the client, along with the version number.\n2. Client 2 adds eggs to the cart, not knowing that client 1 concurrently added milk\n(client 2 thought that its eggs were the only item in the cart). The server assigns\nversion 2 to this write, and stores eggs and milk as two separate values. It then\nreturns both values to the client, along with the version number of 2.\n3. Client 1, oblivious to client 2\u2019s write, wants to add flour to the cart, so it thinks\nthe current cart contents should be [milk, flour]. It sends this value to the\nserver, along with the version number 1 that the server gave client 1 previously.\nThe server can tell from the version number that the write of [milk, flour]\nsupersedes the prior value of [milk] but that it is concurrent with [eggs]. Thus,\nthe server assigns version 3 to [milk, flour], overwrites the version 1 value\n[milk], but keeps the version 2 value [eggs] and returns both remaining values\nto the client.\n4. Meanwhile, client 2 wants to add ham to the cart, unaware that client 1 just added\nflour. Client 2 received the two values [milk] and [eggs] from the server in the\nlast response, so the client now merges those values and adds ham to form a new\nvalue, [eggs, milk, ham]. It sends that value to the server, along with the previ\u2010\nous version number 2. The server detects that version 2 overwrites [eggs] but is\nconcurrent with [milk, flour], so the two remaining values are [milk, flour]\nwith version 3, and [eggs, milk, ham] with version 4.\n5. Finally, client 1 wants to add bacon. It previously received [milk, flour] and\n[eggs] from the server at version 3, so it merges those, adds bacon, and sends the\nfinal value [milk, flour, eggs, bacon] to the server, along with the version\nnumber 3. This overwrites [milk, flour] (note that [eggs] was already over\u2010\nwritten in the last step) but is concurrent with [eggs, milk, ham], so the server\nkeeps those two concurrent values."}
{"211": "Figure 5-13. Capturing causal dependencies between two clients concurrently editing a\nshopping cart.\nThe dataflow between the operations in Figure 5-13 is illustrated graphically in\nFigure 5-14. The arrows indicate which operation happened before which other oper\u2010\nation, in the sense that the later operation knew about or depended on the earlier one.\nIn this example, the clients are never fully up to date with the data on the server, since\nthere is always another operation going on concurrently. But old versions of the value\ndo get overwritten eventually, and no writes are lost.\nFigure 5-14. Graph of causal dependencies in Figure 5-13.\nNote that the server can determine whether two operations are concurrent by looking\nat the version numbers\u2014it does not need to interpret the value itself (so the value\ncould be any data structure). The algorithm works as follows:"}
{"212": "\u2022 The server maintains a version number for every key, increments the version\nnumber every time that key is written, and stores the new version number along\nwith the value written.\n\u2022 When a client reads a key, the server returns all values that have not been over\u2010\nwritten, as well as the latest version number. A client must read a key before\nwriting.\n\u2022 When a client writes a key, it must include the version number from the prior\nread, and it must merge together all values that it received in the prior read. (The\nresponse from a write request can be like a read, returning all current values,\nwhich allows us to chain several writes like in the shopping cart example.)\n\u2022 When the server receives a write with a particular version number, it can over\u2010\nwrite all values with that version number or below (since it knows that they have\nbeen merged into the new value), but it must keep all values with a higher ver\u2010\nsion number (because those values are concurrent with the incoming write).\nWhen a write includes the version number from a prior read, that tells us which pre\u2010\nvious state the write is based on. If you make a write without including a version\nnumber, it is concurrent with all other writes, so it will not overwrite anything\u2014it\nwill just be returned as one of the values on subsequent reads.\nMerging concurrently written values\nThis algorithm ensures that no data is silently dropped, but it unfortunately requires\nthat the clients do some extra work: if several operations happen concurrently, clients\nhave to clean up afterward by merging the concurrently written values. Riak calls\nthese concurrent values siblings.\nMerging sibling values is essentially the same problem as conflict resolution in multi-\nleader replication, which we discussed previously (see \u201cHandling Write Conflicts\u201d on\npage 171). A simple approach is to just pick one of the values based on a version\nnumber or timestamp (last write wins), but that implies losing data. So, you may\nneed to do something more intelligent in application code.\nWith the example of a shopping cart, a reasonable approach to merging siblings is to\njust take the union. In Figure 5-14, the two final siblings are [milk, flour, eggs,\nbacon] and [eggs, milk, ham]; note that milk and eggs appear in both, even\nthough they were each only written once. The merged value might be something like\n[milk, flour, eggs, bacon, ham], without duplicates.\nHowever, if you want to allow people to also remove things from their carts, and not"}
{"213": "lem, an item cannot simply be deleted from the database when it is removed; instead,\nthe system must leave a marker with an appropriate version number to indicate that\nthe item has been removed when merging siblings. Such a deletion marker is known\nas a tombstone. (We previously saw tombstones in the context of log compaction in\n\u201cHash Indexes\u201d on page 72.)\nAs merging siblings in application code is complex and error-prone, there are some\nefforts to design data structures that can perform this merging automatically, as dis\u2010\ncussed in \u201cAutomatic Conflict Resolution\u201d on page 174. For example, Riak\u2019s datatype\nsupport uses a family of data structures called CRDTs [38, 39, 55] that can automati\u2010\ncally merge siblings in sensible ways, including preserving deletions.\nVersion vectors\nThe example in Figure 5-13 used only a single replica. How does the algorithm\nchange when there are multiple replicas, but no leader?\nFigure 5-13 uses a single version number to capture dependencies between opera\u2010\ntions, but that is not sufficient when there are multiple replicas accepting writes con\u2010\ncurrently. Instead, we need to use a version number per replica as well as per key.\nEach replica increments its own version number when processing a write, and also\nkeeps track of the version numbers it has seen from each of the other replicas. This\ninformation indicates which values to overwrite and which values to keep as siblings.\nThe collection of version numbers from all the replicas is called a version vector [56].\nA few variants of this idea are in use, but the most interesting is probably the dotted\nversion vector [57], which is used in Riak 2.0 [58, 59]. We won\u2019t go into the details,\nbut the way it works is quite similar to what we saw in our cart example.\nLike the version numbers in Figure 5-13, version vectors are sent from the database\nreplicas to clients when values are read, and need to be sent back to the database\nwhen a value is subsequently written. (Riak encodes the version vector as a string that\nit calls causal context.) The version vector allows the database to distinguish between\noverwrites and concurrent writes.\nAlso, like in the single-replica example, the application may need to merge siblings.\nThe version vector structure ensures that it is safe to read from one replica and subse\u2010\nquently write back to another replica. Doing so may result in siblings being created,\nbut no data is lost as long as siblings are merged correctly.\nVersion vectors and vector clocks\nA version vector is sometimes also called a vector clock, even though"}
{"214": "Summary\nIn this chapter we looked at the issue of replication. Replication can serve several\npurposes:\nHigh availability\nKeeping the system running, even when one machine (or several machines, or an\nentire datacenter) goes down\nDisconnected operation\nAllowing an application to continue working when there is a network interrup\u2010\ntion\nLatency\nPlacing data geographically close to users, so that users can interact with it faster\nScalability\nBeing able to handle a higher volume of reads than a single machine could han\u2010\ndle, by performing reads on replicas\nDespite being a simple goal\u2014keeping a copy of the same data on several machines\u2014\nreplication turns out to be a remarkably tricky problem. It requires carefully thinking\nabout concurrency and about all the things that can go wrong, and dealing with the\nconsequences of those faults. At a minimum, we need to deal with unavailable nodes\nand network interruptions (and that\u2019s not even considering the more insidious kinds\nof fault, such as silent data corruption due to software bugs).\nWe discussed three main approaches to replication:\nSingle-leader replication\nClients send all writes to a single node (the leader), which sends a stream of data\nchange events to the other replicas (followers). Reads can be performed on any\nreplica, but reads from followers might be stale.\nMulti-leader replication\nClients send each write to one of several leader nodes, any of which can accept\nwrites. The leaders send streams of data change events to each other and to any\nfollower nodes.\nLeaderless replication\nClients send each write to several nodes, and read from several nodes in parallel\nin order to detect and correct nodes with stale data.\nEach approach has advantages and disadvantages. Single-leader replication is popular"}
{"215": "faulty nodes, network interruptions, and latency spikes\u2014at the cost of being harder\nto reason about and providing only very weak consistency guarantees.\nReplication can be synchronous or asynchronous, which has a profound effect on the\nsystem behavior when there is a fault. Although asynchronous replication can be fast\nwhen the system is running smoothly, it\u2019s important to figure out what happens\nwhen replication lag increases and servers fail. If a leader fails and you promote an\nasynchronously updated follower to be the new leader, recently committed data may\nbe lost.\nWe looked at some strange effects that can be caused by replication lag, and we dis\u2010\ncussed a few consistency models which are helpful for deciding how an application\nshould behave under replication lag:\nRead-after-write consistency\nUsers should always see data that they submitted themselves.\nMonotonic reads\nAfter users have seen the data at one point in time, they shouldn\u2019t later see the\ndata from some earlier point in time.\nConsistent prefix reads\nUsers should see the data in a state that makes causal sense: for example, seeing a\nquestion and its reply in the correct order.\nFinally, we discussed the concurrency issues that are inherent in multi-leader and\nleaderless replication approaches: because they allow multiple writes to happen con\u2010\ncurrently, conflicts may occur. We examined an algorithm that a database might use\nto determine whether one operation happened before another, or whether they hap\u2010\npened concurrently. We also touched on methods for resolving conflicts by merging\ntogether concurrent updates.\nIn the next chapter we will continue looking at data that is distributed across multiple\nmachines, through the counterpart of replication: splitting a large dataset into parti\u2010\ntions.\nReferences\n[1] Bruce G. Lindsay, Patricia Griffiths Selinger, C. Galtieri, et al.: \u201cNotes on Dis\u2010\ntributed Databases,\u201d IBM Research, Research Report RJ2571(33471), July 1979.\n[2] \u201cOracle Active Data Guard Real-Time Data Protection and Availability,\u201d Oracle\nWhite Paper, June 2013."}
{"216": "[4] Lin Qiao, Kapil Surlaker, Shirshanka Das, et al.: \u201cOn Brewing Fresh Espresso:\nLinkedIn\u2019s Distributed Data Serving Platform,\u201d at ACM International Conference on\nManagement of Data (SIGMOD), June 2013.\n[5] Jun Rao: \u201cIntra-Cluster Replication for Apache Kafka,\u201d at ApacheCon North\nAmerica, February 2013.\n[6] \u201cHighly Available Queues,\u201d in RabbitMQ Server Documentation, Pivotal Software,\nInc., 2014.\n[7] Yoshinori Matsunobu: \u201cSemi-Synchronous Replication at Facebook,\u201d yoshinori\u2010\nmatsunobu.blogspot.co.uk, April 1, 2014.\n[8] Robbert van Renesse and Fred B. Schneider: \u201cChain Replication for Supporting\nHigh Throughput and Availability,\u201d at 6th USENIX Symposium on Operating System\nDesign and Implementation (OSDI), December 2004.\n[9] Jeff Terrace and Michael J. Freedman: \u201cObject Storage on CRAQ: High-\nThroughput Chain Replication for Read-Mostly Workloads,\u201d at USENIX Annual\nTechnical Conference (ATC), June 2009.\n[10] Brad Calder, Ju Wang, Aaron Ogus, et al.: \u201cWindows Azure Storage: A Highly\nAvailable Cloud Storage Service with Strong Consistency,\u201d at 23rd ACM Symposium\non Operating Systems Principles (SOSP), October 2011.\n[11] Andrew Wang: \u201cWindows Azure Storage,\u201d umbrant.com, February 4, 2016.\n[12] \u201cPercona Xtrabackup - Documentation,\u201d Percona LLC, 2014.\n[13] Jesse Newland: \u201cGitHub Availability This Week,\u201d github.com, September 14,\n2012.\n[14] Mark Imbriaco: \u201cDowntime Last Saturday,\u201d github.com, December 26, 2012.\n[15] John Hugg: \u201c\u2018All in\u2019 with Determinism for Performance and Testing in Dis\u2010\ntributed Systems,\u201d at Strange Loop, September 2015.\n[16] Amit Kapila: \u201cWAL Internals of PostgreSQL,\u201d at PostgreSQL Conference\n(PGCon), May 2012.\n[17] MySQL Internals Manual. Oracle, 2014.\n[18] Yogeshwer Sharma, Philippe Ajoux, Petchean Ang, et al.: \u201cWormhole: Reliable\nPub-Sub to Support Geo-Replicated Internet Services,\u201d at 12th USENIX Symposium\non Networked Systems Design and Implementation (NSDI), May 2015.\n[19] \u201cOracle GoldenGate 12c: Real-Time Access to Real-Time Information,\u201d Oracle"}
{"217": "[21] Greg Sabino Mullane: \u201cVersion 5 of Bucardo Database Replication System,\u201d\nblog.endpoint.com, June 23, 2014.\n[22] Werner Vogels: \u201cEventually Consistent,\u201d ACM Queue, volume 6, number 6,\npages 14\u201319, October 2008. doi:10.1145/1466443.1466448\n[23] Douglas B. Terry: \u201cReplicated Data Consistency Explained Through Baseball,\u201d\nMicrosoft Research, Technical Report MSR-TR-2011-137, October 2011.\n[24] Douglas B. Terry, Alan J. Demers, Karin Petersen, et al.: \u201cSession Guarantees for\nWeakly Consistent Replicated Data,\u201d at 3rd International Conference on Parallel and\nDistributed Information Systems (PDIS), September 1994. doi:10.1109/PDIS.\n1994.331722\n[25] Terry Pratchett: Reaper Man: A Discworld Novel. Victor Gollancz, 1991. ISBN:\n978-0-575-04979-6\n[26] \u201cTungsten Replicator,\u201d Continuent, Inc., 2014.\n[27] \u201cBDR 0.10.0 Documentation,\u201d The PostgreSQL Global Development Group,\nbdr-project.org, 2015.\n[28] Robert Hodges: \u201cIf You *Must* Deploy Multi-Master Replication, Read This\nFirst,\u201d scale-out-blog.blogspot.co.uk, March 30, 2012.\n[29] J. Chris Anderson, Jan Lehnardt, and Noah Slater: CouchDB: The Definitive\nGuide. O\u2019Reilly Media, 2010. ISBN: 978-0-596-15589-6\n[30] AppJet, Inc.: \u201cEtherpad and EasySync Technical Manual,\u201d github.com, March 26,\n2011.\n[31] John Day-Richter: \u201cWhat\u2019s Different About the New Google Docs: Making Col\u2010\nlaboration Fast,\u201d googledrive.blogspot.com, 23 September 2010.\n[32] Martin Kleppmann and Alastair R. Beresford: \u201cA Conflict-Free Replicated JSON\nDatatype,\u201d arXiv:1608.03960, August 13, 2016.\n[33] Frazer Clement: \u201cEventual Consistency \u2013 Detecting Conflicts,\u201d messagepass\u2010\ning.blogspot.co.uk, October 20, 2011.\n[34] Robert Hodges: \u201cState of the Art for MySQL Multi-Master Replication,\u201d at Per\u2010\ncona Live: MySQL Conference & Expo, April 2013.\n[35] John Daily: \u201cClocks Are Bad, or, Welcome to the Wonderful World of Dis\u2010\ntributed Systems,\u201d basho.com, November 12, 2013.\n[36] Riley Berton: \u201cIs Bi-Directional Replication (BDR) in Postgres Transactional?,\u201d"}
{"218": "[37] Giuseppe DeCandia, Deniz Hastorun, Madan Jampani, et al.: \u201cDynamo: Ama\u2010\nzon\u2019s Highly Available Key-Value Store,\u201d at 21st ACM Symposium on Operating Sys\u2010\ntems Principles (SOSP), October 2007.\n[38] Marc Shapiro, Nuno Pregui\u00e7a, Carlos Baquero, and Marek Zawirski: \u201cA Com\u2010\nprehensive Study of Convergent and Commutative Replicated Data Types,\u201d INRIA\nResearch Report no. 7506, January 2011.\n[39] Sam Elliott: \u201cCRDTs: An UPDATE (or Maybe Just a PUT),\u201d at RICON West,\nOctober 2013.\n[40] Russell Brown: \u201cA Bluffers Guide to CRDTs in Riak,\u201d gist.github.com, October\n28, 2013.\n[41] Benjamin Farinier, Thomas Gazagnaire, and Anil Madhavapeddy: \u201cMergeable\nPersistent Data Structures,\u201d at 26es Journ\u00e9es Francophones des Langages Applicatifs\n(JFLA), January 2015.\n[42] Chengzheng Sun and Clarence Ellis: \u201cOperational Transformation in Real-Time\nGroup Editors: Issues, Algorithms, and Achievements,\u201d at ACM Conference on Com\u2010\nputer Supported Cooperative Work (CSCW), November 1998.\n[43] Lars Hofhansl: \u201cHBASE-7709: Infinite Loop Possible in Master/Master Replica\u2010\ntion,\u201d issues.apache.org, January 29, 2013.\n[44] David K. Gifford: \u201cWeighted Voting for Replicated Data,\u201d at 7th ACM Sympo\u2010\nsium on Operating Systems Principles (SOSP), December 1979. doi:\n10.1145/800215.806583\n[45] Heidi Howard, Dahlia Malkhi, and Alexander Spiegelman: \u201cFlexible Paxos: Quo\u2010\nrum Intersection Revisited,\u201d arXiv:1608.06696, August 24, 2016.\n[46] Joseph Blomstedt: \u201cRe: Absolute Consistency,\u201d email to riak-users mailing list,\nlists.basho.com, January 11, 2012.\n[47] Joseph Blomstedt: \u201cBringing Consistency to Riak,\u201d at RICON West, October\n2012.\n[48] Peter Bailis, Shivaram Venkataraman, Michael J. Franklin, et al.: \u201cQuantifying\nEventual Consistency with PBS,\u201d Communications of the ACM, volume 57, number 8,\npages 93\u2013102, August 2014. doi:10.1145/2632792\n[49] Jonathan Ellis: \u201cModern Hinted Handoff,\u201d datastax.com, December 11, 2012.\n[50] \u201cProject Voldemort Wiki,\u201d github.com, 2013."}
{"219": "[53] Jonathan Ellis: \u201cWhy Cassandra Doesn\u2019t Need Vector Clocks,\u201d datastax.com,\nSeptember 2, 2013.\n[54] Leslie Lamport: \u201cTime, Clocks, and the Ordering of Events in a Distributed Sys\u2010\ntem,\u201d Communications of the ACM, volume 21, number 7, pages 558\u2013565, July 1978.\ndoi:10.1145/359545.359563\n[55] Joel Jacobson: \u201cRiak 2.0: Data Types,\u201d blog.joeljacobson.com, March 23, 2014.\n[56] D. Stott Parker Jr., Gerald J. Popek, Gerard Rudisin, et al.: \u201cDetection of Mutual\nInconsistency in Distributed Systems,\u201d IEEE Transactions on Software Engineering,\nvolume 9, number 3, pages 240\u2013247, May 1983. doi:10.1109/TSE.1983.236733\n[57] Nuno Pregui\u00e7a, Carlos Baquero, Paulo S\u00e9rgio Almeida, et al.: \u201cDotted Version\nVectors: Logical Clocks for Optimistic Replication,\u201d arXiv:1011.5808, November 26,\n2010.\n[58] Sean Cribbs: \u201cA Brief History of Time in Riak,\u201d at RICON, October 2014.\n[59] Russell Brown: \u201cVector Clocks Revisited Part 2: Dotted Version Vectors,\u201d\nbasho.com, November 10, 2015.\n[60] Carlos Baquero: \u201cVersion Vectors Are Not Vector Clocks,\u201d haslab.word\u2010\npress.com, July 8, 2011.\n[61] Reinhard Schwarz and Friedemann Mattern: \u201cDetecting Causal Relationships in\nDistributed Computations: In Search of the Holy Grail,\u201d Distributed Computing, vol\u2010\nume 7, number 3, pages 149\u2013174, March 1994. doi:10.1007/BF02277859"}
{"220": ""}
{"221": "CHAPTER 6\nPartitioning\nClearly, we must break away from the sequential and not limit the computers. We must\nstate definitions and provide for priorities and descriptions of data. We must state relation\u2010\nships, not procedures.\n\u2014Grace Murray Hopper, Management and the Computer of the Future (1962)\nIn Chapter 5 we discussed replication\u2014that is, having multiple copies of the same\ndata on different nodes. For very large datasets, or very high query throughput, that is\nnot sufficient: we need to break the data up into partitions, also known as sharding.i\nTerminological confusion\nWhat we call a partition here is called a shard in MongoDB, Elas\u2010\nticsearch, and SolrCloud; it\u2019s known as a region in HBase, a tablet\nin Bigtable, a vnode in Cassandra and Riak, and a vBucket in\nCouchbase. However, partitioning is the most established term, so\nwe\u2019ll stick with that.\nNormally, partitions are defined in such a way that each piece of data (each record,\nrow, or document) belongs to exactly one partition. There are various ways of achiev\u2010\ning this, which we discuss in depth in this chapter. In effect, each partition is a small\ndatabase of its own, although the database may support operations that touch multi\u2010\nple partitions at the same time.\nThe main reason for wanting to partition data is scalability. Different partitions can\nbe placed on different nodes in a shared-nothing cluster (see the introduction to"}
{"222": "Part II for a definition of shared nothing). Thus, a large dataset can be distributed\nacross many disks, and the query load can be distributed across many processors.\nFor queries that operate on a single partition, each node can independently execute\nthe queries for its own partition, so query throughput can be scaled by adding more\nnodes. Large, complex queries can potentially be parallelized across many nodes,\nalthough this gets significantly harder.\nPartitioned databases were pioneered in the 1980s by products such as Teradata and\nTandem NonStop SQL [1], and more recently rediscovered by NoSQL databases and\nHadoop-based data warehouses. Some systems are designed for transactional work\u2010\nloads, and others for analytics (see \u201cTransaction Processing or Analytics?\u201d on page\n90): this difference affects how the system is tuned, but the fundamentals of partition\u2010\ning apply to both kinds of workloads.\nIn this chapter we will first look at different approaches for partitioning large datasets\nand observe how the indexing of data interacts with partitioning. We\u2019ll then talk\nabout rebalancing, which is necessary if you want to add or remove nodes in your\ncluster. Finally, we\u2019ll get an overview of how databases route requests to the right par\u2010\ntitions and execute queries.\nPartitioning and Replication\nPartitioning is usually combined with replication so that copies of each partition are\nstored on multiple nodes. This means that, even though each record belongs to\nexactly one partition, it may still be stored on several different nodes for fault toler\u2010\nance.\nA node may store more than one partition. If a leader\u2013follower replication model is\nused, the combination of partitioning and replication can look like Figure 6-1. Each\npartition\u2019s leader is assigned to one node, and its followers are assigned to other\nnodes. Each node may be the leader for some partitions and a follower for other par\u2010\ntitions.\nEverything we discussed in Chapter 5 about replication of databases applies equally\nto replication of partitions. The choice of partitioning scheme is mostly independent\nof the choice of replication scheme, so we will keep things simple and ignore replica\u2010\ntion in this chapter."}
{"223": "Figure 6-1. Combining replication and partitioning: each node acts as leader for some\npartitions and follower for other partitions.\nPartitioning of Key-Value Data\nSay you have a large amount of data, and you want to partition it. How do you decide\nwhich records to store on which nodes?\nOur goal with partitioning is to spread the data and the query load evenly across\nnodes. If every node takes a fair share, then\u2014in theory\u201410 nodes should be able to\nhandle 10 times as much data and 10 times the read and write throughput of a single\nnode (ignoring replication for now).\nIf the partitioning is unfair, so that some partitions have more data or queries than\nothers, we call it skewed. The presence of skew makes partitioning much less effective.\nIn an extreme case, all the load could end up on one partition, so 9 out of 10 nodes\nare idle and your bottleneck is the single busy node. A partition with disproportion\u2010\nately high load is called a hot spot.\nThe simplest approach for avoiding hot spots would be to assign records to nodes\nrandomly. That would distribute the data quite evenly across the nodes, but it has a\nbig disadvantage: when you\u2019re trying to read a particular item, you have no way of\nknowing which node it is on, so you have to query all nodes in parallel.\nWe can do better. Let\u2019s assume for now that you have a simple key-value data model,\nin which you always access a record by its primary key. For example, in an old-"}
{"224": "Partitioning by Key Range\nOne way of partitioning is to assign a continuous range of keys (from some mini\u2010\nmum to some maximum) to each partition, like the volumes of a paper encyclopedia\n(Figure 6-2). If you know the boundaries between the ranges, you can easily deter\u2010\nmine which partition contains a given key. If you also know which partition is\nassigned to which node, then you can make your request directly to the appropriate\nnode (or, in the case of the encyclopedia, pick the correct book off the shelf).\nFigure 6-2. A print encyclopedia is partitioned by key range.\nThe ranges of keys are not necessarily evenly spaced, because your data may not be\nevenly distributed. For example, in Figure 6-2, volume 1 contains words starting with\nA and B, but volume 12 contains words starting with T, U, V, X, Y, and Z. Simply\nhaving one volume per two letters of the alphabet would lead to some volumes being\nmuch bigger than others. In order to distribute the data evenly, the partition bound\u2010\naries need to adapt to the data.\nThe partition boundaries might be chosen manually by an administrator, or the data\u2010\nbase can choose them automatically (we will discuss choices of partition boundaries\nin more detail in \u201cRebalancing Partitions\u201d on page 209). This partitioning strategy is\nused by Bigtable, its open source equivalent HBase [2, 3], RethinkDB, and MongoDB\nbefore version 2.4 [4].\nWithin each partition, we can keep keys in sorted order (see \u201cSSTables and LSM-\nTrees\u201d on page 76). This has the advantage that range scans are easy, and you can\ntreat the key as a concatenated index in order to fetch several related records in one\nquery (see \u201cMulti-column indexes\u201d on page 87). For example, consider an application\nthat stores data from a network of sensors, where the key is the timestamp of the\nmeasurement (year-month-day-hour-minute-second). Range scans are very useful in"}
{"225": "However, the downside of key range partitioning is that certain access patterns can\nlead to hot spots. If the key is a timestamp, then the partitions correspond to ranges\nof time\u2014e.g., one partition per day. Unfortunately, because we write data from the\nsensors to the database as the measurements happen, all the writes end up going to\nthe same partition (the one for today), so that partition can be overloaded with writes\nwhile others sit idle [5].\nTo avoid this problem in the sensor database, you need to use something other than\nthe timestamp as the first element of the key. For example, you could prefix each\ntimestamp with the sensor name so that the partitioning is first by sensor name and\nthen by time. Assuming you have many sensors active at the same time, the write\nload will end up more evenly spread across the partitions. Now, when you want to\nfetch the values of multiple sensors within a time range, you need to perform a sepa\u2010\nrate range query for each sensor name.\nPartitioning by Hash of Key\nBecause of this risk of skew and hot spots, many distributed datastores use a hash\nfunction to determine the partition for a given key.\nA good hash function takes skewed data and makes it uniformly distributed. Say you\nhave a 32-bit hash function that takes a string. Whenever you give it a new string, it\nreturns a seemingly random number between 0 and 232 \u2212 1. Even if the input strings\nare very similar, their hashes are evenly distributed across that range of numbers.\nFor partitioning purposes, the hash function need not be cryptographically strong:\nfor example, Cassandra and MongoDB use MD5, and Voldemort uses the Fowler\u2013\nNoll\u2013Vo function. Many programming languages have simple hash functions built in\n(as they are used for hash tables), but they may not be suitable for partitioning: for\nexample, in Java\u2019s Object.hashCode() and Ruby\u2019s Object#hash, the same key may\nhave a different hash value in different processes [6].\nOnce you have a suitable hash function for keys, you can assign each partition a\nrange of hashes (rather than a range of keys), and every key whose hash falls within a\npartition\u2019s range will be stored in that partition. This is illustrated in Figure 6-3."}
{"226": "Figure 6-3. Partitioning by hash of key.\nThis technique is good at distributing keys fairly among the partitions. The partition\nboundaries can be evenly spaced, or they can be chosen pseudorandomly (in which\ncase the technique is sometimes known as consistent hashing).\nConsistent Hashing\nConsistent hashing, as defined by Karger et al. [7], is a way of evenly distributing load\nacross an internet-wide system of caches such as a content delivery network (CDN).\nIt uses randomly chosen partition boundaries to avoid the need for central control or\ndistributed consensus. Note that consistent here has nothing to do with replica consis\u2010\ntency (see Chapter 5) or ACID consistency (see Chapter 7), but rather describes a\nparticular approach to rebalancing.\nAs we shall see in \u201cRebalancing Partitions\u201d on page 209, this particular approach\nactually doesn\u2019t work very well for databases [8], so it is rarely used in practice (the\ndocumentation of some databases still refers to consistent hashing, but it is often\ninaccurate). Because this is so confusing, it\u2019s best to avoid the term consistent hashing\nand just call it hash partitioning instead.\nUnfortunately however, by using the hash of the key for partitioning we lose a nice\nproperty of key-range partitioning: the ability to do efficient range queries. Keys that\nwere once adjacent are now scattered across all the partitions, so their sort order is\nlost. In MongoDB, if you have enabled hash-based sharding mode, any range query\nhas to be sent to all partitions [4]. Range queries on the primary key are not sup\u2010\nported by Riak [9], Couchbase [10], or Voldemort.\nCassandra achieves a compromise between the two partitioning strategies [11, 12,\n13]. A table in Cassandra can be declared with a compound primary key consisting of"}
{"227": "first column of a compound key, but if it specifies a fixed value for the first column, it\ncan perform an efficient range scan over the other columns of the key.\nThe concatenated index approach enables an elegant data model for one-to-many\nrelationships. For example, on a social media site, one user may post many updates. If\nthe primary key for updates is chosen to be (user_id, update_timestamp), then you\ncan efficiently retrieve all updates made by a particular user within some time inter\u2010\nval, sorted by timestamp. Different users may be stored on different partitions, but\nwithin each user, the updates are stored ordered by timestamp on a single partition.\nSkewed Workloads and Relieving Hot Spots\nAs discussed, hashing a key to determine its partition can help reduce hot spots.\nHowever, it can\u2019t avoid them entirely: in the extreme case where all reads and writes\nare for the same key, you still end up with all requests being routed to the same parti\u2010\ntion.\nThis kind of workload is perhaps unusual, but not unheard of: for example, on a\nsocial media site, a celebrity user with millions of followers may cause a storm of\nactivity when they do something [14]. This event can result in a large volume of\nwrites to the same key (where the key is perhaps the user ID of the celebrity, or the ID\nof the action that people are commenting on). Hashing the key doesn\u2019t help, as the\nhash of two identical IDs is still the same.\nToday, most data systems are not able to automatically compensate for such a highly\nskewed workload, so it\u2019s the responsibility of the application to reduce the skew. For\nexample, if one key is known to be very hot, a simple technique is to add a random\nnumber to the beginning or end of the key. Just a two-digit decimal random number\nwould split the writes to the key evenly across 100 different keys, allowing those keys\nto be distributed to different partitions.\nHowever, having split the writes across different keys, any reads now have to do addi\u2010\ntional work, as they have to read the data from all 100 keys and combine it. This tech\u2010\nnique also requires additional bookkeeping: it only makes sense to append the\nrandom number for the small number of hot keys; for the vast majority of keys with\nlow write throughput this would be unnecessary overhead. Thus, you also need some\nway of keeping track of which keys are being split.\nPerhaps in the future, data systems will be able to automatically detect and compen\u2010\nsate for skewed workloads; but for now, you need to think through the trade-offs for\nyour own application."}
{"228": "Partitioning and Secondary Indexes\nThe partitioning schemes we have discussed so far rely on a key-value data model. If\nrecords are only ever accessed via their primary key, we can determine the partition\nfrom that key and use it to route read and write requests to the partition responsible\nfor that key.\nThe situation becomes more complicated if secondary indexes are involved (see also\n\u201cOther Indexing Structures\u201d on page 85). A secondary index usually doesn\u2019t identify\na record uniquely but rather is a way of searching for occurrences of a particular\nvalue: find all actions by user 123, find all articles containing the word hogwash, find\nall cars whose color is red, and so on.\nSecondary indexes are the bread and butter of relational databases, and they are com\u2010\nmon in document databases too. Many key-value stores (such as HBase and Volde\u2010\nmort) have avoided secondary indexes because of their added implementation\ncomplexity, but some (such as Riak) have started adding them because they are so\nuseful for data modeling. And finally, secondary indexes are the raison d\u2019\u00eatre of\nsearch servers such as Solr and Elasticsearch.\nThe problem with secondary indexes is that they don\u2019t map neatly to partitions.\nThere are two main approaches to partitioning a database with secondary indexes:\ndocument-based partitioning and term-based partitioning.\nPartitioning Secondary Indexes by Document\nFor example, imagine you are operating a website for selling used cars (illustrated in\nFigure 6-4). Each listing has a unique ID\u2014call it the document ID\u2014and you partition\nthe database by the document ID (for example, IDs 0 to 499 in partition 0, IDs 500 to\n999 in partition 1, etc.).\nYou want to let users search for cars, allowing them to filter by color and by make, so\nyou need a secondary index on color and make (in a document database these would\nbe fields; in a relational database they would be columns). If you have declared the\nindex, the database can perform the indexing automatically.ii For example, whenever\na red car is added to the database, the database partition automatically adds it to the\nlist of document IDs for the index entry color:red."}
{"229": "Figure 6-4. Partitioning secondary indexes by document.\nIn this indexing approach, each partition is completely separate: each partition main\u2010\ntains its own secondary indexes, covering only the documents in that partition. It\ndoesn\u2019t care what data is stored in other partitions. Whenever you need to write to\nthe database\u2014to add, remove, or update a document\u2014you only need to deal with the\npartition that contains the document ID that you are writing. For that reason, a\ndocument-partitioned index is also known as a local index (as opposed to a global\nindex, described in the next section).\nHowever, reading from a document-partitioned index requires care: unless you have\ndone something special with the document IDs, there is no reason why all the cars\nwith a particular color or a particular make would be in the same partition. In\nFigure 6-4, red cars appear in both partition 0 and partition 1. Thus, if you want to\nsearch for red cars, you need to send the query to all partitions, and combine all the\nresults you get back.\nThis approach to querying a partitioned database is sometimes known as scatter/\ngather, and it can make read queries on secondary indexes quite expensive. Even if\nyou query the partitions in parallel, scatter/gather is prone to tail latency amplifica\u2010\ntion (see \u201cPercentiles in Practice\u201d on page 16). Nevertheless, it is widely used: Mon\u2010\ngoDB, Riak [15], Cassandra [16], Elasticsearch [17], SolrCloud [18], and VoltDB [19]\nall use document-partitioned secondary indexes. Most database vendors recommend\nthat you structure your partitioning scheme so that secondary index queries can be\nserved from a single partition, but that is not always possible, especially when you\u2019re\nusing multiple secondary indexes in a single query (such as filtering cars by color and"}
{"230": "Figure 6-5. Partitioning secondary indexes by term.\nPartitioning Secondary Indexes by Term\nRather than each partition having its own secondary index (a local index), we can\nconstruct a global index that covers data in all partitions. However, we can\u2019t just store\nthat index on one node, since it would likely become a bottleneck and defeat the pur\u2010\npose of partitioning. A global index must also be partitioned, but it can be partitioned\ndifferently from the primary key index.\nFigure 6-5 illustrates what this could look like: red cars from all partitions appear\nunder color:red in the index, but the index is partitioned so that colors starting with\nthe letters a to r appear in partition 0 and colors starting with s to z appear in parti\u2010\ntion 1. The index on the make of car is partitioned similarly (with the partition\nboundary being between f and h).\nWe call this kind of index term-partitioned, because the term we\u2019re looking for deter\u2010\nmines the partition of the index. Here, a term would be color:red, for example. The\nname term comes from full-text indexes (a particular kind of secondary index), where\nthe terms are all the words that occur in a document.\nAs before, we can partition the index by the term itself, or using a hash of the term.\nPartitioning by the term itself can be useful for range scans (e.g., on a numeric prop\u2010\nerty, such as the asking price of the car), whereas partitioning on a hash of the term\ngives a more even distribution of load.\nThe advantage of a global (term-partitioned) index over a document-partitioned\nindex is that it can make reads more efficient: rather than doing scatter/gather over"}
{"231": "partitions of the index (every term in the document might be on a different partition,\non a different node).\nIn an ideal world, the index would always be up to date, and every document written\nto the database would immediately be reflected in the index. However, in a term-\npartitioned index, that would require a distributed transaction across all partitions\naffected by a write, which is not supported in all databases (see Chapter 7 and Chap\u2010\nter 9).\nIn practice, updates to global secondary indexes are often asynchronous (that is, if\nyou read the index shortly after a write, the change you just made may not yet be\nreflected in the index). For example, Amazon DynamoDB states that its global secon\u2010\ndary indexes are updated within a fraction of a second in normal circumstances, but\nmay experience longer propagation delays in cases of faults in the infrastructure [20].\nOther uses of global term-partitioned indexes include Riak\u2019s search feature [21] and\nthe Oracle data warehouse, which lets you choose between local and global indexing\n[22]. We will return to the topic of implementing term-partitioned secondary indexes\nin Chapter 12.\nRebalancing Partitions\nOver time, things change in a database:\n\u2022 The query throughput increases, so you want to add more CPUs to handle the\nload.\n\u2022 The dataset size increases, so you want to add more disks and RAM to store it.\n\u2022 A machine fails, and other machines need to take over the failed machine\u2019s\nresponsibilities.\nAll of these changes call for data and requests to be moved from one node to another.\nThe process of moving load from one node in the cluster to another is called reba\u2010\nlancing.\nNo matter which partitioning scheme is used, rebalancing is usually expected to meet\nsome minimum requirements:\n\u2022 After rebalancing, the load (data storage, read and write requests) should be\nshared fairly between the nodes in the cluster.\n\u2022 While rebalancing is happening, the database should continue accepting reads\nand writes."}
{"232": "Strategies for Rebalancing\nThere are a few different ways of assigning partitions to nodes [23]. Let\u2019s briefly dis\u2010\ncuss each in turn.\nHow not to do it: hash mod N\nWhen partitioning by the hash of a key, we said earlier (Figure 6-3) that it\u2019s best to\ndivide the possible hashes into ranges and assign each range to a partition (e.g., assign\nkey to partition 0 if 0 \u2264 hash(key) < b , to partition 1 if b \u2264 hash(key) < b , etc.).\n0 0 1\nPerhaps you wondered why we don\u2019t just use mod (the % operator in many program\u2010\nming languages). For example, hash(key) mod 10 would return a number between 0\nand 9 (if we write the hash as a decimal number, the hash mod 10 would be the last\ndigit). If we have 10 nodes, numbered 0 to 9, that seems like an easy way of assigning\neach key to a node.\nThe problem with the mod N approach is that if the number of nodes N changes,\nmost of the keys will need to be moved from one node to another. For example, say\nhash(key) = 123456. If you initially have 10 nodes, that key starts out on node 6\n(because 123456 mod 10 = 6). When you grow to 11 nodes, the key needs to move to\nnode 3 (123456 mod 11 = 3), and when you grow to 12 nodes, it needs to move to\nnode 0 (123456 mod 12 = 0). Such frequent moves make rebalancing excessively\nexpensive.\nWe need an approach that doesn\u2019t move data around more than necessary.\nFixed number of partitions\nFortunately, there is a fairly simple solution: create many more partitions than there\nare nodes, and assign several partitions to each node. For example, a database run\u2010\nning on a cluster of 10 nodes may be split into 1,000 partitions from the outset so that\napproximately 100 partitions are assigned to each node.\nNow, if a node is added to the cluster, the new node can steal a few partitions from\nevery existing node until partitions are fairly distributed once again. This process is\nillustrated in Figure 6-6. If a node is removed from the cluster, the same happens in\nreverse.\nOnly entire partitions are moved between nodes. The number of partitions does not\nchange, nor does the assignment of keys to partitions. The only thing that changes is\nthe assignment of partitions to nodes. This change of assignment is not immediate\u2014\nit takes some time to transfer a large amount of data over the network\u2014so the old"}
{"233": "Figure 6-6. Adding a new node to a database cluster with multiple partitions per node.\nIn principle, you can even account for mismatched hardware in your cluster: by\nassigning more partitions to nodes that are more powerful, you can force those nodes\nto take a greater share of the load.\nThis approach to rebalancing is used in Riak [15], Elasticsearch [24], Couchbase [10],\nand Voldemort [25].\nIn this configuration, the number of partitions is usually fixed when the database is\nfirst set up and not changed afterward. Although in principle it\u2019s possible to split and\nmerge partitions (see the next section), a fixed number of partitions is operationally\nsimpler, and so many fixed-partition databases choose not to implement partition\nsplitting. Thus, the number of partitions configured at the outset is the maximum\nnumber of nodes you can have, so you need to choose it high enough to accommo\u2010\ndate future growth. However, each partition also has management overhead, so it\u2019s\ncounterproductive to choose too high a number.\nChoosing the right number of partitions is difficult if the total size of the dataset is\nhighly variable (for example, if it starts small but may grow much larger over time).\nSince each partition contains a fixed fraction of the total data, the size of each parti\u2010\ntion grows proportionally to the total amount of data in the cluster. If partitions are\nvery large, rebalancing and recovery from node failures become expensive. But if par\u2010\ntitions are too small, they incur too much overhead. The best performance is\nachieved when the size of partitions is \u201cjust right,\u201d neither too big nor too small,"}
{"234": "Dynamic partitioning\nFor databases that use key range partitioning (see \u201cPartitioning by Key Range\u201d on\npage 202), a fixed number of partitions with fixed boundaries would be very incon\u2010\nvenient: if you got the boundaries wrong, you could end up with all of the data in one\npartition and all of the other partitions empty. Reconfiguring the partition bound\u2010\naries manually would be very tedious.\nFor that reason, key range\u2013partitioned databases such as HBase and RethinkDB cre\u2010\nate partitions dynamically. When a partition grows to exceed a configured size (on\nHBase, the default is 10 GB), it is split into two partitions so that approximately half\nof the data ends up on each side of the split [26]. Conversely, if lots of data is deleted\nand a partition shrinks below some threshold, it can be merged with an adjacent par\u2010\ntition. This process is similar to what happens at the top level of a B-tree (see \u201cB-\nTrees\u201d on page 79).\nEach partition is assigned to one node, and each node can handle multiple partitions,\nlike in the case of a fixed number of partitions. After a large partition has been split,\none of its two halves can be transferred to another node in order to balance the load.\nIn the case of HBase, the transfer of partition files happens through HDFS, the\nunderlying distributed filesystem [3].\nAn advantage of dynamic partitioning is that the number of partitions adapts to the\ntotal data volume. If there is only a small amount of data, a small number of parti\u2010\ntions is sufficient, so overheads are small; if there is a huge amount of data, the size of\neach individual partition is limited to a configurable maximum [23].\nHowever, a caveat is that an empty database starts off with a single partition, since\nthere is no a priori information about where to draw the partition boundaries. While\nthe dataset is small\u2014until it hits the point at which the first partition is split\u2014all\nwrites have to be processed by a single node while the other nodes sit idle. To miti\u2010\ngate this issue, HBase and MongoDB allow an initial set of partitions to be configured\non an empty database (this is called pre-splitting). In the case of key-range partition\u2010\ning, pre-splitting requires that you already know what the key distribution is going to\nlook like [4, 26].\nDynamic partitioning is not only suitable for key range\u2013partitioned data, but can\nequally well be used with hash-partitioned data. MongoDB since version 2.4 supports\nboth key-range and hash partitioning, and it splits partitions dynamically in either\ncase.\nPartitioning proportionally to nodes"}
{"235": "ber of partitions, the size of each partition is proportional to the size of the dataset. In\nboth of these cases, the number of partitions is independent of the number of nodes.\nA third option, used by Cassandra and Ketama, is to make the number of partitions\nproportional to the number of nodes\u2014in other words, to have a fixed number of par\u2010\ntitions per node [23, 27, 28]. In this case, the size of each partition grows proportion\u2010\nally to the dataset size while the number of nodes remains unchanged, but when you\nincrease the number of nodes, the partitions become smaller again. Since a larger\ndata volume generally requires a larger number of nodes to store, this approach also\nkeeps the size of each partition fairly stable.\nWhen a new node joins the cluster, it randomly chooses a fixed number of existing\npartitions to split, and then takes ownership of one half of each of those split parti\u2010\ntions while leaving the other half of each partition in place. The randomization can\nproduce unfair splits, but when averaged over a larger number of partitions (in Cas\u2010\nsandra, 256 partitions per node by default), the new node ends up taking a fair share\nof the load from the existing nodes. Cassandra 3.0 introduced an alternative rebalanc\u2010\ning algorithm that avoids unfair splits [29].\nPicking partition boundaries randomly requires that hash-based partitioning is used\n(so the boundaries can be picked from the range of numbers produced by the hash\nfunction). Indeed, this approach corresponds most closely to the original definition\nof consistent hashing [7] (see \u201cConsistent Hashing\u201d on page 204). Newer hash func\u2010\ntions can achieve a similar effect with lower metadata overhead [8].\nOperations: Automatic or Manual Rebalancing\nThere is one important question with regard to rebalancing that we have glossed\nover: does the rebalancing happen automatically or manually?\nThere is a gradient between fully automatic rebalancing (the system decides automat\u2010\nically when to move partitions from one node to another, without any administrator\ninteraction) and fully manual (the assignment of partitions to nodes is explicitly con\u2010\nfigured by an administrator, and only changes when the administrator explicitly\nreconfigures it). For example, Couchbase, Riak, and Voldemort generate a suggested\npartition assignment automatically, but require an administrator to commit it before\nit takes effect.\nFully automated rebalancing can be convenient, because there is less operational\nwork to do for normal maintenance. However, it can be unpredictable. Rebalancing\nis an expensive operation, because it requires rerouting requests and moving a large\namount of data from one node to another. If it is not done carefully, this process can"}
{"236": "Such automation can be dangerous in combination with automatic failure detection.\nFor example, say one node is overloaded and is temporarily slow to respond to\nrequests. The other nodes conclude that the overloaded node is dead, and automati\u2010\ncally rebalance the cluster to move load away from it. This puts additional load on the\noverloaded node, other nodes, and the network\u2014making the situation worse and\npotentially causing a cascading failure.\nFor that reason, it can be a good thing to have a human in the loop for rebalancing.\nIt\u2019s slower than a fully automatic process, but it can help prevent operational\nsurprises.\nRequest Routing\nWe have now partitioned our dataset across multiple nodes running on multiple\nmachines. But there remains an open question: when a client wants to make a\nrequest, how does it know which node to connect to? As partitions are rebalanced,\nthe assignment of partitions to nodes changes. Somebody needs to stay on top of\nthose changes in order to answer the question: if I want to read or write the key \u201cfoo\u201d,\nwhich IP address and port number do I need to connect to?\nThis is an instance of a more general problem called service discovery, which isn\u2019t\nlimited to just databases. Any piece of software that is accessible over a network has\nthis problem, especially if it is aiming for high availability (running in a redundant\nconfiguration on multiple machines). Many companies have written their own in-\nhouse service discovery tools, and many of these have been released as open source\n[30].\nOn a high level, there are a few different approaches to this problem (illustrated in\nFigure 6-7):\n1. Allow clients to contact any node (e.g., via a round-robin load balancer). If that\nnode coincidentally owns the partition to which the request applies, it can handle\nthe request directly; otherwise, it forwards the request to the appropriate node,\nreceives the reply, and passes the reply along to the client.\n2. Send all requests from clients to a routing tier first, which determines the node\nthat should handle each request and forwards it accordingly. This routing tier\ndoes not itself handle any requests; it only acts as a partition-aware load balancer.\n3. Require that clients be aware of the partitioning and the assignment of partitions\nto nodes. In this case, a client can connect directly to the appropriate node,\nwithout any intermediary."}
{"237": "Figure 6-7. Three different ways of routing a request to the right node.\nThis is a challenging problem, because it is important that all participants agree\u2014\notherwise requests would be sent to the wrong nodes and not handled correctly.\nThere are protocols for achieving consensus in a distributed system, but they are hard\nto implement correctly (see Chapter 9).\nMany distributed data systems rely on a separate coordination service such as Zoo\u2010\nKeeper to keep track of this cluster metadata, as illustrated in Figure 6-8. Each node\nregisters itself in ZooKeeper, and ZooKeeper maintains the authoritative mapping of\npartitions to nodes. Other actors, such as the routing tier or the partitioning-aware\nclient, can subscribe to this information in ZooKeeper. Whenever a partition changes\nownership, or a node is added or removed, ZooKeeper notifies the routing tier so that\nit can keep its routing information up to date."}
{"238": "For example, LinkedIn\u2019s Espresso uses Helix [31] for cluster management (which in\nturn relies on ZooKeeper), implementing a routing tier as shown in Figure 6-8.\nHBase, SolrCloud, and Kafka also use ZooKeeper to track partition assignment.\nMongoDB has a similar architecture, but it relies on its own config server implemen\u2010\ntation and mongos daemons as the routing tier.\nCassandra and Riak take a different approach: they use a gossip protocol among the\nnodes to disseminate any changes in cluster state. Requests can be sent to any node,\nand that node forwards them to the appropriate node for the requested partition\n(approach 1 in Figure 6-7). This model puts more complexity in the database nodes\nbut avoids the dependency on an external coordination service such as ZooKeeper.\nCouchbase does not rebalance automatically, which simplifies the design. Normally it\nis configured with a routing tier called moxi, which learns about routing changes\nfrom the cluster nodes [32].\nWhen using a routing tier or when sending requests to a random node, clients still\nneed to find the IP addresses to connect to. These are not as fast-changing as the\nassignment of partitions to nodes, so it is often sufficient to use DNS for this purpose.\nParallel Query Execution\nSo far we have focused on very simple queries that read or write a single key (plus\nscatter/gather queries in the case of document-partitioned secondary indexes). This is\nabout the level of access supported by most NoSQL distributed datastores.\nHowever, massively parallel processing (MPP) relational database products, often\nused for analytics, are much more sophisticated in the types of queries they support.\nA typical data warehouse query contains several join, filtering, grouping, and aggre\u2010\ngation operations. The MPP query optimizer breaks this complex query into a num\u2010\nber of execution stages and partitions, many of which can be executed in parallel on\ndifferent nodes of the database cluster. Queries that involve scanning over large parts\nof the dataset particularly benefit from such parallel execution.\nFast parallel execution of data warehouse queries is a specialized topic, and given the\nbusiness importance of analytics, it receives a lot of commercial interest. We will dis\u2010\ncuss some techniques for parallel query execution in Chapter 10. For a more detailed\noverview of techniques used in parallel databases, please see the references [1, 33].\nSummary\nIn this chapter we explored different ways of partitioning a large dataset into smaller"}
{"239": "The goal of partitioning is to spread the data and query load evenly across multiple\nmachines, avoiding hot spots (nodes with disproportionately high load). This\nrequires choosing a partitioning scheme that is appropriate to your data, and reba\u2010\nlancing the partitions when nodes are added to or removed from the cluster.\nWe discussed two main approaches to partitioning:\n\u2022 Key range partitioning, where keys are sorted, and a partition owns all the keys\nfrom some minimum up to some maximum. Sorting has the advantage that effi\u2010\ncient range queries are possible, but there is a risk of hot spots if the application\noften accesses keys that are close together in the sorted order.\nIn this approach, partitions are typically rebalanced dynamically by splitting the\nrange into two subranges when a partition gets too big.\n\u2022 Hash partitioning, where a hash function is applied to each key, and a partition\nowns a range of hashes. This method destroys the ordering of keys, making range\nqueries inefficient, but may distribute load more evenly.\nWhen partitioning by hash, it is common to create a fixed number of partitions\nin advance, to assign several partitions to each node, and to move entire parti\u2010\ntions from one node to another when nodes are added or removed. Dynamic\npartitioning can also be used.\nHybrid approaches are also possible, for example with a compound key: using one\npart of the key to identify the partition and another part for the sort order.\nWe also discussed the interaction between partitioning and secondary indexes. A sec\u2010\nondary index also needs to be partitioned, and there are two methods:\n\u2022 Document-partitioned indexes (local indexes), where the secondary indexes are\nstored in the same partition as the primary key and value. This means that only a\nsingle partition needs to be updated on write, but a read of the secondary index\nrequires a scatter/gather across all partitions.\n\u2022 Term-partitioned indexes (global indexes), where the secondary indexes are parti\u2010\ntioned separately, using the indexed values. An entry in the secondary index may\ninclude records from all partitions of the primary key. When a document is writ\u2010\nten, several partitions of the secondary index need to be updated; however, a read\ncan be served from a single partition.\nFinally, we discussed techniques for routing queries to the appropriate partition,\nwhich range from simple partition-aware load balancing to sophisticated parallel\nquery execution engines."}
{"240": "to several partitions can be difficult to reason about: for example, what happens if the\nwrite to one partition succeeds, but another fails? We will address that question in the\nfollowing chapters.\nReferences\n[1] David J. DeWitt and Jim N. Gray: \u201cParallel Database Systems: The Future of High\nPerformance Database Systems,\u201d Communications of the ACM, volume 35, number 6,\npages 85\u201398, June 1992. doi:10.1145/129888.129894\n[2] Lars George: \u201cHBase vs. BigTable Comparison,\u201d larsgeorge.com, November 2009.\n[3] \u201cThe Apache HBase Reference Guide,\u201d Apache Software Foundation,\nhbase.apache.org, 2014.\n[4] MongoDB, Inc.: \u201cNew Hash-Based Sharding Feature in MongoDB 2.4,\u201d blog.mon\u2010\ngodb.org, April 10, 2013.\n[5] Ikai Lan: \u201cApp Engine Datastore Tip: Monotonically Increasing Values Are Bad,\u201d\nikaisays.com, January 25, 2011.\n[6] Martin Kleppmann: \u201cJava\u2019s hashCode Is Not Safe for Distributed Systems,\u201d mar\u2010\ntin.kleppmann.com, June 18, 2012.\n[7] David Karger, Eric Lehman, Tom Leighton, et al.: \u201cConsistent Hashing and Ran\u2010\ndom Trees: Distributed Caching Protocols for Relieving Hot Spots on the World\nWide Web,\u201d at 29th Annual ACM Symposium on Theory of Computing (STOC),\npages 654\u2013663, 1997. doi:10.1145/258533.258660\n[8] John Lamping and Eric Veach: \u201cA Fast, Minimal Memory, Consistent Hash Algo\u2010\nrithm,\u201d arxiv.org, June 2014.\n[9] Eric Redmond: \u201cA Little Riak Book,\u201d Version 1.4.0, Basho Technologies, Septem\u2010\nber 2013.\n[10] \u201cCouchbase 2.5 Administrator Guide,\u201d Couchbase, Inc., 2014.\n[11] Avinash Lakshman and Prashant Malik: \u201cCassandra \u2013 A Decentralized Struc\u2010\ntured Storage System,\u201d at 3rd ACM SIGOPS International Workshop on Large Scale\nDistributed Systems and Middleware (LADIS), October 2009.\n[12] Jonathan Ellis: \u201cFacebook\u2019s Cassandra Paper, Annotated and Compared to\nApache Cassandra 2.0,\u201d datastax.com, September 12, 2013.\n[13] \u201cIntroduction to Cassandra Query Language,\u201d DataStax, Inc., 2014."}
{"241": "[16] Richard Low: \u201cThe Sweet Spot for Cassandra Secondary Indexing,\u201d wentnet.com,\nOctober 21, 2013.\n[17] Zachary Tong: \u201cCustomizing Your Document Routing,\u201d elasticsearch.org, June\n3, 2013.\n[18] \u201cApache Solr Reference Guide,\u201d Apache Software Foundation, 2014.\n[19] Andrew Pavlo: \u201cH-Store Frequently Asked Questions,\u201d hstore.cs.brown.edu,\nOctober 2013.\n[20] \u201cAmazon DynamoDB Developer Guide,\u201d Amazon Web Services, Inc., 2014.\n[21] Rusty Klophaus: \u201cDifference Between 2I and Search,\u201d email to riak-users mailing\nlist, lists.basho.com, October 25, 2011.\n[22] Donald K. Burleson: \u201cObject Partitioning in Oracle,\u201d dba-oracle.com, November\n8, 2000.\n[23] Eric Evans: \u201cRethinking Topology in Cassandra,\u201d at ApacheCon Europe, Novem\u2010\nber 2012.\n[24] Rafa\u0142 Ku\u0107: \u201cReroute API Explained,\u201d elasticsearchserverbook.com, September 30,\n2013.\n[25] \u201cProject Voldemort Documentation,\u201d project-voldemort.com.\n[26] Enis Soztutar: \u201cApache HBase Region Splitting and Merging,\u201d hortonworks.com,\nFebruary 1, 2013.\n[27] Brandon Williams: \u201cVirtual Nodes in Cassandra 1.2,\u201d datastax.com, December\n4, 2012.\n[28] Richard Jones: \u201clibketama: Consistent Hashing Library for Memcached Clients,\u201d\nmetabrew.com, April 10, 2007.\n[29] Branimir Lambov: \u201cNew Token Allocation Algorithm in Cassandra 3.0,\u201d data\u2010\nstax.com, January 28, 2016.\n[30] Jason Wilder: \u201cOpen-Source Service Discovery,\u201d jasonwilder.com, February\n2014.\n[31] Kishore Gopalakrishna, Shi Lu, Zhen Zhang, et al.: \u201cUntangling Cluster Manage\u2010\nment with Helix,\u201d at ACM Symposium on Cloud Computing (SoCC), October 2012.\ndoi:10.1145/2391229.2391248\n[32] \u201cMoxi 1.8 Manual,\u201d Couchbase, Inc., 2014."}
{"242": ""}
{"243": "CHAPTER 7\nTransactions\nSome authors have claimed that general two-phase commit is too expensive to support,\nbecause of the performance or availability problems that it brings. We believe it is better to\nhave application programmers deal with performance problems due to overuse of transac\u2010\ntions as bottlenecks arise, rather than always coding around the lack of transactions.\n\u2014James Corbett et al., Spanner: Google\u2019s Globally-Distributed Database (2012)\nIn the harsh reality of data systems, many things can go wrong:\n\u2022 The database software or hardware may fail at any time (including in the middle\nof a write operation).\n\u2022 The application may crash at any time (including halfway through a series of\noperations).\n\u2022 Interruptions in the network can unexpectedly cut off the application from the\ndatabase, or one database node from another.\n\u2022 Several clients may write to the database at the same time, overwriting each\nother\u2019s changes.\n\u2022 A client may read data that doesn\u2019t make sense because it has only partially been\nupdated.\n\u2022 Race conditions between clients can cause surprising bugs.\nIn order to be reliable, a system has to deal with these faults and ensure that they\ndon\u2019t cause catastrophic failure of the entire system. However, implementing fault-\ntolerance mechanisms is a lot of work. It requires a lot of careful thinking about all\nthe things that can go wrong, and a lot of testing to ensure that the solution actually"}
{"244": "For decades, transactions have been the mechanism of choice for simplifying these\nissues. A transaction is a way for an application to group several reads and writes\ntogether into a logical unit. Conceptually, all the reads and writes in a transaction are\nexecuted as one operation: either the entire transaction succeeds (commit) or it fails\n(abort, rollback). If it fails, the application can safely retry. With transactions, error\nhandling becomes much simpler for an application, because it doesn\u2019t need to worry\nabout partial failure\u2014i.e., the case where some operations succeed and some fail (for\nwhatever reason).\nIf you have spent years working with transactions, they may seem obvious, but we\nshouldn\u2019t take them for granted. Transactions are not a law of nature; they were cre\u2010\nated with a purpose, namely to simplify the programming model for applications\naccessing a database. By using transactions, the application is free to ignore certain\npotential error scenarios and concurrency issues, because the database takes care of\nthem instead (we call these safety guarantees).\nNot every application needs transactions, and sometimes there are advantages to\nweakening transactional guarantees or abandoning them entirely (for example, to\nachieve higher performance or higher availability). Some safety properties can be\nachieved without transactions.\nHow do you figure out whether you need transactions? In order to answer that ques\u2010\ntion, we first need to understand exactly what safety guarantees transactions can pro\u2010\nvide, and what costs are associated with them. Although transactions seem\nstraightforward at first glance, there are actually many subtle but important details\nthat come into play.\nIn this chapter, we will examine many examples of things that can go wrong, and\nexplore the algorithms that databases use to guard against those issues. We will go\nespecially deep in the area of concurrency control, discussing various kinds of race\nconditions that can occur and how databases implement isolation levels such as read\ncommitted, snapshot isolation, and serializability.\nThis chapter applies to both single-node and distributed databases; in Chapter 8 we\nwill focus the discussion on the particular challenges that arise only in distributed\nsystems.\nThe Slippery Concept of a Transaction\nAlmost all relational databases today, and some nonrelational databases, support\ntransactions. Most of them follow the style that was introduced in 1975 by IBM Sys\u2010\ntem R, the first SQL database [1, 2, 3]. Although some implementation details have"}
{"245": "In the late 2000s, nonrelational (NoSQL) databases started gaining popularity. They\naimed to improve upon the relational status quo by offering a choice of new data\nmodels (see Chapter 2), and by including replication (Chapter 5) and partitioning\n(Chapter 6) by default. Transactions were the main casualty of this movement: many\nof this new generation of databases abandoned transactions entirely, or redefined the\nword to describe a much weaker set of guarantees than had previously been under\u2010\nstood [4].\nWith the hype around this new crop of distributed databases, there emerged a popu\u2010\nlar belief that transactions were the antithesis of scalability, and that any large-scale\nsystem would have to abandon transactions in order to maintain good performance\nand high availability [5, 6]. On the other hand, transactional guarantees are some\u2010\ntimes presented by database vendors as an essential requirement for \u201cserious applica\u2010\ntions\u201d with \u201cvaluable data.\u201d Both viewpoints are pure hyperbole.\nThe truth is not that simple: like every other technical design choice, transactions\nhave advantages and limitations. In order to understand those trade-offs, let\u2019s go into\nthe details of the guarantees that transactions can provide\u2014both in normal operation\nand in various extreme (but realistic) circumstances.\nThe Meaning of ACID\nThe safety guarantees provided by transactions are often described by the well-\nknown acronym ACID, which stands for Atomicity, Consistency, Isolation, and Dura\u2010\nbility. It was coined in 1983 by Theo H\u00e4rder and Andreas Reuter [7] in an effort to\nestablish precise terminology for fault-tolerance mechanisms in databases.\nHowever, in practice, one database\u2019s implementation of ACID does not equal\nanother\u2019s implementation. For example, as we shall see, there is a lot of ambiguity\naround the meaning of isolation [8]. The high-level idea is sound, but the devil is in\nthe details. Today, when a system claims to be \u201cACID compliant,\u201d it\u2019s unclear what\nguarantees you can actually expect. ACID has unfortunately become mostly a mar\u2010\nketing term.\n(Systems that do not meet the ACID criteria are sometimes called BASE, which\nstands for Basically Available, Soft state, and Eventual consistency [9]. This is even\nmore vague than the definition of ACID. It seems that the only sensible definition of\nBASE is \u201cnot ACID\u201d; i.e., it can mean almost anything you want.)\nLet\u2019s dig into the definitions of atomicity, consistency, isolation, and durability, as\nthis will let us refine our idea of transactions."}
{"246": "ing. For example, in multi-threaded programming, if one thread executes an atomic\noperation, that means there is no way that another thread could see the half-finished\nresult of the operation. The system can only be in the state it was before the operation\nor after the operation, not something in between.\nBy contrast, in the context of ACID, atomicity is not about concurrency. It does not\ndescribe what happens if several processes try to access the same data at the same\ntime, because that is covered under the letter I, for isolation (see \u201cIsolation\u201d on page\n225).\nRather, ACID atomicity describes what happens if a client wants to make several\nwrites, but a fault occurs after some of the writes have been processed\u2014for example,\na process crashes, a network connection is interrupted, a disk becomes full, or some\nintegrity constraint is violated. If the writes are grouped together into an atomic\ntransaction, and the transaction cannot be completed (committed) due to a fault, then\nthe transaction is aborted and the database must discard or undo any writes it has\nmade so far in that transaction.\nWithout atomicity, if an error occurs partway through making multiple changes, it\u2019s\ndifficult to know which changes have taken effect and which haven\u2019t. The application\ncould try again, but that risks making the same change twice, leading to duplicate or\nincorrect data. Atomicity simplifies this problem: if a transaction was aborted, the\napplication can be sure that it didn\u2019t change anything, so it can safely be retried.\nThe ability to abort a transaction on error and have all writes from that transaction\ndiscarded is the defining feature of ACID atomicity. Perhaps abortability would have\nbeen a better term than atomicity, but we will stick with atomicity since that\u2019s the\nusual word.\nConsistency\nThe word consistency is terribly overloaded:\n\u2022 In Chapter 5 we discussed replica consistency and the issue of eventual consis\u2010\ntency that arises in asynchronously replicated systems (see \u201cProblems with Repli\u2010\ncation Lag\u201d on page 161).\n\u2022 Consistent hashing is an approach to partitioning that some systems use for reba\u2010\nlancing (see \u201cConsistent Hashing\u201d on page 204).\n\u2022 In the CAP theorem (see Chapter 9), the word consistency is used to mean linear\u2010\nizability (see \u201cLinearizability\u201d on page 324).\n\u2022 In the context of ACID, consistency refers to an application-specific notion of the"}
{"247": "The idea of ACID consistency is that you have certain statements about your data\n(invariants) that must always be true\u2014for example, in an accounting system, credits\nand debits across all accounts must always be balanced. If a transaction starts with a\ndatabase that is valid according to these invariants, and any writes during the transac\u2010\ntion preserve the validity, then you can be sure that the invariants are always satisfied.\nHowever, this idea of consistency depends on the application\u2019s notion of invariants,\nand it\u2019s the application\u2019s responsibility to define its transactions correctly so that they\npreserve consistency. This is not something that the database can guarantee: if you\nwrite bad data that violates your invariants, the database can\u2019t stop you. (Some spe\u2010\ncific kinds of invariants can be checked by the database, for example using foreign\nkey constraints or uniqueness constraints. However, in general, the application\ndefines what data is valid or invalid\u2014the database only stores it.)\nAtomicity, isolation, and durability are properties of the database, whereas consis\u2010\ntency (in the ACID sense) is a property of the application. The application may rely\non the database\u2019s atomicity and isolation properties in order to achieve consistency,\nbut it\u2019s not up to the database alone. Thus, the letter C doesn\u2019t really belong in ACID.i\nIsolation\nMost databases are accessed by several clients at the same time. That is no problem if\nthey are reading and writing different parts of the database, but if they are accessing\nthe same database records, you can run into concurrency problems (race conditions).\nFigure 7-1 is a simple example of this kind of problem. Say you have two clients\nsimultaneously incrementing a counter that is stored in a database. Each client needs\nto read the current value, add 1, and write the new value back (assuming there is no\nincrement operation built into the database). In Figure 7-1 the counter should have\nincreased from 42 to 44, because two increments happened, but it actually only went\nto 43 because of the race condition.\nIsolation in the sense of ACID means that concurrently executing transactions are\nisolated from each other: they cannot step on each other\u2019s toes. The classic database\ntextbooks formalize isolation as serializability, which means that each transaction can\npretend that it is the only transaction running on the entire database. The database\nensures that when the transactions have committed, the result is the same as if they\nhad run serially (one after another), even though in reality they may have run con\u2010\ncurrently [10]."}
{"248": "Figure 7-1. A race condition between two clients concurrently incrementing a counter.\nHowever, in practice, serializable isolation is rarely used, because it carries a perfor\u2010\nmance penalty. Some popular databases, such as Oracle 11g, don\u2019t even implement it.\nIn Oracle there is an isolation level called \u201cserializable,\u201d but it actually implements\nsomething called snapshot isolation, which is a weaker guarantee than serializability\n[8, 11]. We will explore snapshot isolation and other forms of isolation in \u201cWeak Iso\u2010\nlation Levels\u201d on page 233.\nDurability\nThe purpose of a database system is to provide a safe place where data can be stored\nwithout fear of losing it. Durability is the promise that once a transaction has com\u2010\nmitted successfully, any data it has written will not be forgotten, even if there is a\nhardware fault or the database crashes.\nIn a single-node database, durability typically means that the data has been written to\nnonvolatile storage such as a hard drive or SSD. It usually also involves a write-ahead\nlog or similar (see \u201cMaking B-trees reliable\u201d on page 82), which allows recovery in the\nevent that the data structures on disk are corrupted. In a replicated database, durabil\u2010\nity may mean that the data has been successfully copied to some number of nodes. In\norder to provide a durability guarantee, a database must wait until these writes or\nreplications are complete before reporting a transaction as successfully committed.\nAs discussed in \u201cReliability\u201d on page 6, perfect durability does not exist: if all your\nhard disks and all your backups are destroyed at the same time, there\u2019s obviously\nnothing your database can do to save you."}
{"249": "Replication and Durability\nHistorically, durability meant writing to an archive tape. Then it was understood as\nwriting to a disk or SSD. More recently, it has been adapted to mean replication.\nWhich implementation is better?\nThe truth is, nothing is perfect:\n\u2022 If you write to disk and the machine dies, even though your data isn\u2019t lost, it is\ninaccessible until you either fix the machine or transfer the disk to another\nmachine. Replicated systems can remain available.\n\u2022 A correlated fault\u2014a power outage or a bug that crashes every node on a particu\u2010\nlar input\u2014can knock out all replicas at once (see \u201cReliability\u201d on page 6), losing\nany data that is only in memory. Writing to disk is therefore still relevant for in-\nmemory databases.\n\u2022 In an asynchronously replicated system, recent writes may be lost when the\nleader becomes unavailable (see \u201cHandling Node Outages\u201d on page 156).\n\u2022 When the power is suddenly cut, SSDs in particular have been shown to some\u2010\ntimes violate the guarantees they are supposed to provide: even fsync isn\u2019t guar\u2010\nanteed to work correctly [12]. Disk firmware can have bugs, just like any other\nkind of software [13, 14].\n\u2022 Subtle interactions between the storage engine and the filesystem implementa\u2010\ntion can lead to bugs that are hard to track down, and may cause files on disk to\nbe corrupted after a crash [15, 16].\n\u2022 Data on disk can gradually become corrupted without this being detected [17]. If\ndata has been corrupted for some time, replicas and recent backups may also be\ncorrupted. In this case, you will need to try to restore the data from a historical\nbackup.\n\u2022 One study of SSDs found that between 30% and 80% of drives develop at least\none bad block during the first four years of operation [18]. Magnetic hard drives\nhave a lower rate of bad sectors, but a higher rate of complete failure than SSDs.\n\u2022 If an SSD is disconnected from power, it can start losing data within a few weeks,\ndepending on the temperature [19].\nIn practice, there is no one technique that can provide absolute guarantees. There are\nonly various risk-reduction techniques, including writing to disk, replicating to\nremote machines, and backups\u2014and they can and should be used together. As\nalways, it\u2019s wise to take any theoretical \u201cguarantees\u201d with a healthy grain of salt."}
{"250": "Single-Object and Multi-Object Operations\nTo recap, in ACID, atomicity and isolation describe what the database should do if a\nclient makes several writes within the same transaction:\nAtomicity\nIf an error occurs halfway through a sequence of writes, the transaction should\nbe aborted, and the writes made up to that point should be discarded. In other\nwords, the database saves you from having to worry about partial failure, by giv\u2010\ning an all-or-nothing guarantee.\nIsolation\nConcurrently running transactions shouldn\u2019t interfere with each other. For\nexample, if one transaction makes several writes, then another transaction should\nsee either all or none of those writes, but not some subset.\nThese definitions assume that you want to modify several objects (rows, documents,\nrecords) at once. Such multi-object transactions are often needed if several pieces of\ndata need to be kept in sync. Figure 7-2 shows an example from an email application.\nTo display the number of unread messages for a user, you could query something\nlike:\nSELECT COUNT(*) FROM emails WHERE recipient_id = 2 AND unread_flag = true\nHowever, you might find this query to be too slow if there are many emails, and\ndecide to store the number of unread messages in a separate field (a kind of denorm\u2010\nalization). Now, whenever a new message comes in, you have to increment the\nunread counter as well, and whenever a message is marked as read, you also have to\ndecrement the unread counter.\nIn Figure 7-2, user 2 experiences an anomaly: the mailbox listing shows an unread\nmessage, but the counter shows zero unread messages because the counter increment\nhas not yet happened.ii Isolation would have prevented this issue by ensuring that\nuser 2 sees either both the inserted email and the updated counter, or neither, but not\nan inconsistent halfway point."}
{"251": "Figure 7-2. Violating isolation: one transaction reads another transaction\u2019s uncommit\u2010\nted writes (a \u201cdirty read\u201d).\nFigure 7-3 illustrates the need for atomicity: if an error occurs somewhere over the\ncourse of the transaction, the contents of the mailbox and the unread counter might\nbecome out of sync. In an atomic transaction, if the update to the counter fails, the\ntransaction is aborted and the inserted email is rolled back.\nFigure 7-3. Atomicity ensures that if an error occurs any prior writes from that transac\u2010\ntion are undone, to avoid an inconsistent state.\nMulti-object transactions require some way of determining which read and write\noperations belong to the same transaction. In relational databases, that is typically\ndone based on the client\u2019s TCP connection to the database server: on any particular\nconnection, everything between a BEGIN TRANSACTION and a COMMIT statement is\nconsidered to be part of the same transaction.iii"}
{"252": "On the other hand, many nonrelational databases don\u2019t have such a way of grouping\noperations together. Even if there is a multi-object API (for example, a key-value\nstore may have a multi-put operation that updates several keys in one operation), that\ndoesn\u2019t necessarily mean it has transaction semantics: the command may succeed for\nsome keys and fail for others, leaving the database in a partially updated state.\nSingle-object writes\nAtomicity and isolation also apply when a single object is being changed. For exam\u2010\nple, imagine you are writing a 20 KB JSON document to a database:\n\u2022 If the network connection is interrupted after the first 10 KB have been sent, does\nthe database store that unparseable 10 KB fragment of JSON?\n\u2022 If the power fails while the database is in the middle of overwriting the previous\nvalue on disk, do you end up with the old and new values spliced together?\n\u2022 If another client reads that document while the write is in progress, will it see a\npartially updated value?\nThose issues would be incredibly confusing, so storage engines almost universally\naim to provide atomicity and isolation on the level of a single object (such as a key-\nvalue pair) on one node. Atomicity can be implemented using a log for crash recov\u2010\nery (see \u201cMaking B-trees reliable\u201d on page 82), and isolation can be implemented\nusing a lock on each object (allowing only one thread to access an object at any one\ntime).\nSome databases also provide more complex atomic operations,iv such as an increment\noperation, which removes the need for a read-modify-write cycle like that in\nFigure 7-1. Similarly popular is a compare-and-set operation, which allows a write to\nhappen only if the value has not been concurrently changed by someone else (see\n\u201cCompare-and-set\u201d on page 245).\nThese single-object operations are useful, as they can prevent lost updates when sev\u2010\neral clients try to write to the same object concurrently (see \u201cPreventing Lost\nUpdates\u201d on page 242). However, they are not transactions in the usual sense of the\nword. Compare-and-set and other single-object operations have been dubbed \u201clight\u2010\nweight transactions\u201d or even \u201cACID\u201d for marketing purposes [20, 21, 22], but that\nterminology is misleading. A transaction is usually understood as a mechanism for\ngrouping multiple operations on multiple objects into one unit of execution."}
{"253": "The need for multi-object transactions\nMany distributed datastores have abandoned multi-object transactions because they\nare difficult to implement across partitions, and they can get in the way in some sce\u2010\nnarios where very high availability or performance is required. However, there is\nnothing that fundamentally prevents transactions in a distributed database, and we\nwill discuss implementations of distributed transactions in Chapter 9.\nBut do we need multi-object transactions at all? Would it be possible to implement\nany application with only a key-value data model and single-object operations?\nThere are some use cases in which single-object inserts, updates, and deletes are suffi\u2010\ncient. However, in many other cases writes to several different objects need to be\ncoordinated:\n\u2022 In a relational data model, a row in one table often has a foreign key reference to\na row in another table. (Similarly, in a graph-like data model, a vertex has edges\nto other vertices.) Multi-object transactions allow you to ensure that these refer\u2010\nences remain valid: when inserting several records that refer to one another, the\nforeign keys have to be correct and up to date, or the data becomes nonsensical.\n\u2022 In a document data model, the fields that need to be updated together are often\nwithin the same document, which is treated as a single object\u2014no multi-object\ntransactions are needed when updating a single document. However, document\ndatabases lacking join functionality also encourage denormalization (see \u201cRela\u2010\ntional Versus Document Databases Today\u201d on page 38). When denormalized\ninformation needs to be updated, like in the example of Figure 7-2, you need to\nupdate several documents in one go. Transactions are very useful in this situation\nto prevent denormalized data from going out of sync.\n\u2022 In databases with secondary indexes (almost everything except pure key-value\nstores), the indexes also need to be updated every time you change a value. These\nindexes are different database objects from a transaction point of view: for exam\u2010\nple, without transaction isolation, it\u2019s possible for a record to appear in one index\nbut not another, because the update to the second index hasn\u2019t happened yet.\nSuch applications can still be implemented without transactions. However, error han\u2010\ndling becomes much more complicated without atomicity, and the lack of isolation\ncan cause concurrency problems. We will discuss those in \u201cWeak Isolation Levels\u201d on\npage 233, and explore alternative approaches in Chapter 12.\nHandling errors and aborts"}
{"254": "of violating its guarantee of atomicity, isolation, or durability, it would rather aban\u2010\ndon the transaction entirely than allow it to remain half-finished.\nNot all systems follow that philosophy, though. In particular, datastores with leader\u2010\nless replication (see \u201cLeaderless Replication\u201d on page 177) work much more on a\n\u201cbest effort\u201d basis, which could be summarized as \u201cthe database will do as much as it\ncan, and if it runs into an error, it won\u2019t undo something it has already done\u201d\u2014so it\u2019s\nthe application\u2019s responsibility to recover from errors.\nErrors will inevitably happen, but many software developers prefer to think only\nabout the happy path rather than the intricacies of error handling. For example, pop\u2010\nular object-relational mapping (ORM) frameworks such as Rails\u2019s ActiveRecord and\nDjango don\u2019t retry aborted transactions\u2014the error usually results in an exception\nbubbling up the stack, so any user input is thrown away and the user gets an error\nmessage. This is a shame, because the whole point of aborts is to enable safe retries.\nAlthough retrying an aborted transaction is a simple and effective error handling\nmechanism, it isn\u2019t perfect:\n\u2022 If the transaction actually succeeded, but the network failed while the server tried\nto acknowledge the successful commit to the client (so the client thinks it failed),\nthen retrying the transaction causes it to be performed twice\u2014unless you have an\nadditional application-level deduplication mechanism in place.\n\u2022 If the error is due to overload, retrying the transaction will make the problem\nworse, not better. To avoid such feedback cycles, you can limit the number of\nretries, use exponential backoff, and handle overload-related errors differently\nfrom other errors (if possible).\n\u2022 It is only worth retrying after transient errors (for example due to deadlock, iso\u2010\nlation violation, temporary network interruptions, and failover); after a perma\u2010\nnent error (e.g., constraint violation) a retry would be pointless.\n\u2022 If the transaction also has side effects outside of the database, those side effects\nmay happen even if the transaction is aborted. For example, if you\u2019re sending an\nemail, you wouldn\u2019t want to send the email again every time you retry the trans\u2010\naction. If you want to make sure that several different systems either commit or\nabort together, two-phase commit can help (we will discuss this in \u201cAtomic\nCommit and Two-Phase Commit (2PC)\u201d on page 354).\n\u2022 If the client process fails while retrying, any data it was trying to write to the\ndatabase is lost."}
{"255": "Weak Isolation Levels\nIf two transactions don\u2019t touch the same data, they can safely be run in parallel,\nbecause neither depends on the other. Concurrency issues (race conditions) only\ncome into play when one transaction reads data that is concurrently modified by\nanother transaction, or when two transactions try to simultaneously modify the same\ndata.\nConcurrency bugs are hard to find by testing, because such bugs are only triggered\nwhen you get unlucky with the timing. Such timing issues might occur very rarely,\nand are usually difficult to reproduce. Concurrency is also very difficult to reason\nabout, especially in a large application where you don\u2019t necessarily know which other\npieces of code are accessing the database. Application development is difficult\nenough if you just have one user at a time; having many concurrent users makes it\nmuch harder still, because any piece of data could unexpectedly change at any time.\nFor that reason, databases have long tried to hide concurrency issues from applica\u2010\ntion developers by providing transaction isolation. In theory, isolation should make\nyour life easier by letting you pretend that no concurrency is happening: serializable\nisolation means that the database guarantees that transactions have the same effect as\nif they ran serially (i.e., one at a time, without any concurrency).\nIn practice, isolation is unfortunately not that simple. Serializable isolation has a per\u2010\nformance cost, and many databases don\u2019t want to pay that price [8]. It\u2019s therefore\ncommon for systems to use weaker levels of isolation, which protect against some\nconcurrency issues, but not all. Those levels of isolation are much harder to under\u2010\nstand, and they can lead to subtle bugs, but they are nevertheless used in practice\n[23].\nConcurrency bugs caused by weak transaction isolation are not just a theoretical\nproblem. They have caused substantial loss of money [24, 25], led to investigation by\nfinancial auditors [26], and caused customer data to be corrupted [27]. A popular\ncomment on revelations of such problems is \u201cUse an ACID database if you\u2019re han\u2010\ndling financial data!\u201d\u2014but that misses the point. Even many popular relational data\u2010\nbase systems (which are usually considered \u201cACID\u201d) use weak isolation, so they\nwouldn\u2019t necessarily have prevented these bugs from occurring.\nRather than blindly relying on tools, we need to develop a good understanding of the\nkinds of concurrency problems that exist, and how to prevent them. Then we can\nbuild applications that are reliable and correct, using the tools at our disposal.\nIn this section we will look at several weak (nonserializable) isolation levels that are"}
{"256": "251). Our discussion of isolation levels will be informal, using examples. If you want\nrigorous definitions and analyses of their properties, you can find them in the aca\u2010\ndemic literature [28, 29, 30].\nRead Committed\nThe most basic level of transaction isolation is read committed.v It makes two guaran\u2010\ntees:\n1. When reading from the database, you will only see data that has been committed\n(no dirty reads).\n2. When writing to the database, you will only overwrite data that has been com\u2010\nmitted (no dirty writes).\nLet\u2019s discuss these two guarantees in more detail.\nNo dirty reads\nImagine a transaction has written some data to the database, but the transaction has\nnot yet committed or aborted. Can another transaction see that uncommitted data? If\nyes, that is called a dirty read [2].\nTransactions running at the read committed isolation level must prevent dirty reads.\nThis means that any writes by a transaction only become visible to others when that\ntransaction commits (and then all of its writes become visible at once). This is illus\u2010\ntrated in Figure 7-4, where user 1 has set x = 3, but user 2\u2019s get x still returns the old\nvalue, 2, while user 1 has not yet committed.\nFigure 7-4. No dirty reads: user 2 sees the new value for x only after user 1\u2019s transaction\nhas committed."}
{"257": "There are a few reasons why it\u2019s useful to prevent dirty reads:\n\u2022 If a transaction needs to update several objects, a dirty read means that another\ntransaction may see some of the updates but not others. For example, in\nFigure 7-2, the user sees the new unread email but not the updated counter. This\nis a dirty read of the email. Seeing the database in a partially updated state is con\u2010\nfusing to users and may cause other transactions to take incorrect decisions.\n\u2022 If a transaction aborts, any writes it has made need to be rolled back (like in\nFigure 7-3). If the database allows dirty reads, that means a transaction may see\ndata that is later rolled back\u2014i.e., which is never actually committed to the data\u2010\nbase. Reasoning about the consequences quickly becomes mind-bending.\nNo dirty writes\nWhat happens if two transactions concurrently try to update the same object in a\ndatabase? We don\u2019t know in which order the writes will happen, but we normally\nassume that the later write overwrites the earlier write.\nHowever, what happens if the earlier write is part of a transaction that has not yet\ncommitted, so the later write overwrites an uncommitted value? This is called a dirty\nwrite [28]. Transactions running at the read committed isolation level must prevent\ndirty writes, usually by delaying the second write until the first write\u2019s transaction has\ncommitted or aborted.\nBy preventing dirty writes, this isolation level avoids some kinds of concurrency\nproblems:\n\u2022 If transactions update multiple objects, dirty writes can lead to a bad outcome.\nFor example, consider Figure 7-5, which illustrates a used car sales website on\nwhich two people, Alice and Bob, are simultaneously trying to buy the same car.\nBuying a car requires two database writes: the listing on the website needs to be\nupdated to reflect the buyer, and the sales invoice needs to be sent to the buyer.\nIn the case of Figure 7-5, the sale is awarded to Bob (because he performs the\nwinning update to the listings table), but the invoice is sent to Alice (because\nshe performs the winning update to the invoices table). Read committed pre\u2010\nvents such mishaps.\n\u2022 However, read committed does not prevent the race condition between two\ncounter increments in Figure 7-1. In this case, the second write happens after the\nfirst transaction has committed, so it\u2019s not a dirty write. It\u2019s still incorrect, but for"}
{"258": "Figure 7-5. With dirty writes, conflicting writes from different transactions can be\nmixed up.\nImplementing read committed\nRead committed is a very popular isolation level. It is the default setting in Oracle\n11g, PostgreSQL, SQL Server 2012, MemSQL, and many other databases [8].\nMost commonly, databases prevent dirty writes by using row-level locks: when a\ntransaction wants to modify a particular object (row or document), it must first\nacquire a lock on that object. It must then hold that lock until the transaction is com\u2010\nmitted or aborted. Only one transaction can hold the lock for any given object; if\nanother transaction wants to write to the same object, it must wait until the first\ntransaction is committed or aborted before it can acquire the lock and continue. This\nlocking is done automatically by databases in read committed mode (or stronger iso\u2010\nlation levels).\nHow do we prevent dirty reads? One option would be to use the same lock, and to\nrequire any transaction that wants to read an object to briefly acquire the lock and\nthen release it again immediately after reading. This would ensure that a read\ncouldn\u2019t happen while an object has a dirty, uncommitted value (because during that\ntime the lock would be held by the transaction that has made the write).\nHowever, the approach of requiring read locks does not work well in practice,\nbecause one long-running write transaction can force many read-only transactions to\nwait until the long-running transaction has completed. This harms the response time\nof read-only transactions and is bad for operability: a slowdown in one part of an\napplication can have a knock-on effect in a completely different part of the applica\u2010"}
{"259": "For that reason, most databasesvi prevent dirty reads using the approach illustrated in\nFigure 7-4: for every object that is written, the database remembers both the old com\u2010\nmitted value and the new value set by the transaction that currently holds the write\nlock. While the transaction is ongoing, any other transactions that read the object are\nsimply given the old value. Only when the new value is committed do transactions\nswitch over to reading the new value.\nSnapshot Isolation and Repeatable Read\nIf you look superficially at read committed isolation, you could be forgiven for think\u2010\ning that it does everything that a transaction needs to do: it allows aborts (required\nfor atomicity), it prevents reading the incomplete results of transactions, and it pre\u2010\nvents concurrent writes from getting intermingled. Indeed, those are useful features,\nand much stronger guarantees than you can get from a system that has no transac\u2010\ntions.\nHowever, there are still plenty of ways in which you can have concurrency bugs when\nusing this isolation level. For example, Figure 7-6 illustrates a problem that can occur\nwith read committed.\nFigure 7-6. Read skew: Alice observes the database in an inconsistent state.\nSay Alice has $1,000 of savings at a bank, split across two accounts with $500 each.\nNow a transaction transfers $100 from one of her accounts to the other. If she is\nunlucky enough to look at her list of account balances in the same moment as that\ntransaction is being processed, she may see one account balance at a time before the"}
{"260": "incoming payment has arrived (with a balance of $500), and the other account after\nthe outgoing transfer has been made (the new balance being $400). To Alice it now\nappears as though she only has a total of $900 in her accounts\u2014it seems that $100 has\nvanished into thin air.\nThis anomaly is called a nonrepeatable read or read skew: if Alice were to read the\nbalance of account 1 again at the end of the transaction, she would see a different\nvalue ($600) than she saw in her previous query. Read skew is considered acceptable\nunder read committed isolation: the account balances that Alice saw were indeed\ncommitted at the time when she read them.\nThe term skew is unfortunately overloaded: we previously used it in\nthe sense of an unbalanced workload with hot spots (see \u201cSkewed\nWorkloads and Relieving Hot Spots\u201d on page 205), whereas here it\nmeans timing anomaly.\nIn Alice\u2019s case, this is not a lasting problem, because she will most likely see consis\u2010\ntent account balances if she reloads the online banking website a few seconds later.\nHowever, some situations cannot tolerate such temporary inconsistency:\nBackups\nTaking a backup requires making a copy of the entire database, which may take\nhours on a large database. During the time that the backup process is running,\nwrites will continue to be made to the database. Thus, you could end up with\nsome parts of the backup containing an older version of the data, and other parts\ncontaining a newer version. If you need to restore from such a backup, the\ninconsistencies (such as disappearing money) become permanent.\nAnalytic queries and integrity checks\nSometimes, you may want to run a query that scans over large parts of the data\u2010\nbase. Such queries are common in analytics (see \u201cTransaction Processing or Ana\u2010\nlytics?\u201d on page 90), or may be part of a periodic integrity check that everything\nis in order (monitoring for data corruption). These queries are likely to return\nnonsensical results if they observe parts of the database at different points in\ntime.\nSnapshot isolation [28] is the most common solution to this problem. The idea is that\neach transaction reads from a consistent snapshot of the database\u2014that is, the trans\u2010\naction sees all the data that was committed in the database at the start of the transac\u2010\ntion. Even if the data is subsequently changed by another transaction, each"}
{"261": "it operates is changing at the same time as the query is executing. When a transaction\ncan see a consistent snapshot of the database, frozen at a particular point in time, it is\nmuch easier to understand.\nSnapshot isolation is a popular feature: it is supported by PostgreSQL, MySQL with\nthe InnoDB storage engine, Oracle, SQL Server, and others [23, 31, 32].\nImplementing snapshot isolation\nLike read committed isolation, implementations of snapshot isolation typically use\nwrite locks to prevent dirty writes (see \u201cImplementing read committed\u201d on page 236),\nwhich means that a transaction that makes a write can block the progress of another\ntransaction that writes to the same object. However, reads do not require any locks.\nFrom a performance point of view, a key principle of snapshot isolation is readers\nnever block writers, and writers never block readers. This allows a database to handle\nlong-running read queries on a consistent snapshot at the same time as processing\nwrites normally, without any lock contention between the two.\nTo implement snapshot isolation, databases use a generalization of the mechanism\nwe saw for preventing dirty reads in Figure 7-4. The database must potentially keep\nseveral different committed versions of an object, because various in-progress trans\u2010\nactions may need to see the state of the database at different points in time. Because it\nmaintains several versions of an object side by side, this technique is known as multi-\nversion concurrency control (MVCC).\nIf a database only needed to provide read committed isolation, but not snapshot iso\u2010\nlation, it would be sufficient to keep two versions of an object: the committed version\nand the overwritten-but-not-yet-committed version. However, storage engines that\nsupport snapshot isolation typically use MVCC for their read committed isolation\nlevel as well. A typical approach is that read committed uses a separate snapshot for\neach query, while snapshot isolation uses the same snapshot for an entire transaction.\nFigure 7-7 illustrates how MVCC-based snapshot isolation is implemented in Post\u2010\ngreSQL [31] (other implementations are similar). When a transaction is started, it is\ngiven a unique, always-increasingvii transaction ID (txid). Whenever a transaction\nwrites anything to the database, the data it writes is tagged with the transaction ID of\nthe writer."}
{"262": "Figure 7-7. Implementing snapshot isolation using multi-version objects.\nEach row in a table has a created_by field, containing the ID of the transaction that\ninserted this row into the table. Moreover, each row has a deleted_by field, which is\ninitially empty. If a transaction deletes a row, the row isn\u2019t actually deleted from the\ndatabase, but it is marked for deletion by setting the deleted_by field to the ID of the\ntransaction that requested the deletion. At some later time, when it is certain that no\ntransaction can any longer access the deleted data, a garbage collection process in the\ndatabase removes any rows marked for deletion and frees their space.\nAn update is internally translated into a delete and a create. For example, in\nFigure 7-7, transaction 13 deducts $100 from account 2, changing the balance from\n$500 to $400. The accounts table now actually contains two rows for account 2: a\nrow with a balance of $500 which was marked as deleted by transaction 13, and a row\nwith a balance of $400 which was created by transaction 13.\nVisibility rules for observing a consistent snapshot"}
{"263": "the database can present a consistent snapshot of the database to the application. This\nworks as follows:\n1. At the start of each transaction, the database makes a list of all the other transac\u2010\ntions that are in progress (not yet committed or aborted) at that time. Any writes\nthat those transactions have made are ignored, even if the transactions subse\u2010\nquently commit.\n2. Any writes made by aborted transactions are ignored.\n3. Any writes made by transactions with a later transaction ID (i.e., which started\nafter the current transaction started) are ignored, regardless of whether those\ntransactions have committed.\n4. All other writes are visible to the application\u2019s queries.\nThese rules apply to both creation and deletion of objects. In Figure 7-7, when trans\u2010\naction 12 reads from account 2, it sees a balance of $500 because the deletion of the\n$500 balance was made by transaction 13 (according to rule 3, transaction 12 cannot\nsee a deletion made by transaction 13), and the creation of the $400 balance is not yet\nvisible (by the same rule).\nPut another way, an object is visible if both of the following conditions are true:\n\u2022 At the time when the reader\u2019s transaction started, the transaction that created the\nobject had already committed.\n\u2022 The object is not marked for deletion, or if it is, the transaction that requested\ndeletion had not yet committed at the time when the reader\u2019s transaction started.\nA long-running transaction may continue using a snapshot for a long time, continu\u2010\ning to read values that (from other transactions\u2019 point of view) have long been over\u2010\nwritten or deleted. By never updating values in place but instead creating a new\nversion every time a value is changed, the database can provide a consistent snapshot\nwhile incurring only a small overhead.\nIndexes and snapshot isolation\nHow do indexes work in a multi-version database? One option is to have the index\nsimply point to all versions of an object and require an index query to filter out any\nobject versions that are not visible to the current transaction. When garbage collec\u2010\ntion removes old object versions that are no longer visible to any transaction, the cor\u2010\nresponding index entries can also be removed."}
{"264": "Another approach is used in CouchDB, Datomic, and LMDB. Although they also use\nB-trees (see \u201cB-Trees\u201d on page 79), they use an append-only/copy-on-write variant\nthat does not overwrite pages of the tree when they are updated, but instead creates a\nnew copy of each modified page. Parent pages, up to the root of the tree, are copied\nand updated to point to the new versions of their child pages. Any pages that are not\naffected by a write do not need to be copied, and remain immutable [33, 34, 35].\nWith append-only B-trees, every write transaction (or batch of transactions) creates a\nnew B-tree root, and a particular root is a consistent snapshot of the database at the\npoint in time when it was created. There is no need to filter out objects based on\ntransaction IDs because subsequent writes cannot modify an existing B-tree; they can\nonly create new tree roots. However, this approach also requires a background pro\u2010\ncess for compaction and garbage collection.\nRepeatable read and naming confusion\nSnapshot isolation is a useful isolation level, especially for read-only transactions.\nHowever, many databases that implement it call it by different names. In Oracle it is\ncalled serializable, and in PostgreSQL and MySQL it is called repeatable read [23].\nThe reason for this naming confusion is that the SQL standard doesn\u2019t have the con\u2010\ncept of snapshot isolation, because the standard is based on System R\u2019s 1975 defini\u2010\ntion of isolation levels [2] and snapshot isolation hadn\u2019t yet been invented then.\nInstead, it defines repeatable read, which looks superficially similar to snapshot isola\u2010\ntion. PostgreSQL and MySQL call their snapshot isolation level repeatable read\nbecause it meets the requirements of the standard, and so they can claim standards\ncompliance.\nUnfortunately, the SQL standard\u2019s definition of isolation levels is flawed\u2014it is ambig\u2010\nuous, imprecise, and not as implementation-independent as a standard should be\n[28]. Even though several databases implement repeatable read, there are big differ\u2010\nences in the guarantees they actually provide, despite being ostensibly standardized\n[23]. There has been a formal definition of repeatable read in the research literature\n[29, 30], but most implementations don\u2019t satisfy that formal definition. And to top it\noff, IBM DB2 uses \u201crepeatable read\u201d to refer to serializability [8].\nAs a result, nobody really knows what repeatable read means.\nPreventing Lost Updates\nThe read committed and snapshot isolation levels we\u2019ve discussed so far have been\nprimarily about the guarantees of what a read-only transaction can see in the pres\u2010"}
{"265": "There are several other interesting kinds of conflicts that can occur between concur\u2010\nrently writing transactions. The best known of these is the lost update problem, illus\u2010\ntrated in Figure 7-1 with the example of two concurrent counter increments.\nThe lost update problem can occur if an application reads some value from the data\u2010\nbase, modifies it, and writes back the modified value (a read-modify-write cycle). If\ntwo transactions do this concurrently, one of the modifications can be lost, because\nthe second write does not include the first modification. (We sometimes say that the\nlater write clobbers the earlier write.) This pattern occurs in various different\nscenarios:\n\u2022 Incrementing a counter or updating an account balance (requires reading the\ncurrent value, calculating the new value, and writing back the updated value)\n\u2022 Making a local change to a complex value, e.g., adding an element to a list within\na JSON document (requires parsing the document, making the change, and writ\u2010\ning back the modified document)\n\u2022 Two users editing a wiki page at the same time, where each user saves their\nchanges by sending the entire page contents to the server, overwriting whatever\nis currently in the database\nBecause this is such a common problem, a variety of solutions have been developed.\nAtomic write operations\nMany databases provide atomic update operations, which remove the need to imple\u2010\nment read-modify-write cycles in application code. They are usually the best solution\nif your code can be expressed in terms of those operations. For example, the follow\u2010\ning instruction is concurrency-safe in most relational databases:\nUPDATE counters SET value = value + 1 WHERE key = 'foo';\nSimilarly, document databases such as MongoDB provide atomic operations for\nmaking local modifications to a part of a JSON document, and Redis provides atomic\noperations for modifying data structures such as priority queues. Not all writes can\neasily be expressed in terms of atomic operations\u2014for example, updates to a wiki\npage involve arbitrary text editingviii\u2014but in situations where atomic operations can\nbe used, they are usually the best choice.\nAtomic operations are usually implemented by taking an exclusive lock on the object\nwhen it is read so that no other transaction can read it until the update has been"}
{"266": "applied. This technique is sometimes known as cursor stability [36, 37]. Another\noption is to simply force all atomic operations to be executed on a single thread.\nUnfortunately, object-relational mapping frameworks make it easy to accidentally\nwrite code that performs unsafe read-modify-write cycles instead of using atomic\noperations provided by the database [38]. That\u2019s not a problem if you know what you\nare doing, but it is potentially a source of subtle bugs that are difficult to find by\ntesting.\nExplicit locking\nAnother option for preventing lost updates, if the database\u2019s built-in atomic opera\u2010\ntions don\u2019t provide the necessary functionality, is for the application to explicitly lock\nobjects that are going to be updated. Then the application can perform a read-\nmodify-write cycle, and if any other transaction tries to concurrently read the same\nobject, it is forced to wait until the first read-modify-write cycle has completed.\nFor example, consider a multiplayer game in which several players can move the\nsame figure concurrently. In this case, an atomic operation may not be sufficient,\nbecause the application also needs to ensure that a player\u2019s move abides by the rules\nof the game, which involves some logic that you cannot sensibly implement as a data\u2010\nbase query. Instead, you may use a lock to prevent two players from concurrently\nmoving the same piece, as illustrated in Example 7-1.\nExample 7-1. Explicitly locking rows to prevent lost updates\nBEGIN TRANSACTION;\nSELECT * FROM figures\nWHERE name = 'robot' AND game_id = 222\nFOR UPDATE;\n-- Check whether move is valid, then update the position\n-- of the piece that was returned by the previous SELECT.\nUPDATE figures SET position = 'c4' WHERE id = 1234;\nCOMMIT;\nThe FOR UPDATE clause indicates that the database should take a lock on all rows\nreturned by this query.\nThis works, but to get it right, you need to carefully think about your application\nlogic. It\u2019s easy to forget to add a necessary lock somewhere in the code, and thus"}
{"267": "Automatically detecting lost updates\nAtomic operations and locks are ways of preventing lost updates by forcing the read-\nmodify-write cycles to happen sequentially. An alternative is to allow them to execute\nin parallel and, if the transaction manager detects a lost update, abort the transaction\nand force it to retry its read-modify-write cycle.\nAn advantage of this approach is that databases can perform this check efficiently in\nconjunction with snapshot isolation. Indeed, PostgreSQL\u2019s repeatable read, Oracle\u2019s\nserializable, and SQL Server\u2019s snapshot isolation levels automatically detect when a\nlost update has occurred and abort the offending transaction. However, MySQL/\nInnoDB\u2019s repeatable read does not detect lost updates [23]. Some authors [28, 30]\nargue that a database must prevent lost updates in order to qualify as providing snap\u2010\nshot isolation, so MySQL does not provide snapshot isolation under this definition.\nLost update detection is a great feature, because it doesn\u2019t require application code to\nuse any special database features\u2014you may forget to use a lock or an atomic opera\u2010\ntion and thus introduce a bug, but lost update detection happens automatically and is\nthus less error-prone.\nCompare-and-set\nIn databases that don\u2019t provide transactions, you sometimes find an atomic compare-\nand-set operation (previously mentioned in \u201cSingle-object writes\u201d on page 230). The\npurpose of this operation is to avoid lost updates by allowing an update to happen\nonly if the value has not changed since you last read it. If the current value does not\nmatch what you previously read, the update has no effect, and the read-modify-write\ncycle must be retried.\nFor example, to prevent two users concurrently updating the same wiki page, you\nmight try something like this, expecting the update to occur only if the content of the\npage hasn\u2019t changed since the user started editing it:\n-- This may or may not be safe, depending on the database implementation\nUPDATE wiki_pages SET content = 'new content'\nWHERE id = 1234 AND content = 'old content';\nIf the content has changed and no longer matches 'old content', this update will\nhave no effect, so you need to check whether the update took effect and retry if neces\u2010\nsary. However, if the database allows the WHERE clause to read from an old snapshot,\nthis statement may not prevent lost updates, because the condition may be true even\nthough another concurrent write is occurring. Check whether your database\u2019s\ncompare-and-set operation is safe before relying on it."}
{"268": "Conflict resolution and replication\nIn replicated databases (see Chapter 5), preventing lost updates takes on another\ndimension: since they have copies of the data on multiple nodes, and the data can\npotentially be modified concurrently on different nodes, some additional steps need\nto be taken to prevent lost updates.\nLocks and compare-and-set operations assume that there is a single up-to-date copy\nof the data. However, databases with multi-leader or leaderless replication usually\nallow several writes to happen concurrently and replicate them asynchronously, so\nthey cannot guarantee that there is a single up-to-date copy of the data. Thus, techni\u2010\nques based on locks or compare-and-set do not apply in this context. (We will revisit\nthis issue in more detail in \u201cLinearizability\u201d on page 324.)\nInstead, as discussed in \u201cDetecting Concurrent Writes\u201d on page 184, a common\napproach in such replicated databases is to allow concurrent writes to create several\nconflicting versions of a value (also known as siblings), and to use application code or\nspecial data structures to resolve and merge these versions after the fact.\nAtomic operations can work well in a replicated context, especially if they are com\u2010\nmutative (i.e., you can apply them in a different order on different replicas, and still\nget the same result). For example, incrementing a counter or adding an element to a\nset are commutative operations. That is the idea behind Riak 2.0 datatypes, which\nprevent lost updates across replicas. When a value is concurrently updated by differ\u2010\nent clients, Riak automatically merges together the updates in such a way that no\nupdates are lost [39].\nOn the other hand, the last write wins (LWW) conflict resolution method is prone to\nlost updates, as discussed in \u201cLast write wins (discarding concurrent writes)\u201d on page\n186. Unfortunately, LWW is the default in many replicated databases.\nWrite Skew and Phantoms\nIn the previous sections we saw dirty writes and lost updates, two kinds of race condi\u2010\ntions that can occur when different transactions concurrently try to write to the same\nobjects. In order to avoid data corruption, those race conditions need to be prevented\n\u2014either automatically by the database, or by manual safeguards such as using locks\nor atomic write operations.\nHowever, that is not the end of the list of potential race conditions that can occur\nbetween concurrent writes. In this section we will see some subtler examples of\nconflicts."}
{"269": "can give up their shifts (e.g., if they are sick themselves), provided that at least one\ncolleague remains on call in that shift [40, 41].\nNow imagine that Alice and Bob are the two on-call doctors for a particular shift.\nBoth are feeling unwell, so they both decide to request leave. Unfortunately, they\nhappen to click the button to go off call at approximately the same time. What hap\u2010\npens next is illustrated in Figure 7-8.\nFigure 7-8. Example of write skew causing an application bug.\nIn each transaction, your application first checks that two or more doctors are cur\u2010\nrently on call; if yes, it assumes it\u2019s safe for one doctor to go off call. Since the data\u2010\nbase is using snapshot isolation, both checks return 2, so both transactions proceed to\nthe next stage. Alice updates her own record to take herself off call, and Bob updates\nhis own record likewise. Both transactions commit, and now no doctor is on call.\nYour requirement of having at least one doctor on call has been violated.\nCharacterizing write skew\nThis anomaly is called write skew [28]. It is neither a dirty write nor a lost update,"}
{"270": "doctor would have been prevented from going off call. The anomalous behavior was\nonly possible because the transactions ran concurrently.\nYou can think of write skew as a generalization of the lost update problem. Write\nskew can occur if two transactions read the same objects, and then update some of\nthose objects (different transactions may update different objects). In the special case\nwhere different transactions update the same object, you get a dirty write or lost\nupdate anomaly (depending on the timing).\nWe saw that there are various different ways of preventing lost updates. With write\nskew, our options are more restricted:\n\u2022 Atomic single-object operations don\u2019t help, as multiple objects are involved.\n\u2022 The automatic detection of lost updates that you find in some implementations\nof snapshot isolation unfortunately doesn\u2019t help either: write skew is not auto\u2010\nmatically detected in PostgreSQL\u2019s repeatable read, MySQL/InnoDB\u2019s repeatable\nread, Oracle\u2019s serializable, or SQL Server\u2019s snapshot isolation level [23]. Auto\u2010\nmatically preventing write skew requires true serializable isolation (see \u201cSerializa\u2010\nbility\u201d on page 251).\n\u2022 Some databases allow you to configure constraints, which are then enforced by\nthe database (e.g., uniqueness, foreign key constraints, or restrictions on a partic\u2010\nular value). However, in order to specify that at least one doctor must be on call,\nyou would need a constraint that involves multiple objects. Most databases do\nnot have built-in support for such constraints, but you may be able to implement\nthem with triggers or materialized views, depending on the database [42].\n\u2022 If you can\u2019t use a serializable isolation level, the second-best option in this case is\nprobably to explicitly lock the rows that the transaction depends on. In the doc\u2010\ntors example, you could write something like the following:\nBEGIN TRANSACTION;\nSELECT * FROM doctors\nWHERE on_call = true\nAND shift_id = 1234 FOR UPDATE;\nUPDATE doctors\nSET on_call = false\nWHERE name = 'Alice'\nAND shift_id = 1234;\nCOMMIT;"}
{"271": "More examples of write skew\nWrite skew may seem like an esoteric issue at first, but once you\u2019re aware of it, you\nmay notice more situations in which it can occur. Here are some more examples:\nMeeting room booking system\nSay you want to enforce that there cannot be two bookings for the same meeting\nroom at the same time [43]. When someone wants to make a booking, you first\ncheck for any conflicting bookings (i.e., bookings for the same room with an\noverlapping time range), and if none are found, you create the meeting (see\nExample 7-2).ix\nExample 7-2. A meeting room booking system tries to avoid double-booking (not\nsafe under snapshot isolation)\nBEGIN TRANSACTION;\n-- Check for any existing bookings that overlap with the period of noon-1pm\nSELECT COUNT(*) FROM bookings\nWHERE room_id = 123 AND\nend_time > '2015-01-01 12:00' AND start_time < '2015-01-01 13:00';\n-- If the previous query returned zero:\nINSERT INTO bookings\n(room_id, start_time, end_time, user_id)\nVALUES (123, '2015-01-01 12:00', '2015-01-01 13:00', 666);\nCOMMIT;\nUnfortunately, snapshot isolation does not prevent another user from concur\u2010\nrently inserting a conflicting meeting. In order to guarantee you won\u2019t get sched\u2010\nuling conflicts, you once again need serializable isolation.\nMultiplayer game\nIn Example 7-1, we used a lock to prevent lost updates (that is, making sure that\ntwo players can\u2019t move the same figure at the same time). However, the lock\ndoesn\u2019t prevent players from moving two different figures to the same position\non the board or potentially making some other move that violates the rules of the\ngame. Depending on the kind of rule you are enforcing, you might be able to use\na unique constraint, but otherwise you\u2019re vulnerable to write skew."}
{"272": "Claiming a username\nOn a website where each user has a unique username, two users may try to create\naccounts with the same username at the same time. You may use a transaction to\ncheck whether a name is taken and, if not, create an account with that name.\nHowever, like in the previous examples, that is not safe under snapshot isolation.\nFortunately, a unique constraint is a simple solution here (the second transaction\nthat tries to register the username will be aborted due to violating the constraint).\nPreventing double-spending\nA service that allows users to spend money or points needs to check that a user\ndoesn\u2019t spend more than they have. You might implement this by inserting a ten\u2010\ntative spending item into a user\u2019s account, listing all the items in the account, and\nchecking that the sum is positive [44]. With write skew, it could happen that two\nspending items are inserted concurrently that together cause the balance to go\nnegative, but that neither transaction notices the other.\nPhantoms causing write skew\nAll of these examples follow a similar pattern:\n1. A SELECT query checks whether some requirement is satisfied by searching for\nrows that match some search condition (there are at least two doctors on call,\nthere are no existing bookings for that room at that time, the position on the\nboard doesn\u2019t already have another figure on it, the username isn\u2019t already taken,\nthere is still money in the account).\n2. Depending on the result of the first query, the application code decides how to\ncontinue (perhaps to go ahead with the operation, or perhaps to report an error\nto the user and abort).\n3. If the application decides to go ahead, it makes a write (INSERT, UPDATE, or\nDELETE) to the database and commits the transaction.\nThe effect of this write changes the precondition of the decision of step 2. In\nother words, if you were to repeat the SELECT query from step 1 after commiting\nthe write, you would get a different result, because the write changed the set of\nrows matching the search condition (there is now one fewer doctor on call, the\nmeeting room is now booked for that time, the position on the board is now\ntaken by the figure that was moved, the username is now taken, there is now less\nmoney in the account).\nThe steps may occur in a different order. For example, you could first make the write,"}
{"273": "In the case of the doctor on call example, the row being modified in step 3 was one of\nthe rows returned in step 1, so we could make the transaction safe and avoid write\nskew by locking the rows in step 1 (SELECT FOR UPDATE). However, the other four\nexamples are different: they check for the absence of rows matching some search con\u2010\ndition, and the write adds a row matching the same condition. If the query in step 1\ndoesn\u2019t return any rows, SELECT FOR UPDATE can\u2019t attach locks to anything.\nThis effect, where a write in one transaction changes the result of a search query in\nanother transaction, is called a phantom [3]. Snapshot isolation avoids phantoms in\nread-only queries, but in read-write transactions like the examples we discussed,\nphantoms can lead to particularly tricky cases of write skew.\nMaterializing conflicts\nIf the problem of phantoms is that there is no object to which we can attach the locks,\nperhaps we can artificially introduce a lock object into the database?\nFor example, in the meeting room booking case you could imagine creating a table of\ntime slots and rooms. Each row in this table corresponds to a particular room for a\nparticular time period (say, 15 minutes). You create rows for all possible combina\u2010\ntions of rooms and time periods ahead of time, e.g. for the next six months.\nNow a transaction that wants to create a booking can lock (SELECT FOR UPDATE) the\nrows in the table that correspond to the desired room and time period. After it has\nacquired the locks, it can check for overlapping bookings and insert a new booking as\nbefore. Note that the additional table isn\u2019t used to store information about the book\u2010\ning\u2014it\u2019s purely a collection of locks which is used to prevent bookings on the same\nroom and time range from being modified concurrently.\nThis approach is called materializing conflicts, because it takes a phantom and turns it\ninto a lock conflict on a concrete set of rows that exist in the database [11]. Unfortu\u2010\nnately, it can be hard and error-prone to figure out how to materialize conflicts, and\nit\u2019s ugly to let a concurrency control mechanism leak into the application data model.\nFor those reasons, materializing conflicts should be considered a last resort if no\nalternative is possible. A serializable isolation level is much preferable in most cases.\nSerializability\nIn this chapter we have seen several examples of transactions that are prone to race\nconditions. Some race conditions are prevented by the read committed and snapshot\nisolation levels, but others are not. We encountered some particularly tricky exam\u2010\nples with write skew and phantoms. It\u2019s a sad situation:"}
{"274": "\u2022 If you look at your application code, it\u2019s difficult to tell whether it is safe to run at\na particular isolation level\u2014especially in a large application, where you might not\nbe aware of all the things that may be happening concurrently.\n\u2022 There are no good tools to help us detect race conditions. In principle, static\nanalysis may help [26], but research techniques have not yet found their way into\npractical use. Testing for concurrency issues is hard, because they are usually\nnondeterministic\u2014problems only occur if you get unlucky with the timing.\nThis is not a new problem\u2014it has been like this since the 1970s, when weak isolation\nlevels were first introduced [2]. All along, the answer from researchers has been sim\u2010\nple: use serializable isolation!\nSerializable isolation is usually regarded as the strongest isolation level. It guarantees\nthat even though transactions may execute in parallel, the end result is the same as if\nthey had executed one at a time, serially, without any concurrency. Thus, the database\nguarantees that if the transactions behave correctly when run individually, they con\u2010\ntinue to be correct when run concurrently\u2014in other words, the database prevents all\npossible race conditions.\nBut if serializable isolation is so much better than the mess of weak isolation levels,\nthen why isn\u2019t everyone using it? To answer this question, we need to look at the\noptions for implementing serializability, and how they perform. Most databases that\nprovide serializability today use one of three techniques, which we will explore in the\nrest of this chapter:\n\u2022 Literally executing transactions in a serial order (see \u201cActual Serial Execution\u201d on\npage 252)\n\u2022 Two-phase locking (see \u201cTwo-Phase Locking (2PL)\u201d on page 257), which for sev\u2010\neral decades was the only viable option\n\u2022 Optimistic concurrency control techniques such as serializable snapshot isolation\n(see \u201cSerializable Snapshot Isolation (SSI)\u201d on page 261)\nFor now, we will discuss these techniques primarily in the context of single-node\ndatabases; in Chapter 9 we will examine how they can be generalized to transactions\nthat involve multiple nodes in a distributed system.\nActual Serial Execution\nThe simplest way of avoiding concurrency problems is to remove the concurrency\nentirely: to execute only one transaction at a time, in serial order, on a single thread."}
{"275": "Even though this seems like an obvious idea, database designers only fairly recently\u2014\naround 2007\u2014decided that a single-threaded loop for executing transactions was fea\u2010\nsible [45]. If multi-threaded concurrency was considered essential for getting good\nperformance during the previous 30 years, what changed to make single-threaded\nexecution possible?\nTwo developments caused this rethink:\n\u2022 RAM became cheap enough that for many use cases is now feasible to keep the\nentire active dataset in memory (see \u201cKeeping everything in memory\u201d on page\n88). When all data that a transaction needs to access is in memory, transactions\ncan execute much faster than if they have to wait for data to be loaded from disk.\n\u2022 Database designers realized that OLTP transactions are usually short and only\nmake a small number of reads and writes (see \u201cTransaction Processing or Ana\u2010\nlytics?\u201d on page 90). By contrast, long-running analytic queries are typically read-\nonly, so they can be run on a consistent snapshot (using snapshot isolation)\noutside of the serial execution loop.\nThe approach of executing transactions serially is implemented in VoltDB/H-Store,\nRedis, and Datomic [46, 47, 48]. A system designed for single-threaded execution can\nsometimes perform better than a system that supports concurrency, because it can\navoid the coordination overhead of locking. However, its throughput is limited to\nthat of a single CPU core. In order to make the most of that single thread, transac\u2010\ntions need to be structured differently from their traditional form.\nEncapsulating transactions in stored procedures\nIn the early days of databases, the intention was that a database transaction could\nencompass an entire flow of user activity. For example, booking an airline ticket is a\nmulti-stage process (searching for routes, fares, and available seats; deciding on an\nitinerary; booking seats on each of the flights of the itinerary; entering passenger\ndetails; making payment). Database designers thought that it would be neat if that\nentire process was one transaction so that it could be committed atomically.\nUnfortunately, humans are very slow to make up their minds and respond. If a data\u2010\nbase transaction needs to wait for input from a user, the database needs to support a\npotentially huge number of concurrent transactions, most of them idle. Most data\u2010\nbases cannot do that efficiently, and so almost all OLTP applications keep transac\u2010\ntions short by avoiding interactively waiting for a user within a transaction. On the\nweb, this means that a transaction is committed within the same HTTP request\u2014a\ntransaction does not span multiple requests. A new HTTP request starts a new trans\u2010"}
{"276": "An application makes a query, reads the result, perhaps makes another query\ndepending on the result of the first query, and so on. The queries and results are sent\nback and forth between the application code (running on one machine) and the data\u2010\nbase server (on another machine).\nIn this interactive style of transaction, a lot of time is spent in network communica\u2010\ntion between the application and the database. If you were to disallow concurrency in\nthe database and only process one transaction at a time, the throughput would be\ndreadful because the database would spend most of its time waiting for the applica\u2010\ntion to issue the next query for the current transaction. In this kind of database, it\u2019s\nnecessary to process multiple transactions concurrently in order to get reasonable\nperformance.\nFor this reason, systems with single-threaded serial transaction processing don\u2019t\nallow interactive multi-statement transactions. Instead, the application must submit\nthe entire transaction code to the database ahead of time, as a stored procedure. The\ndifferences between these approaches is illustrated in Figure 7-9. Provided that all\ndata required by a transaction is in memory, the stored procedure can execute very\nfast, without waiting for any network or disk I/O.\nFigure 7-9. The difference between an interactive transaction and a stored procedure\n(using the example transaction of Figure 7-8)."}
{"277": "Pros and cons of stored procedures\nStored procedures have existed for some time in relational databases, and they have\nbeen part of the SQL standard (SQL/PSM) since 1999. They have gained a somewhat\nbad reputation, for various reasons:\n\u2022 Each database vendor has its own language for stored procedures (Oracle has PL/\nSQL, SQL Server has T-SQL, PostgreSQL has PL/pgSQL, etc.). These languages\nhaven\u2019t kept up with developments in general-purpose programming languages,\nso they look quite ugly and archaic from today\u2019s point of view, and they lack the\necosystem of libraries that you find with most programming languages.\n\u2022 Code running in a database is difficult to manage: compared to an application\nserver, it\u2019s harder to debug, more awkward to keep in version control and deploy,\ntrickier to test, and difficult to integrate with a metrics collection system for\nmonitoring.\n\u2022 A database is often much more performance-sensitive than an application server,\nbecause a single database instance is often shared by many application servers. A\nbadly written stored procedure (e.g., using a lot of memory or CPU time) in a\ndatabase can cause much more trouble than equivalent badly written code in an\napplication server.\nHowever, those issues can be overcome. Modern implementations of stored proce\u2010\ndures have abandoned PL/SQL and use existing general-purpose programming lan\u2010\nguages instead: VoltDB uses Java or Groovy, Datomic uses Java or Clojure, and Redis\nuses Lua.\nWith stored procedures and in-memory data, executing all transactions on a single\nthread becomes feasible. As they don\u2019t need to wait for I/O and they avoid the over\u2010\nhead of other concurrency control mechanisms, they can achieve quite good\nthroughput on a single thread.\nVoltDB also uses stored procedures for replication: instead of copying a transaction\u2019s\nwrites from one node to another, it executes the same stored procedure on each rep\u2010\nlica. VoltDB therefore requires that stored procedures are deterministic (when run on\ndifferent nodes, they must produce the same result). If a transaction needs to use the\ncurrent date and time, for example, it must do so through special deterministic APIs.\nPartitioning\nExecuting all transactions serially makes concurrency control much simpler, but lim\u2010\nits the transaction throughput of the database to the speed of a single CPU core on a"}
{"278": "In order to scale to multiple CPU cores, and multiple nodes, you can potentially par\u2010\ntition your data (see Chapter 6), which is supported in VoltDB. If you can find a way\nof partitioning your dataset so that each transaction only needs to read and write data\nwithin a single partition, then each partition can have its own transaction processing\nthread running independently from the others. In this case, you can give each CPU\ncore its own partition, which allows your transaction throughput to scale linearly\nwith the number of CPU cores [47].\nHowever, for any transaction that needs to access multiple partitions, the database\nmust coordinate the transaction across all the partitions that it touches. The stored\nprocedure needs to be performed in lock-step across all partitions to ensure serializa\u2010\nbility across the whole system.\nSince cross-partition transactions have additional coordination overhead, they are\nvastly slower than single-partition transactions. VoltDB reports a throughput of\nabout 1,000 cross-partition writes per second, which is orders of magnitude below its\nsingle-partition throughput and cannot be increased by adding more machines [49].\nWhether transactions can be single-partition depends very much on the structure of\nthe data used by the application. Simple key-value data can often be partitioned very\neasily, but data with multiple secondary indexes is likely to require a lot of cross-\npartition coordination (see \u201cPartitioning and Secondary Indexes\u201d on page 206).\nSummary of serial execution\nSerial execution of transactions has become a viable way of achieving serializable iso\u2010\nlation within certain constraints:\n\u2022 Every transaction must be small and fast, because it takes only one slow transac\u2010\ntion to stall all transaction processing.\n\u2022 It is limited to use cases where the active dataset can fit in memory. Rarely\naccessed data could potentially be moved to disk, but if it needed to be accessed\nin a single-threaded transaction, the system would get very slow.x\n\u2022 Write throughput must be low enough to be handled on a single CPU core, or\nelse transactions need to be partitioned without requiring cross-partition coordi\u2010\nnation.\n\u2022 Cross-partition transactions are possible, but there is a hard limit to the extent to\nwhich they can be used."}
{"279": "Two-Phase Locking (2PL)\nFor around 30 years, there was only one widely used algorithm for serializability in\ndatabases: two-phase locking (2PL).xi\n2PL is not 2PC\nNote that while two-phase locking (2PL) sounds very similar to\ntwo-phase commit (2PC), they are completely different things. We\nwill discuss 2PC in Chapter 9.\nWe saw previously that locks are often used to prevent dirty writes (see \u201cNo dirty\nwrites\u201d on page 235): if two transactions concurrently try to write to the same object,\nthe lock ensures that the second writer must wait until the first one has finished its\ntransaction (aborted or committed) before it may continue.\nTwo-phase locking is similar, but makes the lock requirements much stronger. Sev\u2010\neral transactions are allowed to concurrently read the same object as long as nobody\nis writing to it. But as soon as anyone wants to write (modify or delete) an object,\nexclusive access is required:\n\u2022 If transaction A has read an object and transaction B wants to write to that\nobject, B must wait until A commits or aborts before it can continue. (This\nensures that B can\u2019t change the object unexpectedly behind A\u2019s back.)\n\u2022 If transaction A has written an object and transaction B wants to read that object,\nB must wait until A commits or aborts before it can continue. (Reading an old\nversion of the object, like in Figure 7-1, is not acceptable under 2PL.)\nIn 2PL, writers don\u2019t just block other writers; they also block readers and vice versa.\nSnapshot isolation has the mantra readers never block writers, and writers never block\nreaders (see \u201cImplementing snapshot isolation\u201d on page 239), which captures this key\ndifference between snapshot isolation and two-phase locking. On the other hand,\nbecause 2PL provides serializability, it protects against all the race conditions dis\u2010\ncussed earlier, including lost updates and write skew.\nImplementation of two-phase locking\n2PL is used by the serializable isolation level in MySQL (InnoDB) and SQL Server,\nand the repeatable read isolation level in DB2 [23, 36]."}
{"280": "The blocking of readers and writers is implemented by a having a lock on each object\nin the database. The lock can either be in shared mode or in exclusive mode. The lock\nis used as follows:\n\u2022 If a transaction wants to read an object, it must first acquire the lock in shared\nmode. Several transactions are allowed to hold the lock in shared mode simulta\u2010\nneously, but if another transaction already has an exclusive lock on the object,\nthese transactions must wait.\n\u2022 If a transaction wants to write to an object, it must first acquire the lock in exclu\u2010\nsive mode. No other transaction may hold the lock at the same time (either in\nshared or in exclusive mode), so if there is any existing lock on the object, the\ntransaction must wait.\n\u2022 If a transaction first reads and then writes an object, it may upgrade its shared\nlock to an exclusive lock. The upgrade works the same as getting an exclusive\nlock directly.\n\u2022 After a transaction has acquired the lock, it must continue to hold the lock until\nthe end of the transaction (commit or abort). This is where the name \u201ctwo-\nphase\u201d comes from: the first phase (while the transaction is executing) is when\nthe locks are acquired, and the second phase (at the end of the transaction) is\nwhen all the locks are released.\nSince so many locks are in use, it can happen quite easily that transaction A is stuck\nwaiting for transaction B to release its lock, and vice versa. This situation is called\ndeadlock. The database automatically detects deadlocks between transactions and\naborts one of them so that the others can make progress. The aborted transaction\nneeds to be retried by the application.\nPerformance of two-phase locking\nThe big downside of two-phase locking, and the reason why it hasn\u2019t been used by\neverybody since the 1970s, is performance: transaction throughput and response\ntimes of queries are significantly worse under two-phase locking than under weak\nisolation.\nThis is partly due to the overhead of acquiring and releasing all those locks, but more\nimportantly due to reduced concurrency. By design, if two concurrent transactions\ntry to do anything that may in any way result in a race condition, one has to wait for\nthe other to complete.\nTraditional relational databases don\u2019t limit the duration of a transaction, because"}
{"281": "queue may form if several transactions want to access the same object, so a transac\u2010\ntion may have to wait for several others to complete before it can do anything.\nFor this reason, databases running 2PL can have quite unstable latencies, and they\ncan be very slow at high percentiles (see \u201cDescribing Performance\u201d on page 13) if\nthere is contention in the workload. It may take just one slow transaction, or one\ntransaction that accesses a lot of data and acquires many locks, to cause the rest of the\nsystem to grind to a halt. This instability is problematic when robust operation is\nrequired.\nAlthough deadlocks can happen with the lock-based read committed isolation level,\nthey occur much more frequently under 2PL serializable isolation (depending on the\naccess patterns of your transaction). This can be an additional performance problem:\nwhen a transaction is aborted due to deadlock and is retried, it needs to do its work\nall over again. If deadlocks are frequent, this can mean significant wasted effort.\nPredicate locks\nIn the preceding description of locks, we glossed over a subtle but important detail.\nIn \u201cPhantoms causing write skew\u201d on page 250 we discussed the problem of phan\u2010\ntoms\u2014that is, one transaction changing the results of another transaction\u2019s search\nquery. A database with serializable isolation must prevent phantoms.\nIn the meeting room booking example this means that if one transaction has\nsearched for existing bookings for a room within a certain time window (see\nExample 7-2), another transaction is not allowed to concurrently insert or update\nanother booking for the same room and time range. (It\u2019s okay to concurrently insert\nbookings for other rooms, or for the same room at a different time that doesn\u2019t affect\nthe proposed booking.)\nHow do we implement this? Conceptually, we need a predicate lock [3]. It works sim\u2010\nilarly to the shared/exclusive lock described earlier, but rather than belonging to a\nparticular object (e.g., one row in a table), it belongs to all objects that match some\nsearch condition, such as:\nSELECT * FROM bookings\nWHERE room_id = 123 AND\nend_time > '2018-01-01 12:00' AND\nstart_time < '2018-01-01 13:00';\nA predicate lock restricts access as follows:\n\u2022 If transaction A wants to read objects matching some condition, like in that\nSELECT query, it must acquire a shared-mode predicate lock on the conditions of"}
{"282": "\u2022 If transaction A wants to insert, update, or delete any object, it must first check\nwhether either the old or the new value matches any existing predicate lock. If\nthere is a matching predicate lock held by transaction B, then A must wait until B\nhas committed or aborted before it can continue.\nThe key idea here is that a predicate lock applies even to objects that do not yet exist\nin the database, but which might be added in the future (phantoms). If two-phase\nlocking includes predicate locks, the database prevents all forms of write skew and\nother race conditions, and so its isolation becomes serializable.\nIndex-range locks\nUnfortunately, predicate locks do not perform well: if there are many locks by active\ntransactions, checking for matching locks becomes time-consuming. For that reason,\nmost databases with 2PL actually implement index-range locking (also known as next-\nkey locking), which is a simplified approximation of predicate locking [41, 50].\nIt\u2019s safe to simplify a predicate by making it match a greater set of objects. For exam\u2010\nple, if you have a predicate lock for bookings of room 123 between noon and 1 p.m.,\nyou can approximate it by locking bookings for room 123 at any time, or you can\napproximate it by locking all rooms (not just room 123) between noon and 1 p.m.\nThis is safe, because any write that matches the original predicate will definitely also\nmatch the approximations.\nIn the room bookings database you would probably have an index on the room_id\ncolumn, and/or indexes on start_time and end_time (otherwise the preceding query\nwould be very slow on a large database):\n\u2022 Say your index is on room_id, and the database uses this index to find existing\nbookings for room 123. Now the database can simply attach a shared lock to this\nindex entry, indicating that a transaction has searched for bookings of room 123.\n\u2022 Alternatively, if the database uses a time-based index to find existing bookings, it\ncan attach a shared lock to a range of values in that index, indicating that a trans\u2010\naction has searched for bookings that overlap with the time period of noon to 1\np.m. on January 1, 2018.\nEither way, an approximation of the search condition is attached to one of the\nindexes. Now, if another transaction wants to insert, update, or delete a booking for\nthe same room and/or an overlapping time period, it will have to update the same\npart of the index. In the process of doing so, it will encounter the shared lock, and it\nwill be forced to wait until the lock is released."}
{"283": "objects than is strictly necessary to maintain serializability), but since they have much\nlower overheads, they are a good compromise.\nIf there is no suitable index where a range lock can be attached, the database can fall\nback to a shared lock on the entire table. This will not be good for performance, since\nit will stop all other transactions writing to the table, but it\u2019s a safe fallback position.\nSerializable Snapshot Isolation (SSI)\nThis chapter has painted a bleak picture of concurrency control in databases. On the\none hand, we have implementations of serializability that don\u2019t perform well (two-\nphase locking) or don\u2019t scale well (serial execution). On the other hand, we have weak\nisolation levels that have good performance, but are prone to various race conditions\n(lost updates, write skew, phantoms, etc.). Are serializable isolation and good perfor\u2010\nmance fundamentally at odds with each other?\nPerhaps not: an algorithm called serializable snapshot isolation (SSI) is very promis\u2010\ning. It provides full serializability, but has only a small performance penalty com\u2010\npared to snapshot isolation. SSI is fairly new: it was first described in 2008 [40] and is\nthe subject of Michael Cahill\u2019s PhD thesis [51].\nToday SSI is used both in single-node databases (the serializable isolation level in\nPostgreSQL since version 9.1 [41]) and distributed databases (FoundationDB uses a\nsimilar algorithm). As SSI is so young compared to other concurrency control mech\u2010\nanisms, it is still proving its performance in practice, but it has the possibility of being\nfast enough to become the new default in the future.\nPessimistic versus optimistic concurrency control\nTwo-phase locking is a so-called pessimistic concurrency control mechanism: it is\nbased on the principle that if anything might possibly go wrong (as indicated by a\nlock held by another transaction), it\u2019s better to wait until the situation is safe again\nbefore doing anything. It is like mutual exclusion, which is used to protect data struc\u2010\ntures in multi-threaded programming.\nSerial execution is, in a sense, pessimistic to the extreme: it is essentially equivalent to\neach transaction having an exclusive lock on the entire database (or one partition of\nthe database) for the duration of the transaction. We compensate for the pessimism\nby making each transaction very fast to execute, so it only needs to hold the \u201clock\u201d for\na short time.\nBy contrast, serializable snapshot isolation is an optimistic concurrency control tech\u2010\nnique. Optimistic in this context means that instead of blocking if something poten\u2010"}
{"284": "action is aborted and has to be retried. Only transactions that executed serializably\nare allowed to commit.\nOptimistic concurrency control is an old idea [52], and its advantages and disadvan\u2010\ntages have been debated for a long time [53]. It performs badly if there is high con\u2010\ntention (many transactions trying to access the same objects), as this leads to a high\nproportion of transactions needing to abort. If the system is already close to its maxi\u2010\nmum throughput, the additional transaction load from retried transactions can make\nperformance worse.\nHowever, if there is enough spare capacity, and if contention between transactions is\nnot too high, optimistic concurrency control techniques tend to perform better than\npessimistic ones. Contention can be reduced with commutative atomic operations:\nfor example, if several transactions concurrently want to increment a counter, it\ndoesn\u2019t matter in which order the increments are applied (as long as the counter isn\u2019t\nread in the same transaction), so the concurrent increments can all be applied\nwithout conflicting.\nAs the name suggests, SSI is based on snapshot isolation\u2014that is, all reads within a\ntransaction are made from a consistent snapshot of the database (see \u201cSnapshot Isola\u2010\ntion and Repeatable Read\u201d on page 237). This is the main difference compared to ear\u2010\nlier optimistic concurrency control techniques. On top of snapshot isolation, SSI adds\nan algorithm for detecting serialization conflicts among writes and determining\nwhich transactions to abort.\nDecisions based on an outdated premise\nWhen we previously discussed write skew in snapshot isolation (see \u201cWrite Skew and\nPhantoms\u201d on page 246), we observed a recurring pattern: a transaction reads some\ndata from the database, examines the result of the query, and decides to take some\naction (write to the database) based on the result that it saw. However, under snap\u2010\nshot isolation, the result from the original query may no longer be up-to-date by the\ntime the transaction commits, because the data may have been modified in the mean\u2010\ntime.\nPut another way, the transaction is taking an action based on a premise (a fact that\nwas true at the beginning of the transaction, e.g., \u201cThere are currently two doctors on\ncall\u201d). Later, when the transaction wants to commit, the original data may have\nchanged\u2014the premise may no longer be true.\nWhen the application makes a query (e.g., \u201cHow many doctors are currently on\ncall?\u201d), the database doesn\u2019t know how the application logic uses the result of that"}
{"285": "which a transaction may have acted on an outdated premise and abort the transac\u2010\ntion in that case.\nHow does the database know if a query result might have changed? There are two\ncases to consider:\n\u2022 Detecting reads of a stale MVCC object version (uncommitted write occurred\nbefore the read)\n\u2022 Detecting writes that affect prior reads (the write occurs after the read)\nDetecting stale MVCC reads\nRecall that snapshot isolation is usually implemented by multi-version concurrency\ncontrol (MVCC; see Figure 7-10). When a transaction reads from a consistent snap\u2010\nshot in an MVCC database, it ignores writes that were made by any other transac\u2010\ntions that hadn\u2019t yet committed at the time when the snapshot was taken. In\nFigure 7-10, transaction 43 sees Alice as having on_call = true, because transaction\n42 (which modified Alice\u2019s on-call status) is uncommitted. However, by the time\ntransaction 43 wants to commit, transaction 42 has already committed. This means\nthat the write that was ignored when reading from the consistent snapshot has now\ntaken effect, and transaction 43\u2019s premise is no longer true."}
{"286": "In order to prevent this anomaly, the database needs to track when a transaction\nignores another transaction\u2019s writes due to MVCC visibility rules. When the transac\u2010\ntion wants to commit, the database checks whether any of the ignored writes have\nnow been committed. If so, the transaction must be aborted.\nWhy wait until committing? Why not abort transaction 43 immediately when the\nstale read is detected? Well, if transaction 43 was a read-only transaction, it wouldn\u2019t\nneed to be aborted, because there is no risk of write skew. At the time when transac\u2010\ntion 43 makes its read, the database doesn\u2019t yet know whether that transaction is\ngoing to later perform a write. Moreover, transaction 42 may yet abort or may still be\nuncommitted at the time when transaction 43 is committed, and so the read may\nturn out not to have been stale after all. By avoiding unnecessary aborts, SSI preserves\nsnapshot isolation\u2019s support for long-running reads from a consistent snapshot.\nDetecting writes that affect prior reads\nThe second case to consider is when another transaction modifies data after it has\nbeen read. This case is illustrated in Figure 7-11.\nFigure 7-11. In serializable snapshot isolation, detecting when one transaction modifies\nanother transaction\u2019s reads."}
{"287": "In Figure 7-11, transactions 42 and 43 both search for on-call doctors during shift\n1234. If there is an index on shift_id, the database can use the index entry 1234 to\nrecord the fact that transactions 42 and 43 read this data. (If there is no index, this\ninformation can be tracked at the table level.) This information only needs to be kept\nfor a while: after a transaction has finished (committed or aborted), and all concur\u2010\nrent transactions have finished, the database can forget what data it read.\nWhen a transaction writes to the database, it must look in the indexes for any other\ntransactions that have recently read the affected data. This process is similar to\nacquiring a write lock on the affected key range, but rather than blocking until the\nreaders have committed, the lock acts as a tripwire: it simply notifies the transactions\nthat the data they read may no longer be up to date.\nIn Figure 7-11, transaction 43 notifies transaction 42 that its prior read is outdated,\nand vice versa. Transaction 42 is first to commit, and it is successful: although trans\u2010\naction 43\u2019s write affected 42, 43 hasn\u2019t yet committed, so the write has not yet taken\neffect. However, when transaction 43 wants to commit, the conflicting write from 42\nhas already been committed, so 43 must abort.\nPerformance of serializable snapshot isolation\nAs always, many engineering details affect how well an algorithm works in practice.\nFor example, one trade-off is the granularity at which transactions\u2019 reads and writes\nare tracked. If the database keeps track of each transaction\u2019s activity in great detail, it\ncan be precise about which transactions need to abort, but the bookkeeping overhead\ncan become significant. Less detailed tracking is faster, but may lead to more transac\u2010\ntions being aborted than strictly necessary.\nIn some cases, it\u2019s okay for a transaction to read information that was overwritten by\nanother transaction: depending on what else happened, it\u2019s sometimes possible to\nprove that the result of the execution is nevertheless serializable. PostgreSQL uses this\ntheory to reduce the number of unnecessary aborts [11, 41].\nCompared to two-phase locking, the big advantage of serializable snapshot isolation\nis that one transaction doesn\u2019t need to block waiting for locks held by another trans\u2010\naction. Like under snapshot isolation, writers don\u2019t block readers, and vice versa. This\ndesign principle makes query latency much more predictable and less variable. In\nparticular, read-only queries can run on a consistent snapshot without requiring any\nlocks, which is very appealing for read-heavy workloads.\nCompared to serial execution, serializable snapshot isolation is not limited to the\nthroughput of a single CPU core: FoundationDB distributes the detection of seriali\u2010"}
{"288": "The rate of aborts significantly affects the overall performance of SSI. For example, a\ntransaction that reads and writes data over a long period of time is likely to run into\nconflicts and abort, so SSI requires that read-write transactions be fairly short (long-\nrunning read-only transactions may be okay). However, SSI is probably less sensitive\nto slow transactions than two-phase locking or serial execution.\nSummary\nTransactions are an abstraction layer that allows an application to pretend that cer\u2010\ntain concurrency problems and certain kinds of hardware and software faults don\u2019t\nexist. A large class of errors is reduced down to a simple transaction abort, and the\napplication just needs to try again.\nIn this chapter we saw many examples of problems that transactions help prevent.\nNot all applications are susceptible to all those problems: an application with very\nsimple access patterns, such as reading and writing only a single record, can probably\nmanage without transactions. However, for more complex access patterns, transac\u2010\ntions can hugely reduce the number of potential error cases you need to think about.\nWithout transactions, various error scenarios (processes crashing, network interrup\u2010\ntions, power outages, disk full, unexpected concurrency, etc.) mean that data can\nbecome inconsistent in various ways. For example, denormalized data can easily go\nout of sync with the source data. Without transactions, it becomes very difficult to\nreason about the effects that complex interacting accesses can have on the database.\nIn this chapter, we went particularly deep into the topic of concurrency control. We\ndiscussed several widely used isolation levels, in particular read committed, snapshot\nisolation (sometimes called repeatable read), and serializable. We characterized those\nisolation levels by discussing various examples of race conditions:\nDirty reads\nOne client reads another client\u2019s writes before they have been committed. The\nread committed isolation level and stronger levels prevent dirty reads.\nDirty writes\nOne client overwrites data that another client has written, but not yet committed.\nAlmost all transaction implementations prevent dirty writes.\nRead skew (nonrepeatable reads)\nA client sees different parts of the database at different points in time. This issue\nis most commonly prevented with snapshot isolation, which allows a transaction\nto read from a consistent snapshot at one point in time. It is usually implemented"}
{"289": "Lost updates\nTwo clients concurrently perform a read-modify-write cycle. One overwrites the\nother\u2019s write without incorporating its changes, so data is lost. Some implemen\u2010\ntations of snapshot isolation prevent this anomaly automatically, while others\nrequire a manual lock (SELECT FOR UPDATE).\nWrite skew\nA transaction reads something, makes a decision based on the value it saw, and\nwrites the decision to the database. However, by the time the write is made, the\npremise of the decision is no longer true. Only serializable isolation prevents this\nanomaly.\nPhantom reads\nA transaction reads objects that match some search condition. Another client\nmakes a write that affects the results of that search. Snapshot isolation prevents\nstraightforward phantom reads, but phantoms in the context of write skew\nrequire special treatment, such as index-range locks.\nWeak isolation levels protect against some of those anomalies but leave you, the\napplication developer, to handle others manually (e.g., using explicit locking). Only\nserializable isolation protects against all of these issues. We discussed three different\napproaches to implementing serializable transactions:\nLiterally executing transactions in a serial order\nIf you can make each transaction very fast to execute, and the transaction\nthroughput is low enough to process on a single CPU core, this is a simple and\neffective option.\nTwo-phase locking\nFor decades this has been the standard way of implementing serializability, but\nmany applications avoid using it because of its performance characteristics.\nSerializable snapshot isolation (SSI)\nA fairly new algorithm that avoids most of the downsides of the previous\napproaches. It uses an optimistic approach, allowing transactions to proceed\nwithout blocking. When a transaction wants to commit, it is checked, and it is\naborted if the execution was not serializable.\nThe examples in this chapter used a relational data model. However, as discussed in\n\u201cThe need for multi-object transactions\u201d on page 231, transactions are a valuable\ndatabase feature, no matter which data model is used.\nIn this chapter, we explored ideas and algorithms mostly in the context of a database"}
{"290": "References\n[1] Donald D. Chamberlin, Morton M. Astrahan, Michael W. Blasgen, et al.: \u201cA His\u2010\ntory and Evaluation of System R,\u201d Communications of the ACM, volume 24, number\n10, pages 632\u2013646, October 1981. doi:10.1145/358769.358784\n[2] Jim N. Gray, Raymond A. Lorie, Gianfranco R. Putzolu, and Irving L. Traiger:\n\u201cGranularity of Locks and Degrees of Consistency in a Shared Data Base,\u201d in Model\u2010\nling in Data Base Management Systems: Proceedings of the IFIP Working Conference\non Modelling in Data Base Management Systems, edited by G. M. Nijssen, pages 364\u2013\n394, Elsevier/North Holland Publishing, 1976. Also in Readings in Database Systems,\n4th edition, edited by Joseph M. Hellerstein and Michael Stonebraker, MIT Press,\n2005. ISBN: 978-0-262-69314-1\n[3] Kapali P. Eswaran, Jim N. Gray, Raymond A. Lorie, and Irving L. Traiger: \u201cThe\nNotions of Consistency and Predicate Locks in a Database System,\u201d Communications\nof the ACM, volume 19, number 11, pages 624\u2013633, November 1976.\n[4] \u201cACID Transactions Are Incredibly Helpful,\u201d FoundationDB, LLC, 2013.\n[5] John D. Cook: \u201cACID Versus BASE for Database Transactions,\u201d johndcook.com,\nJuly 6, 2009.\n[6] Gavin Clarke: \u201cNoSQL\u2019s CAP Theorem Busters: We Don\u2019t Drop ACID,\u201d theregis\u2010\nter.co.uk, November 22, 2012.\n[7] Theo H\u00e4rder and Andreas Reuter: \u201cPrinciples of Transaction-Oriented Database\nRecovery,\u201d ACM Computing Surveys, volume 15, number 4, pages 287\u2013317, Decem\u2010\nber 1983. doi:10.1145/289.291\n[8] Peter Bailis, Alan Fekete, Ali Ghodsi, et al.: \u201cHAT, not CAP: Towards Highly\nAvailable Transactions,\u201d at 14th USENIX Workshop on Hot Topics in Operating Sys\u2010\ntems (HotOS), May 2013.\n[9] Armando Fox, Steven D. Gribble, Yatin Chawathe, et al.: \u201cCluster-Based Scalable\nNetwork Services,\u201d at 16th ACM Symposium on Operating Systems Principles (SOSP),\nOctober 1997.\n[10] Philip A. Bernstein, Vassos Hadzilacos, and Nathan Goodman: Concurrency\nControl and Recovery in Database Systems. Addison-Wesley, 1987. ISBN:\n978-0-201-10715-9, available online at research.microsoft.com.\n[11] Alan Fekete, Dimitrios Liarokapis, Elizabeth O\u2019Neil, et al.: \u201cMaking Snapshot\nIsolation Serializable,\u201d ACM Transactions on Database Systems, volume 30, number"}
{"291": "[12] Mai Zheng, Joseph Tucek, Feng Qin, and Mark Lillibridge: \u201cUnderstanding the\nRobustness of SSDs Under Power Fault,\u201d at 11th USENIX Conference on File and\nStorage Technologies (FAST), February 2013.\n[13] Laurie Denness: \u201cSSDs: A Gift and a Curse,\u201d laur.ie, June 2, 2015.\n[14] Adam Surak: \u201cWhen Solid State Drives Are Not That Solid,\u201d blog.algolia.com,\nJune 15, 2015.\n[15] Thanumalayan Sankaranarayana Pillai, Vijay Chidambaram, Ramnatthan Ala\u2010\ngappan, et al.: \u201cAll File Systems Are Not Created Equal: On the Complexity of Craft\u2010\ning Crash-Consistent Applications,\u201d at 11th USENIX Symposium on Operating\nSystems Design and Implementation (OSDI), October 2014.\n[16] Chris Siebenmann: \u201cUnix\u2019s File Durability Problem,\u201d utcc.utoronto.ca, April 14,\n2016.\n[17] Lakshmi N. Bairavasundaram, Garth R. Goodson, Bianca Schroeder, et al.: \u201cAn\nAnalysis of Data Corruption in the Storage Stack,\u201d at 6th USENIX Conference on File\nand Storage Technologies (FAST), February 2008.\n[18] Bianca Schroeder, Raghav Lagisetty, and Arif Merchant: \u201cFlash Reliability in\nProduction: The Expected and the Unexpected,\u201d at 14th USENIX Conference on File\nand Storage Technologies (FAST), February 2016.\n[19] Don Allison: \u201cSSD Storage \u2013 Ignorance of Technology Is No Excuse,\u201d blog.kore\u2010\nlogic.com, March 24, 2015.\n[20] Dave Scherer: \u201cThose Are Not Transactions (Cassandra 2.0),\u201d blog.founda\u2010\ntiondb.com, September 6, 2013.\n[21] Kyle Kingsbury: \u201cCall Me Maybe: Cassandra,\u201d aphyr.com, September 24, 2013.\n[22] \u201cACID Support in Aerospike,\u201d Aerospike, Inc., June 2014.\n[23] Martin Kleppmann: \u201cHermitage: Testing the \u2018I\u2019 in ACID,\u201d martin.klepp\u2010\nmann.com, November 25, 2014.\n[24] Tristan D\u2019Agosta: \u201cBTC Stolen from Poloniex,\u201d bitcointalk.org, March 4, 2014.\n[25] bitcointhief2: \u201cHow I Stole Roughly 100 BTC from an Exchange and How I\nCould Have Stolen More!,\u201d reddit.com, February 2, 2014.\n[26] Sudhir Jorwekar, Alan Fekete, Krithi Ramamritham, and S. Sudarshan: \u201cAuto\u2010\nmating the Detection of Snapshot Isolation Anomalies,\u201d at 33rd International Confer\u2010\nence on Very Large Data Bases (VLDB), September 2007."}
{"292": "[28] Hal Berenson, Philip A. Bernstein, Jim N. Gray, et al.: \u201cA Critique of ANSI SQL\nIsolation Levels,\u201d at ACM International Conference on Management of Data (SIG\u2010\nMOD), May 1995.\n[29] Atul Adya: \u201cWeak Consistency: A Generalized Theory and Optimistic Imple\u2010\nmentations for Distributed Transactions,\u201d PhD Thesis, Massachusetts Institute of\nTechnology, March 1999.\n[30] Peter Bailis, Aaron Davidson, Alan Fekete, et al.: \u201cHighly Available Transactions:\nVirtues and Limitations (Extended Version),\u201d at 40th International Conference on\nVery Large Data Bases (VLDB), September 2014.\n[31] Bruce Momjian: \u201cMVCC Unmasked,\u201d momjian.us, July 2014.\n[32] Annamalai Gurusami: \u201cRepeatable Read Isolation Level in InnoDB \u2013 How Con\u2010\nsistent Read View Works,\u201d blogs.oracle.com, January 15, 2013.\n[33] Nikita Prokopov: \u201cUnofficial Guide to Datomic Internals,\u201d tonsky.me, May 6,\n2014.\n[34] Baron Schwartz: \u201cImmutability, MVCC, and Garbage Collection,\u201d xaprb.com,\nDecember 28, 2013.\n[35] J. Chris Anderson, Jan Lehnardt, and Noah Slater: CouchDB: The Definitive\nGuide. O\u2019Reilly Media, 2010. ISBN: 978-0-596-15589-6\n[36] Rikdeb Mukherjee: \u201cIsolation in DB2 (Repeatable Read, Read Stability, Cursor\nStability, Uncommitted Read) with Examples,\u201d mframes.blogspot.co.uk, July 4, 2013.\n[37] Steve Hilker: \u201cCursor Stability (CS) \u2013 IBM DB2 Community,\u201d toadworld.com,\nMarch 14, 2013.\n[38] Nate Wiger: \u201cAn Atomic Rant,\u201d nateware.com, February 18, 2010.\n[39] Joel Jacobson: \u201cRiak 2.0: Data Types,\u201d blog.joeljacobson.com, March 23, 2014.\n[40] Michael J. Cahill, Uwe R\u00f6hm, and Alan Fekete: \u201cSerializable Isolation for Snap\u2010\nshot Databases,\u201d at ACM International Conference on Management of Data (SIG\u2010\nMOD), June 2008. doi:10.1145/1376616.1376690\n[41] Dan R. K. Ports and Kevin Grittner: \u201cSerializable Snapshot Isolation in Post\u2010\ngreSQL,\u201d at 38th International Conference on Very Large Databases (VLDB), August\n2012.\n[42] Tony Andrews: \u201cEnforcing Complex Constraints in Oracle,\u201d tonyandrews.blog\u2010\nspot.co.uk, October 15, 2004."}
{"293": "Symposium on Operating Systems Principles (SOSP), December 1995. doi:\n10.1145/224056.224070\n[44] Gary Fredericks: \u201cPostgres Serializability Bug,\u201d github.com, September 2015.\n[45] Michael Stonebraker, Samuel Madden, Daniel J. Abadi, et al.: \u201cThe End of an\nArchitectural Era (It\u2019s Time for a Complete Rewrite),\u201d at 33rd International Confer\u2010\nence on Very Large Data Bases (VLDB), September 2007.\n[46] John Hugg: \u201cH-Store/VoltDB Architecture vs. CEP Systems and Newer Stream\u2010\ning Architectures,\u201d at Data @Scale Boston, November 2014.\n[47] Robert Kallman, Hideaki Kimura, Jonathan Natkins, et al.: \u201cH-Store: A High-\nPerformance, Distributed Main Memory Transaction Processing System,\u201d Proceed\u2010\nings of the VLDB Endowment, volume 1, number 2, pages 1496\u20131499, August 2008.\n[48] Rich Hickey: \u201cThe Architecture of Datomic,\u201d infoq.com, November 2, 2012.\n[49] John Hugg: \u201cDebunking Myths About the VoltDB In-Memory Database,\u201d\nvoltdb.com, May 12, 2014.\n[50] Joseph M. Hellerstein, Michael Stonebraker, and James Hamilton: \u201cArchitecture\nof a Database System,\u201d Foundations and Trends in Databases, volume 1, number 2,\npages 141\u2013259, November 2007. doi:10.1561/1900000002\n[51] Michael J. Cahill: \u201cSerializable Isolation for Snapshot Databases,\u201d PhD Thesis,\nUniversity of Sydney, July 2009.\n[52] D. Z. Badal: \u201cCorrectness of Concurrency Control and Implications in Dis\u2010\ntributed Databases,\u201d at 3rd International IEEE Computer Software and Applications\nConference (COMPSAC), November 1979.\n[53] Rakesh Agrawal, Michael J. Carey, and Miron Livny: \u201cConcurrency Control Per\u2010\nformance Modeling: Alternatives and Implications,\u201d ACM Transactions on Database\nSystems (TODS), volume 12, number 4, pages 609\u2013654, December 1987. doi:\n10.1145/32204.32220\n[54] Dave Rosenthal: \u201cDatabases at 14.4MHz,\u201d blog.foundationdb.com, December 10,\n2014."}
{"294": ""}
{"295": "CHAPTER 8\nThe Trouble with Distributed Systems\nHey I just met you\nThe network\u2019s laggy\nBut here\u2019s my data\nSo store it maybe\n\u2014Kyle Kingsbury, Carly Rae Jepsen and the Perils of Network Partitions (2013)\nA recurring theme in the last few chapters has been how systems handle things going\nwrong. For example, we discussed replica failover (\u201cHandling Node Outages\u201d on\npage 156), replication lag (\u201cProblems with Replication Lag\u201d on page 161), and con\u2010\ncurrency control for transactions (\u201cWeak Isolation Levels\u201d on page 233). As we come\nto understand various edge cases that can occur in real systems, we get better at han\u2010\ndling them.\nHowever, even though we have talked a lot about faults, the last few chapters have\nstill been too optimistic. The reality is even darker. We will now turn our pessimism\nto the maximum and assume that anything that can go wrong will go wrong.i (Expe\u2010\nrienced systems operators will tell you that is a reasonable assumption. If you ask\nnicely, they might tell you some frightening stories while nursing their scars of past\nbattles.)\nWorking with distributed systems is fundamentally different from writing software\non a single computer\u2014and the main difference is that there are lots of new and excit\u2010\ning ways for things to go wrong [1, 2]. In this chapter, we will get a taste of the prob\u2010\nlems that arise in practice, and an understanding of the things we can and cannot rely\non."}
{"296": "In the end, our task as engineers is to build systems that do their job (i.e., meet the\nguarantees that users are expecting), in spite of everything going wrong. In Chapter 9,\nwe will look at some examples of algorithms that can provide such guarantees in a\ndistributed system. But first, in this chapter, we must understand what challenges we\nare up against.\nThis chapter is a thoroughly pessimistic and depressing overview of things that may\ngo wrong in a distributed system. We will look into problems with networks (\u201cUnre\u2010\nliable Networks\u201d on page 277); clocks and timing issues (\u201cUnreliable Clocks\u201d on page\n287); and we\u2019ll discuss to what degree they are avoidable. The consequences of all\nthese issues are disorienting, so we\u2019ll explore how to think about the state of a dis\u2010\ntributed system and how to reason about things that have happened (\u201cKnowledge,\nTruth, and Lies\u201d on page 300).\nFaults and Partial Failures\nWhen you are writing a program on a single computer, it normally behaves in a fairly\npredictable way: either it works or it doesn\u2019t. Buggy software may give the appearance\nthat the computer is sometimes \u201chaving a bad day\u201d (a problem that is often fixed by a\nreboot), but that is mostly just a consequence of badly written software.\nThere is no fundamental reason why software on a single computer should be flaky:\nwhen the hardware is working correctly, the same operation always produces the\nsame result (it is deterministic). If there is a hardware problem (e.g., memory corrup\u2010\ntion or a loose connector), the consequence is usually a total system failure (e.g., ker\u2010\nnel panic, \u201cblue screen of death,\u201d failure to start up). An individual computer with\ngood software is usually either fully functional or entirely broken, but not something\nin between.\nThis is a deliberate choice in the design of computers: if an internal fault occurs, we\nprefer a computer to crash completely rather than returning a wrong result, because\nwrong results are difficult and confusing to deal with. Thus, computers hide the fuzzy\nphysical reality on which they are implemented and present an idealized system\nmodel that operates with mathematical perfection. A CPU instruction always does\nthe same thing; if you write some data to memory or disk, that data remains intact\nand doesn\u2019t get randomly corrupted. This design goal of always-correct computation\ngoes all the way back to the very first digital computer [3].\nWhen you are writing software that runs on several computers, connected by a net\u2010\nwork, the situation is fundamentally different. In distributed systems, we are no\nlonger operating in an idealized system model\u2014we have no choice but to confront"}
{"297": "In my limited experience I\u2019ve dealt with long-lived network partitions in a single data\ncenter (DC), PDU [power distribution unit] failures, switch failures, accidental power\ncycles of whole racks, whole-DC backbone failures, whole-DC power failures, and a\nhypoglycemic driver smashing his Ford pickup truck into a DC\u2019s HVAC [heating, ven\u2010\ntilation, and air conditioning] system. And I\u2019m not even an ops guy.\n\u2014Coda Hale\nIn a distributed system, there may well be some parts of the system that are broken in\nsome unpredictable way, even though other parts of the system are working fine. This\nis known as a partial failure. The difficulty is that partial failures are nondeterministic:\nif you try to do anything involving multiple nodes and the network, it may sometimes\nwork and sometimes unpredictably fail. As we shall see, you may not even know\nwhether something succeeded or not, as the time it takes for a message to travel\nacross a network is also nondeterministic!\nThis nondeterminism and possibility of partial failures is what makes distributed sys\u2010\ntems hard to work with [5].\nCloud Computing and Supercomputing\nThere is a spectrum of philosophies on how to build large-scale computing systems:\n\u2022 At one end of the scale is the field of high-performance computing (HPC). Super\u2010\ncomputers with thousands of CPUs are typically used for computationally inten\u2010\nsive scientific computing tasks, such as weather forecasting or molecular\ndynamics (simulating the movement of atoms and molecules).\n\u2022 At the other extreme is cloud computing, which is not very well defined [6] but is\noften associated with multi-tenant datacenters, commodity computers connected\nwith an IP network (often Ethernet), elastic/on-demand resource allocation, and\nmetered billing.\n\u2022 Traditional enterprise datacenters lie somewhere between these extremes.\nWith these philosophies come very different approaches to handling faults. In a\nsupercomputer, a job typically checkpoints the state of its computation to durable\nstorage from time to time. If one node fails, a common solution is to simply stop the\nentire cluster workload. After the faulty node is repaired, the computation is restarted\nfrom the last checkpoint [7, 8]. Thus, a supercomputer is more like a single-node\ncomputer than a distributed system: it deals with partial failure by letting it escalate\ninto total failure\u2014if any part of the system fails, just let everything crash (like a kernel\npanic on a single machine)."}
{"298": "\u2022 Many internet-related applications are online, in the sense that they need to be\nable to serve users with low latency at any time. Making the service unavailable\u2014\nfor example, stopping the cluster for repair\u2014is not acceptable. In contrast, off\u2010\nline (batch) jobs like weather simulations can be stopped and restarted with fairly\nlow impact.\n\u2022 Supercomputers are typically built from specialized hardware, where each node\nis quite reliable, and nodes communicate through shared memory and remote\ndirect memory access (RDMA). On the other hand, nodes in cloud services are\nbuilt from commodity machines, which can provide equivalent performance at\nlower cost due to economies of scale, but also have higher failure rates.\n\u2022 Large datacenter networks are often based on IP and Ethernet, arranged in Clos\ntopologies to provide high bisection bandwidth [9]. Supercomputers often use\nspecialized network topologies, such as multi-dimensional meshes and toruses\n[10], which yield better performance for HPC workloads with known communi\u2010\ncation patterns.\n\u2022 The bigger a system gets, the more likely it is that one of its components is bro\u2010\nken. Over time, broken things get fixed and new things break, but in a system\nwith thousands of nodes, it is reasonable to assume that something is always bro\u2010\nken [7]. When the error handling strategy consists of simply giving up, a large\nsystem can end up spending a lot of its time recovering from faults rather than\ndoing useful work [8].\n\u2022 If the system can tolerate failed nodes and still keep working as a whole, that is a\nvery useful feature for operations and maintenance: for example, you can per\u2010\nform a rolling upgrade (see Chapter 4), restarting one node at a time, while the\nservice continues serving users without interruption. In cloud environments, if\none virtual machine is not performing well, you can just kill it and request a new\none (hoping that the new one will be faster).\n\u2022 In a geographically distributed deployment (keeping data geographically close to\nyour users to reduce access latency), communication most likely goes over the\ninternet, which is slow and unreliable compared to local networks. Supercom\u2010\nputers generally assume that all of their nodes are close together.\nIf we want to make distributed systems work, we must accept the possibility of partial\nfailure and build fault-tolerance mechanisms into the software. In other words, we\nneed to build a reliable system from unreliable components. (As discussed in \u201cRelia\u2010\nbility\u201d on page 6, there is no such thing as perfect reliability, so we\u2019ll need to under\u2010\nstand the limits of what we can realistically promise.)"}
{"299": "will become faulty, and the software will have to somehow handle it. The fault han\u2010\ndling must be part of the software design, and you (as operator of the software) need\nto know what behavior to expect from the software in the case of a fault.\nIt would be unwise to assume that faults are rare and simply hope for the best. It is\nimportant to consider a wide range of possible faults\u2014even fairly unlikely ones\u2014and\nto artificially create such situations in your testing environment to see what happens.\nIn distributed systems, suspicion, pessimism, and paranoia pay off.\nBuilding a Reliable System from Unreliable Components\nYou may wonder whether this makes any sense\u2014intuitively it may seem like a system\ncan only be as reliable as its least reliable component (its weakest link). This is not the\ncase: in fact, it is an old idea in computing to construct a more reliable system from a\nless reliable underlying base [11]. For example:\n\u2022 Error-correcting codes allow digital data to be transmitted accurately across a\ncommunication channel that occasionally gets some bits wrong, for example due\nto radio interference on a wireless network [12].\n\u2022 IP (the Internet Protocol) is unreliable: it may drop, delay, duplicate, or reorder\npackets. TCP (the Transmission Control Protocol) provides a more reliable\ntransport layer on top of IP: it ensures that missing packets are retransmitted,\nduplicates are eliminated, and packets are reassembled into the order in which\nthey were sent.\nAlthough the system can be more reliable than its underlying parts, there is always a\nlimit to how much more reliable it can be. For example, error-correcting codes can\ndeal with a small number of single-bit errors, but if your signal is swamped by inter\u2010\nference, there is a fundamental limit to how much data you can get through your\ncommunication channel [13]. TCP can hide packet loss, duplication, and reordering\nfrom you, but it cannot magically remove delays in the network.\nAlthough the more reliable higher-level system is not perfect, it\u2019s still useful because it\ntakes care of some of the tricky low-level faults, and so the remaining faults are usu\u2010\nally easier to reason about and deal with. We will explore this matter further in \u201cThe\nend-to-end argument\u201d on page 519.\nUnreliable Networks\nAs discussed in the introduction to Part II, the distributed systems we focus on in this"}
{"300": "machine has its own memory and disk, and one machine cannot access another\nmachine\u2019s memory or disk (except by making requests to a service over the network).\nShared-nothing is not the only way of building systems, but it has become the domi\u2010\nnant approach for building internet services, for several reasons: it\u2019s comparatively\ncheap because it requires no special hardware, it can make use of commoditized\ncloud computing services, and it can achieve high reliability through redundancy\nacross multiple geographically distributed datacenters.\nThe internet and most internal networks in datacenters (often Ethernet) are asyn\u2010\nchronous packet networks. In this kind of network, one node can send a message (a\npacket) to another node, but the network gives no guarantees as to when it will arrive,\nor whether it will arrive at all. If you send a request and expect a response, many\nthings could go wrong (some of which are illustrated in Figure 8-1):\n1. Your request may have been lost (perhaps someone unplugged a network cable).\n2. Your request may be waiting in a queue and will be delivered later (perhaps the\nnetwork or the recipient is overloaded).\n3. The remote node may have failed (perhaps it crashed or it was powered down).\n4. The remote node may have temporarily stopped responding (perhaps it is expe\u2010\nriencing a long garbage collection pause; see \u201cProcess Pauses\u201d on page 295), but it\nwill start responding again later.\n5. The remote node may have processed your request, but the response has been\nlost on the network (perhaps a network switch has been misconfigured).\n6. The remote node may have processed your request, but the response has been\ndelayed and will be delivered later (perhaps the network or your own machine is\noverloaded).\nFigure 8-1. If you send a request and don\u2019t get a response, it\u2019s not possible to distinguish"}
{"301": "The sender can\u2019t even tell whether the packet was delivered: the only option is for the\nrecipient to send a response message, which may in turn be lost or delayed. These\nissues are indistinguishable in an asynchronous network: the only information you\nhave is that you haven\u2019t received a response yet. If you send a request to another node\nand don\u2019t receive a response, it is impossible to tell why.\nThe usual way of handling this issue is a timeout: after some time you give up waiting\nand assume that the response is not going to arrive. However, when a timeout occurs,\nyou still don\u2019t know whether the remote node got your request or not (and if the\nrequest is still queued somewhere, it may still be delivered to the recipient, even if the\nsender has given up on it).\nNetwork Faults in Practice\nWe have been building computer networks for decades\u2014one might hope that by now\nwe would have figured out how to make them reliable. However, it seems that we\nhave not yet succeeded.\nThere are some systematic studies, and plenty of anecdotal evidence, showing that\nnetwork problems can be surprisingly common, even in controlled environments like\na datacenter operated by one company [14]. One study in a medium-sized datacenter\nfound about 12 network faults per month, of which half disconnected a single\nmachine, and half disconnected an entire rack [15]. Another study measured the fail\u2010\nure rates of components like top-of-rack switches, aggregation switches, and load bal\u2010\nancers [16]. It found that adding redundant networking gear doesn\u2019t reduce faults as\nmuch as you might hope, since it doesn\u2019t guard against human error (e.g., misconfig\u2010\nured switches), which is a major cause of outages.\nPublic cloud services such as EC2 are notorious for having frequent transient net\u2010\nwork glitches [14], and well-managed private datacenter networks can be stabler\nenvironments. Nevertheless, nobody is immune from network problems: for exam\u2010\nple, a problem during a software upgrade for a switch could trigger a network topol\u2010\nogy reconfiguration, during which network packets could be delayed for more than a\nminute [17]. Sharks might bite undersea cables and damage them [18]. Other surpris\u2010\ning faults include a network interface that sometimes drops all inbound packets but\nsends outbound packets successfully [19]: just because a network link works in one\ndirection doesn\u2019t guarantee it\u2019s also working in the opposite direction.\nNetwork partitions\nWhen one part of the network is cut off from the rest due to a net\u2010"}
{"302": "Even if network faults are rare in your environment, the fact that faults can occur\nmeans that your software needs to be able to handle them. Whenever any communi\u2010\ncation happens over a network, it may fail\u2014there is no way around it.\nIf the error handling of network faults is not defined and tested, arbitrarily bad things\ncould happen: for example, the cluster could become deadlocked and permanently\nunable to serve requests, even when the network recovers [20], or it could even delete\nall of your data [21]. If software is put in an unanticipated situation, it may do arbi\u2010\ntrary unexpected things.\nHandling network faults doesn\u2019t necessarily mean tolerating them: if your network is\nnormally fairly reliable, a valid approach may be to simply show an error message to\nusers while your network is experiencing problems. However, you do need to know\nhow your software reacts to network problems and ensure that the system can\nrecover from them. It may make sense to deliberately trigger network problems and\ntest the system\u2019s response (this is the idea behind Chaos Monkey; see \u201cReliability\u201d on\npage 6).\nDetecting Faults\nMany systems need to automatically detect faulty nodes. For example:\n\u2022 A load balancer needs to stop sending requests to a node that is dead (i.e., take it\nout of rotation).\n\u2022 In a distributed database with single-leader replication, if the leader fails, one of\nthe followers needs to be promoted to be the new leader (see \u201cHandling Node\nOutages\u201d on page 156).\nUnfortunately, the uncertainty about the network makes it difficult to tell whether a\nnode is working or not. In some specific circumstances you might get some feedback\nto explicitly tell you that something is not working:\n\u2022 If you can reach the machine on which the node should be running, but no pro\u2010\ncess is listening on the destination port (e.g., because the process crashed), the\noperating system will helpfully close or refuse TCP connections by sending a RST\nor FIN packet in reply. However, if the node crashed while it was handling your\nrequest, you have no way of knowing how much data was actually processed by\nthe remote node [22].\n\u2022 If a node process crashed (or was killed by an administrator) but the node\u2019s oper\u2010\nating system is still running, a script can notify other nodes about the crash so"}
{"303": "\u2022 If you have access to the management interface of the network switches in your\ndatacenter, you can query them to detect link failures at a hardware level (e.g., if\nthe remote machine is powered down). This option is ruled out if you\u2019re con\u2010\nnecting via the internet, or if you\u2019re in a shared datacenter with no access to the\nswitches themselves, or if you can\u2019t reach the management interface due to a net\u2010\nwork problem.\n\u2022 If a router is sure that the IP address you\u2019re trying to connect to is unreachable, it\nmay reply to you with an ICMP Destination Unreachable packet. However, the\nrouter doesn\u2019t have a magic failure detection capability either\u2014it is subject to the\nsame limitations as other participants of the network.\nRapid feedback about a remote node being down is useful, but you can\u2019t count on it.\nEven if TCP acknowledges that a packet was delivered, the application may have\ncrashed before handling it. If you want to be sure that a request was successful, you\nneed a positive response from the application itself [24].\nConversely, if something has gone wrong, you may get an error response at some\nlevel of the stack, but in general you have to assume that you will get no response at\nall. You can retry a few times (TCP retries transparently, but you may also retry at the\napplication level), wait for a timeout to elapse, and eventually declare the node dead if\nyou don\u2019t hear back within the timeout.\nTimeouts and Unbounded Delays\nIf a timeout is the only sure way of detecting a fault, then how long should the time\u2010\nout be? There is unfortunately no simple answer.\nA long timeout means a long wait until a node is declared dead (and during this time,\nusers may have to wait or see error messages). A short timeout detects faults faster,\nbut carries a higher risk of incorrectly declaring a node dead when in fact it has only\nsuffered a temporary slowdown (e.g., due to a load spike on the node or the network).\nPrematurely declaring a node dead is problematic: if the node is actually alive and in\nthe middle of performing some action (for example, sending an email), and another\nnode takes over, the action may end up being performed twice. We will discuss this\nissue in more detail in \u201cKnowledge, Truth, and Lies\u201d on page 300, and in Chapters 9\nand 11.\nWhen a node is declared dead, its responsibilities need to be transferred to other\nnodes, which places additional load on other nodes and the network. If the system is\nalready struggling with high load, declaring nodes dead prematurely can make the"}
{"304": "Imagine a fictitious system with a network that guaranteed a maximum delay for\npackets\u2014every packet is either delivered within some time d, or it is lost, but delivery\nnever takes longer than d. Furthermore, assume that you can guarantee that a non-\nfailed node always handles a request within some time r. In this case, you could guar\u2010\nantee that every successful request receives a response within time 2d + r\u2014and if you\ndon\u2019t receive a response within that time, you know that either the network or the\nremote node is not working. If this was true, 2d + r would be a reasonable timeout to\nuse.\nUnfortunately, most systems we work with have neither of those guarantees: asyn\u2010\nchronous networks have unbounded delays (that is, they try to deliver packets as\nquickly as possible, but there is no upper limit on the time it may take for a packet to\narrive), and most server implementations cannot guarantee that they can handle\nrequests within some maximum time (see \u201cResponse time guarantees\u201d on page 298).\nFor failure detection, it\u2019s not sufficient for the system to be fast most of the time: if\nyour timeout is low, it only takes a transient spike in round-trip times to throw the\nsystem off-balance.\nNetwork congestion and queueing\nWhen driving a car, travel times on road networks often vary most due to traffic con\u2010\ngestion. Similarly, the variability of packet delays on computer networks is most often\ndue to queueing [25]:\n\u2022 If several different nodes simultaneously try to send packets to the same destina\u2010\ntion, the network switch must queue them up and feed them into the destination\nnetwork link one by one (as illustrated in Figure 8-2). On a busy network link, a\npacket may have to wait a while until it can get a slot (this is called network con\u2010\ngestion). If there is so much incoming data that the switch queue fills up, the\npacket is dropped, so it needs to be resent\u2014even though the network is function\u2010\ning fine.\n\u2022 When a packet reaches the destination machine, if all CPU cores are currently\nbusy, the incoming request from the network is queued by the operating system\nuntil the application is ready to handle it. Depending on the load on the machine,\nthis may take an arbitrary length of time.\n\u2022 In virtualized environments, a running operating system is often paused for tens\nof milliseconds while another virtual machine uses a CPU core. During this time,\nthe VM cannot consume any data from the network, so the incoming data is\nqueued (buffered) by the virtual machine monitor [26], further increasing the\nvariability of network delays."}
{"305": "network link or the receiving node [27]. This means additional queueing at the\nsender before the data even enters the network.\nFigure 8-2. If several machines send network traffic to the same destination, its switch\nqueue can fill up. Here, ports 1, 2, and 4 are all trying to send packets to port 3.\nMoreover, TCP considers a packet to be lost if it is not acknowledged within some\ntimeout (which is calculated from observed round-trip times), and lost packets are\nautomatically retransmitted. Although the application does not see the packet loss\nand retransmission, it does see the resulting delay (waiting for the timeout to expire,\nand then waiting for the retransmitted packet to be acknowledged).\nTCP Versus UDP\nSome latency-sensitive applications, such as videoconferencing and Voice over IP\n(VoIP), use UDP rather than TCP. It\u2019s a trade-off between reliability and variability\nof delays: as UDP does not perform flow control and does not retransmit lost packets,\nit avoids some of the reasons for variable network delays (although it is still suscepti\u2010\nble to switch queues and scheduling delays).\nUDP is a good choice in situations where delayed data is worthless. For example, in a\nVoIP phone call, there probably isn\u2019t enough time to retransmit a lost packet before\nits data is due to be played over the loudspeakers. In this case, there\u2019s no point in\nretransmitting the packet\u2014the application must instead fill the missing packet\u2019s time\nslot with silence (causing a brief interruption in the sound) and move on in the\nstream. The retry happens at the human layer instead. (\u201cCould you repeat that please?\nThe sound just cut out for a moment.\u201d)"}
{"306": "tem with plenty of spare capacity can easily drain queues, whereas in a highly utilized\nsystem, long queues can build up very quickly.\nIn public clouds and multi-tenant datacenters, resources are shared among many\ncustomers: the network links and switches, and even each machine\u2019s network inter\u2010\nface and CPUs (when running on virtual machines), are shared. Batch workloads\nsuch as MapReduce (see Chapter 10) can easily saturate network links. As you have\nno control over or insight into other customers\u2019 usage of the shared resources, net\u2010\nwork delays can be highly variable if someone near you (a noisy neighbor) is using a\nlot of resources [28, 29].\nIn such environments, you can only choose timeouts experimentally: measure the\ndistribution of network round-trip times over an extended period, and over many\nmachines, to determine the expected variability of delays. Then, taking into account\nyour application\u2019s characteristics, you can determine an appropriate trade-off\nbetween failure detection delay and risk of premature timeouts.\nEven better, rather than using configured constant timeouts, systems can continually\nmeasure response times and their variability (jitter), and automatically adjust time\u2010\nouts according to the observed response time distribution. This can be done with a\nPhi Accrual failure detector [30], which is used for example in Akka and Cassandra\n[31]. TCP retransmission timeouts also work similarly [27].\nSynchronous Versus Asynchronous Networks\nDistributed systems would be a lot simpler if we could rely on the network to deliver\npackets with some fixed maximum delay, and not to drop packets. Why can\u2019t we\nsolve this at the hardware level and make the network reliable so that the software\ndoesn\u2019t need to worry about it?\nTo answer this question, it\u2019s interesting to compare datacenter networks to the tradi\u2010\ntional fixed-line telephone network (non-cellular, non-VoIP), which is extremely\nreliable: delayed audio frames and dropped calls are very rare. A phone call requires a\nconstantly low end-to-end latency and enough bandwidth to transfer the audio sam\u2010\nples of your voice. Wouldn\u2019t it be nice to have similar reliability and predictability in\ncomputer networks?\nWhen you make a call over the telephone network, it establishes a circuit: a fixed,\nguaranteed amount of bandwidth is allocated for the call, along the entire route\nbetween the two callers. This circuit remains in place until the call ends [32]. For\nexample, an ISDN network runs at a fixed rate of 4,000 frames per second. When a\ncall is established, it is allocated 16 bits of space within each frame (in each direction)."}
{"307": "This kind of network is synchronous: even as data passes through several routers, it\ndoes not suffer from queueing, because the 16 bits of space for the call have already\nbeen reserved in the next hop of the network. And because there is no queueing, the\nmaximum end-to-end latency of the network is fixed. We call this a bounded delay.\nCan we not simply make network delays predictable?\nNote that a circuit in a telephone network is very different from a TCP connection: a\ncircuit is a fixed amount of reserved bandwidth which nobody else can use while the\ncircuit is established, whereas the packets of a TCP connection opportunistically use\nwhatever network bandwidth is available. You can give TCP a variable-sized block of\ndata (e.g., an email or a web page), and it will try to transfer it in the shortest time\npossible. While a TCP connection is idle, it doesn\u2019t use any bandwidth.ii\nIf datacenter networks and the internet were circuit-switched networks, it would be\npossible to establish a guaranteed maximum round-trip time when a circuit was set\nup. However, they are not: Ethernet and IP are packet-switched protocols, which suf\u2010\nfer from queueing and thus unbounded delays in the network. These protocols do\nnot have the concept of a circuit.\nWhy do datacenter networks and the internet use packet switching? The answer is\nthat they are optimized for bursty traffic. A circuit is good for an audio or video call,\nwhich needs to transfer a fairly constant number of bits per second for the duration\nof the call. On the other hand, requesting a web page, sending an email, or transfer\u2010\nring a file doesn\u2019t have any particular bandwidth requirement\u2014we just want it to\ncomplete as quickly as possible.\nIf you wanted to transfer a file over a circuit, you would have to guess a bandwidth\nallocation. If you guess too low, the transfer is unnecessarily slow, leaving network\ncapacity unused. If you guess too high, the circuit cannot be set up (because the net\u2010\nwork cannot allow a circuit to be created if its bandwidth allocation cannot be guar\u2010\nanteed). Thus, using circuits for bursty data transfers wastes network capacity and\nmakes transfers unnecessarily slow. By contrast, TCP dynamically adapts the rate of\ndata transfer to the available network capacity.\nThere have been some attempts to build hybrid networks that support both circuit\nswitching and packet switching, such as ATM.iii InfiniBand has some similarities [35]:\nit implements end-to-end flow control at the link layer, which reduces the need for\nii. Except perhaps for an occasional keepalive packet, if TCP keepalive is enabled."}
{"308": "queueing in the network, although it can still suffer from delays due to link conges\u2010\ntion [36]. With careful use of quality of service (QoS, prioritization and scheduling of\npackets) and admission control (rate-limiting senders), it is possible to emulate circuit\nswitching on packet networks, or provide statistically bounded delay [25, 32].\nLatency and Resource Utilization\nMore generally, you can think of variable delays as a consequence of dynamic\nresource partitioning.\nSay you have a wire between two telephone switches that can carry up to 10,000\nsimultaneous calls. Each circuit that is switched over this wire occupies one of those\ncall slots. Thus, you can think of the wire as a resource that can be shared by up to\n10,000 simultaneous users. The resource is divided up in a static way: even if you\u2019re\nthe only call on the wire right now, and all other 9,999 slots are unused, your circuit is\nstill allocated the same fixed amount of bandwidth as when the wire is fully utilized.\nBy contrast, the internet shares network bandwidth dynamically. Senders push and\njostle with each other to get their packets over the wire as quickly as possible, and the\nnetwork switches decide which packet to send (i.e., the bandwidth allocation) from\none moment to the next. This approach has the downside of queueing, but the advan\u2010\ntage is that it maximizes utilization of the wire. The wire has a fixed cost, so if you\nutilize it better, each byte you send over the wire is cheaper.\nA similar situation arises with CPUs: if you share each CPU core dynamically\nbetween several threads, one thread sometimes has to wait in the operating system\u2019s\nrun queue while another thread is running, so a thread can be paused for varying\nlengths of time. However, this utilizes the hardware better than if you allocated a\nstatic number of CPU cycles to each thread (see \u201cResponse time guarantees\u201d on page\n298). Better hardware utilization is also a significant motivation for using virtual\nmachines.\nLatency guarantees are achievable in certain environments, if resources are statically\npartitioned (e.g., dedicated hardware and exclusive bandwidth allocations). However,\nit comes at the cost of reduced utilization\u2014in other words, it is more expensive. On\nthe other hand, multi-tenancy with dynamic resource partitioning provides better\nutilization, so it is cheaper, but it has the downside of variable delays.\nVariable delays in networks are not a law of nature, but simply the result of a cost/\nbenefit trade-off."}
{"309": "However, such quality of service is currently not enabled in multi-tenant datacenters\nand public clouds, or when communicating via the internet.iv Currently deployed\ntechnology does not allow us to make any guarantees about delays or reliability of the\nnetwork: we have to assume that network congestion, queueing, and unbounded\ndelays will happen. Consequently, there\u2019s no \u201ccorrect\u201d value for timeouts\u2014they need\nto be determined experimentally.\nUnreliable Clocks\nClocks and time are important. Applications depend on clocks in various ways to\nanswer questions like the following:\n1. Has this request timed out yet?\n2. What\u2019s the 99th percentile response time of this service?\n3. How many queries per second did this service handle on average in the last five\nminutes?\n4. How long did the user spend on our site?\n5. When was this article published?\n6. At what date and time should the reminder email be sent?\n7. When does this cache entry expire?\n8. What is the timestamp on this error message in the log file?\nExamples 1\u20134 measure durations (e.g., the time interval between a request being sent\nand a response being received), whereas examples 5\u20138 describe points in time (events\nthat occur on a particular date, at a particular time).\nIn a distributed system, time is a tricky business, because communication is not\ninstantaneous: it takes time for a message to travel across the network from one\nmachine to another. The time when a message is received is always later than the\ntime when it is sent, but due to variable delays in the network, we don\u2019t know how\nmuch later. This fact sometimes makes it difficult to determine the order in which\nthings happened when multiple machines are involved.\nMoreover, each machine on the network has its own clock, which is an actual hard\u2010\nware device: usually a quartz crystal oscillator. These devices are not perfectly accu\u2010\nrate, so each machine has its own notion of time, which may be slightly faster or"}
{"310": "slower than on other machines. It is possible to synchronize clocks to some degree:\nthe most commonly used mechanism is the Network Time Protocol (NTP), which\nallows the computer clock to be adjusted according to the time reported by a group of\nservers [37]. The servers in turn get their time from a more accurate time source,\nsuch as a GPS receiver.\nMonotonic Versus Time-of-Day Clocks\nModern computers have at least two different kinds of clocks: a time-of-day clock and\na monotonic clock. Although they both measure time, it is important to distinguish\nthe two, since they serve different purposes.\nTime-of-day clocks\nA time-of-day clock does what you intuitively expect of a clock: it returns the current\ndate and time according to some calendar (also known as wall-clock time). For exam\u2010\nple, clock_gettime(CLOCK_REALTIME) on Linuxv and System.currentTimeMillis()\nin Java return the number of seconds (or milliseconds) since the epoch: midnight\nUTC on January 1, 1970, according to the Gregorian calendar, not counting leap sec\u2010\nonds. Some systems use other dates as their reference point.\nTime-of-day clocks are usually synchronized with NTP, which means that a time\u2010\nstamp from one machine (ideally) means the same as a timestamp on another\nmachine. However, time-of-day clocks also have various oddities, as described in the\nnext section. In particular, if the local clock is too far ahead of the NTP server, it may\nbe forcibly reset and appear to jump back to a previous point in time. These jumps, as\nwell as the fact that they often ignore leap seconds, make time-of-day clocks unsuita\u2010\nble for measuring elapsed time [38].\nTime-of-day clocks have also historically had quite a coarse-grained resolution, e.g.,\nmoving forward in steps of 10 ms on older Windows systems [39]. On recent sys\u2010\ntems, this is less of a problem.\nMonotonic clocks\nA monotonic clock is suitable for measuring a duration (time interval), such as a\ntimeout or a service\u2019s response time: clock_gettime(CLOCK_MONOTONIC) on Linux\nand System.nanoTime() in Java are monotonic clocks, for example. The name comes\nfrom the fact that they are guaranteed to always move forward (whereas a time-of-\nday clock may jump back in time)."}
{"311": "You can check the value of the monotonic clock at one point in time, do something,\nand then check the clock again at a later time. The difference between the two values\ntells you how much time elapsed between the two checks. However, the absolute\nvalue of the clock is meaningless: it might be the number of nanoseconds since the\ncomputer was started, or something similarly arbitrary. In particular, it makes no\nsense to compare monotonic clock values from two different computers, because they\ndon\u2019t mean the same thing.\nOn a server with multiple CPU sockets, there may be a separate timer per CPU,\nwhich is not necessarily synchronized with other CPUs. Operating systems compen\u2010\nsate for any discrepancy and try to present a monotonic view of the clock to applica\u2010\ntion threads, even as they are scheduled across different CPUs. However, it is wise to\ntake this guarantee of monotonicity with a pinch of salt [40].\nNTP may adjust the frequency at which the monotonic clock moves forward (this is\nknown as slewing the clock) if it detects that the computer\u2019s local quartz is moving\nfaster or slower than the NTP server. By default, NTP allows the clock rate to be spee\u2010\nded up or slowed down by up to 0.05%, but NTP cannot cause the monotonic clock\nto jump forward or backward. The resolution of monotonic clocks is usually quite\ngood: on most systems they can measure time intervals in microseconds or less.\nIn a distributed system, using a monotonic clock for measuring elapsed time (e.g.,\ntimeouts) is usually fine, because it doesn\u2019t assume any synchronization between dif\u2010\nferent nodes\u2019 clocks and is not sensitive to slight inaccuracies of measurement.\nClock Synchronization and Accuracy\nMonotonic clocks don\u2019t need synchronization, but time-of-day clocks need to be set\naccording to an NTP server or other external time source in order to be useful.\nUnfortunately, our methods for getting a clock to tell the correct time aren\u2019t nearly as\nreliable or accurate as you might hope\u2014hardware clocks and NTP can be fickle\nbeasts. To give just a few examples:\n\u2022 The quartz clock in a computer is not very accurate: it drifts (runs faster or\nslower than it should). Clock drift varies depending on the temperature of the\nmachine. Google assumes a clock drift of 200 ppm (parts per million) for its\nservers [41], which is equivalent to 6 ms drift for a clock that is resynchronized\nwith a server every 30 seconds, or 17 seconds drift for a clock that is resynchron\u2010\nized once a day. This drift limits the best possible accuracy you can achieve, even\nif everything is working correctly.\n\u2022 If a computer\u2019s clock differs too much from an NTP server, it may refuse to syn\u2010"}
{"312": "\u2022 If a node is accidentally firewalled off from NTP servers, the misconfiguration\nmay go unnoticed for some time. Anecdotal evidence suggests that this does hap\u2010\npen in practice.\n\u2022 NTP synchronization can only be as good as the network delay, so there is a limit\nto its accuracy when you\u2019re on a congested network with variable packet delays.\nOne experiment showed that a minimum error of 35 ms is achievable when syn\u2010\nchronizing over the internet [42], though occasional spikes in network delay lead\nto errors of around a second. Depending on the configuration, large network\ndelays can cause the NTP client to give up entirely.\n\u2022 Some NTP servers are wrong or misconfigured, reporting time that is off by\nhours [43, 44]. NTP clients are quite robust, because they query several servers\nand ignore outliers. Nevertheless, it\u2019s somewhat worrying to bet the correctness\nof your systems on the time that you were told by a stranger on the internet.\n\u2022 Leap seconds result in a minute that is 59 seconds or 61 seconds long, which\nmesses up timing assumptions in systems that are not designed with leap seconds\nin mind [45]. The fact that leap seconds have crashed many large systems [38,\n46] shows how easy it is for incorrect assumptions about clocks to sneak into a\nsystem. The best way of handling leap seconds may be to make NTP servers \u201clie,\u201d\nby performing the leap second adjustment gradually over the course of a day\n(this is known as smearing) [47, 48], although actual NTP server behavior varies\nin practice [49].\n\u2022 In virtual machines, the hardware clock is virtualized, which raises additional\nchallenges for applications that need accurate timekeeping [50]. When a CPU\ncore is shared between virtual machines, each VM is paused for tens of milli\u2010\nseconds while another VM is running. From an application\u2019s point of view, this\npause manifests itself as the clock suddenly jumping forward [26].\n\u2022 If you run software on devices that you don\u2019t fully control (e.g., mobile or\nembedded devices), you probably cannot trust the device\u2019s hardware clock at all.\nSome users deliberately set their hardware clock to an incorrect date and time,\nfor example to circumvent timing limitations in games. As a result, the clock\nmight be set to a time wildly in the past or the future.\nIt is possible to achieve very good clock accuracy if you care about it sufficiently to\ninvest significant resources. For example, the MiFID II draft European regulation for\nfinancial institutions requires all high-frequency trading funds to synchronize their\nclocks to within 100 microseconds of UTC, in order to help debug market anomalies\nsuch as \u201cflash crashes\u201d and to help detect market manipulation [51]."}
{"313": "wrong. If your NTP daemon is misconfigured, or a firewall is blocking NTP traffic,\nthe clock error due to drift can quickly become large.\nRelying on Synchronized Clocks\nThe problem with clocks is that while they seem simple and easy to use, they have a\nsurprising number of pitfalls: a day may not have exactly 86,400 seconds, time-of-day\nclocks may move backward in time, and the time on one node may be quite different\nfrom the time on another node.\nEarlier in this chapter we discussed networks dropping and arbitrarily delaying pack\u2010\nets. Even though networks are well behaved most of the time, software must be\ndesigned on the assumption that the network will occasionally be faulty, and the soft\u2010\nware must handle such faults gracefully. The same is true with clocks: although they\nwork quite well most of the time, robust software needs to be prepared to deal with\nincorrect clocks.\nPart of the problem is that incorrect clocks easily go unnoticed. If a machine\u2019s CPU is\ndefective or its network is misconfigured, it most likely won\u2019t work at all, so it will\nquickly be noticed and fixed. On the other hand, if its quartz clock is defective or its\nNTP client is misconfigured, most things will seem to work fine, even though its\nclock gradually drifts further and further away from reality. If some piece of software\nis relying on an accurately synchronized clock, the result is more likely to be silent\nand subtle data loss than a dramatic crash [53, 54].\nThus, if you use software that requires synchronized clocks, it is essential that you\nalso carefully monitor the clock offsets between all the machines. Any node whose\nclock drifts too far from the others should be declared dead and removed from the\ncluster. Such monitoring ensures that you notice the broken clocks before they can\ncause too much damage.\nTimestamps for ordering events\nLet\u2019s consider one particular situation in which it is tempting, but dangerous, to rely\non clocks: ordering of events across multiple nodes. For example, if two clients write\nto a distributed database, who got there first? Which write is the more recent one?\nFigure 8-3 illustrates a dangerous use of time-of-day clocks in a database with multi-\nleader replication (the example is similar to Figure 5-9). Client A writes x = 1 on node\n1; the write is replicated to node 3; client B increments x on node 3 (we now have\nx = 2); and finally, both writes are replicated to node 2."}
{"314": "Figure 8-3. The write by client B is causally later than the write by client A, but B\u2019s\nwrite has an earlier timestamp.\nIn Figure 8-3, when a write is replicated to other nodes, it is tagged with a timestamp\naccording to the time-of-day clock on the node where the write originated. The clock\nsynchronization is very good in this example: the skew between node 1 and node 3 is\nless than 3 ms, which is probably better than you can expect in practice.\nNevertheless, the timestamps in Figure 8-3 fail to order the events correctly: the write\nx = 1 has a timestamp of 42.004 seconds, but the write x = 2 has a timestamp of\n42.003 seconds, even though x = 2 occurred unambiguously later. When node 2\nreceives these two events, it will incorrectly conclude that x = 1 is the more recent\nvalue and drop the write x = 2. In effect, client B\u2019s increment operation will be lost.\nThis conflict resolution strategy is called last write wins (LWW), and it is widely used\nin both multi-leader replication and leaderless databases such as Cassandra [53] and\nRiak [54] (see \u201cLast write wins (discarding concurrent writes)\u201d on page 186). Some\nimplementations generate timestamps on the client rather than the server, but this\ndoesn\u2019t change the fundamental problems with LWW:\n\u2022 Database writes can mysteriously disappear: a node with a lagging clock is unable\nto overwrite values previously written by a node with a fast clock until the clock\nskew between the nodes has elapsed [54, 55]. This scenario can cause arbitrary\namounts of data to be silently dropped without any error being reported to the\napplication.\n\u2022 LWW cannot distinguish between writes that occurred sequentially in quick suc\u2010"}
{"315": "needed in order to prevent violations of causality (see \u201cDetecting Concurrent\nWrites\u201d on page 184).\n\u2022 It is possible for two nodes to independently generate writes with the same time\u2010\nstamp, especially when the clock only has millisecond resolution. An additional\ntiebreaker value (which can simply be a large random number) is required to\nresolve such conflicts, but this approach can also lead to violations of causality\n[53].\nThus, even though it is tempting to resolve conflicts by keeping the most \u201crecent\u201d\nvalue and discarding others, it\u2019s important to be aware that the definition of \u201crecent\u201d\ndepends on a local time-of-day clock, which may well be incorrect. Even with tightly\nNTP-synchronized clocks, you could send a packet at timestamp 100 ms (according\nto the sender\u2019s clock) and have it arrive at timestamp 99 ms (according to the recipi\u2010\nent\u2019s clock)\u2014so it appears as though the packet arrived before it was sent, which is\nimpossible.\nCould NTP synchronization be made accurate enough that such incorrect orderings\ncannot occur? Probably not, because NTP\u2019s synchronization accuracy is itself limited\nby the network round-trip time, in addition to other sources of error such as quartz\ndrift. For correct ordering, you would need the clock source to be significantly more\naccurate than the thing you are measuring (namely network delay).\nSo-called logical clocks [56, 57], which are based on incrementing counters rather\nthan an oscillating quartz crystal, are a safer alternative for ordering events (see\n\u201cDetecting Concurrent Writes\u201d on page 184). Logical clocks do not measure the time\nof day or the number of seconds elapsed, only the relative ordering of events\n(whether one event happened before or after another). In contrast, time-of-day and\nmonotonic clocks, which measure actual elapsed time, are also known as physical\nclocks. We\u2019ll look at ordering a bit more in \u201cOrdering Guarantees\u201d on page 339.\nClock readings have a confidence interval\nYou may be able to read a machine\u2019s time-of-day clock with microsecond or even\nnanosecond resolution. But even if you can get such a fine-grained measurement,\nthat doesn\u2019t mean the value is actually accurate to such precision. In fact, it most\nlikely is not\u2014as mentioned previously, the drift in an imprecise quartz clock can\neasily be several milliseconds, even if you synchronize with an NTP server on the\nlocal network every minute. With an NTP server on the public internet, the best pos\u2010\nsible accuracy is probably to the tens of milliseconds, and the error may easily spike\nto over 100 ms when there is network congestion [57]."}
{"316": "doesn\u2019t know any more precisely than that [58]. If we only know the time +/\u2013 100 ms,\nthe microsecond digits in the timestamp are essentially meaningless.\nThe uncertainty bound can be calculated based on your time source. If you have a\nGPS receiver or atomic (caesium) clock directly attached to your computer, the\nexpected error range is reported by the manufacturer. If you\u2019re getting the time from\na server, the uncertainty is based on the expected quartz drift since your last sync\nwith the server, plus the NTP server\u2019s uncertainty, plus the network round-trip time\nto the server (to a first approximation, and assuming you trust the server).\nUnfortunately, most systems don\u2019t expose this uncertainty: for example, when you\ncall clock_gettime(), the return value doesn\u2019t tell you the expected error of the\ntimestamp, so you don\u2019t know if its confidence interval is five milliseconds or five\nyears.\nAn interesting exception is Google\u2019s TrueTime API in Spanner [41], which explicitly\nreports the confidence interval on the local clock. When you ask it for the current\ntime, you get back two values: [earliest, latest], which are the earliest possible\nand the latest possible timestamp. Based on its uncertainty calculations, the clock\nknows that the actual current time is somewhere within that interval. The width of\nthe interval depends, among other things, on how long it has been since the local\nquartz clock was last synchronized with a more accurate clock source.\nSynchronized clocks for global snapshots\nIn \u201cSnapshot Isolation and Repeatable Read\u201d on page 237 we discussed snapshot iso\u2010\nlation, which is a very useful feature in databases that need to support both small, fast\nread-write transactions and large, long-running read-only transactions (e.g., for\nbackups or analytics). It allows read-only transactions to see the database in a consis\u2010\ntent state at a particular point in time, without locking and interfering with read-\nwrite transactions.\nThe most common implementation of snapshot isolation requires a monotonically\nincreasing transaction ID. If a write happened later than the snapshot (i.e., the write\nhas a greater transaction ID than the snapshot), that write is invisible to the snapshot\ntransaction. On a single-node database, a simple counter is sufficient for generating\ntransaction IDs.\nHowever, when a database is distributed across many machines, potentially in multi\u2010\nple datacenters, a global, monotonically increasing transaction ID (across all parti\u2010\ntions) is difficult to generate, because it requires coordination. The transaction ID\nmust reflect causality: if transaction B reads a value that was written by transaction A,"}
{"317": "be consistent. With lots of small, rapid transactions, creating transaction IDs in a dis\u2010\ntributed system becomes an untenable bottleneck.vi\nCan we use the timestamps from synchronized time-of-day clocks as transaction IDs?\nIf we could get the synchronization good enough, they would have the right proper\u2010\nties: later transactions have a higher timestamp. The problem, of course, is the uncer\u2010\ntainty about clock accuracy.\nSpanner implements snapshot isolation across datacenters in this way [59, 60]. It uses\nthe clock\u2019s confidence interval as reported by the TrueTime API, and is based on the\nfollowing observation: if you have two confidence intervals, each consisting of an ear\u2010\nliest and latest possible timestamp (A = [A , A ] and B = [B , B ]), and\nearliest latest earliest latest\nthose two intervals do not overlap (i.e., A < A < B < B ), then B defi\u2010\nearliest latest earliest latest\nnitely happened after A\u2014there can be no doubt. Only if the intervals overlap are we\nunsure in which order A and B happened.\nIn order to ensure that transaction timestamps reflect causality, Spanner deliberately\nwaits for the length of the confidence interval before committing a read-write trans\u2010\naction. By doing so, it ensures that any transaction that may read the data is at a suffi\u2010\nciently later time, so their confidence intervals do not overlap. In order to keep the\nwait time as short as possible, Spanner needs to keep the clock uncertainty as small as\npossible; for this purpose, Google deploys a GPS receiver or atomic clock in each\ndatacenter, allowing clocks to be synchronized to within about 7 ms [41].\nUsing clock synchronization for distributed transaction semantics is an area of active\nresearch [57, 61, 62]. These ideas are interesting, but they have not yet been imple\u2010\nmented in mainstream databases outside of Google.\nProcess Pauses\nLet\u2019s consider another example of dangerous clock use in a distributed system. Say\nyou have a database with a single leader per partition. Only the leader is allowed to\naccept writes. How does a node know that it is still leader (that it hasn\u2019t been declared\ndead by the others), and that it may safely accept writes?\nOne option is for the leader to obtain a lease from the other nodes, which is similar to\na lock with a timeout [63]. Only one node can hold the lease at any one time\u2014thus,\nwhen a node obtains a lease, it knows that it is the leader for some amount of time,\nuntil the lease expires. In order to remain leader, the node must periodically renew"}
{"318": "the lease before it expires. If the node fails, it stops renewing the lease, so another\nnode can take over when it expires.\nYou can imagine the request-handling loop looking something like this:\nwhile (true) {\nrequest = getIncomingRequest();\n// Ensure that the lease always has at least 10 seconds remaining\nif (lease.expiryTimeMillis - System.currentTimeMillis() < 10000) {\nlease = lease.renew();\n}\nif (lease.isValid()) {\nprocess(request);\n}\n}\nWhat\u2019s wrong with this code? Firstly, it\u2019s relying on synchronized clocks: the expiry\ntime on the lease is set by a different machine (where the expiry may be calculated as\nthe current time plus 30 seconds, for example), and it\u2019s being compared to the local\nsystem clock. If the clocks are out of sync by more than a few seconds, this code will\nstart doing strange things.\nSecondly, even if we change the protocol to only use the local monotonic clock, there\nis another problem: the code assumes that very little time passes between the point\nthat it checks the time (System.currentTimeMillis()) and the time when the\nrequest is processed (process(request)). Normally this code runs very quickly, so\nthe 10 second buffer is more than enough to ensure that the lease doesn\u2019t expire in\nthe middle of processing a request.\nHowever, what if there is an unexpected pause in the execution of the program? For\nexample, imagine the thread stops for 15 seconds around the line lease.isValid()\nbefore finally continuing. In that case, it\u2019s likely that the lease will have expired by the\ntime the request is processed, and another node has already taken over as leader.\nHowever, there is nothing to tell this thread that it was paused for so long, so this\ncode won\u2019t notice that the lease has expired until the next iteration of the loop\u2014by\nwhich time it may have already done something unsafe by processing the request.\nIs it crazy to assume that a thread might be paused for so long? Unfortunately not.\nThere are various reasons why this could happen:\n\u2022 Many programming language runtimes (such as the Java Virtual Machine) have\na garbage collector (GC) that occasionally needs to stop all running threads."}
{"319": "reduced by changing allocation patterns or tuning GC settings [66], we must\nassume the worst if we want to offer robust guarantees.\n\u2022 In virtualized environments, a virtual machine can be suspended (pausing the\nexecution of all processes and saving the contents of memory to disk) and\nresumed (restoring the contents of memory and continuing execution). This\npause can occur at any time in a process\u2019s execution and can last for an arbitrary\nlength of time. This feature is sometimes used for live migration of virtual\nmachines from one host to another without a reboot, in which case the length of\nthe pause depends on the rate at which processes are writing to memory [67].\n\u2022 On end-user devices such as laptops, execution may also be suspended and\nresumed arbitrarily, e.g., when the user closes the lid of their laptop.\n\u2022 When the operating system context-switches to another thread, or when the\nhypervisor switches to a different virtual machine (when running in a virtual\nmachine), the currently running thread can be paused at any arbitrary point in\nthe code. In the case of a virtual machine, the CPU time spent in other virtual\nmachines is known as steal time. If the machine is under heavy load\u2014i.e., if there\nis a long queue of threads waiting to run\u2014it may take some time before the\npaused thread gets to run again.\n\u2022 If the application performs synchronous disk access, a thread may be paused\nwaiting for a slow disk I/O operation to complete [68]. In many languages, disk\naccess can happen surprisingly, even if the code doesn\u2019t explicitly mention file\naccess\u2014for example, the Java classloader lazily loads class files when they are first\nused, which could happen at any time in the program execution. I/O pauses and\nGC pauses may even conspire to combine their delays [69]. If the disk is actually\na network filesystem or network block device (such as Amazon\u2019s EBS), the I/O\nlatency is further subject to the variability of network delays [29].\n\u2022 If the operating system is configured to allow swapping to disk (paging), a simple\nmemory access may result in a page fault that requires a page from disk to be\nloaded into memory. The thread is paused while this slow I/O operation takes\nplace. If memory pressure is high, this may in turn require a different page to be\nswapped out to disk. In extreme circumstances, the operating system may spend\nmost of its time swapping pages in and out of memory and getting little actual\nwork done (this is known as thrashing). To avoid this problem, paging is often\ndisabled on server machines (if you would rather kill a process to free up mem\u2010\nory than risk thrashing).\n\u2022 A Unix process can be paused by sending it the SIGSTOP signal, for example by\npressing Ctrl-Z in a shell. This signal immediately stops the process from getting"}
{"320": "All of these occurrences can preempt the running thread at any point and resume it at\nsome later time, without the thread even noticing. The problem is similar to making\nmulti-threaded code on a single machine thread-safe: you can\u2019t assume anything\nabout timing, because arbitrary context switches and parallelism may occur.\nWhen writing multi-threaded code on a single machine, we have fairly good tools for\nmaking it thread-safe: mutexes, semaphores, atomic counters, lock-free data struc\u2010\ntures, blocking queues, and so on. Unfortunately, these tools don\u2019t directly translate\nto distributed systems, because a distributed system has no shared memory\u2014only\nmessages sent over an unreliable network.\nA node in a distributed system must assume that its execution can be paused for a\nsignificant length of time at any point, even in the middle of a function. During the\npause, the rest of the world keeps moving and may even declare the paused node\ndead because it\u2019s not responding. Eventually, the paused node may continue running,\nwithout even noticing that it was asleep until it checks its clock sometime later.\nResponse time guarantees\nIn many programming languages and operating systems, threads and processes may\npause for an unbounded amount of time, as discussed. Those reasons for pausing can\nbe eliminated if you try hard enough.\nSome software runs in environments where a failure to respond within a specified\ntime can cause serious damage: computers that control aircraft, rockets, robots, cars,\nand other physical objects must respond quickly and predictably to their sensor\ninputs. In these systems, there is a specified deadline by which the software must\nrespond; if it doesn\u2019t meet the deadline, that may cause a failure of the entire system.\nThese are so-called hard real-time systems.\nIs real-time really real?\nIn embedded systems, real-time means that a system is carefully\ndesigned and tested to meet specified timing guarantees in all cir\u2010\ncumstances. This meaning is in contrast to the more vague use of\nthe term real-time on the web, where it describes servers pushing\ndata to clients and stream processing without hard response time\nconstraints (see Chapter 11).\nFor example, if your car\u2019s onboard sensors detect that you are currently experiencing\na crash, you wouldn\u2019t want the release of the airbag to be delayed due to an inoppor\u2010\ntune GC pause in the airbag release system."}
{"321": "functions must document their worst-case execution times; dynamic memory alloca\u2010\ntion may be restricted or disallowed entirely (real-time garbage collectors exist, but\nthe application must still ensure that it doesn\u2019t give the GC too much work to do);\nand an enormous amount of testing and measurement must be done to ensure that\nguarantees are being met.\nAll of this requires a large amount of additional work and severely restricts the range\nof programming languages, libraries, and tools that can be used (since most lan\u2010\nguages and tools do not provide real-time guarantees). For these reasons, developing\nreal-time systems is very expensive, and they are most commonly used in safety-\ncritical embedded devices. Moreover, \u201creal-time\u201d is not the same as \u201chigh-\nperformance\u201d\u2014in fact, real-time systems may have lower throughput, since they\nhave to prioritize timely responses above all else (see also \u201cLatency and Resource Uti\u2010\nlization\u201d on page 286).\nFor most server-side data processing systems, real-time guarantees are simply not\neconomical or appropriate. Consequently, these systems must suffer the pauses and\nclock instability that come from operating in a non-real-time environment.\nLimiting the impact of garbage collection\nThe negative effects of process pauses can be mitigated without resorting to expen\u2010\nsive real-time scheduling guarantees. Language runtimes have some flexibility\naround when they schedule garbage collections, because they can track the rate of\nobject allocation and the remaining free memory over time.\nAn emerging idea is to treat GC pauses like brief planned outages of a node, and to\nlet other nodes handle requests from clients while one node is collecting its garbage.\nIf the runtime can warn the application that a node soon requires a GC pause, the\napplication can stop sending new requests to that node, wait for it to finish process\u2010\ning outstanding requests, and then perform the GC while no requests are in progress.\nThis trick hides GC pauses from clients and reduces the high percentiles of response\ntime [70, 71]. Some latency-sensitive financial trading systems [72] use this approach.\nA variant of this idea is to use the garbage collector only for short-lived objects\n(which are fast to collect) and to restart processes periodically, before they accumu\u2010\nlate enough long-lived objects to require a full GC of long-lived objects [65, 73]. One\nnode can be restarted at a time, and traffic can be shifted away from the node before\nthe planned restart, like in a rolling upgrade (see Chapter 4).\nThese measures cannot fully prevent garbage collection pauses, but they can usefully\nreduce their impact on the application."}
{"322": "Knowledge, Truth, and Lies\nSo far in this chapter we have explored the ways in which distributed systems are dif\u2010\nferent from programs running on a single computer: there is no shared memory, only\nmessage passing via an unreliable network with variable delays, and the systems may\nsuffer from partial failures, unreliable clocks, and processing pauses.\nThe consequences of these issues are profoundly disorienting if you\u2019re not used to\ndistributed systems. A node in the network cannot know anything for sure\u2014it can\nonly make guesses based on the messages it receives (or doesn\u2019t receive) via the net\u2010\nwork. A node can only find out what state another node is in (what data it has stored,\nwhether it is correctly functioning, etc.) by exchanging messages with it. If a remote\nnode doesn\u2019t respond, there is no way of knowing what state it is in, because prob\u2010\nlems in the network cannot reliably be distinguished from problems at a node.\nDiscussions of these systems border on the philosophical: What do we know to be\ntrue or false in our system? How sure can we be of that knowledge, if the mechanisms\nfor perception and measurement are unreliable? Should software systems obey the\nlaws that we expect of the physical world, such as cause and effect?\nFortunately, we don\u2019t need to go as far as figuring out the meaning of life. In a dis\u2010\ntributed system, we can state the assumptions we are making about the behavior (the\nsystem model) and design the actual system in such a way that it meets those assump\u2010\ntions. Algorithms can be proved to function correctly within a certain system model.\nThis means that reliable behavior is achievable, even if the underlying system model\nprovides very few guarantees.\nHowever, although it is possible to make software well behaved in an unreliable sys\u2010\ntem model, it is not straightforward to do so. In the rest of this chapter we will further\nexplore the notions of knowledge and truth in distributed systems, which will help us\nthink about the kinds of assumptions we can make and the guarantees we may want\nto provide. In Chapter 9 we will proceed to look at some examples of distributed sys\u2010\ntems, algorithms that provide particular guarantees under particular assumptions.\nThe Truth Is Defined by the Majority\nImagine a network with an asymmetric fault: a node is able to receive all messages\nsent to it, but any outgoing messages from that node are dropped or delayed [19].\nEven though that node is working perfectly well, and is receiving requests from other\nnodes, the other nodes cannot hear its responses. After some timeout, the other\nnodes declare it dead, because they haven\u2019t heard from the node. The situation"}
{"323": "In a slightly less nightmarish scenario, the semi-disconnected node may notice that\nthe messages it is sending are not being acknowledged by other nodes, and so realize\nthat there must be a fault in the network. Nevertheless, the node is wrongly declared\ndead by the other nodes, and the semi-disconnected node cannot do anything about\nit.\nAs a third scenario, imagine a node that experiences a long stop-the-world garbage\ncollection pause. All of the node\u2019s threads are preempted by the GC and paused for\none minute, and consequently, no requests are processed and no responses are sent.\nThe other nodes wait, retry, grow impatient, and eventually declare the node dead\nand load it onto the hearse. Finally, the GC finishes and the node\u2019s threads continue\nas if nothing had happened. The other nodes are surprised as the supposedly dead\nnode suddenly raises its head out of the coffin, in full health, and starts cheerfully\nchatting with bystanders. At first, the GCing node doesn\u2019t even realize that an entire\nminute has passed and that it was declared dead\u2014from its perspective, hardly any\ntime has passed since it was last talking to the other nodes.\nThe moral of these stories is that a node cannot necessarily trust its own judgment of\na situation. A distributed system cannot exclusively rely on a single node, because a\nnode may fail at any time, potentially leaving the system stuck and unable to recover.\nInstead, many distributed algorithms rely on a quorum, that is, voting among the\nnodes (see \u201cQuorums for reading and writing\u201d on page 179): decisions require some\nminimum number of votes from several nodes in order to reduce the dependence on\nany one particular node.\nThat includes decisions about declaring nodes dead. If a quorum of nodes declares\nanother node dead, then it must be considered dead, even if that node still very much\nfeels alive. The individual node must abide by the quorum decision and step down.\nMost commonly, the quorum is an absolute majority of more than half the nodes\n(although other kinds of quorums are possible). A majority quorum allows the sys\u2010\ntem to continue working if individual nodes have failed (with three nodes, one failure\ncan be tolerated; with five nodes, two failures can be tolerated). However, it is still\nsafe, because there can only be only one majority in the system\u2014there cannot be two\nmajorities with conflicting decisions at the same time. We will discuss the use of quo\u2010\nrums in more detail when we get to consensus algorithms in Chapter 9.\nThe leader and the lock\nFrequently, a system requires there to be only one of some thing. For example:\n\u2022 Only one node is allowed to be the leader for a database partition, to avoid split"}
{"324": "\u2022 Only one user is allowed to register a particular username, because a username\nmust uniquely identify a user.\nImplementing this in a distributed system requires care: even if a node believes that it\nis \u201cthe chosen one\u201d (the leader of the partition, the holder of the lock, the request\nhandler of the user who successfully grabbed the username), that doesn\u2019t necessarily\nmean a quorum of nodes agrees! A node may have formerly been the leader, but if\nthe other nodes declared it dead in the meantime (e.g., due to a network interruption\nor GC pause), it may have been demoted and another leader may have already been\nelected.\nIf a node continues acting as the chosen one, even though the majority of nodes have\ndeclared it dead, it could cause problems in a system that is not carefully designed.\nSuch a node could send messages to other nodes in its self-appointed capacity, and if\nother nodes believe it, the system as a whole may do something incorrect.\nFor example, Figure 8-4 shows a data corruption bug due to an incorrect implemen\u2010\ntation of locking. (The bug is not theoretical: HBase used to have this problem [74,\n75].) Say you want to ensure that a file in a storage service can only be accessed by\none client at a time, because if multiple clients tried to write to it, the file would\nbecome corrupted. You try to implement this by requiring a client to obtain a lease\nfrom a lock service before accessing the file.\nFigure 8-4. Incorrect implementation of a distributed lock: client 1 believes that it still\nhas a valid lease, even though it has expired, and thus corrupts a file in storage.\nThe problem is an example of what we discussed in \u201cProcess Pauses\u201d on page 295: if\nthe client holding the lease is paused for too long, its lease expires. Another client can\nobtain a lease for the same file, and start writing to the file. When the paused client"}
{"325": "Fencing tokens\nWhen using a lock or lease to protect access to some resource, such as the file storage\nin Figure 8-4, we need to ensure that a node that is under a false belief of being \u201cthe\nchosen one\u201d cannot disrupt the rest of the system. A fairly simple technique that ach\u2010\nieves this goal is called fencing, and is illustrated in Figure 8-5.\nFigure 8-5. Making access to storage safe by allowing writes only in the order of increas\u2010\ning fencing tokens.\nLet\u2019s assume that every time the lock server grants a lock or lease, it also returns a\nfencing token, which is a number that increases every time a lock is granted (e.g.,\nincremented by the lock service). We can then require that every time a client sends a\nwrite request to the storage service, it must include its current fencing token.\nIn Figure 8-5, client 1 acquires the lease with a token of 33, but then it goes into a\nlong pause and the lease expires. Client 2 acquires the lease with a token of 34 (the\nnumber always increases) and then sends its write request to the storage service,\nincluding the token of 34. Later, client 1 comes back to life and sends its write to the\nstorage service, including its token value 33. However, the storage server remembers\nthat it has already processed a write with a higher token number (34), and so it rejects\nthe request with token 33.\nIf ZooKeeper is used as lock service, the transaction ID zxid or the node version\ncversion can be used as fencing token. Since they are guaranteed to be monotoni\u2010\ncally increasing, they have the required properties [74].\nNote that this mechanism requires the resource itself to take an active role in check\u2010\ning tokens by rejecting any writes with an older token than one that has already been\nprocessed\u2014it is not sufficient to rely on clients checking their lock status themselves."}
{"326": "Checking a token on the server side may seem like a downside, but it is arguably a\ngood thing: it is unwise for a service to assume that its clients will always be well\nbehaved, because the clients are often run by people whose priorities are very differ\u2010\nent from the priorities of the people running the service [76]. Thus, it is a good idea\nfor any service to protect itself from accidentally abusive clients.\nByzantine Faults\nFencing tokens can detect and block a node that is inadvertently acting in error (e.g.,\nbecause it hasn\u2019t yet found out that its lease has expired). However, if the node delib\u2010\nerately wanted to subvert the system\u2019s guarantees, it could easily do so by sending\nmessages with a fake fencing token.\nIn this book we assume that nodes are unreliable but honest: they may be slow or\nnever respond (due to a fault), and their state may be outdated (due to a GC pause or\nnetwork delays), but we assume that if a node does respond, it is telling the \u201ctruth\u201d: to\nthe best of its knowledge, it is playing by the rules of the protocol.\nDistributed systems problems become much harder if there is a risk that nodes may\n\u201clie\u201d (send arbitrary faulty or corrupted responses)\u2014for example, if a node may claim\nto have received a particular message when in fact it didn\u2019t. Such behavior is known\nas a Byzantine fault, and the problem of reaching consensus in this untrusting envi\u2010\nronment is known as the Byzantine Generals Problem [77].\nThe Byzantine Generals Problem\nThe Byzantine Generals Problem is a generalization of the so-called Two Generals\nProblem [78], which imagines a situation in which two army generals need to agree\non a battle plan. As they have set up camp on two different sites, they can only com\u2010\nmunicate by messenger, and the messengers sometimes get delayed or lost (like pack\u2010\nets in a network). We will discuss this problem of consensus in Chapter 9.\nIn the Byzantine version of the problem, there are n generals who need to agree, and\ntheir endeavor is hampered by the fact that there are some traitors in their midst.\nMost of the generals are loyal, and thus send truthful messages, but the traitors may\ntry to deceive and confuse the others by sending fake or untrue messages (while try\u2010\ning to remain undiscovered). It is not known in advance who the traitors are.\nByzantium was an ancient Greek city that later became Constantinople, in the place\nwhich is now Istanbul in Turkey. There isn\u2019t any historic evidence that the generals of\nByzantium were any more prone to intrigue and conspiracy than those elsewhere.\nRather, the name is derived from Byzantine in the sense of excessively complicated,"}
{"327": "A system is Byzantine fault-tolerant if it continues to operate correctly even if some\nof the nodes are malfunctioning and not obeying the protocol, or if malicious attack\u2010\ners are interfering with the network. This concern is relevant in certain specific cir\u2010\ncumstances. For example:\n\u2022 In aerospace environments, the data in a computer\u2019s memory or CPU register\ncould become corrupted by radiation, leading it to respond to other nodes in\narbitrarily unpredictable ways. Since a system failure would be very expensive\n(e.g., an aircraft crashing and killing everyone on board, or a rocket colliding\nwith the International Space Station), flight control systems must tolerate Byzan\u2010\ntine faults [81, 82].\n\u2022 In a system with multiple participating organizations, some participants may\nattempt to cheat or defraud others. In such circumstances, it is not safe for a\nnode to simply trust another node\u2019s messages, since they may be sent with mali\u2010\ncious intent. For example, peer-to-peer networks like Bitcoin and other block\u2010\nchains can be considered to be a way of getting mutually untrusting parties to\nagree whether a transaction happened or not, without relying on a central\nauthority [83].\nHowever, in the kinds of systems we discuss in this book, we can usually safely\nassume that there are no Byzantine faults. In your datacenter, all the nodes are con\u2010\ntrolled by your organization (so they can hopefully be trusted) and radiation levels\nare low enough that memory corruption is not a major problem. Protocols for mak\u2010\ning systems Byzantine fault-tolerant are quite complicated [84], and fault-tolerant\nembedded systems rely on support from the hardware level [81]. In most server-side\ndata systems, the cost of deploying Byzantine fault-tolerant solutions makes them\nimpractical.\nWeb applications do need to expect arbitrary and malicious behavior of clients that\nare under end-user control, such as web browsers. This is why input validation, sani\u2010\ntization, and output escaping are so important: to prevent SQL injection and cross-\nsite scripting, for example. However, we typically don\u2019t use Byzantine fault-tolerant\nprotocols here, but simply make the server the authority on deciding what client\nbehavior is and isn\u2019t allowed. In peer-to-peer networks, where there is no such cen\u2010\ntral authority, Byzantine fault tolerance is more relevant.\nA bug in the software could be regarded as a Byzantine fault, but if you deploy the\nsame software to all nodes, then a Byzantine fault-tolerant algorithm cannot save you.\nMost Byzantine fault-tolerant algorithms require a supermajority of more than two-\nthirds of the nodes to be functioning correctly (i.e., if you have four nodes, at most"}
{"328": "Similarly, it would be appealing if a protocol could protect us from vulnerabilities,\nsecurity compromises, and malicious attacks. Unfortunately, this is not realistic\neither: in most systems, if an attacker can compromise one node, they can probably\ncompromise all of them, because they are probably running the same software. Thus,\ntraditional mechanisms (authentication, access control, encryption, firewalls, and so\non) continue to be the main protection against attackers.\nWeak forms of lying\nAlthough we assume that nodes are generally honest, it can be worth adding mecha\u2010\nnisms to software that guard against weak forms of \u201clying\u201d\u2014for example, invalid\nmessages due to hardware issues, software bugs, and misconfiguration. Such protec\u2010\ntion mechanisms are not full-blown Byzantine fault tolerance, as they would not\nwithstand a determined adversary, but they are nevertheless simple and pragmatic\nsteps toward better reliability. For example:\n\u2022 Network packets do sometimes get corrupted due to hardware issues or bugs in\noperating systems, drivers, routers, etc. Usually, corrupted packets are caught by\nthe checksums built into TCP and UDP, but sometimes they evade detection [85,\n86, 87]. Simple measures are usually sufficient protection against such corrup\u2010\ntion, such as checksums in the application-level protocol.\n\u2022 A publicly accessible application must carefully sanitize any inputs from users,\nfor example checking that a value is within a reasonable range and limiting the\nsize of strings to prevent denial of service through large memory allocations. An\ninternal service behind a firewall may be able to get away with less strict checks\non inputs, but some basic sanity-checking of values (e.g., in protocol parsing\n[85]) is a good idea.\n\u2022 NTP clients can be configured with multiple server addresses. When synchroniz\u2010\ning, the client contacts all of them, estimates their errors, and checks that a\nmajority of servers agree on some time range. As long as most of the servers are\nokay, a misconfigured NTP server that is reporting an incorrect time is detected\nas an outlier and is excluded from synchronization [37]. The use of multiple\nservers makes NTP more robust than if it only uses a single server.\nSystem Model and Reality\nMany algorithms have been designed to solve distributed systems problems\u2014for\nexample, we will examine solutions for the consensus problem in Chapter 9. In order\nto be useful, these algorithms need to tolerate the various faults of distributed systems"}
{"329": "turn requires that we somehow formalize the kinds of faults that we expect to happen\nin a system. We do this by defining a system model, which is an abstraction that\ndescribes what things an algorithm may assume.\nWith regard to timing assumptions, three system models are in common use:\nSynchronous model\nThe synchronous model assumes bounded network delay, bounded process pau\u2010\nses, and bounded clock error. This does not imply exactly synchronized clocks or\nzero network delay; it just means you know that network delay, pauses, and clock\ndrift will never exceed some fixed upper bound [88]. The synchronous model is\nnot a realistic model of most practical systems, because (as discussed in this\nchapter) unbounded delays and pauses do occur.\nPartially synchronous model\nPartial synchrony means that a system behaves like a synchronous system most of\nthe time, but it sometimes exceeds the bounds for network delay, process pauses,\nand clock drift [88]. This is a realistic model of many systems: most of the time,\nnetworks and processes are quite well behaved\u2014otherwise we would never be\nable to get anything done\u2014but we have to reckon with the fact that any timing\nassumptions may be shattered occasionally. When this happens, network delay,\npauses, and clock error may become arbitrarily large.\nAsynchronous model\nIn this model, an algorithm is not allowed to make any timing assumptions\u2014in\nfact, it does not even have a clock (so it cannot use timeouts). Some algorithms\ncan be designed for the asynchronous model, but it is very restrictive.\nMoreover, besides timing issues, we have to consider node failures. The three most\ncommon system models for nodes are:\nCrash-stop faults\nIn the crash-stop model, an algorithm may assume that a node can fail in only\none way, namely by crashing. This means that the node may suddenly stop\nresponding at any moment, and thereafter that node is gone forever\u2014it never\ncomes back.\nCrash-recovery faults\nWe assume that nodes may crash at any moment, and perhaps start responding\nagain after some unknown time. In the crash-recovery model, nodes are assumed\nto have stable storage (i.e., nonvolatile disk storage) that is preserved across\ncrashes, while the in-memory state is assumed to be lost."}
{"330": "For modeling real systems, the partially synchronous model with crash-recovery\nfaults is generally the most useful model. But how do distributed algorithms cope\nwith that model?\nCorrectness of an algorithm\nTo define what it means for an algorithm to be correct, we can describe its properties.\nFor example, the output of a sorting algorithm has the property that for any two dis\u2010\ntinct elements of the output list, the element further to the left is smaller than the ele\u2010\nment further to the right. That is simply a formal way of defining what it means for a\nlist to be sorted.\nSimilarly, we can write down the properties we want of a distributed algorithm to\ndefine what it means to be correct. For example, if we are generating fencing tokens\nfor a lock (see \u201cFencing tokens\u201d on page 303), we may require the algorithm to have\nthe following properties:\nUniqueness\nNo two requests for a fencing token return the same value.\nMonotonic sequence\nIf request x returned token t, and request y returned token t, and x completed\nx y\nbefore y began, then t < t.\nx y\nAvailability\nA node that requests a fencing token and does not crash eventually receives a\nresponse.\nAn algorithm is correct in some system model if it always satisfies its properties in all\nsituations that we assume may occur in that system model. But how does this make\nsense? If all nodes crash, or all network delays suddenly become infinitely long, then\nno algorithm will be able to get anything done.\nSafety and liveness\nTo clarify the situation, it is worth distinguishing between two different kinds of\nproperties: safety and liveness properties. In the example just given, uniqueness and\nmonotonic sequence are safety properties, but availability is a liveness property.\nWhat distinguishes the two kinds of properties? A giveaway is that liveness properties\noften include the word \u201ceventually\u201d in their definition. (And yes, you guessed it\u2014\neventual consistency is a liveness property [89].)\nSafety is often informally defined as nothing bad happens, and liveness as something"}
{"331": "\u2022 If a safety property is violated, we can point at a particular point in time at which\nit was broken (for example, if the uniqueness property was violated, we can iden\u2010\ntify the particular operation in which a duplicate fencing token was returned).\nAfter a safety property has been violated, the violation cannot be undone\u2014the\ndamage is already done.\n\u2022 A liveness property works the other way round: it may not hold at some point in\ntime (for example, a node may have sent a request but not yet received a\nresponse), but there is always hope that it may be satisfied in the future (namely\nby receiving a response).\nAn advantage of distinguishing between safety and liveness properties is that it helps\nus deal with difficult system models. For distributed algorithms, it is common to\nrequire that safety properties always hold, in all possible situations of a system model\n[88]. That is, even if all nodes crash, or the entire network fails, the algorithm must\nnevertheless ensure that it does not return a wrong result (i.e., that the safety proper\u2010\nties remain satisfied).\nHowever, with liveness properties we are allowed to make caveats: for example, we\ncould say that a request needs to receive a response only if a majority of nodes have\nnot crashed, and only if the network eventually recovers from an outage. The defini\u2010\ntion of the partially synchronous model requires that eventually the system returns to\na synchronous state\u2014that is, any period of network interruption lasts only for a finite\nduration and is then repaired.\nMapping system models to the real world\nSafety and liveness properties and system models are very useful for reasoning about\nthe correctness of a distributed algorithm. However, when implementing an algo\u2010\nrithm in practice, the messy facts of reality come back to bite you again, and it\nbecomes clear that the system model is a simplified abstraction of reality.\nFor example, algorithms in the crash-recovery model generally assume that data in\nstable storage survives crashes. However, what happens if the data on disk is corrup\u2010\nted, or the data is wiped out due to hardware error or misconfiguration [91]? What\nhappens if a server has a firmware bug and fails to recognize its hard drives on\nreboot, even though the drives are correctly attached to the server [92]?\nQuorum algorithms (see \u201cQuorums for reading and writing\u201d on page 179) rely on a\nnode remembering the data that it claims to have stored. If a node may suffer from\namnesia and forget previously stored data, that breaks the quorum condition, and\nthus breaks the correctness of the algorithm. Perhaps a new system model is needed,"}
{"332": "The theoretical description of an algorithm can declare that certain things are simply\nassumed not to happen\u2014and in non-Byzantine systems, we do have to make some\nassumptions about faults that can and cannot happen. However, a real implementa\u2010\ntion may still have to include code to handle the case where something happens that\nwas assumed to be impossible, even if that handling boils down to printf(\"Sucks to\nbe you\") and exit(666)\u2014i.e., letting a human operator clean up the mess [93].\n(This is arguably the difference between computer science and software engineering.)\nThat is not to say that theoretical, abstract system models are worthless\u2014quite the\nopposite. They are incredibly helpful for distilling down the complexity of real sys\u2010\ntems to a manageable set of faults that we can reason about, so that we can under\u2010\nstand the problem and try to solve it systematically. We can prove algorithms correct\nby showing that their properties always hold in some system model.\nProving an algorithm correct does not mean its implementation on a real system will\nnecessarily always behave correctly. But it\u2019s a very good first step, because the theo\u2010\nretical analysis can uncover problems in an algorithm that might remain hidden for a\nlong time in a real system, and that only come to bite you when your assumptions\n(e.g., about timing) are defeated due to unusual circumstances. Theoretical analysis\nand empirical testing are equally important.\nSummary\nIn this chapter we have discussed a wide range of problems that can occur in dis\u2010\ntributed systems, including:\n\u2022 Whenever you try to send a packet over the network, it may be lost or arbitrarily\ndelayed. Likewise, the reply may be lost or delayed, so if you don\u2019t get a reply,\nyou have no idea whether the message got through.\n\u2022 A node\u2019s clock may be significantly out of sync with other nodes (despite your\nbest efforts to set up NTP), it may suddenly jump forward or back in time, and\nrelying on it is dangerous because you most likely don\u2019t have a good measure of\nyour clock\u2019s error interval.\n\u2022 A process may pause for a substantial amount of time at any point in its execu\u2010\ntion (perhaps due to a stop-the-world garbage collector), be declared dead by\nother nodes, and then come back to life again without realizing that it was\npaused.\nThe fact that such partial failures can occur is the defining characteristic of dis\u2010\ntributed systems. Whenever software tries to do anything involving other nodes,"}
{"333": "ance of partial failures into software, so that the system as a whole may continue\nfunctioning even when some of its constituent parts are broken.\nTo tolerate faults, the first step is to detect them, but even that is hard. Most systems\ndon\u2019t have an accurate mechanism of detecting whether a node has failed, so most\ndistributed algorithms rely on timeouts to determine whether a remote node is still\navailable. However, timeouts can\u2019t distinguish between network and node failures,\nand variable network delay sometimes causes a node to be falsely suspected of crash\u2010\ning. Moreover, sometimes a node can be in a degraded state: for example, a Gigabit\nnetwork interface could suddenly drop to 1 Kb/s throughput due to a driver bug [94].\nSuch a node that is \u201climping\u201d but not dead can be even more difficult to deal with\nthan a cleanly failed node.\nOnce a fault is detected, making a system tolerate it is not easy either: there is no\nglobal variable, no shared memory, no common knowledge or any other kind of\nshared state between the machines. Nodes can\u2019t even agree on what time it is, let\nalone on anything more profound. The only way information can flow from one\nnode to another is by sending it over the unreliable network. Major decisions cannot\nbe safely made by a single node, so we require protocols that enlist help from other\nnodes and try to get a quorum to agree.\nIf you\u2019re used to writing software in the idealized mathematical perfection of a single\ncomputer, where the same operation always deterministically returns the same result,\nthen moving to the messy physical reality of distributed systems can be a bit of a\nshock. Conversely, distributed systems engineers will often regard a problem as triv\u2010\nial if it can be solved on a single computer [5], and indeed a single computer can do a\nlot nowadays [95]. If you can avoid opening Pandora\u2019s box and simply keep things on\na single machine, it is generally worth doing so.\nHowever, as discussed in the introduction to Part II, scalability is not the only reason\nfor wanting to use a distributed system. Fault tolerance and low latency (by placing\ndata geographically close to users) are equally important goals, and those things can\u2010\nnot be achieved with a single node.\nIn this chapter we also went on some tangents to explore whether the unreliability of\nnetworks, clocks, and processes is an inevitable law of nature. We saw that it isn\u2019t: it\nis possible to give hard real-time response guarantees and bounded delays in net\u2010\nworks, but doing so is very expensive and results in lower utilization of hardware\nresources. Most non-safety-critical systems choose cheap and unreliable over expen\u2010\nsive and reliable.\nWe also touched on supercomputers, which assume reliable components and thus"}
{"334": "theory. (In practice, if a bad configuration change is rolled out to all nodes, that will\nstill bring a distributed system to its knees.)\nThis chapter has been all about problems, and has given us a bleak outlook. In the\nnext chapter we will move on to solutions, and discuss some algorithms that have\nbeen designed to cope with all the problems in distributed systems.\nReferences\n[1] Mark Cavage: \u201cThere\u2019s Just No Getting Around It: You\u2019re Building a Distributed\nSystem,\u201d ACM Queue, volume 11, number 4, pages 80-89, April 2013. doi:\n10.1145/2466486.2482856\n[2] Jay Kreps: \u201cGetting Real About Distributed System Reliability,\u201d blog.empathy\u2010\nbox.com, March 19, 2012.\n[3] Sydney Padua: The Thrilling Adventures of Lovelace and Babbage: The (Mostly)\nTrue Story of the First Computer. Particular Books, April 2015. ISBN:\n978-0-141-98151-2\n[4] Coda Hale: \u201cYou Can\u2019t Sacrifice Partition Tolerance,\u201d codahale.com, October 7,\n2010.\n[5] Jeff Hodges: \u201cNotes on Distributed Systems for Young Bloods,\u201d somethingsimi\u2010\nlar.com, January 14, 2013.\n[6] Antonio Regalado: \u201cWho Coined \u2018Cloud Computing\u2019?,\u201d technologyreview.com,\nOctober 31, 2011.\n[7] Luiz Andr\u00e9 Barroso, Jimmy Clidaras, and Urs H\u00f6lzle: \u201cThe Datacenter as a Com\u2010\nputer: An Introduction to the Design of Warehouse-Scale Machines, Second Edi\u2010\ntion,\u201d Synthesis Lectures on Computer Architecture, volume 8, number 3, Morgan &\nClaypool Publishers, July 2013. doi:10.2200/S00516ED2V01Y201306CAC024, ISBN:\n978-1-627-05010-4\n[8] David Fiala, Frank Mueller, Christian Engelmann, et al.: \u201cDetection and Correc\u2010\ntion of Silent Data Corruption for Large-Scale High-Performance Computing,\u201d at\nInternational Conference for High Performance Computing, Networking, Storage and\nAnalysis (SC12), November 2012.\n[9] Arjun Singh, Joon Ong, Amit Agarwal, et al.: \u201cJupiter Rising: A Decade of Clos\nTopologies and Centralized Control in Google\u2019s Datacenter Network,\u201d at Annual\nConference of the ACM Special Interest Group on Data Communication (SIGCOMM),\nAugust 2015. doi:10.1145/2785956.2787508"}
{"335": "[11] John von Neumann: \u201cProbabilistic Logics and the Synthesis of Reliable Organ\u2010\nisms from Unreliable Components,\u201d in Automata Studies (AM-34), edited by Claude\nE. Shannon and John McCarthy, Princeton University Press, 1956. ISBN:\n978-0-691-07916-5\n[12] Richard W. Hamming: The Art of Doing Science and Engineering. Taylor & Fran\u2010\ncis, 1997. ISBN: 978-9-056-99500-3\n[13] Claude E. Shannon: \u201cA Mathematical Theory of Communication,\u201d The Bell Sys\u2010\ntem Technical Journal, volume 27, number 3, pages 379\u2013423 and 623\u2013656, July 1948.\n[14] Peter Bailis and Kyle Kingsbury: \u201cThe Network Is Reliable,\u201d ACM Queue, vol\u2010\nume 12, number 7, pages 48-55, July 2014. doi:10.1145/2639988.2639988\n[15] Joshua B. Leners, Trinabh Gupta, Marcos K. Aguilera, and Michael Walfish:\n\u201cTaming Uncertainty in Distributed Systems with Help from the Network,\u201d at 10th\nEuropean Conference on Computer Systems (EuroSys), April 2015. doi:\n10.1145/2741948.2741976\n[16] Phillipa Gill, Navendu Jain, and Nachiappan Nagappan: \u201cUnderstanding Net\u2010\nwork Failures in Data Centers: Measurement, Analysis, and Implications,\u201d at ACM\nSIGCOMM Conference, August 2011. doi:10.1145/2018436.2018477\n[17] Mark Imbriaco: \u201cDowntime Last Saturday,\u201d github.com, December 26, 2012.\n[18] Will Oremus: \u201cThe Global Internet Is Being Attacked by Sharks, Google Con\u2010\nfirms,\u201d slate.com, August 15, 2014.\n[19] Marc A. Donges: \u201cRe: bnx2 cards Intermittantly Going Offline,\u201d Message to\nLinux netdev mailing list, spinics.net, September 13, 2012.\n[20] Kyle Kingsbury: \u201cCall Me Maybe: Elasticsearch,\u201d aphyr.com, June 15, 2014.\n[21] Salvatore Sanfilippo: \u201cA Few Arguments About Redis Sentinel Properties and\nFail Scenarios,\u201d antirez.com, October 21, 2014.\n[22] Bert Hubert: \u201cThe Ultimate SO_LINGER Page, or: Why Is My TCP Not Relia\u2010\nble,\u201d blog.netherlabs.nl, January 18, 2009.\n[23] Nicolas Liochon: \u201cCAP: If All You Have Is a Timeout, Everything Looks Like a\nPartition,\u201d blog.thislongrun.com, May 25, 2015.\n[24] Jerome H. Saltzer, David P. Reed, and David D. Clark: \u201cEnd-To-End Arguments\nin System Design,\u201d ACM Transactions on Computer Systems, volume 2, number 4,\npages 277\u2013288, November 1984. doi:10.1145/357401.357402"}
{"336": "[26] Guohui Wang and T. S. Eugene Ng: \u201cThe Impact of Virtualization on Network\nPerformance of Amazon EC2 Data Center,\u201d at 29th IEEE International Conference on\nComputer Communications (INFOCOM), March 2010. doi:10.1109/INFCOM.\n2010.5461931\n[27] Van Jacobson: \u201cCongestion Avoidance and Control,\u201d at ACM Symposium on\nCommunications Architectures and Protocols (SIGCOMM), August 1988. doi:\n10.1145/52324.52356\n[28] Brandon Philips: \u201cetcd: Distributed Locking and Service Discovery,\u201d at Strange\nLoop, September 2014.\n[29] Steve Newman: \u201cA Systematic Look at EC2 I/O,\u201d blog.scalyr.com, October 16,\n2012.\n[30] Naohiro Hayashibara, Xavier D\u00e9fago, Rami Yared, and Takuya Katayama: \u201cThe\n\u03d5 Accrual Failure Detector,\u201d Japan Advanced Institute of Science and Technology,\nSchool of Information Science, Technical Report IS-RR-2004-010, May 2004.\n[31] Jeffrey Wang: \u201cPhi Accrual Failure Detector,\u201d ternarysearch.blogspot.co.uk,\nAugust 11, 2013.\n[32] Srinivasan Keshav: An Engineering Approach to Computer Networking: ATM\nNetworks, the Internet, and the Telephone Network. Addison-Wesley Professional,\nMay 1997. ISBN: 978-0-201-63442-6\n[33] Cisco, \u201cIntegrated Services Digital Network,\u201d docwiki.cisco.com.\n[34] Othmar Kyas: ATM Networks. International Thomson Publishing, 1995. ISBN:\n978-1-850-32128-6\n[35] \u201cInfiniBand FAQ,\u201d Mellanox Technologies, December 22, 2014.\n[36] Jose Renato Santos, Yoshio Turner, and G. (John) Janakiraman: \u201cEnd-to-End\nCongestion Control for InfiniBand,\u201d at 22nd Annual Joint Conference of the IEEE\nComputer and Communications Societies (INFOCOM), April 2003. Also published by\nHP Laboratories Palo Alto, Tech Report HPL-2002-359. doi:10.1109/INFCOM.\n2003.1208949\n[37] Ulrich Windl, David Dalton, Marc Martinec, and Dale R. Worley: \u201cThe NTP\nFAQ and HOWTO,\u201d ntp.org, November 2006.\n[38] John Graham-Cumming: \u201cHow and why the leap second affected Cloudflare\nDNS,\u201d blog.cloudflare.com, January 1, 2017.\n[39] David Holmes: \u201cInside the Hotspot VM: Clocks, Timers and Scheduling Events"}
{"337": "[41] James C. Corbett, Jeffrey Dean, Michael Epstein, et al.: \u201cSpanner: Google\u2019s\nGlobally-Distributed Database,\u201d at 10th USENIX Symposium on Operating System\nDesign and Implementation (OSDI), October 2012.\n[42] M. Caporaloni and R. Ambrosini: \u201cHow Closely Can a Personal Computer\nClock Track the UTC Timescale Via the Internet?,\u201d European Journal of Physics, vol\u2010\nume 23, number 4, pages L17\u2013L21, June 2012. doi:10.1088/0143-0807/23/4/103\n[43] Nelson Minar: \u201cA Survey of the NTP Network,\u201d alumni.media.mit.edu, Decem\u2010\nber 1999.\n[44] Viliam Holub: \u201cSynchronizing Clocks in a Cassandra Cluster Pt. 1 \u2013 The Prob\u2010\nlem,\u201d blog.logentries.com, March 14, 2014.\n[45] Poul-Henning Kamp: \u201cThe One-Second War (What Time Will You Die?),\u201d\nACM Queue, volume 9, number 4, pages 44\u201348, April 2011. doi:\n10.1145/1966989.1967009\n[46] Nelson Minar: \u201cLeap Second Crashes Half the Internet,\u201d somebits.com, July 3,\n2012.\n[47] Christopher Pascoe: \u201cTime, Technology and Leaping Seconds,\u201d googleblog.blog\u2010\nspot.co.uk, September 15, 2011.\n[48] Mingxue Zhao and Jeff Barr: \u201cLook Before You Leap \u2013 The Coming Leap Second\nand AWS,\u201d aws.amazon.com, May 18, 2015.\n[49] Darryl Veitch and Kanthaiah Vijayalayan: \u201cNetwork Timing and the 2015 Leap\nSecond,\u201d at 17th International Conference on Passive and Active Measurement\n(PAM), April 2016. doi:10.1007/978-3-319-30505-9_29\n[50] \u201cTimekeeping in VMware Virtual Machines,\u201d Information Guide, VMware, Inc.,\nDecember 2011.\n[51] \u201cMiFID II / MiFIR: Regulatory Technical and Implementing Standards \u2013 Annex\nI (Draft),\u201d European Securities and Markets Authority, Report ESMA/2015/1464,\nSeptember 2015.\n[52] Luke Bigum: \u201cSolving MiFID II Clock Synchronisation With Minimum Spend\n(Part 1),\u201d lmax.com, November 27, 2015.\n[53] Kyle Kingsbury: \u201cCall Me Maybe: Cassandra,\u201d aphyr.com, September 24, 2013.\n[54] John Daily: \u201cClocks Are Bad, or, Welcome to the Wonderful World of Dis\u2010\ntributed Systems,\u201d basho.com, November 12, 2013."}
{"338": "[56] Leslie Lamport: \u201cTime, Clocks, and the Ordering of Events in a Distributed Sys\u2010\ntem,\u201d Communications of the ACM, volume 21, number 7, pages 558\u2013565, July 1978.\ndoi:10.1145/359545.359563\n[57] Sandeep Kulkarni, Murat Demirbas, Deepak Madeppa, et al.: \u201cLogical Physical\nClocks and Consistent Snapshots in Globally Distributed Databases,\u201d State University\nof New York at Buffalo, Computer Science and Engineering Technical Report\n2014-04, May 2014.\n[58] Justin Sheehy: \u201cThere Is No Now: Problems With Simultaneity in Distributed\nSystems,\u201d ACM Queue, volume 13, number 3, pages 36\u201341, March 2015. doi:\n10.1145/2733108\n[59] Murat Demirbas: \u201cSpanner: Google\u2019s Globally-Distributed Database,\u201d muratbuf\u2010\nfalo.blogspot.co.uk, July 4, 2013.\n[60] Dahlia Malkhi and Jean-Philippe Martin: \u201cSpanner\u2019s Concurrency Control,\u201d\nACM SIGACT News, volume 44, number 3, pages 73\u201377, September 2013. doi:\n10.1145/2527748.2527767\n[61] Manuel Bravo, Nuno Diegues, Jingna Zeng, et al.: \u201cOn the Use of Clocks to\nEnforce Consistency in the Cloud,\u201d IEEE Data Engineering Bulletin, volume 38, num\u2010\nber 1, pages 18\u201331, March 2015.\n[62] Spencer Kimball: \u201cLiving Without Atomic Clocks,\u201d cockroachlabs.com, February\n17, 2016.\n[63] Cary G. Gray and David R. Cheriton: \u201cLeases: An Efficient Fault-Tolerant Mech\u2010\nanism for Distributed File Cache Consistency,\u201d at 12th ACM Symposium on Operat\u2010\ning Systems Principles (SOSP), December 1989. doi:10.1145/74850.74870\n[64] Todd Lipcon: \u201cAvoiding Full GCs in Apache HBase with MemStore-Local Allo\u2010\ncation Buffers: Part 1,\u201d blog.cloudera.com, February 24, 2011.\n[65] Martin Thompson: \u201cJava Garbage Collection Distilled,\u201d mechanical-\nsympathy.blogspot.co.uk, July 16, 2013.\n[66] Alexey Ragozin: \u201cHow to Tame Java GC Pauses? Surviving 16GiB Heap and\nGreater,\u201d java.dzone.com, June 28, 2011.\n[67] Christopher Clark, Keir Fraser, Steven Hand, et al.: \u201cLive Migration of Virtual\nMachines,\u201d at 2nd USENIX Symposium on Symposium on Networked Systems Design\n& Implementation (NSDI), May 2005.\n[68] Mike Shaver: \u201cfsyncers and Curveballs,\u201d shaver.off.net, May 25, 2008."}
{"339": "[70] David Terei and Amit Levy: \u201cBlade: A Data Center Garbage Collector,\u201d arXiv:\n1504.02578, April 13, 2015.\n[71] Martin Maas, Tim Harris, Krste Asanovi\u0107, and John Kubiatowicz: \u201cTrash Day:\nCoordinating Garbage Collection in Distributed Systems,\u201d at 15th USENIX Workshop\non Hot Topics in Operating Systems (HotOS), May 2015.\n[72] \u201cPredictable Low Latency,\u201d Cinnober Financial Technology AB, cinnober.com,\nNovember 24, 2013.\n[73] Martin Fowler: \u201cThe LMAX Architecture,\u201d martinfowler.com, July 12, 2011.\n[74] Flavio P. Junqueira and Benjamin Reed: ZooKeeper: Distributed Process Coordi\u2010\nnation. O\u2019Reilly Media, 2013. ISBN: 978-1-449-36130-3\n[75] Enis S\u00f6ztutar: \u201cHBase and HDFS: Understanding Filesystem Usage in HBase,\u201d at\nHBaseCon, June 2013.\n[76] Caitie McCaffrey: \u201cClients Are Jerks: AKA How Halo 4 DoSed the Services at\nLaunch & How We Survived,\u201d caitiem.com, June 23, 2015.\n[77] Leslie Lamport, Robert Shostak, and Marshall Pease: \u201cThe Byzantine Generals\nProblem,\u201d ACM Transactions on Programming Languages and Systems (TOPLAS),\nvolume 4, number 3, pages 382\u2013401, July 1982. doi:10.1145/357172.357176\n[78] Jim N. Gray: \u201cNotes on Data Base Operating Systems,\u201d in Operating Systems: An\nAdvanced Course, Lecture Notes in Computer Science, volume 60, edited by R. Bayer,\nR. M. Graham, and G. Seegm\u00fcller, pages 393\u2013481, Springer-Verlag, 1978. ISBN:\n978-3-540-08755-7\n[79] Brian Palmer: \u201cHow Complicated Was the Byzantine Empire?,\u201d slate.com, Octo\u2010\nber 20, 2011.\n[80] Leslie Lamport: \u201cMy Writings,\u201d research.microsoft.com, December 16, 2014. This\npage can be found by searching the web for the 23-character string obtained by\nremoving the hyphens from the string allla-mport-spubso-ntheweb.\n[81] John Rushby: \u201cBus Architectures for Safety-Critical Embedded Systems,\u201d at 1st\nInternational Workshop on Embedded Software (EMSOFT), October 2001.\n[82] Jake Edge: \u201cELC: SpaceX Lessons Learned,\u201d lwn.net, March 6, 2013.\n[83] Andrew Miller and Joseph J. LaViola, Jr.: \u201cAnonymous Byzantine Consensus\nfrom Moderately-Hard Puzzles: A Model for Bitcoin,\u201d University of Central Florida,\nTechnical Report CS-TR-14-01, April 2014."}
{"340": "[86] Jonathan Stone and Craig Partridge: \u201cWhen the CRC and TCP Checksum Disa\u2010\ngree,\u201d at ACM Conference on Applications, Technologies, Architectures, and Protocols\nfor Computer Communication (SIGCOMM), August 2000. doi:\n10.1145/347059.347561\n[87] Evan Jones: \u201cHow Both TCP and Ethernet Checksums Fail,\u201d evanjones.ca, Octo\u2010\nber 5, 2015.\n[88] Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer: \u201cConsensus in the Pres\u2010\nence of Partial Synchrony,\u201d Journal of the ACM, volume 35, number 2, pages 288\u2013\n323, April 1988. doi:10.1145/42282.42283\n[89] Peter Bailis and Ali Ghodsi: \u201cEventual Consistency Today: Limitations, Exten\u2010\nsions, and Beyond,\u201d ACM Queue, volume 11, number 3, pages 55-63, March 2013.\ndoi:10.1145/2460276.2462076\n[90] Bowen Alpern and Fred B. Schneider: \u201cDefining Liveness,\u201d Information Process\u2010\ning Letters, volume 21, number 4, pages 181\u2013185, October 1985. doi:\n10.1016/0020-0190(85)90056-0\n[91] Flavio P. Junqueira: \u201cDude, Where\u2019s My Metadata?,\u201d fpj.me, May 28, 2015.\n[92] Scott Sanders: \u201cJanuary 28th Incident Report,\u201d github.com, February 3, 2016.\n[93] Jay Kreps: \u201cA Few Notes on Kafka and Jepsen,\u201d blog.empathybox.com, Septem\u2010\nber 25, 2013.\n[94] Thanh Do, Mingzhe Hao, Tanakorn Leesatapornwongsa, et al.: \u201cLimplock:\nUnderstanding the Impact of Limpware on Scale-out Cloud Systems,\u201d at 4th ACM\nSymposium on Cloud Computing (SoCC), October 2013. doi:\n10.1145/2523616.2523627\n[95] Frank McSherry, Michael Isard, and Derek G. Murray: \u201cScalability! But at What\nCOST?,\u201d at 15th USENIX Workshop on Hot Topics in Operating Systems (HotOS),\nMay 2015."}
{"341": ""}
{"342": ""}
{"343": "CHAPTER 9\nConsistency and Consensus\nIs it better to be alive and wrong or right and dead?\n\u2014Jay Kreps, A Few Notes on Kafka and Jepsen (2013)\nLots of things can go wrong in distributed systems, as discussed in Chapter 8. The\nsimplest way of handling such faults is to simply let the entire service fail, and show\nthe user an error message. If that solution is unacceptable, we need to find ways of\ntolerating faults\u2014that is, of keeping the service functioning correctly, even if some\ninternal component is faulty.\nIn this chapter, we will talk about some examples of algorithms and protocols for\nbuilding fault-tolerant distributed systems. We will assume that all the problems\nfrom Chapter 8 can occur: packets can be lost, reordered, duplicated, or arbitrarily\ndelayed in the network; clocks are approximate at best; and nodes can pause (e.g., due\nto garbage collection) or crash at any time.\nThe best way of building fault-tolerant systems is to find some general-purpose\nabstractions with useful guarantees, implement them once, and then let applications\nrely on those guarantees. This is the same approach as we used with transactions in\nChapter 7: by using a transaction, the application can pretend that there are no\ncrashes (atomicity), that nobody else is concurrently accessing the database (isola\u2010\ntion), and that storage devices are perfectly reliable (durability). Even though crashes,\nrace conditions, and disk failures do occur, the transaction abstraction hides those\nproblems so that the application doesn\u2019t need to worry about them.\nWe will now continue along the same lines, and seek abstractions that can allow an\napplication to ignore some of the problems with distributed systems. For example,"}
{"344": "reaching consensus in spite of network faults and process failures is a surprisingly\ntricky problem.\nOnce you have an implementation of consensus, applications can use it for various\npurposes. For example, say you have a database with single-leader replication. If the\nleader dies and you need to fail over to another node, the remaining database nodes\ncan use consensus to elect a new leader. As discussed in \u201cHandling Node Outages\u201d on\npage 156, it\u2019s important that there is only one leader, and that all nodes agree who the\nleader is. If two nodes both believe that they are the leader, that situation is called split\nbrain, and it often leads to data loss. Correct implementations of consensus help\navoid such problems.\nLater in this chapter, in \u201cDistributed Transactions and Consensus\u201d on page 352, we\nwill look into algorithms to solve consensus and related problems. But first we first\nneed to explore the range of guarantees and abstractions that can be provided in a\ndistributed system.\nWe need to understand the scope of what can and cannot be done: in some situa\u2010\ntions, it\u2019s possible for the system to tolerate faults and continue working; in other sit\u2010\nuations, that is not possible. The limits of what is and isn\u2019t possible have been\nexplored in depth, both in theoretical proofs and in practical implementations. We\nwill get an overview of those fundamental limits in this chapter.\nResearchers in the field of distributed systems have been studying these topics for\ndecades, so there is a lot of material\u2014we\u2019ll only be able to scratch the surface. In this\nbook we don\u2019t have space to go into details of the formal models and proofs, so we\nwill stick with informal intuitions. The literature references offer plenty of additional\ndepth if you\u2019re interested.\nConsistency Guarantees\nIn \u201cProblems with Replication Lag\u201d on page 161 we looked at some timing issues that\noccur in a replicated database. If you look at two database nodes at the same moment\nin time, you\u2019re likely to see different data on the two nodes, because write requests\narrive on different nodes at different times. These inconsistencies occur no matter\nwhat replication method the database uses (single-leader, multi-leader, or leaderless\nreplication).\nMost replicated databases provide at least eventual consistency, which means that if\nyou stop writing to the database and wait for some unspecified length of time, then\neventually all read requests will return the same value [1]. In other words, the incon\u2010\nsistency is temporary, and it eventually resolves itself (assuming that any faults in the"}
{"345": "However, this is a very weak guarantee\u2014it doesn\u2019t say anything about when the repli\u2010\ncas will converge. Until the time of convergence, reads could return anything or\nnothing [1]. For example, if you write a value and then immediately read it again,\nthere is no guarantee that you will see the value you just wrote, because the read may\nbe routed to a different replica (see \u201cReading Your Own Writes\u201d on page 162).\nEventual consistency is hard for application developers because it is so different from\nthe behavior of variables in a normal single-threaded program. If you assign a value\nto a variable and then read it shortly afterward, you don\u2019t expect to read back the old\nvalue, or for the read to fail. A database looks superficially like a variable that you can\nread and write, but in fact it has much more complicated semantics [3].\nWhen working with a database that provides only weak guarantees, you need to be\nconstantly aware of its limitations and not accidentally assume too much. Bugs are\noften subtle and hard to find by testing, because the application may work well most\nof the time. The edge cases of eventual consistency only become apparent when there\nis a fault in the system (e.g., a network interruption) or at high concurrency.\nIn this chapter we will explore stronger consistency models that data systems may\nchoose to provide. They don\u2019t come for free: systems with stronger guarantees may\nhave worse performance or be less fault-tolerant than systems with weaker guaran\u2010\ntees. Nevertheless, stronger guarantees can be appealing because they are easier to use\ncorrectly. Once you have seen a few different consistency models, you\u2019ll be in a better\nposition to decide which one best fits your needs.\nThere is some similarity between distributed consistency models and the hierarchy of\ntransaction isolation levels we discussed previously [4, 5] (see \u201cWeak Isolation Lev\u2010\nels\u201d on page 233). But while there is some overlap, they are mostly independent con\u2010\ncerns: transaction isolation is primarily about avoiding race conditions due to\nconcurrently executing transactions, whereas distributed consistency is mostly about\ncoordinating the state of replicas in the face of delays and faults.\nThis chapter covers a broad range of topics, but as we shall see, these areas are in fact\ndeeply linked:\n\u2022 We will start by looking at one of the strongest consistency models in common\nuse, linearizability, and examine its pros and cons.\n\u2022 We\u2019ll then examine the issue of ordering events in a distributed system (\u201cOrder\u2010\ning Guarantees\u201d on page 339), particularly around causality and total ordering.\n\u2022 In the third section (\u201cDistributed Transactions and Consensus\u201d on page 352) we\nwill explore how to atomically commit a distributed transaction, which will"}
{"346": "Linearizability\nIn an eventually consistent database, if you ask two different replicas the same ques\u2010\ntion at the same time, you may get two different answers. That\u2019s confusing. Wouldn\u2019t\nit be a lot simpler if the database could give the illusion that there is only one replica\n(i.e., only one copy of the data)? Then every client would have the same view of the\ndata, and you wouldn\u2019t have to worry about replication lag.\nThis is the idea behind linearizability [6] (also known as atomic consistency [7], strong\nconsistency, immediate consistency, or external consistency [8]). The exact definition\nof linearizability is quite subtle, and we will explore it in the rest of this section. But\nthe basic idea is to make a system appear as if there were only one copy of the data,\nand all operations on it are atomic. With this guarantee, even though there may be\nmultiple replicas in reality, the application does not need to worry about them.\nIn a linearizable system, as soon as one client successfully completes a write, all cli\u2010\nents reading from the database must be able to see the value just written. Maintaining\nthe illusion of a single copy of the data means guaranteeing that the value read is the\nmost recent, up-to-date value, and doesn\u2019t come from a stale cache or replica. In\nother words, linearizability is a recency guarantee. To clarify this idea, let\u2019s look at an\nexample of a system that is not linearizable."}
{"347": "Figure 9-1 shows an example of a nonlinearizable sports website [9]. Alice and Bob\nare sitting in the same room, both checking their phones to see the outcome of the\n2014 FIFA World Cup final. Just after the final score is announced, Alice refreshes\nthe page, sees the winner announced, and excitedly tells Bob about it. Bob incredu\u2010\nlously hits reload on his own phone, but his request goes to a database replica that is\nlagging, and so his phone shows that the game is still ongoing.\nIf Alice and Bob had hit reload at the same time, it would have been less surprising if\nthey had gotten two different query results, because they wouldn\u2019t know at exactly\nwhat time their respective requests were processed by the server. However, Bob\nknows that he hit the reload button (initiated his query) after he heard Alice exclaim\nthe final score, and therefore he expects his query result to be at least as recent as\nAlice\u2019s. The fact that his query returned a stale result is a violation of linearizability.\nWhat Makes a System Linearizable?\nThe basic idea behind linearizability is simple: to make a system appear as if there is\nonly a single copy of the data. However, nailing down precisely what that means\nactually requires some care. In order to understand linearizability better, let\u2019s look at\nsome more examples.\nFigure 9-2 shows three clients concurrently reading and writing the same key x in a\nlinearizable database. In the distributed systems literature, x is called a register\u2014in\npractice, it could be one key in a key-value store, one row in a relational database, or\none document in a document database, for example.\nFigure 9-2. If a read request is concurrent with a write request, it may return either the\nold or the new value.\nFor simplicity, Figure 9-2 shows only the requests from the clients\u2019 point of view, not\nthe internals of the database. Each bar is a request made by a client, where the start of\na bar is the time when the request was sent, and the end of a bar is when the response"}
{"348": "exactly when the database processed its request\u2014it only knows that it must have hap\u2010\npened sometime between the client sending the request and receiving the response.i\nIn this example, the register has two types of operations:\n\u2022 read(x) \u21d2 v means the client requested to read the value of register x, and the\ndatabase returned the value v.\n\u2022 write(x, v) \u21d2 r means the client requested to set the register x to value v, and the\ndatabase returned response r (which could be ok or error).\nIn Figure 9-2, the value of x is initially 0, and client C performs a write request to set\nit to 1. While this is happening, clients A and B are repeatedly polling the database to\nread the latest value. What are the possible responses that A and B might get for their\nread requests?\n\u2022 The first read operation by client A completes before the write begins, so it must\ndefinitely return the old value 0.\n\u2022 The last read by client A begins after the write has completed, so it must defi\u2010\nnitely return the new value 1 if the database is linearizable: we know that the\nwrite must have been processed sometime between the start and end of the write\noperation, and the read must have been processed sometime between the start\nand end of the read operation. If the read started after the write ended, then the\nread must have been processed after the write, and therefore it must see the new\nvalue that was written.\n\u2022 Any read operations that overlap in time with the write operation might return\neither 0 or 1, because we don\u2019t know whether or not the write has taken effect at\nthe time when the read operation is processed. These operations are concurrent\nwith the write.\nHowever, that is not yet sufficient to fully describe linearizability: if reads that are\nconcurrent with a write can return either the old or the new value, then readers could\nsee a value flip back and forth between the old and the new value several times while\na write is going on. That is not what we expect of a system that emulates a \u201csingle\ncopy of the data.\u201dii\ni. A subtle detail of this diagram is that it assumes the existence of a global clock, represented by the horizon\u2010\ntal axis. Even though real systems typically don\u2019t have accurate clocks (see \u201cUnreliable Clocks\u201d on page 287),\nthis assumption is okay: for the purposes of analyzing a distributed algorithm, we may pretend that an accu\u2010"}
{"349": "To make the system linearizable, we need to add another constraint, illustrated in\nFigure 9-3.\nFigure 9-3. After any one read has returned the new value, all following reads (on the\nsame or other clients) must also return the new value.\nIn a linearizable system we imagine that there must be some point in time (between\nthe start and end of the write operation) at which the value of x atomically flips from\n0 to 1. Thus, if one client\u2019s read returns the new value 1, all subsequent reads must\nalso return the new value, even if the write operation has not yet completed.\nThis timing dependency is illustrated with an arrow in Figure 9-3. Client A is the first\nto read the new value, 1. Just after A\u2019s read returns, B begins a new read. Since B\u2019s\nread occurs strictly after A\u2019s read, it must also return 1, even though the write by C is\nstill ongoing. (It\u2019s the same situation as with Alice and Bob in Figure 9-1: after Alice\nhas read the new value, Bob also expects to read the new value.)\nWe can further refine this timing diagram to visualize each operation taking effect\natomically at some point in time. A more complex example is shown in Figure 9-4\n[10].\nIn Figure 9-4 we add a third type of operation besides read and write:\n\u2022 cas(x, v , v ) \u21d2 r means the client requested an atomic compare-and-set oper\u2010\nold new\nation (see \u201cCompare-and-set\u201d on page 245). If the current value of the register x\nequals v , it should be atomically set to v . If x \u2260 v then the operation should\nold new old\nleave the register unchanged and return an error. r is the database\u2019s response (ok\nor error).\nEach operation in Figure 9-4 is marked with a vertical line (inside the bar for each\noperation) at the time when we think the operation was executed. Those markers are\njoined up in a sequential order, and the result must be a valid sequence of reads and"}
{"350": "ensures the recency guarantee we discussed earlier: once a new value has been written\nor read, all subsequent reads see the value that was written, until it is overwritten\nagain.\nFigure 9-4. Visualizing the points in time at which the reads and writes appear to have\ntaken effect. The final read by B is not linearizable.\nThere are a few interesting details to point out in Figure 9-4:\n\u2022 First client B sent a request to read x, then client D sent a request to set x to 0,\nand then client A sent a request to set x to 1. Nevertheless, the value returned to\nB\u2019s read is 1 (the value written by A). This is okay: it means that the database first\nprocessed D\u2019s write, then A\u2019s write, and finally B\u2019s read. Although this is not the\norder in which the requests were sent, it\u2019s an acceptable order, because the three\nrequests are concurrent. Perhaps B\u2019s read request was slightly delayed in the net\u2010\nwork, so it only reached the database after the two writes.\n\u2022 Client B\u2019s read returned 1 before client A received its response from the database,\nsaying that the write of the value 1 was successful. This is also okay: it doesn\u2019t\nmean the value was read before it was written, it just means the ok response from\nthe database to client A was slightly delayed in the network.\n\u2022 This model doesn\u2019t assume any transaction isolation: another client may change\na value at any time. For example, C first reads 1 and then reads 2, because the\nvalue was changed by B between the two reads. An atomic compare-and-set (cas)\noperation can be used to check the value hasn\u2019t been concurrently changed by\nanother client: B and C\u2019s cas requests succeed, but D\u2019s cas request fails (by the"}
{"351": "other requests, it would be okay for B\u2019s read to return 2. However, client A has\nalready read the new value 4 before B\u2019s read started, so B is not allowed to read\nan older value than A. Again, it\u2019s the same situation as with Alice and Bob in\nFigure 9-1.\nThat is the intuition behind linearizability; the formal definition [6] describes it more\nprecisely. It is possible (though computationally expensive) to test whether a system\u2019s\nbehavior is linearizable by recording the timings of all requests and responses, and\nchecking whether they can be arranged into a valid sequential order [11].\nLinearizability Versus Serializability\nLinearizability is easily confused with serializability (see \u201cSerializability\u201d on page 251),\nas both words seem to mean something like \u201ccan be arranged in a sequential order.\u201d\nHowever, they are two quite different guarantees, and it is important to distinguish\nbetween them:\nSerializability\nSerializability is an isolation property of transactions, where every transaction\nmay read and write multiple objects (rows, documents, records)\u2014see \u201cSingle-\nObject and Multi-Object Operations\u201d on page 228. It guarantees that transac\u2010\ntions behave the same as if they had executed in some serial order (each\ntransaction running to completion before the next transaction starts). It is okay\nfor that serial order to be different from the order in which transactions were\nactually run [12].\nLinearizability\nLinearizability is a recency guarantee on reads and writes of a register (an indi\u2010\nvidual object). It doesn\u2019t group operations together into transactions, so it does\nnot prevent problems such as write skew (see \u201cWrite Skew and Phantoms\u201d on\npage 246), unless you take additional measures such as materializing conflicts\n(see \u201cMaterializing conflicts\u201d on page 251).\nA database may provide both serializability and linearizability, and this combination\nis known as strict serializability or strong one-copy serializability (strong-1SR) [4, 13].\nImplementations of serializability based on two-phase locking (see \u201cTwo-Phase Lock\u2010\ning (2PL)\u201d on page 257) or actual serial execution (see \u201cActual Serial Execution\u201d on\npage 252) are typically linearizable.\nHowever, serializable snapshot isolation (see \u201cSerializable Snapshot Isolation (SSI)\u201d\non page 261) is not linearizable: by design, it makes reads from a consistent snapshot,\nto avoid lock contention between readers and writers. The whole point of a consistent"}
{"352": "Relying on Linearizability\nIn what circumstances is linearizability useful? Viewing the final score of a sporting\nmatch is perhaps a frivolous example: a result that is outdated by a few seconds is\nunlikely to cause any real harm in this situation. However, there a few areas in which\nlinearizability is an important requirement for making a system work correctly.\nLocking and leader election\nA system that uses single-leader replication needs to ensure that there is indeed only\none leader, not several (split brain). One way of electing a leader is to use a lock: every\nnode that starts up tries to acquire the lock, and the one that succeeds becomes the\nleader [14]. No matter how this lock is implemented, it must be linearizable: all nodes\nmust agree which node owns the lock; otherwise it is useless.\nCoordination services like Apache ZooKeeper [15] and etcd [16] are often used to\nimplement distributed locks and leader election. They use consensus algorithms to\nimplement linearizable operations in a fault-tolerant way (we discuss such algorithms\nlater in this chapter, in \u201cFault-Tolerant Consensus\u201d on page 364).iii There are still\nmany subtle details to implementing locks and leader election correctly (see for\nexample the fencing issue in \u201cThe leader and the lock\u201d on page 301), and libraries like\nApache Curator [17] help by providing higher-level recipes on top of ZooKeeper.\nHowever, a linearizable storage service is the basic foundation for these coordination\ntasks.\nDistributed locking is also used at a much more granular level in some distributed\ndatabases, such as Oracle Real Application Clusters (RAC) [18]. RAC uses a lock per\ndisk page, with multiple nodes sharing access to the same disk storage system. Since\nthese linearizable locks are on the critical path of transaction execution, RAC deploy\u2010\nments usually have a dedicated cluster interconnect network for communication\nbetween database nodes.\nConstraints and uniqueness guarantees\nUniqueness constraints are common in databases: for example, a username or email\naddress must uniquely identify one user, and in a file storage service there cannot be\ntwo files with the same path and filename. If you want to enforce this constraint as\nthe data is written (such that if two people try to concurrently create a user or a file\nwith the same name, one of them will be returned an error), you need linearizability."}
{"353": "This situation is actually similar to a lock: when a user registers for your service, you\ncan think of them acquiring a \u201clock\u201d on their chosen username. The operation is also\nvery similar to an atomic compare-and-set, setting the username to the ID of the user\nwho claimed it, provided that the username is not already taken.\nSimilar issues arise if you want to ensure that a bank account balance never goes neg\u2010\native, or that you don\u2019t sell more items than you have in stock in the warehouse, or\nthat two people don\u2019t concurrently book the same seat on a flight or in a theater.\nThese constraints all require there to be a single up-to-date value (the account bal\u2010\nance, the stock level, the seat occupancy) that all nodes agree on.\nIn real applications, it is sometimes acceptable to treat such constraints loosely (for\nexample, if a flight is overbooked, you can move customers to a different flight and\noffer them compensation for the inconvenience). In such cases, linearizability may\nnot be needed, and we will discuss such loosely interpreted constraints in \u201cTimeliness\nand Integrity\u201d on page 524.\nHowever, a hard uniqueness constraint, such as the one you typically find in rela\u2010\ntional databases, requires linearizability. Other kinds of constraints, such as foreign\nkey or attribute constraints, can be implemented without requiring linearizability\n[19].\nCross-channel timing dependencies\nNotice a detail in Figure 9-1: if Alice hadn\u2019t exclaimed the score, Bob wouldn\u2019t have\nknown that the result of his query was stale. He would have just refreshed the page\nagain a few seconds later, and eventually seen the final score. The linearizability viola\u2010\ntion was only noticed because there was an additional communication channel in the\nsystem (Alice\u2019s voice to Bob\u2019s ears).\nSimilar situations can arise in computer systems. For example, say you have a website\nwhere users can upload a photo, and a background process resizes the photos to\nlower resolution for faster download (thumbnails). The architecture and dataflow of\nthis system is illustrated in Figure 9-5.\nThe image resizer needs to be explicitly instructed to perform a resizing job, and this\ninstruction is sent from the web server to the resizer via a message queue (see Chap\u2010\nter 11). The web server doesn\u2019t place the entire photo on the queue, since most mes\u2010\nsage brokers are designed for small messages, and a photo may be several megabytes\nin size. Instead, the photo is first written to a file storage service, and once the write is\ncomplete, the instruction to the resizer is placed on the queue."}
{"354": "Figure 9-5. The web server and image resizer communicate both through file storage\nand a message queue, opening the potential for race conditions.\nIf the file storage service is linearizable, then this system should work fine. If it is not\nlinearizable, there is the risk of a race condition: the message queue (steps 3 and 4 in\nFigure 9-5) might be faster than the internal replication inside the storage service. In\nthis case, when the resizer fetches the image (step 5), it might see an old version of the\nimage, or nothing at all. If it processes an old version of the image, the full-size and\nresized images in the file storage become permanently inconsistent.\nThis problem arises because there are two different communication channels\nbetween the web server and the resizer: the file storage and the message queue.\nWithout the recency guarantee of linearizability, race conditions between these two\nchannels are possible. This situation is analogous to Figure 9-1, where there was also\na race condition between two communication channels: the database replication and\nthe real-life audio channel between Alice\u2019s mouth and Bob\u2019s ears.\nLinearizability is not the only way of avoiding this race condition, but it\u2019s the simplest\nto understand. If you control the additional communication channel (like in the case\nof the message queue, but not in the case of Alice and Bob), you can use alternative\napproaches similar to what we discussed in \u201cReading Your Own Writes\u201d on page 162,\nat the cost of additional complexity.\nImplementing Linearizable Systems\nNow that we\u2019ve looked at a few examples in which linearizability is useful, let\u2019s think\nabout how we might implement a system that offers linearizable semantics.\nSince linearizability essentially means \u201cbehave as though there is only a single copy of\nthe data, and all operations on it are atomic,\u201d the simplest answer would be to really\nonly use a single copy of the data. However, that approach would not be able to toler\u2010"}
{"355": "The most common approach to making a system fault-tolerant is to use replication.\nLet\u2019s revisit the replication methods from Chapter 5, and compare whether they can\nbe made linearizable:\nSingle-leader replication (potentially linearizable)\nIn a system with single-leader replication (see \u201cLeaders and Followers\u201d on page\n152), the leader has the primary copy of the data that is used for writes, and the\nfollowers maintain backup copies of the data on other nodes. If you make reads\nfrom the leader, or from synchronously updated followers, they have the poten\u2010\ntial to be linearizable.iv However, not every single-leader database is actually line\u2010\narizable, either by design (e.g., because it uses snapshot isolation) or due to\nconcurrency bugs [10].\nUsing the leader for reads relies on the assumption that you know for sure who\nthe leader is. As discussed in \u201cThe Truth Is Defined by the Majority\u201d on page\n300, it is quite possible for a node to think that it is the leader, when in fact it is\nnot\u2014and if the delusional leader continues to serve requests, it is likely to violate\nlinearizability [20]. With asynchronous replication, failover may even lose com\u2010\nmitted writes (see \u201cHandling Node Outages\u201d on page 156), which violates both\ndurability and linearizability.\nConsensus algorithms (linearizable)\nSome consensus algorithms, which we will discuss later in this chapter, bear a\nresemblance to single-leader replication. However, consensus protocols contain\nmeasures to prevent split brain and stale replicas. Thanks to these details, con\u2010\nsensus algorithms can implement linearizable storage safely. This is how Zoo\u2010\nKeeper [21] and etcd [22] work, for example.\nMulti-leader replication (not linearizable)\nSystems with multi-leader replication are generally not linearizable, because they\nconcurrently process writes on multiple nodes and asynchronously replicate\nthem to other nodes. For this reason, they can produce conflicting writes that\nrequire resolution (see \u201cHandling Write Conflicts\u201d on page 171). Such conflicts\nare an artifact of the lack of a single copy of the data.\nLeaderless replication (probably not linearizable)\nFor systems with leaderless replication (Dynamo-style; see \u201cLeaderless Replica\u2010\ntion\u201d on page 177), people sometimes claim that you can obtain \u201cstrong consis\u2010\ntency\u201d by requiring quorum reads and writes (w + r > n). Depending on the exact"}
{"356": "configuration of the quorums, and depending on how you define strong consis\u2010\ntency, this is not quite true.\n\u201cLast write wins\u201d conflict resolution methods based on time-of-day clocks (e.g.,\nin Cassandra; see \u201cRelying on Synchronized Clocks\u201d on page 291) are almost cer\u2010\ntainly nonlinearizable, because clock timestamps cannot be guaranteed to be\nconsistent with actual event ordering due to clock skew. Sloppy quorums\n(\u201cSloppy Quorums and Hinted Handoff\u201d on page 183) also ruin any chance of\nlinearizability. Even with strict quorums, nonlinearizable behavior is possible, as\ndemonstrated in the next section.\nLinearizability and quorums\nIntuitively, it seems as though strict quorum reads and writes should be linearizable\nin a Dynamo-style model. However, when we have variable network delays, it is pos\u2010\nsible to have race conditions, as demonstrated in Figure 9-6.\nFigure 9-6. A nonlinearizable execution, despite using a strict quorum.\nIn Figure 9-6, the initial value of x is 0, and a writer client is updating x to 1 by send\u2010\ning the write to all three replicas (n = 3, w = 3). Concurrently, client A reads from a\nquorum of two nodes (r = 2) and sees the new value 1 on one of the nodes. Also con\u2010\ncurrently with the write, client B reads from a different quorum of two nodes, and\ngets back the old value 0 from both."}
{"357": "while A returns the new value. (It\u2019s once again the Alice and Bob situation from\nFigure 9-1.)\nInterestingly, it is possible to make Dynamo-style quorums linearizable at the cost of\nreduced performance: a reader must perform read repair (see \u201cRead repair and anti-\nentropy\u201d on page 178) synchronously, before returning results to the application\n[23], and a writer must read the latest state of a quorum of nodes before sending its\nwrites [24, 25]. However, Riak does not perform synchronous read repair due to the\nperformance penalty [26]. Cassandra does wait for read repair to complete on quo\u2010\nrum reads [27], but it loses linearizability if there are multiple concurrent writes to\nthe same key, due to its use of last-write-wins conflict resolution.\nMoreover, only linearizable read and write operations can be implemented in this\nway; a linearizable compare-and-set operation cannot, because it requires a consen\u2010\nsus algorithm [28].\nIn summary, it is safest to assume that a leaderless system with Dynamo-style replica\u2010\ntion does not provide linearizability.\nThe Cost of Linearizability\nAs some replication methods can provide linearizability and others cannot, it is inter\u2010\nesting to explore the pros and cons of linearizability in more depth.\nWe already discussed some use cases for different replication methods in Chapter 5;\nfor example, we saw that multi-leader replication is often a good choice for multi-\ndatacenter replication (see \u201cMulti-datacenter operation\u201d on page 168). An example of\nsuch a deployment is illustrated in Figure 9-7."}
{"358": "Consider what happens if there is a network interruption between the two datacen\u2010\nters. Let\u2019s assume that the network within each datacenter is working, and clients can\nreach the datacenters, but the datacenters cannot connect to each other.\nWith a multi-leader database, each datacenter can continue operating normally: since\nwrites from one datacenter are asynchronously replicated to the other, the writes are\nsimply queued up and exchanged when network connectivity is restored.\nOn the other hand, if single-leader replication is used, then the leader must be in one\nof the datacenters. Any writes and any linearizable reads must be sent to the leader\u2014\nthus, for any clients connected to a follower datacenter, those read and write requests\nmust be sent synchronously over the network to the leader datacenter.\nIf the network between datacenters is interrupted in a single-leader setup, clients con\u2010\nnected to follower datacenters cannot contact the leader, so they cannot make any\nwrites to the database, nor any linearizable reads. They can still make reads from the\nfollower, but they might be stale (nonlinearizable). If the application requires linear\u2010\nizable reads and writes, the network interruption causes the application to become\nunavailable in the datacenters that cannot contact the leader.\nIf clients can connect directly to the leader datacenter, this is not a problem, since the\napplication continues to work normally there. But clients that can only reach a fol\u2010\nlower datacenter will experience an outage until the network link is repaired.\nThe CAP theorem\nThis issue is not just a consequence of single-leader and multi-leader replication: any\nlinearizable database has this problem, no matter how it is implemented. The issue\nalso isn\u2019t specific to multi-datacenter deployments, but can occur on any unreliable\nnetwork, even within one datacenter. The trade-off is as follows:v\n\u2022 If your application requires linearizability, and some replicas are disconnected\nfrom the other replicas due to a network problem, then some replicas cannot\nprocess requests while they are disconnected: they must either wait until the net\u2010\nwork problem is fixed, or return an error (either way, they become unavailable).\n\u2022 If your application does not require linearizability, then it can be written in a way\nthat each replica can process requests independently, even if it is disconnected\nfrom other replicas (e.g., multi-leader). In this case, the application can remain\navailable in the face of a network problem, but its behavior is not linearizable."}
{"359": "Thus, applications that don\u2019t require linearizability can be more tolerant of network\nproblems. This insight is popularly known as the CAP theorem [29, 30, 31, 32],\nnamed by Eric Brewer in 2000, although the trade-off has been known to designers of\ndistributed databases since the 1970s [33, 34, 35, 36].\nCAP was originally proposed as a rule of thumb, without precise definitions, with the\ngoal of starting a discussion about trade-offs in databases. At the time, many dis\u2010\ntributed databases focused on providing linearizable semantics on a cluster of\nmachines with shared storage [18], and CAP encouraged database engineers to\nexplore a wider design space of distributed shared-nothing systems, which were more\nsuitable for implementing large-scale web services [37]. CAP deserves credit for this\nculture shift\u2014witness the explosion of new database technologies since the\nmid-2000s (known as NoSQL).\nThe Unhelpful CAP Theorem\nCAP is sometimes presented as Consistency, Availability, Partition tolerance: pick 2\nout of 3. Unfortunately, putting it this way is misleading [32] because network parti\u2010\ntions are a kind of fault, so they aren\u2019t something about which you have a choice: they\nwill happen whether you like it or not [38].\nAt times when the network is working correctly, a system can provide both consis\u2010\ntency (linearizability) and total availability. When a network fault occurs, you have to\nchoose between either linearizability or total availability. Thus, a better way of phras\u2010\ning CAP would be either Consistent or Available when Partitioned [39]. A more relia\u2010\nble network needs to make this choice less often, but at some point the choice is\ninevitable.\nIn discussions of CAP there are several contradictory definitions of the term availa\u2010\nbility, and the formalization as a theorem [30] does not match its usual meaning [40].\nMany so-called \u201chighly available\u201d (fault-tolerant) systems actually do not meet CAP\u2019s\nidiosyncratic definition of availability. All in all, there is a lot of misunderstanding\nand confusion around CAP, and it does not help us understand systems better, so\nCAP is best avoided.\nThe CAP theorem as formally defined [30] is of very narrow scope: it only considers\none consistency model (namely linearizability) and one kind of fault (network parti\u2010\ntions,vi or nodes that are alive but disconnected from each other). It doesn\u2019t say any\u2010"}
{"360": "thing about network delays, dead nodes, or other trade-offs. Thus, although CAP has\nbeen historically influential, it has little practical value for designing systems [9, 40].\nThere are many more interesting impossibility results in distributed systems [41],\nand CAP has now been superseded by more precise results [2, 42], so it is of mostly\nhistorical interest today.\nLinearizability and network delays\nAlthough linearizability is a useful guarantee, surprisingly few systems are actually\nlinearizable in practice. For example, even RAM on a modern multi-core CPU is not\nlinearizable [43]: if a thread running on one CPU core writes to a memory address,\nand a thread on another CPU core reads the same address shortly afterward, it is not\nguaranteed to read the value written by the first thread (unless a memory barrier or\nfence [44] is used).\nThe reason for this behavior is that every CPU core has its own memory cache and\nstore buffer. Memory access first goes to the cache by default, and any changes are\nasynchronously written out to main memory. Since accessing data in the cache is\nmuch faster than going to main memory [45], this feature is essential for good per\u2010\nformance on modern CPUs. However, there are now several copies of the data (one\nin main memory, and perhaps several more in various caches), and these copies are\nasynchronously updated, so linearizability is lost.\nWhy make this trade-off? It makes no sense to use the CAP theorem to justify the\nmulti-core memory consistency model: within one computer we usually assume reli\u2010\nable communication, and we don\u2019t expect one CPU core to be able to continue oper\u2010\nating normally if it is disconnected from the rest of the computer. The reason for\ndropping linearizability is performance, not fault tolerance.\nThe same is true of many distributed databases that choose not to provide lineariza\u2010\nble guarantees: they do so primarily to increase performance, not so much for fault\ntolerance [46]. Linearizability is slow\u2014and this is true all the time, not only during a\nnetwork fault.\nCan\u2019t we maybe find a more efficient implementation of linearizable storage? It\nseems the answer is no: Attiya and Welch [47] prove that if you want linearizability,\nthe response time of read and write requests is at least proportional to the uncertainty\nof delays in the network. In a network with highly variable delays, like most com\u2010\nputer networks (see \u201cTimeouts and Unbounded Delays\u201d on page 281), the response\ntime of linearizable reads and writes is inevitably going to be high. A faster algorithm\nfor linearizability does not exist, but weaker consistency models can be much faster,"}
{"361": "Ordering Guarantees\nWe said previously that a linearizable register behaves as if there is only a single copy\nof the data, and that every operation appears to take effect atomically at one point in\ntime. This definition implies that operations are executed in some well-defined order.\nWe illustrated the ordering in Figure 9-4 by joining up the operations in the order in\nwhich they seem to have executed.\nOrdering has been a recurring theme in this book, which suggests that it might be an\nimportant fundamental idea. Let\u2019s briefly recap some of the other contexts in which\nwe have discussed ordering:\n\u2022 In Chapter 5 we saw that the main purpose of the leader in single-leader replica\u2010\ntion is to determine the order of writes in the replication log\u2014that is, the order in\nwhich followers apply those writes. If there is no single leader, conflicts can occur\ndue to concurrent operations (see \u201cHandling Write Conflicts\u201d on page 171).\n\u2022 Serializability, which we discussed in Chapter 7, is about ensuring that transac\u2010\ntions behave as if they were executed in some sequential order. It can be achieved\nby literally executing transactions in that serial order, or by allowing concurrent\nexecution while preventing serialization conflicts (by locking or aborting).\n\u2022 The use of timestamps and clocks in distributed systems that we discussed in\nChapter 8 (see \u201cRelying on Synchronized Clocks\u201d on page 291) is another\nattempt to introduce order into a disorderly world, for example to determine\nwhich one of two writes happened later.\nIt turns out that there are deep connections between ordering, linearizability, and\nconsensus. Although this notion is a bit more theoretical and abstract than the rest of\nthis book, it is very helpful for clarifying our understanding of what systems can and\ncannot do. We will explore this topic in the next few sections.\nOrdering and Causality\nThere are several reasons why ordering keeps coming up, and one of the reasons is\nthat it helps preserve causality. We have already seen several examples over the\ncourse of this book where causality has been important:\n\u2022 In \u201cConsistent Prefix Reads\u201d on page 165 (Figure 5-5) we saw an example where\nthe observer of a conversation saw first the answer to a question, and then the\nquestion being answered. This is confusing because it violates our intuition of\ncause and effect: if a question is answered, then clearly the question had to be"}
{"362": "\u2022 A similar pattern appeared in Figure 5-9, where we looked at the replication\nbetween three leaders and noticed that some writes could \u201covertake\u201d others due\nto network delays. From the perspective of one of the replicas it would look as\nthough there was an update to a row that did not exist. Causality here means that\na row must first be created before it can be updated.\n\u2022 In \u201cDetecting Concurrent Writes\u201d on page 184 we observed that if you have two\noperations A and B, there are three possibilities: either A happened before B, or B\nhappened before A, or A and B are concurrent. This happened before relationship\nis another expression of causality: if A happened before B, that means B might\nhave known about A, or built upon A, or depended on A. If A and B are concur\u2010\nrent, there is no causal link between them; in other words, we are sure that nei\u2010\nther knew about the other.\n\u2022 In the context of snapshot isolation for transactions (\u201cSnapshot Isolation and\nRepeatable Read\u201d on page 237), we said that a transaction reads from a consistent\nsnapshot. But what does \u201cconsistent\u201d mean in this context? It means consistent\nwith causality: if the snapshot contains an answer, it must also contain the ques\u2010\ntion being answered [48]. Observing the entire database at a single point in time\nmakes it consistent with causality: the effects of all operations that happened cau\u2010\nsally before that point in time are visible, but no operations that happened cau\u2010\nsally afterward can be seen. Read skew (non-repeatable reads, as illustrated in\nFigure 7-6) means reading data in a state that violates causality.\n\u2022 Our examples of write skew between transactions (see \u201cWrite Skew and Phan\u2010\ntoms\u201d on page 246) also demonstrated causal dependencies: in Figure 7-8, Alice\nwas allowed to go off call because the transaction thought that Bob was still on\ncall, and vice versa. In this case, the action of going off call is causally dependent\non the observation of who is currently on call. Serializable snapshot isolation (see\n\u201cSerializable Snapshot Isolation (SSI)\u201d on page 261) detects write skew by track\u2010\ning the causal dependencies between transactions.\n\u2022 In the example of Alice and Bob watching football (Figure 9-1), the fact that Bob\ngot a stale result from the server after hearing Alice exclaim the result is a causal\u2010\nity violation: Alice\u2019s exclamation is causally dependent on the announcement of\nthe score, so Bob should also be able to see the score after hearing Alice. The\nsame pattern appeared again in \u201cCross-channel timing dependencies\u201d on page\n331 in the guise of an image resizing service.\nCausality imposes an ordering on events: cause comes before effect; a message is sent\nbefore that message is received; the question comes before the answer. And, like in\nreal life, one thing leads to another: one node reads some data and then writes some\u2010"}
{"363": "If a system obeys the ordering imposed by causality, we say that it is causally consis\u2010\ntent. For example, snapshot isolation provides causal consistency: when you read\nfrom the database, and you see some piece of data, then you must also be able to see\nany data that causally precedes it (assuming it has not been deleted in the meantime).\nThe causal order is not a total order\nA total order allows any two elements to be compared, so if you have two elements,\nyou can always say which one is greater and which one is smaller. For example, natu\u2010\nral numbers are totally ordered: if I give you any two numbers, say 5 and 13, you can\ntell me that 13 is greater than 5.\nHowever, mathematical sets are not totally ordered: is {a, b} greater than {b, c}? Well,\nyou can\u2019t really compare them, because neither is a subset of the other. We say they\nare incomparable, and therefore mathematical sets are partially ordered: in some cases\none set is greater than another (if one set contains all the elements of another), but in\nother cases they are incomparable.\nThe difference between a total order and a partial order is reflected in different data\u2010\nbase consistency models:\nLinearizability\nIn a linearizable system, we have a total order of operations: if the system behaves\nas if there is only a single copy of the data, and every operation is atomic, this\nmeans that for any two operations we can always say which one happened first.\nThis total ordering is illustrated as a timeline in Figure 9-4.\nCausality\nWe said that two operations are concurrent if neither happened before the other\n(see \u201cThe \u201chappens-before\u201d relationship and concurrency\u201d on page 186). Put\nanother way, two events are ordered if they are causally related (one happened\nbefore the other), but they are incomparable if they are concurrent. This means\nthat causality defines a partial order, not a total order: some operations are\nordered with respect to each other, but some are incomparable.\nTherefore, according to this definition, there are no concurrent operations in a line\u2010\narizable datastore: there must be a single timeline along which all operations are\ntotally ordered. There might be several requests waiting to be handled, but the data\u2010\nstore ensures that every request is handled atomically at a single point in time, acting\non a single copy of the data, along a single timeline, without any concurrency.\nConcurrency would mean that the timeline branches and merges again\u2014and in this\ncase, operations on different branches are incomparable (i.e., concurrent). We saw"}
{"364": "If you are familiar with distributed version control systems such as Git, their version\nhistories are very much like the graph of causal dependencies. Often one commit\nhappens after another, in a straight line, but sometimes you get branches (when sev\u2010\neral people concurrently work on a project), and merges are created when those con\u2010\ncurrently created commits are combined.\nLinearizability is stronger than causal consistency\nSo what is the relationship between the causal order and linearizability? The answer is\nthat linearizability implies causality: any system that is linearizable will preserve cau\u2010\nsality correctly [7]. In particular, if there are multiple communication channels in a\nsystem (such as the message queue and the file storage service in Figure 9-5), lineariz\u2010\nability ensures that causality is automatically preserved without the system having to\ndo anything special (such as passing around timestamps between different compo\u2010\nnents).\nThe fact that linearizability ensures causality is what makes linearizable systems sim\u2010\nple to understand and appealing. However, as discussed in \u201cThe Cost of Linearizabil\u2010\nity\u201d on page 335, making a system linearizable can harm its performance and\navailability, especially if the system has significant network delays (for example, if it\u2019s\ngeographically distributed). For this reason, some distributed data systems have\nabandoned linearizability, which allows them to achieve better performance but can\nmake them difficult to work with.\nThe good news is that a middle ground is possible. Linearizability is not the only way\nof preserving causality\u2014there are other ways too. A system can be causally consistent\nwithout incurring the performance hit of making it linearizable (in particular, the\nCAP theorem does not apply). In fact, causal consistency is the strongest possible\nconsistency model that does not slow down due to network delays, and remains\navailable in the face of network failures [2, 42].\nIn many cases, systems that appear to require linearizability in fact only really require\ncausal consistency, which can be implemented more efficiently. Based on this obser\u2010\nvation, researchers are exploring new kinds of databases that preserve causality, with\nperformance and availability characteristics that are similar to those of eventually\nconsistent systems [49, 50, 51].\nAs this research is quite recent, not much of it has yet made its way into production\nsystems, and there are still challenges to be overcome [52, 53]. However, it is a prom\u2010\nising direction for future systems.\nCapturing causal dependencies"}
{"365": "In order to maintain causality, you need to know which operation happened before\nwhich other operation. This is a partial order: concurrent operations may be pro\u2010\ncessed in any order, but if one operation happened before another, then they must be\nprocessed in that order on every replica. Thus, when a replica processes an operation,\nit must ensure that all causally preceding operations (all operations that happened\nbefore) have already been processed; if some preceding operation is missing, the later\noperation must wait until the preceding operation has been processed.\nIn order to determine causal dependencies, we need some way of describing the\n\u201cknowledge\u201d of a node in the system. If a node had already seen the value X when it\nissued the write Y, then X and Y may be causally related. The analysis uses the kinds\nof questions you would expect in a criminal investigation of fraud charges: did the\nCEO know about X at the time when they made decision Y?\nThe techniques for determining which operation happened before which other oper\u2010\nation are similar to what we discussed in \u201cDetecting Concurrent Writes\u201d on page 184.\nThat section discussed causality in a leaderless datastore, where we need to detect\nconcurrent writes to the same key in order to prevent lost updates. Causal consis\u2010\ntency goes further: it needs to track causal dependencies across the entire database,\nnot just for a single key. Version vectors can be generalized to do this [54].\nIn order to determine the causal ordering, the database needs to know which version\nof the data was read by the application. This is why, in Figure 5-13, the version num\u2010\nber from the prior operation is passed back to the database on a write. A similar idea\nappears in the conflict detection of SSI, as discussed in \u201cSerializable Snapshot Isola\u2010\ntion (SSI)\u201d on page 261: when a transaction wants to commit, the database checks\nwhether the version of the data that it read is still up to date. To this end, the database\nkeeps track of which data has been read by which transaction.\nSequence Number Ordering\nAlthough causality is an important theoretical concept, actually keeping track of all\ncausal dependencies can become impractical. In many applications, clients read lots\nof data before writing something, and then it is not clear whether the write is causally\ndependent on all or only some of those prior reads. Explicitly tracking all the data\nthat has been read would mean a large overhead.\nHowever, there is a better way: we can use sequence numbers or timestamps to order\nevents. A timestamp need not come from a time-of-day clock (or physical clock,\nwhich have many problems, as discussed in \u201cUnreliable Clocks\u201d on page 287). It can\ninstead come from a logical clock, which is an algorithm to generate a sequence of"}
{"366": "Such sequence numbers or timestamps are compact (only a few bytes in size), and\nthey provide a total order: that is, every operation has a unique sequence number, and\nyou can always compare two sequence numbers to determine which is greater (i.e.,\nwhich operation happened later).\nIn particular, we can create sequence numbers in a total order that is consistent with\ncausality:vii we promise that if operation A causally happened before B, then A occurs\nbefore B in the total order (A has a lower sequence number than B). Concurrent\noperations may be ordered arbitrarily. Such a total order captures all the causality\ninformation, but also imposes more ordering than strictly required by causality.\nIn a database with single-leader replication (see \u201cLeaders and Followers\u201d on page\n152), the replication log defines a total order of write operations that is consistent\nwith causality. The leader can simply increment a counter for each operation, and\nthus assign a monotonically increasing sequence number to each operation in the\nreplication log. If a follower applies the writes in the order they appear in the replica\u2010\ntion log, the state of the follower is always causally consistent (even if it is lagging\nbehind the leader).\nNoncausal sequence number generators\nIf there is not a single leader (perhaps because you are using a multi-leader or leader\u2010\nless database, or because the database is partitioned), it is less clear how to generate\nsequence numbers for operations. Various methods are used in practice:\n\u2022 Each node can generate its own independent set of sequence numbers. For exam\u2010\nple, if you have two nodes, one node can generate only odd numbers and the\nother only even numbers. In general, you could reserve some bits in the binary\nrepresentation of the sequence number to contain a unique node identifier, and\nthis would ensure that two different nodes can never generate the same sequence\nnumber.\n\u2022 You can attach a timestamp from a time-of-day clock (physical clock) to each\noperation [55]. Such timestamps are not sequential, but if they have sufficiently\nhigh resolution, they might be sufficient to totally order operations. This fact is\nused in the last write wins conflict resolution method (see \u201cTimestamps for\nordering events\u201d on page 291).\n\u2022 You can preallocate blocks of sequence numbers. For example, node A might\nclaim the block of sequence numbers from 1 to 1,000, and node B might claim"}
{"367": "the block from 1,001 to 2,000. Then each node can independently assign\nsequence numbers from its block, and allocate a new block when its supply of\nsequence numbers begins to run low.\nThese three options all perform better and are more scalable than pushing all opera\u2010\ntions through a single leader that increments a counter. They generate a unique,\napproximately increasing sequence number for each operation. However, they all\nhave a problem: the sequence numbers they generate are not consistent with causality.\nThe causality problems occur because these sequence number generators do not cor\u2010\nrectly capture the ordering of operations across different nodes:\n\u2022 Each node may process a different number of operations per second. Thus, if one\nnode generates even numbers and the other generates odd numbers, the counter\nfor even numbers may lag behind the counter for odd numbers, or vice versa. If\nyou have an odd-numbered operation and an even-numbered operation, you\ncannot accurately tell which one causally happened first.\n\u2022 Timestamps from physical clocks are subject to clock skew, which can make\nthem inconsistent with causality. For example, see Figure 8-3, which shows a sce\u2010\nnario in which an operation that happened causally later was actually assigned a\nlower timestamp.viii\n\u2022 In the case of the block allocator, one operation may be given a sequence number\nin the range from 1,001 to 2,000, and a causally later operation may be given a\nnumber in the range from 1 to 1,000. Here, again, the sequence number is incon\u2010\nsistent with causality.\nLamport timestamps\nAlthough the three sequence number generators just described are inconsistent with\ncausality, there is actually a simple method for generating sequence numbers that is\nconsistent with causality. It is called a Lamport timestamp, proposed in 1978 by Leslie\nLamport [56], in what is now one of the most-cited papers in the field of distributed\nsystems.\nThe use of Lamport timestamps is illustrated in Figure 9-8. Each node has a unique\nidentifier, and each node keeps a counter of the number of operations it has pro\u2010\ncessed. The Lamport timestamp is then simply a pair of (counter, node ID). Two"}
{"368": "nodes may sometimes have the same counter value, but by including the node ID in\nthe timestamp, each timestamp is made unique.\nFigure 9-8. Lamport timestamps provide a total ordering consistent with causality.\nA Lamport timestamp bears no relationship to a physical time-of-day clock, but it\nprovides total ordering: if you have two timestamps, the one with a greater counter\nvalue is the greater timestamp; if the counter values are the same, the one with the\ngreater node ID is the greater timestamp.\nSo far this description is essentially the same as the even/odd counters described in\nthe last section. The key idea about Lamport timestamps, which makes them consis\u2010\ntent with causality, is the following: every node and every client keeps track of the\nmaximum counter value it has seen so far, and includes that maximum on every\nrequest. When a node receives a request or response with a maximum counter value\ngreater than its own counter value, it immediately increases its own counter to that\nmaximum.\nThis is shown in Figure 9-8, where client A receives a counter value of 5 from node 2,\nand then sends that maximum of 5 to node 1. At that time, node 1\u2019s counter was only\n1, but it was immediately moved forward to 5, so the next operation had an incre\u2010\nmented counter value of 6.\nAs long as the maximum counter value is carried along with every operation, this\nscheme ensures that the ordering from the Lamport timestamps is consistent with\ncausality, because every causal dependency results in an increased timestamp.\nLamport timestamps are sometimes confused with version vectors, which we saw in\n\u201cDetecting Concurrent Writes\u201d on page 184. Although there are some similarities,"}
{"369": "stamps, you cannot tell whether two operations are concurrent or whether they are\ncausally dependent. The advantage of Lamport timestamps over version vectors is\nthat they are more compact.\nTimestamp ordering is not sufficient\nAlthough Lamport timestamps define a total order of operations that is consistent\nwith causality, they are not quite sufficient to solve many common problems in dis\u2010\ntributed systems.\nFor example, consider a system that needs to ensure that a username uniquely identi\u2010\nfies a user account. If two users concurrently try to create an account with the same\nusername, one of the two should succeed and the other should fail. (We touched on\nthis problem previously in \u201cThe leader and the lock\u201d on page 301.)\nAt first glance, it seems as though a total ordering of operations (e.g., using Lamport\ntimestamps) should be sufficient to solve this problem: if two accounts with the same\nusername are created, pick the one with the lower timestamp as the winner (the one\nwho grabbed the username first), and let the one with the greater timestamp fail.\nSince timestamps are totally ordered, this comparison is always valid.\nThis approach works for determining the winner after the fact: once you have collec\u2010\nted all the username creation operations in the system, you can compare their time\u2010\nstamps. However, it is not sufficient when a node has just received a request from a\nuser to create a username, and needs to decide right now whether the request should\nsucceed or fail. At that moment, the node does not know whether another node is\nconcurrently in the process of creating an account with the same username, and what\ntimestamp that other node may assign to the operation.\nIn order to be sure that no other node is in the process of concurrently creating an\naccount with the same username and a lower timestamp, you would have to check\nwith every other node to see what it is doing [56]. If one of the other nodes has failed\nor cannot be reached due to a network problem, this system would grind to a halt.\nThis is not the kind of fault-tolerant system that we need.\nThe problem here is that the total order of operations only emerges after you have\ncollected all of the operations. If another node has generated some operations, but\nyou don\u2019t yet know what they are, you cannot construct the final ordering of opera\u2010\ntions: the unknown operations from the other node may need to be inserted at vari\u2010\nous positions in the total order.\nTo conclude: in order to implement something like a uniqueness constraint for user\u2010\nnames, it\u2019s not sufficient to have a total ordering of operations\u2014you also need to"}
{"370": "This idea of knowing when your total order is finalized is captured in the topic of\ntotal order broadcast.\nTotal Order Broadcast\nIf your program runs only on a single CPU core, it is easy to define a total ordering of\noperations: it is simply the order in which they were executed by the CPU. However,\nin a distributed system, getting all nodes to agree on the same total ordering of opera\u2010\ntions is tricky. In the last section we discussed ordering by timestamps or sequence\nnumbers, but found that it is not as powerful as single-leader replication (if you use\ntimestamp ordering to implement a uniqueness constraint, you cannot tolerate any\nfaults).\nAs discussed, single-leader replication determines a total order of operations by\nchoosing one node as the leader and sequencing all operations on a single CPU core\non the leader. The challenge then is how to scale the system if the throughput is\ngreater than a single leader can handle, and also how to handle failover if the leader\nfails (see \u201cHandling Node Outages\u201d on page 156). In the distributed systems litera\u2010\nture, this problem is known as total order broadcast or atomic broadcast [25, 57, 58].ix\nScope of ordering guarantee\nPartitioned databases with a single leader per partition often main\u2010\ntain ordering only per partition, which means they cannot offer\nconsistency guarantees (e.g., consistent snapshots, foreign key ref\u2010\nerences) across partitions. Total ordering across all partitions is\npossible, but requires additional coordination [59].\nTotal order broadcast is usually described as a protocol for exchanging messages\nbetween nodes. Informally, it requires that two safety properties always be satisfied:\nReliable delivery\nNo messages are lost: if a message is delivered to one node, it is delivered to all\nnodes.\nTotally ordered delivery\nMessages are delivered to every node in the same order.\nA correct algorithm for total order broadcast must ensure that the reliability and\nordering properties are always satisfied, even if a node or the network is faulty. Of"}
{"371": "course, messages will not be delivered while the network is interrupted, but an algo\u2010\nrithm can keep retrying so that the messages get through when the network is even\u2010\ntually repaired (and then they must still be delivered in the correct order).\nUsing total order broadcast\nConsensus services such as ZooKeeper and etcd actually implement total order\nbroadcast. This fact is a hint that there is a strong connection between total order\nbroadcast and consensus, which we will explore later in this chapter.\nTotal order broadcast is exactly what you need for database replication: if every mes\u2010\nsage represents a write to the database, and every replica processes the same writes in\nthe same order, then the replicas will remain consistent with each other (aside from\nany temporary replication lag). This principle is known as state machine replication\n[60], and we will return to it in Chapter 11.\nSimilarly, total order broadcast can be used to implement serializable transactions: as\ndiscussed in \u201cActual Serial Execution\u201d on page 252, if every message represents a\ndeterministic transaction to be executed as a stored procedure, and if every node pro\u2010\ncesses those messages in the same order, then the partitions and replicas of the data\u2010\nbase are kept consistent with each other [61].\nAn important aspect of total order broadcast is that the order is fixed at the time the\nmessages are delivered: a node is not allowed to retroactively insert a message into an\nearlier position in the order if subsequent messages have already been delivered. This\nfact makes total order broadcast stronger than timestamp ordering.\nAnother way of looking at total order broadcast is that it is a way of creating a log (as\nin a replication log, transaction log, or write-ahead log): delivering a message is like\nappending to the log. Since all nodes must deliver the same messages in the same\norder, all nodes can read the log and see the same sequence of messages.\nTotal order broadcast is also useful for implementing a lock service that provides\nfencing tokens (see \u201cFencing tokens\u201d on page 303). Every request to acquire the lock\nis appended as a message to the log, and all messages are sequentially numbered in\nthe order they appear in the log. The sequence number can then serve as a fencing\ntoken, because it is monotonically increasing. In ZooKeeper, this sequence number is\ncalled zxid [15]."}
{"372": "Implementing linearizable storage using total order broadcast\nAs illustrated in Figure 9-4, in a linearizable system there is a total order of opera\u2010\ntions. Does that mean linearizability is the same as total order broadcast? Not quite,\nbut there are close links between the two.x\nTotal order broadcast is asynchronous: messages are guaranteed to be delivered relia\u2010\nbly in a fixed order, but there is no guarantee about when a message will be delivered\n(so one recipient may lag behind the others). By contrast, linearizability is a recency\nguarantee: a read is guaranteed to see the latest value written.\nHowever, if you have total order broadcast, you can build linearizable storage on top\nof it. For example, you can ensure that usernames uniquely identify user accounts.\nImagine that for every possible username, you can have a linearizable register with an\natomic compare-and-set operation. Every register initially has the value null (indi\u2010\ncating that the username is not taken). When a user wants to create a username, you\nexecute a compare-and-set operation on the register for that username, setting it to\nthe user account ID, under the condition that the previous register value is null. If\nmultiple users try to concurrently grab the same username, only one of the compare-\nand-set operations will succeed, because the others will see a value other than null\n(due to linearizability).\nYou can implement such a linearizable compare-and-set operation as follows by\nusing total order broadcast as an append-only log [62, 63]:\n1. Append a message to the log, tentatively indicating the username you want to\nclaim.\n2. Read the log, and wait for the message you appended to be delivered back to\nyou.xi\n3. Check for any messages claiming the username that you want. If the first message\nfor your desired username is your own message, then you are successful: you can\ncommit the username claim (perhaps by appending another message to the log)\nand acknowledge it to the client. If the first message for your desired username is\nfrom another user, you abort the operation.\nx. In a formal sense, a linearizable read-write register is an \u201ceasier\u201d problem. Total order broadcast is equiva\u2010\nlent to consensus [67], which has no deterministic solution in the asynchronous crash-stop model [68],\nwhereas a linearizable read-write register can be implemented in the same system model [23, 24, 25]. How\u2010\never, supporting atomic operations such as compare-and-set or increment-and-get in a register makes it"}
{"373": "Because log entries are delivered to all nodes in the same order, if there are several\nconcurrent writes, all nodes will agree on which one came first. Choosing the first of\nthe conflicting writes as the winner and aborting later ones ensures that all nodes\nagree on whether a write was committed or aborted. A similar approach can be used\nto implement serializable multi-object transactions on top of a log [62].\nWhile this procedure ensures linearizable writes, it doesn\u2019t guarantee linearizable\nreads\u2014if you read from a store that is asynchronously updated from the log, it may\nbe stale. (To be precise, the procedure described here provides sequential consistency\n[47, 64], sometimes also known as timeline consistency [65, 66], a slightly weaker\nguarantee than linearizability.) To make reads linearizable, there are a few options:\n\u2022 You can sequence reads through the log by appending a message, reading the log,\nand performing the actual read when the message is delivered back to you. The\nmessage\u2019s position in the log thus defines the point in time at which the read\nhappens. (Quorum reads in etcd work somewhat like this [16].)\n\u2022 If the log allows you to fetch the position of the latest log message in a lineariza\u2010\nble way, you can query that position, wait for all entries up to that position to be\ndelivered to you, and then perform the read. (This is the idea behind Zoo\u2010\nKeeper\u2019s sync() operation [15].)\n\u2022 You can make your read from a replica that is synchronously updated on writes,\nand is thus sure to be up to date. (This technique is used in chain replication\n[63]; see also \u201cResearch on Replication\u201d on page 155.)\nImplementing total order broadcast using linearizable storage\nThe last section showed how to build a linearizable compare-and-set operation from\ntotal order broadcast. We can also turn it around, assume that we have linearizable\nstorage, and show how to build total order broadcast from it.\nThe easiest way is to assume you have a linearizable register that stores an integer and\nthat has an atomic increment-and-get operation [28]. Alternatively, an atomic\ncompare-and-set operation would also do the job.\nThe algorithm is simple: for every message you want to send through total order\nbroadcast, you increment-and-get the linearizable integer, and then attach the value\nyou got from the register as a sequence number to the message. You can then send\nthe message to all nodes (resending any lost messages), and the recipients will deliver\nthe messages consecutively by sequence number."}
{"374": "with Lamport timestamps\u2014in fact, this is the key difference between total order\nbroadcast and timestamp ordering.\nHow hard could it be to make a linearizable integer with an atomic increment-and-\nget operation? As usual, if things never failed, it would be easy: you could just keep it\nin a variable on one node. The problem lies in handling the situation when network\nconnections to that node are interrupted, and restoring the value when that node fails\n[59]. In general, if you think hard enough about linearizable sequence number gener\u2010\nators, you inevitably end up with a consensus algorithm.\nThis is no coincidence: it can be proved that a linearizable compare-and-set (or\nincrement-and-get) register and total order broadcast are both equivalent to consen\u2010\nsus [28, 67]. That is, if you can solve one of these problems, you can transform it into\na solution for the others. This is quite a profound and surprising insight!\nIt is time to finally tackle the consensus problem head-on, which we will do in the\nrest of this chapter.\nDistributed Transactions and Consensus\nConsensus is one of the most important and fundamental problems in distributed\ncomputing. On the surface, it seems simple: informally, the goal is simply to get sev\u2010\neral nodes to agree on something. You might think that this shouldn\u2019t be too hard.\nUnfortunately, many broken systems have been built in the mistaken belief that this\nproblem is easy to solve.\nAlthough consensus is very important, the section about it appears late in this book\nbecause the topic is quite subtle, and appreciating the subtleties requires some pre\u2010\nrequisite knowledge. Even in the academic research community, the understanding\nof consensus only gradually crystallized over the course of decades, with many mis\u2010\nunderstandings along the way. Now that we have discussed replication (Chapter 5),\ntransactions (Chapter 7), system models (Chapter 8), linearizability, and total order\nbroadcast (this chapter), we are finally ready to tackle the consensus problem.\nThere are a number of situations in which it is important for nodes to agree. For\nexample:\nLeader election\nIn a database with single-leader replication, all nodes need to agree on which\nnode is the leader. The leadership position might become contested if some\nnodes can\u2019t communicate with others due to a network fault. In this case, con\u2010\nsensus is important to avoid a bad failover, resulting in a split brain situation in"}
{"375": "Atomic commit\nIn a database that supports transactions spanning several nodes or partitions, we\nhave the problem that a transaction may fail on some nodes but succeed on oth\u2010\ners. If we want to maintain transaction atomicity (in the sense of ACID; see\n\u201cAtomicity\u201d on page 223), we have to get all nodes to agree on the outcome of the\ntransaction: either they all abort/roll back (if anything goes wrong) or they all\ncommit (if nothing goes wrong). This instance of consensus is known as the\natomic commit problem.xii\nThe Impossibility of Consensus\nYou may have heard about the FLP result [68]\u2014named after the authors Fischer,\nLynch, and Paterson\u2014which proves that there is no algorithm that is always able to\nreach consensus if there is a risk that a node may crash. In a distributed system, we\nmust assume that nodes may crash, so reliable consensus is impossible. Yet, here we\nare, discussing algorithms for achieving consensus. What is going on here?\nThe answer is that the FLP result is proved in the asynchronous system model (see\n\u201cSystem Model and Reality\u201d on page 306), a very restrictive model that assumes a\ndeterministic algorithm that cannot use any clocks or timeouts. If the algorithm is\nallowed to use timeouts, or some other way of identifying suspected crashed nodes\n(even if the suspicion is sometimes wrong), then consensus becomes solvable [67].\nEven just allowing the algorithm to use random numbers is sufficient to get around\nthe impossibility result [69].\nThus, although the FLP result about the impossibility of consensus is of great theoret\u2010\nical importance, distributed systems can usually achieve consensus in practice.\nIn this section we will first examine the atomic commit problem in more detail. In\nparticular, we will discuss the two-phase commit (2PC) algorithm, which is the most\ncommon way of solving atomic commit and which is implemented in various data\u2010\nbases, messaging systems, and application servers. It turns out that 2PC is a kind of\nconsensus algorithm\u2014but not a very good one [70, 71].\nBy learning from 2PC we will then work our way toward better consensus algorithms,\nsuch as those used in ZooKeeper (Zab) and etcd (Raft)."}
{"376": "Atomic Commit and Two-Phase Commit (2PC)\nIn Chapter 7 we learned that the purpose of transaction atomicity is to provide sim\u2010\nple semantics in the case where something goes wrong in the middle of making sev\u2010\neral writes. The outcome of a transaction is either a successful commit, in which case\nall of the transaction\u2019s writes are made durable, or an abort, in which case all of the\ntransaction\u2019s writes are rolled back (i.e., undone or discarded).\nAtomicity prevents failed transactions from littering the database with half-finished\nresults and half-updated state. This is especially important for multi-object transac\u2010\ntions (see \u201cSingle-Object and Multi-Object Operations\u201d on page 228) and databases\nthat maintain secondary indexes. Each secondary index is a separate data structure\nfrom the primary data\u2014thus, if you modify some data, the corresponding change\nneeds to also be made in the secondary index. Atomicity ensures that the secondary\nindex stays consistent with the primary data (if the index became inconsistent with\nthe primary data, it would not be very useful).\nFrom single-node to distributed atomic commit\nFor transactions that execute at a single database node, atomicity is commonly imple\u2010\nmented by the storage engine. When the client asks the database node to commit the\ntransaction, the database makes the transaction\u2019s writes durable (typically in a write-\nahead log; see \u201cMaking B-trees reliable\u201d on page 82) and then appends a commit\nrecord to the log on disk. If the database crashes in the middle of this process, the\ntransaction is recovered from the log when the node restarts: if the commit record\nwas successfully written to disk before the crash, the transaction is considered com\u2010\nmitted; if not, any writes from that transaction are rolled back.\nThus, on a single node, transaction commitment crucially depends on the order in\nwhich data is durably written to disk: first the data, then the commit record [72]. The\nkey deciding moment for whether the transaction commits or aborts is the moment\nat which the disk finishes writing the commit record: before that moment, it is still\npossible to abort (due to a crash), but after that moment, the transaction is commit\u2010\nted (even if the database crashes). Thus, it is a single device (the controller of one par\u2010\nticular disk drive, attached to one particular node) that makes the commit atomic.\nHowever, what if multiple nodes are involved in a transaction? For example, perhaps\nyou have a multi-object transaction in a partitioned database, or a term-partitioned\nsecondary index (in which the index entry may be on a different node from the pri\u2010\nmary data; see \u201cPartitioning and Secondary Indexes\u201d on page 206). Most \u201cNoSQL\u201d\ndistributed datastores do not support such distributed transactions, but various clus\u2010"}
{"377": "happen that the commit succeeds on some nodes and fails on other nodes, which\nwould violate the atomicity guarantee:\n\u2022 Some nodes may detect a constraint violation or conflict, making an abort neces\u2010\nsary, while other nodes are successfully able to commit.\n\u2022 Some of the commit requests might be lost in the network, eventually aborting\ndue to a timeout, while other commit requests get through.\n\u2022 Some nodes may crash before the commit record is fully written and roll back on\nrecovery, while others successfully commit.\nIf some nodes commit the transaction but others abort it, the nodes become inconsis\u2010\ntent with each other (like in Figure 7-3). And once a transaction has been committed\non one node, it cannot be retracted again if it later turns out that it was aborted on\nanother node. For this reason, a node must only commit once it is certain that all\nother nodes in the transaction are also going to commit.\nA transaction commit must be irrevocable\u2014you are not allowed to change your\nmind and retroactively abort a transaction after it has been committed. The reason\nfor this rule is that once data has been committed, it becomes visible to other transac\u2010\ntions, and thus other clients may start relying on that data; this principle forms the\nbasis of read committed isolation, discussed in \u201cRead Committed\u201d on page 234. If a\ntransaction was allowed to abort after committing, any transactions that read the\ncommitted data would be based on data that was retroactively declared not to have\nexisted\u2014so they would have to be reverted as well.\n(It is possible for the effects of a committed transaction to later be undone by\nanother, compensating transaction [73, 74]. However, from the database\u2019s point of\nview this is a separate transaction, and thus any cross-transaction correctness\nrequirements are the application\u2019s problem.)\nIntroduction to two-phase commit\nTwo-phase commit is an algorithm for achieving atomic transaction commit across\nmultiple nodes\u2014i.e., to ensure that either all nodes commit or all nodes abort. It is a\nclassic algorithm in distributed databases [13, 35, 75]. 2PC is used internally in some\ndatabases and also made available to applications in the form of XA transactions [76,\n77] (which are supported by the Java Transaction API, for example) or via WS-\nAtomicTransaction for SOAP web services [78, 79].\nThe basic flow of 2PC is illustrated in Figure 9-9. Instead of a single commit request,\nas with a single-node transaction, the commit/abort process in 2PC is split into two"}
{"378": "Figure 9-9. A successful execution of two-phase commit (2PC).\nDon\u2019t confuse 2PC and 2PL\nTwo-phase commit (2PC) and two-phase locking (see \u201cTwo-Phase\nLocking (2PL)\u201d on page 257) are two very different things. 2PC\nprovides atomic commit in a distributed database, whereas 2PL\nprovides serializable isolation. To avoid confusion, it\u2019s best to think\nof them as entirely separate concepts and to ignore the unfortunate\nsimilarity in the names.\n2PC uses a new component that does not normally appear in single-node transac\u2010\ntions: a coordinator (also known as transaction manager). The coordinator is often\nimplemented as a library within the same application process that is requesting the\ntransaction (e.g., embedded in a Java EE container), but it can also be a separate pro\u2010\ncess or service. Examples of such coordinators include Narayana, JOTM, BTM, or\nMSDTC.\nA 2PC transaction begins with the application reading and writing data on multiple\ndatabase nodes, as normal. We call these database nodes participants in the transac\u2010\ntion. When the application is ready to commit, the coordinator begins phase 1: it\nsends a prepare request to each of the nodes, asking them whether they are able to\ncommit. The coordinator then tracks the responses from the participants:\n\u2022 If all participants reply \u201cyes,\u201d indicating they are ready to commit, then the coor\u2010\ndinator sends out a commit request in phase 2, and the commit actually takes\nplace.\n\u2022 If any of the participants replies \u201cno,\u201d the coordinator sends an abort request to\nall nodes in phase 2."}
{"379": "acknowledgments, the minister pronounces the couple husband and wife: the trans\u2010\naction is committed, and the happy fact is broadcast to all attendees. If either bride or\ngroom does not say \u201cyes,\u201d the ceremony is aborted [73].\nA system of promises\nFrom this short description it might not be clear why two-phase commit ensures\natomicity, while one-phase commit across several nodes does not. Surely the prepare\nand commit requests can just as easily be lost in the two-phase case. What makes 2PC\ndifferent?\nTo understand why it works, we have to break down the process in a bit more detail:\n1. When the application wants to begin a distributed transaction, it requests a\ntransaction ID from the coordinator. This transaction ID is globally unique.\n2. The application begins a single-node transaction on each of the participants, and\nattaches the globally unique transaction ID to the single-node transaction. All\nreads and writes are done in one of these single-node transactions. If anything\ngoes wrong at this stage (for example, a node crashes or a request times out), the\ncoordinator or any of the participants can abort.\n3. When the application is ready to commit, the coordinator sends a prepare\nrequest to all participants, tagged with the global transaction ID. If any of these\nrequests fails or times out, the coordinator sends an abort request for that trans\u2010\naction ID to all participants.\n4. When a participant receives the prepare request, it makes sure that it can defi\u2010\nnitely commit the transaction under all circumstances. This includes writing all\ntransaction data to disk (a crash, a power failure, or running out of disk space is\nnot an acceptable excuse for refusing to commit later), and checking for any con\u2010\nflicts or constraint violations. By replying \u201cyes\u201d to the coordinator, the node\npromises to commit the transaction without error if requested. In other words,\nthe participant surrenders the right to abort the transaction, but without actually\ncommitting it.\n5. When the coordinator has received responses to all prepare requests, it makes a\ndefinitive decision on whether to commit or abort the transaction (committing\nonly if all participants voted \u201cyes\u201d). The coordinator must write that decision to\nits transaction log on disk so that it knows which way it decided in case it subse\u2010\nquently crashes. This is called the commit point.\n6. Once the coordinator\u2019s decision has been written to disk, the commit or abort\nrequest is sent to all participants. If this request fails or times out, the coordinator"}
{"380": "mitted when it recovers\u2014since the participant voted \u201cyes,\u201d it cannot refuse to\ncommit when it recovers.\nThus, the protocol contains two crucial \u201cpoints of no return\u201d: when a participant\nvotes \u201cyes,\u201d it promises that it will definitely be able to commit later (although the\ncoordinator may still choose to abort); and once the coordinator decides, that deci\u2010\nsion is irrevocable. Those promises ensure the atomicity of 2PC. (Single-node atomic\ncommit lumps these two events into one: writing the commit record to the transac\u2010\ntion log.)\nReturning to the marriage analogy, before saying \u201cI do,\u201d you and your bride/groom\nhave the freedom to abort the transaction by saying \u201cNo way!\u201d (or something to that\neffect). However, after saying \u201cI do,\u201d you cannot retract that statement. If you faint\nafter saying \u201cI do\u201d and you don\u2019t hear the minister speak the words \u201cYou are now\nhusband and wife,\u201d that doesn\u2019t change the fact that the transaction was committed.\nWhen you recover consciousness later, you can find out whether you are married or\nnot by querying the minister for the status of your global transaction ID, or you can\nwait for the minister\u2019s next retry of the commit request (since the retries will have\ncontinued throughout your period of unconsciousness).\nCoordinator failure\nWe have discussed what happens if one of the participants or the network fails during\n2PC: if any of the prepare requests fail or time out, the coordinator aborts the trans\u2010\naction; if any of the commit or abort requests fail, the coordinator retries them indefi\u2010\nnitely. However, it is less clear what happens if the coordinator crashes.\nIf the coordinator fails before sending the prepare requests, a participant can safely\nabort the transaction. But once the participant has received a prepare request and\nvoted \u201cyes,\u201d it can no longer abort unilaterally\u2014it must wait to hear back from the\ncoordinator whether the transaction was committed or aborted. If the coordinator\ncrashes or the network fails at this point, the participant can do nothing but wait. A\nparticipant\u2019s transaction in this state is called in doubt or uncertain.\nThe situation is illustrated in Figure 9-10. In this particular example, the coordinator\nactually decided to commit, and database 2 received the commit request. However,\nthe coordinator crashed before it could send the commit request to database 1, and so\ndatabase 1 does not know whether to commit or abort. Even a timeout does not help\nhere: if database 1 unilaterally aborts after a timeout, it will end up inconsistent with\ndatabase 2, which has committed. Similarly, it is not safe to unilaterally commit,\nbecause another participant may have aborted."}
{"381": "Figure 9-10. The coordinator crashes after participants vote \u201cyes.\u201d Database 1 does not\nknow whether to commit or abort.\nWithout hearing from the coordinator, the participant has no way of knowing\nwhether to commit or abort. In principle, the participants could communicate among\nthemselves to find out how each participant voted and come to some agreement, but\nthat is not part of the 2PC protocol.\nThe only way 2PC can complete is by waiting for the coordinator to recover. This is\nwhy the coordinator must write its commit or abort decision to a transaction log on\ndisk before sending commit or abort requests to participants: when the coordinator\nrecovers, it determines the status of all in-doubt transactions by reading its transac\u2010\ntion log. Any transactions that don\u2019t have a commit record in the coordinator\u2019s log\nare aborted. Thus, the commit point of 2PC comes down to a regular single-node\natomic commit on the coordinator.\nThree-phase commit\nTwo-phase commit is called a blocking atomic commit protocol due to the fact that\n2PC can become stuck waiting for the coordinator to recover. In theory, it is possible\nto make an atomic commit protocol nonblocking, so that it does not get stuck if a\nnode fails. However, making this work in practice is not so straightforward.\nAs an alternative to 2PC, an algorithm called three-phase commit (3PC) has been pro\u2010\nposed [13, 80]. However, 3PC assumes a network with bounded delay and nodes with\nbounded response times; in most practical systems with unbounded network delay\nand process pauses (see Chapter 8), it cannot guarantee atomicity.\nIn general, nonblocking atomic commit requires a perfect failure detector [67, 71]\u2014\ni.e., a reliable mechanism for telling whether a node has crashed or not. In a network"}
{"382": "Distributed Transactions in Practice\nDistributed transactions, especially those implemented with two-phase commit, have\na mixed reputation. On the one hand, they are seen as providing an important safety\nguarantee that would be hard to achieve otherwise; on the other hand, they are criti\u2010\ncized for causing operational problems, killing performance, and promising more\nthan they can deliver [81, 82, 83, 84]. Many cloud services choose not to implement\ndistributed transactions due to the operational problems they engender [85, 86].\nSome implementations of distributed transactions carry a heavy performance penalty\n\u2014for example, distributed transactions in MySQL are reported to be over 10 times\nslower than single-node transactions [87], so it is not surprising when people advise\nagainst using them. Much of the performance cost inherent in two-phase commit is\ndue to the additional disk forcing (fsync) that is required for crash recovery [88], and\nthe additional network round-trips.\nHowever, rather than dismissing distributed transactions outright, we should exam\u2010\nine them in some more detail, because there are important lessons to be learned from\nthem. To begin, we should be precise about what we mean by \u201cdistributed transac\u2010\ntions.\u201d Two quite different types of distributed transactions are often conflated:\nDatabase-internal distributed transactions\nSome distributed databases (i.e., databases that use replication and partitioning\nin their standard configuration) support internal transactions among the nodes\nof that database. For example, VoltDB and MySQL Cluster\u2019s NDB storage engine\nhave such internal transaction support. In this case, all the nodes participating in\nthe transaction are running the same database software.\nHeterogeneous distributed transactions\nIn a heterogeneous transaction, the participants are two or more different tech\u2010\nnologies: for example, two databases from different vendors, or even non-\ndatabase systems such as message brokers. A distributed transaction across these\nsystems must ensure atomic commit, even though the systems may be entirely\ndifferent under the hood.\nDatabase-internal transactions do not have to be compatible with any other system,\nso they can use any protocol and apply optimizations specific to that particular tech\u2010\nnology. For that reason, database-internal distributed transactions can often work\nquite well. On the other hand, transactions spanning heterogeneous technologies are\na lot more challenging.\nExactly-once message processing"}
{"383": "successfully committed. This is implemented by atomically committing the message\nacknowledgment and the database writes in a single transaction. With distributed\ntransaction support, this is possible, even if the message broker and the database are\ntwo unrelated technologies running on different machines.\nIf either the message delivery or the database transaction fails, both are aborted, and\nso the message broker may safely redeliver the message later. Thus, by atomically\ncommitting the message and the side effects of its processing, we can ensure that the\nmessage is effectively processed exactly once, even if it required a few retries before it\nsucceeded. The abort discards any side effects of the partially completed transaction.\nSuch a distributed transaction is only possible if all systems affected by the transac\u2010\ntion are able to use the same atomic commit protocol, however. For example, say a\nside effect of processing a message is to send an email, and the email server does not\nsupport two-phase commit: it could happen that the email is sent two or more times\nif message processing fails and is retried. But if all side effects of processing a message\nare rolled back on transaction abort, then the processing step can safely be retried as\nif nothing had happened.\nWe will return to the topic of exactly-once message processing in Chapter 11. Let\u2019s\nlook first at the atomic commit protocol that allows such heterogeneous distributed\ntransactions.\nXA transactions\nX/Open XA (short for eXtended Architecture) is a standard for implementing two-\nphase commit across heterogeneous technologies [76, 77]. It was introduced in 1991\nand has been widely implemented: XA is supported by many traditional relational\ndatabases (including PostgreSQL, MySQL, DB2, SQL Server, and Oracle) and mes\u2010\nsage brokers (including ActiveMQ, HornetQ, MSMQ, and IBM MQ).\nXA is not a network protocol\u2014it is merely a C API for interfacing with a transaction\ncoordinator. Bindings for this API exist in other languages; for example, in the world\nof Java EE applications, XA transactions are implemented using the Java Transaction\nAPI (JTA), which in turn is supported by many drivers for databases using Java Data\u2010\nbase Connectivity (JDBC) and drivers for message brokers using the Java Message\nService (JMS) APIs.\nXA assumes that your application uses a network driver or client library to commu\u2010\nnicate with the participant databases or messaging services. If the driver supports XA,\nthat means it calls the XA API to find out whether an operation should be part of a\ndistributed transaction\u2014and if so, it sends the necessary information to the database"}
{"384": "The transaction coordinator implements the XA API. The standard does not specify\nhow it should be implemented, but in practice the coordinator is often simply a\nlibrary that is loaded into the same process as the application issuing the transaction\n(not a separate service). It keeps track of the participants in a transaction, collects\npartipants\u2019 responses after asking them to prepare (via a callback into the driver), and\nuses a log on the local disk to keep track of the commit/abort decision for each trans\u2010\naction.\nIf the application process crashes, or the machine on which the application is running\ndies, the coordinator goes with it. Any participants with prepared but uncommitted\ntransactions are then stuck in doubt. Since the coordinator\u2019s log is on the application\nserver\u2019s local disk, that server must be restarted, and the coordinator library must\nread the log to recover the commit/abort outcome of each transaction. Only then can\nthe coordinator use the database driver\u2019s XA callbacks to ask participants to commit\nor abort, as appropriate. The database server cannot contact the coordinator directly,\nsince all communication must go via its client library.\nHolding locks while in doubt\nWhy do we care so much about a transaction being stuck in doubt? Can\u2019t the rest of\nthe system just get on with its work, and ignore the in-doubt transaction that will be\ncleaned up eventually?\nThe problem is with locking. As discussed in \u201cRead Committed\u201d on page 234, data\u2010\nbase transactions usually take a row-level exclusive lock on any rows they modify, to\nprevent dirty writes. In addition, if you want serializable isolation, a database using\ntwo-phase locking would also have to take a shared lock on any rows read by the\ntransaction (see \u201cTwo-Phase Locking (2PL)\u201d on page 257).\nThe database cannot release those locks until the transaction commits or aborts\n(illustrated as a shaded area in Figure 9-9). Therefore, when using two-phase commit,\na transaction must hold onto the locks throughout the time it is in doubt. If the coor\u2010\ndinator has crashed and takes 20 minutes to start up again, those locks will be held\nfor 20 minutes. If the coordinator\u2019s log is entirely lost for some reason, those locks\nwill be held forever\u2014or at least until the situation is manually resolved by an admin\u2010\nistrator.\nWhile those locks are held, no other transaction can modify those rows. Depending\non the database, other transactions may even be blocked from reading those rows.\nThus, other transactions cannot simply continue with their business\u2014if they want to\naccess that same data, they will be blocked. This can cause large parts of your applica\u2010\ntion to become unavailable until the in-doubt transaction is resolved."}
{"385": "Recovering from coordinator failure\nIn theory, if the coordinator crashes and is restarted, it should cleanly recover its state\nfrom the log and resolve any in-doubt transactions. However, in practice, orphaned\nin-doubt transactions do occur [89, 90]\u2014that is, transactions for which the coordina\u2010\ntor cannot decide the outcome for whatever reason (e.g., because the transaction log\nhas been lost or corrupted due to a software bug). These transactions cannot be\nresolved automatically, so they sit forever in the database, holding locks and blocking\nother transactions.\nEven rebooting your database servers will not fix this problem, since a correct imple\u2010\nmentation of 2PC must preserve the locks of an in-doubt transaction even across\nrestarts (otherwise it would risk violating the atomicity guarantee). It\u2019s a sticky\nsituation.\nThe only way out is for an administrator to manually decide whether to commit or\nroll back the transactions. The administrator must examine the participants of each\nin-doubt transaction, determine whether any participant has committed or aborted\nalready, and then apply the same outcome to the other participants. Resolving the\nproblem potentially requires a lot of manual effort, and most likely needs to be done\nunder high stress and time pressure during a serious production outage (otherwise,\nwhy would the coordinator be in such a bad state?).\nMany XA implementations have an emergency escape hatch called heuristic decisions:\nallowing a participant to unilaterally decide to abort or commit an in-doubt transac\u2010\ntion without a definitive decision from the coordinator [76, 77, 91]. To be clear, heu\u2010\nristic here is a euphemism for probably breaking atomicity, since it violates the system\nof promises in two-phase commit. Thus, heuristic decisions are intended only for\ngetting out of catastrophic situations, and not for regular use.\nLimitations of distributed transactions\nXA transactions solve the real and important problem of keeping several participant\ndata systems consistent with each other, but as we have seen, they also introduce\nmajor operational problems. In particular, the key realization is that the transaction\ncoordinator is itself a kind of database (in which transaction outcomes are stored),\nand so it needs to be approached with the same care as any other important database:\n\u2022 If the coordinator is not replicated but runs only on a single machine, it is a sin\u2010\ngle point of failure for the entire system (since its failure causes other application\nservers to block on locks held by in-doubt transactions). Surprisingly, many\ncoordinator implementations are not highly available by default, or have only"}
{"386": "that application servers can be added and removed at will. However, when the\ncoordinator is part of the application server, it changes the nature of the deploy\u2010\nment. Suddenly, the coordinator\u2019s logs become a crucial part of the durable sys\u2010\ntem state\u2014as important as the databases themselves, since the coordinator logs\nare required in order to recover in-doubt transactions after a crash. Such applica\u2010\ntion servers are no longer stateless.\n\u2022 Since XA needs to be compatible with a wide range of data systems, it is necessar\u2010\nily a lowest common denominator. For example, it cannot detect deadlocks\nacross different systems (since that would require a standardized protocol for\nsystems to exchange information on the locks that each transaction is waiting\nfor), and it does not work with SSI (see \u201cSerializable Snapshot Isolation (SSI)\u201d on\npage 261), since that would require a protocol for identifying conflicts across dif\u2010\nferent systems.\n\u2022 For database-internal distributed transactions (not XA), the limitations are not\nso great\u2014for example, a distributed version of SSI is possible. However, there\nremains the problem that for 2PC to successfully commit a transaction, all par\u2010\nticipants must respond. Consequently, if any part of the system is broken, the\ntransaction also fails. Distributed transactions thus have a tendency of amplifying\nfailures, which runs counter to our goal of building fault-tolerant systems.\nDo these facts mean we should give up all hope of keeping several systems consistent\nwith each other? Not quite\u2014there are alternative methods that allow us to achieve\nthe same thing without the pain of heterogeneous distributed transactions. We will\nreturn to these in Chapters 11 and 12. But first, we should wrap up the topic of\nconsensus.\nFault-Tolerant Consensus\nInformally, consensus means getting several nodes to agree on something. For exam\u2010\nple, if several people concurrently try to book the last seat on an airplane, or the same\nseat in a theater, or try to register an account with the same username, then a consen\u2010\nsus algorithm could be used to determine which one of these mutually incompatible\noperations should be the winner.\nThe consensus problem is normally formalized as follows: one or more nodes may\npropose values, and the consensus algorithm decides on one of those values. In the\nseat-booking example, when several customers are concurrently trying to buy the last\nseat, each node handling a customer request may propose the ID of the customer it is\nserving, and the decision indicates which one of those customers got the seat."}
{"387": "In this formalism, a consensus algorithm must satisfy the following properties [25]:xiii\nUniform agreement\nNo two nodes decide differently.\nIntegrity\nNo node decides twice.\nValidity\nIf a node decides value v, then v was proposed by some node.\nTermination\nEvery node that does not crash eventually decides some value.\nThe uniform agreement and integrity properties define the core idea of consensus:\neveryone decides on the same outcome, and once you have decided, you cannot\nchange your mind. The validity property exists mostly to rule out trivial solutions: for\nexample, you could have an algorithm that always decides null, no matter what was\nproposed; this algorithm would satisfy the agreement and integrity properties, but\nnot the validity property.\nIf you don\u2019t care about fault tolerance, then satisfying the first three properties is\neasy: you can just hardcode one node to be the \u201cdictator,\u201d and let that node make all\nof the decisions. However, if that one node fails, then the system can no longer make\nany decisions. This is, in fact, what we saw in the case of two-phase commit: if the\ncoordinator fails, in-doubt participants cannot decide whether to commit or abort.\nThe termination property formalizes the idea of fault tolerance. It essentially says that\na consensus algorithm cannot simply sit around and do nothing forever\u2014in other\nwords, it must make progress. Even if some nodes fail, the other nodes must still\nreach a decision. (Termination is a liveness property, whereas the other three are\nsafety properties\u2014see \u201cSafety and liveness\u201d on page 308.)\nThe system model of consensus assumes that when a node \u201ccrashes,\u201d it suddenly dis\u2010\nappears and never comes back. (Instead of a software crash, imagine that there is an\nearthquake, and the datacenter containing your node is destroyed by a landslide. You\nmust assume that your node is buried under 30 feet of mud and is never going to\ncome back online.) In this system model, any algorithm that has to wait for a node to\nrecover is not going to be able to satisfy the termination property. In particular, 2PC\ndoes not meet the requirements for termination."}
{"388": "Of course, if all nodes crash and none of them are running, then it is not possible for\nany algorithm to decide anything. There is a limit to the number of failures that an\nalgorithm can tolerate: in fact, it can be proved that any consensus algorithm requires\nat least a majority of nodes to be functioning correctly in order to assure termination\n[67]. That majority can safely form a quorum (see \u201cQuorums for reading and writ\u2010\ning\u201d on page 179).\nThus, the termination property is subject to the assumption that fewer than half of\nthe nodes are crashed or unreachable. However, most implementations of consensus\nensure that the safety properties\u2014agreement, integrity, and validity\u2014are always met,\neven if a majority of nodes fail or there is a severe network problem [92]. Thus, a\nlarge-scale outage can stop the system from being able to process requests, but it can\u2010\nnot corrupt the consensus system by causing it to make invalid decisions.\nMost consensus algorithms assume that there are no Byzantine faults, as discussed in\n\u201cByzantine Faults\u201d on page 304. That is, if a node does not correctly follow the proto\u2010\ncol (for example, if it sends contradictory messages to different nodes), it may break\nthe safety properties of the protocol. It is possible to make consensus robust against\nByzantine faults as long as fewer than one-third of the nodes are Byzantine-faulty [25,\n93], but we don\u2019t have space to discuss those algorithms in detail in this book.\nConsensus algorithms and total order broadcast\nThe best-known fault-tolerant consensus algorithms are Viewstamped Replication\n(VSR) [94, 95], Paxos [96, 97, 98, 99], Raft [22, 100, 101], and Zab [15, 21, 102]. There\nare quite a few similarities between these algorithms, but they are not the same [103].\nIn this book we won\u2019t go into full details of the different algorithms: it\u2019s sufficient to\nbe aware of some of the high-level ideas that they have in common, unless you\u2019re\nimplementing a consensus system yourself (which is probably not advisable\u2014it\u2019s\nhard [98, 104]).\nMost of these algorithms actually don\u2019t directly use the formal model described here\n(proposing and deciding on a single value, while satisfying the agreement, integrity,\nvalidity, and termination properties). Instead, they decide on a sequence of values,\nwhich makes them total order broadcast algorithms, as discussed previously in this\nchapter (see \u201cTotal Order Broadcast\u201d on page 348).\nRemember that total order broadcast requires messages to be delivered exactly once,\nin the same order, to all nodes. If you think about it, this is equivalent to performing\nseveral rounds of consensus: in each round, nodes propose the message that they\nwant to send next, and then decide on the next message to be delivered in the total\norder [67]."}
{"389": "\u2022 Due to the agreement property of consensus, all nodes decide to deliver the same\nmessages in the same order.\n\u2022 Due to the integrity property, messages are not duplicated.\n\u2022 Due to the validity property, messages are not corrupted and not fabricated out\nof thin air.\n\u2022 Due to the termination property, messages are not lost.\nViewstamped Replication, Raft, and Zab implement total order broadcast directly,\nbecause that is more efficient than doing repeated rounds of one-value-at-a-time\nconsensus. In the case of Paxos, this optimization is known as Multi-Paxos.\nSingle-leader replication and consensus\nIn Chapter 5 we discussed single-leader replication (see \u201cLeaders and Followers\u201d on\npage 152), which takes all the writes to the leader and applies them to the followers in\nthe same order, thus keeping replicas up to date. Isn\u2019t this essentially total order\nbroadcast? How come we didn\u2019t have to worry about consensus in Chapter 5?\nThe answer comes down to how the leader is chosen. If the leader is manually chosen\nand configured by the humans in your operations team, you essentially have a \u201ccon\u2010\nsensus algorithm\u201d of the dictatorial variety: only one node is allowed to accept writes\n(i.e., make decisions about the order of writes in the replication log), and if that node\ngoes down, the system becomes unavailable for writes until the operators manually\nconfigure a different node to be the leader. Such a system can work well in practice,\nbut it does not satisfy the termination property of consensus because it requires\nhuman intervention in order to make progress.\nSome databases perform automatic leader election and failover, promoting a follower\nto be the new leader if the old leader fails (see \u201cHandling Node Outages\u201d on page\n156). This brings us closer to fault-tolerant total order broadcast, and thus to solving\nconsensus.\nHowever, there is a problem. We previously discussed the problem of split brain, and\nsaid that all nodes need to agree who the leader is\u2014otherwise two different nodes\ncould each believe themselves to be the leader, and consequently get the database into\nan inconsistent state. Thus, we need consensus in order to elect a leader. But if the\nconsensus algorithms described here are actually total order broadcast algorithms,\nand total order broadcast is like single-leader replication, and single-leader replica\u2010\ntion requires a leader, then\u2026\nIt seems that in order to elect a leader, we first need a leader. In order to solve con\u2010"}
{"390": "Epoch numbering and quorums\nAll of the consensus protocols discussed so far internally use a leader in some form or\nanother, but they don\u2019t guarantee that the leader is unique. Instead, they can make a\nweaker guarantee: the protocols define an epoch number (called the ballot number in\nPaxos, view number in Viewstamped Replication, and term number in Raft) and\nguarantee that within each epoch, the leader is unique.\nEvery time the current leader is thought to be dead, a vote is started among the nodes\nto elect a new leader. This election is given an incremented epoch number, and thus\nepoch numbers are totally ordered and monotonically increasing. If there is a conflict\nbetween two different leaders in two different epochs (perhaps because the previous\nleader actually wasn\u2019t dead after all), then the leader with the higher epoch number\nprevails.\nBefore a leader is allowed to decide anything, it must first check that there isn\u2019t some\nother leader with a higher epoch number which might take a conflicting decision.\nHow does a leader know that it hasn\u2019t been ousted by another node? Recall \u201cThe\nTruth Is Defined by the Majority\u201d on page 300: a node cannot necessarily trust its\nown judgment\u2014just because a node thinks that it is the leader, that does not neces\u2010\nsarily mean the other nodes accept it as their leader.\nInstead, it must collect votes from a quorum of nodes (see \u201cQuorums for reading and\nwriting\u201d on page 179). For every decision that a leader wants to make, it must send\nthe proposed value to the other nodes and wait for a quorum of nodes to respond in\nfavor of the proposal. The quorum typically, but not always, consists of a majority of\nnodes [105]. A node votes in favor of a proposal only if it is not aware of any other\nleader with a higher epoch.\nThus, we have two rounds of voting: once to choose a leader, and a second time to\nvote on a leader\u2019s proposal. The key insight is that the quorums for those two votes\nmust overlap: if a vote on a proposal succeeds, at least one of the nodes that voted for\nit must have also participated in the most recent leader election [105]. Thus, if the\nvote on a proposal does not reveal any higher-numbered epoch, the current leader\ncan conclude that no leader election with a higher epoch number has happened, and\ntherefore be sure that it still holds the leadership. It can then safely decide the pro\u2010\nposed value.\nThis voting process looks superficially similar to two-phase commit. The biggest dif\u2010\nferences are that in 2PC the coordinator is not elected, and that fault-tolerant consen\u2010\nsus algorithms only require votes from a majority of nodes, whereas 2PC requires a\n\u201cyes\u201d vote from every participant. Moreover, consensus algorithms define a recovery"}
{"391": "Limitations of consensus\nConsensus algorithms are a huge breakthrough for distributed systems: they bring\nconcrete safety properties (agreement, integrity, and validity) to systems where every\u2010\nthing else is uncertain, and they nevertheless remain fault-tolerant (able to make pro\u2010\ngress as long as a majority of nodes are working and reachable). They provide total\norder broadcast, and therefore they can also implement linearizable atomic opera\u2010\ntions in a fault-tolerant way (see \u201cImplementing linearizable storage using total order\nbroadcast\u201d on page 350).\nNevertheless, they are not used everywhere, because the benefits come at a cost.\nThe process by which nodes vote on proposals before they are decided is a kind of\nsynchronous replication. As discussed in \u201cSynchronous Versus Asynchronous Repli\u2010\ncation\u201d on page 153, databases are often configured to use asynchronous replication.\nIn this configuration, some committed data can potentially be lost on failover\u2014but\nmany people choose to accept this risk for the sake of better performance.\nConsensus systems always require a strict majority to operate. This means you need a\nminimum of three nodes in order to tolerate one failure (the remaining two out of\nthree form a majority), or a minimum of five nodes to tolerate two failures (the\nremaining three out of five form a majority). If a network failure cuts off some nodes\nfrom the rest, only the majority portion of the network can make progress, and the\nrest is blocked (see also \u201cThe Cost of Linearizability\u201d on page 335).\nMost consensus algorithms assume a fixed set of nodes that participate in voting,\nwhich means that you can\u2019t just add or remove nodes in the cluster. Dynamic mem\u2010\nbership extensions to consensus algorithms allow the set of nodes in the cluster to\nchange over time, but they are much less well understood than static membership\nalgorithms.\nConsensus systems generally rely on timeouts to detect failed nodes. In environments\nwith highly variable network delays, especially geographically distributed systems, it\noften happens that a node falsely believes the leader to have failed due to a transient\nnetwork issue. Although this error does not harm the safety properties, frequent\nleader elections result in terrible performance because the system can end up spend\u2010\ning more time choosing a leader than doing any useful work.\nSometimes, consensus algorithms are particularly sensitive to network problems. For\nexample, Raft has been shown to have unpleasant edge cases [106]: if the entire net\u2010\nwork is working correctly except for one particular network link that is consistently\nunreliable, Raft can get into situations where leadership continually bounces between\ntwo nodes, or the current leader is continually forced to resign, so the system effec\u2010"}
{"392": "Membership and Coordination Services\nProjects like ZooKeeper or etcd are often described as \u201cdistributed key-value stores\u201d\nor \u201ccoordination and configuration services.\u201d The API of such a service looks pretty\nmuch like that of a database: you can read and write the value for a given key, and\niterate over keys. So if they\u2019re basically databases, why do they go to all the effort of\nimplementing a consensus algorithm? What makes them different from any other\nkind of database?\nTo understand this, it is helpful to briefly explore how a service like ZooKeeper is\nused. As an application developer, you will rarely need to use ZooKeeper directly,\nbecause it is actually not well suited as a general-purpose database. It is more likely\nthat you will end up relying on it indirectly via some other project: for example,\nHBase, Hadoop YARN, OpenStack Nova, and Kafka all rely on ZooKeeper running\nin the background. What is it that these projects get from it?\nZooKeeper and etcd are designed to hold small amounts of data that can fit entirely\nin memory (although they still write to disk for durability)\u2014so you wouldn\u2019t want to\nstore all of your application\u2019s data here. That small amount of data is replicated\nacross all the nodes using a fault-tolerant total order broadcast algorithm. As dis\u2010\ncussed previously, total order broadcast is just what you need for database replica\u2010\ntion: if each message represents a write to the database, applying the same writes in\nthe same order keeps replicas consistent with each other.\nZooKeeper is modeled after Google\u2019s Chubby lock service [14, 98], implementing not\nonly total order broadcast (and hence consensus), but also an interesting set of other\nfeatures that turn out to be particularly useful when building distributed systems:\nLinearizable atomic operations\nUsing an atomic compare-and-set operation, you can implement a lock: if several\nnodes concurrently try to perform the same operation, only one of them will suc\u2010\nceed. The consensus protocol guarantees that the operation will be atomic and\nlinearizable, even if a node fails or the network is interrupted at any point. A dis\u2010\ntributed lock is usually implemented as a lease, which has an expiry time so that\nit is eventually released in case the client fails (see \u201cProcess Pauses\u201d on page 295).\nTotal ordering of operations\nAs discussed in \u201cThe leader and the lock\u201d on page 301, when some resource is\nprotected by a lock or lease, you need a fencing token to prevent clients from con\u2010\nflicting with each other in the case of a process pause. The fencing token is some\nnumber that monotonically increases every time the lock is acquired. ZooKeeper\nprovides this by totally ordering all operations and giving each operation a"}
{"393": "Failure detection\nClients maintain a long-lived session on ZooKeeper servers, and the client and\nserver periodically exchange heartbeats to check that the other node is still alive.\nEven if the connection is temporarily interrupted, or a ZooKeeper node fails, the\nsession remains active. However, if the heartbeats cease for a duration that is\nlonger than the session timeout, ZooKeeper declares the session to be dead. Any\nlocks held by a session can be configured to be automatically released when the\nsession times out (ZooKeeper calls these ephemeral nodes).\nChange notifications\nNot only can one client read locks and values that were created by another client,\nbut it can also watch them for changes. Thus, a client can find out when another\nclient joins the cluster (based on the value it writes to ZooKeeper), or if another\nclient fails (because its session times out and its ephemeral nodes disappear). By\nsubscribing to notifications, a client avoids having to frequently poll to find out\nabout changes.\nOf these features, only the linearizable atomic operations really require consensus.\nHowever, it is the combination of these features that makes systems like ZooKeeper\nso useful for distributed coordination.\nAllocating work to nodes\nOne example in which the ZooKeeper/Chubby model works well is if you have sev\u2010\neral instances of a process or service, and one of them needs to be chosen as leader or\nprimary. If the leader fails, one of the other nodes should take over. This is of course\nuseful for single-leader databases, but it\u2019s also useful for job schedulers and similar\nstateful systems.\nAnother example arises when you have some partitioned resource (database, message\nstreams, file storage, distributed actor system, etc.) and need to decide which parti\u2010\ntion to assign to which node. As new nodes join the cluster, some of the partitions\nneed to be moved from existing nodes to the new nodes in order to rebalance the\nload (see \u201cRebalancing Partitions\u201d on page 209). As nodes are removed or fail, other\nnodes need to take over the failed nodes\u2019 work.\nThese kinds of tasks can be achieved by judicious use of atomic operations, ephem\u2010\neral nodes, and notifications in ZooKeeper. If done correctly, this approach allows\nthe application to automatically recover from faults without human intervention. It\u2019s\nnot easy, despite the appearance of libraries such as Apache Curator [17] that have\nsprung up to provide higher-level tools on top of the ZooKeeper client API\u2014but it is\nstill much better than attempting to implement the necessary consensus algorithms"}
{"394": "An application may initially run only on a single node, but eventually may grow to\nthousands of nodes. Trying to perform majority votes over so many nodes would be\nterribly inefficient. Instead, ZooKeeper runs on a fixed number of nodes (usually\nthree or five) and performs its majority votes among those nodes while supporting a\npotentially large number of clients. Thus, ZooKeeper provides a way of \u201coutsourcing\u201d\nsome of the work of coordinating nodes (consensus, operation ordering, and failure\ndetection) to an external service.\nNormally, the kind of data managed by ZooKeeper is quite slow-changing: it repre\u2010\nsents information like \u201cthe node running on 10.1.1.23 is the leader for partition 7,\u201d\nwhich may change on a timescale of minutes or hours. It is not intended for storing\nthe runtime state of the application, which may change thousands or even millions of\ntimes per second. If application state needs to be replicated from one node to\nanother, other tools (such as Apache BookKeeper [108]) can be used.\nService discovery\nZooKeeper, etcd, and Consul are also often used for service discovery\u2014that is, to find\nout which IP address you need to connect to in order to reach a particular service. In\ncloud datacenter environments, where it is common for virtual machines to continu\u2010\nally come and go, you often don\u2019t know the IP addresses of your services ahead of\ntime. Instead, you can configure your services such that when they start up they reg\u2010\nister their network endpoints in a service registry, where they can then be found by\nother services.\nHowever, it is less clear whether service discovery actually requires consensus. DNS is\nthe traditional way of looking up the IP address for a service name, and it uses multi\u2010\nple layers of caching to achieve good performance and availability. Reads from DNS\nare absolutely not linearizable, and it is usually not considered problematic if the\nresults from a DNS query are a little stale [109]. It is more important that DNS is reli\u2010\nably available and robust to network interruptions.\nAlthough service discovery does not require consensus, leader election does. Thus, if\nyour consensus system already knows who the leader is, then it can make sense to\nalso use that information to help other services discover who the leader is. For this\npurpose, some consensus systems support read-only caching replicas. These replicas\nasynchronously receive the log of all decisions of the consensus algorithm, but do not\nactively participate in voting. They are therefore able to serve read requests that do\nnot need to be linearizable.\nMembership services"}
{"395": "A membership service determines which nodes are currently active and live members\nof a cluster. As we saw throughout Chapter 8, due to unbounded network delays it\u2019s\nnot possible to reliably detect whether another node has failed. However, if you cou\u2010\nple failure detection with consensus, nodes can come to an agreement about which\nnodes should be considered alive or not.\nIt could still happen that a node is incorrectly declared dead by consensus, even\nthough it is actually alive. But it is nevertheless very useful for a system to have agree\u2010\nment on which nodes constitute the current membership. For example, choosing a\nleader could mean simply choosing the lowest-numbered among the current mem\u2010\nbers, but this approach would not work if different nodes have divergent opinions on\nwho the current members are.\nSummary\nIn this chapter we examined the topics of consistency and consensus from several dif\u2010\nferent angles. We looked in depth at linearizability, a popular consistency model: its\ngoal is to make replicated data appear as though there were only a single copy, and to\nmake all operations act on it atomically. Although linearizability is appealing because\nit is easy to understand\u2014it makes a database behave like a variable in a single-\nthreaded program\u2014it has the downside of being slow, especially in environments\nwith large network delays.\nWe also explored causality, which imposes an ordering on events in a system (what\nhappened before what, based on cause and effect). Unlike linearizability, which puts\nall operations in a single, totally ordered timeline, causality provides us with a weaker\nconsistency model: some things can be concurrent, so the version history is like a\ntimeline with branching and merging. Causal consistency does not have the coordi\u2010\nnation overhead of linearizability and is much less sensitive to network problems.\nHowever, even if we capture the causal ordering (for example using Lamport time\u2010\nstamps), we saw that some things cannot be implemented this way: in \u201cTimestamp\nordering is not sufficient\u201d on page 347 we considered the example of ensuring that a\nusername is unique and rejecting concurrent registrations for the same username. If\none node is going to accept a registration, it needs to somehow know that another\nnode isn\u2019t concurrently in the process of registering the same name. This problem led\nus toward consensus.\nWe saw that achieving consensus means deciding something in such a way that all\nnodes agree on what was decided, and such that the decision is irrevocable. With\nsome digging, it turns out that a wide range of problems are actually reducible to"}
{"396": "Linearizable compare-and-set registers\nThe register needs to atomically decide whether to set its value, based on whether\nits current value equals the parameter given in the operation.\nAtomic transaction commit\nA database must decide whether to commit or abort a distributed transaction.\nTotal order broadcast\nThe messaging system must decide on the order in which to deliver messages.\nLocks and leases\nWhen several clients are racing to grab a lock or lease, the lock decides which one\nsuccessfully acquired it.\nMembership/coordination service\nGiven a failure detector (e.g., timeouts), the system must decide which nodes are\nalive, and which should be considered dead because their sessions timed out.\nUniqueness constraint\nWhen several transactions concurrently try to create conflicting records with the\nsame key, the constraint must decide which one to allow and which should fail\nwith a constraint violation.\nAll of these are straightforward if you only have a single node, or if you are willing to\nassign the decision-making capability to a single node. This is what happens in a\nsingle-leader database: all the power to make decisions is vested in the leader, which\nis why such databases are able to provide linearizable operations, uniqueness con\u2010\nstraints, a totally ordered replication log, and more.\nHowever, if that single leader fails, or if a network interruption makes the leader\nunreachable, such a system becomes unable to make any progress. There are three\nways of handling that situation:\n1. Wait for the leader to recover, and accept that the system will be blocked in the\nmeantime. Many XA/JTA transaction coordinators choose this option. This\napproach does not fully solve consensus because it does not satisfy the termina\u2010\ntion property: if the leader does not recover, the system can be blocked forever.\n2. Manually fail over by getting humans to choose a new leader node and reconfig\u2010\nure the system to use it. Many relational databases take this approach. It is a kind\nof consensus by \u201cact of God\u201d\u2014the human operator, outside of the computer sys\u2010\ntem, makes the decision. The speed of failover is limited by the speed at which\nhumans can act, which is generally slower than computers."}
{"397": "3. Use an algorithm to automatically choose a new leader. This approach requires a\nconsensus algorithm, and it is advisable to use a proven algorithm that correctly\nhandles adverse network conditions [107].\nAlthough a single-leader database can provide linearizability without executing a\nconsensus algorithm on every write, it still requires consensus to maintain its leader\u2010\nship and for leadership changes. Thus, in some sense, having a leader only \u201ckicks the\ncan down the road\u201d: consensus is still required, only in a different place, and less fre\u2010\nquently. The good news is that fault-tolerant algorithms and systems for consensus\nexist, and we briefly discussed them in this chapter.\nTools like ZooKeeper play an important role in providing an \u201coutsourced\u201d consen\u2010\nsus, failure detection, and membership service that applications can use. It\u2019s not easy\nto use, but it is much better than trying to develop your own algorithms that can\nwithstand all the problems discussed in Chapter 8. If you find yourself wanting to do\none of those things that is reducible to consensus, and you want it to be fault-tolerant,\nthen it is advisable to use something like ZooKeeper.\nNevertheless, not every system necessarily requires consensus: for example, leaderless\nand multi-leader replication systems typically do not use global consensus. The con\u2010\nflicts that occur in these systems (see \u201cHandling Write Conflicts\u201d on page 171) are a\nconsequence of not having consensus across different leaders, but maybe that\u2019s okay:\nmaybe we simply need to cope without linearizability and learn to work better with\ndata that has branching and merging version histories.\nThis chapter referenced a large body of research on the theory of distributed systems.\nAlthough the theoretical papers and proofs are not always easy to understand, and\nsometimes make unrealistic assumptions, they are incredibly valuable for informing\npractical work in this field: they help us reason about what can and cannot be done,\nand help us find the counterintuitive ways in which distributed systems are often\nflawed. If you have the time, the references are well worth exploring.\nThis brings us to the end of Part II of this book, in which we covered replication\n(Chapter 5), partitioning (Chapter 6), transactions (Chapter 7), distributed system\nfailure models (Chapter 8), and finally consistency and consensus (Chapter 9). Now\nthat we have laid a firm foundation of theory, in Part III we will turn once again to\nmore practical systems, and discuss how to build powerful applications from hetero\u2010\ngeneous building blocks.\nReferences"}
{"398": "[2] Prince Mahajan, Lorenzo Alvisi, and Mike Dahlin: \u201cConsistency, Availability, and\nConvergence,\u201d University of Texas at Austin, Department of Computer Science, Tech\nReport UTCS TR-11-22, May 2011.\n[3] Alex Scotti: \u201cAdventures in Building Your Own Database,\u201d at All Your Base,\nNovember 2015.\n[4] Peter Bailis, Aaron Davidson, Alan Fekete, et al.: \u201cHighly Available Transactions:\nVirtues and Limitations,\u201d at 40th International Conference on Very Large Data Bases\n(VLDB), September 2014. Extended version published as pre-print arXiv:1302.0309\n[cs.DB].\n[5] Paolo Viotti and Marko Vukoli\u0107: \u201cConsistency in Non-Transactional Distributed\nStorage Systems,\u201d arXiv:1512.00168, 12 April 2016.\n[6] Maurice P. Herlihy and Jeannette M. Wing: \u201cLinearizability: A Correctness Con\u2010\ndition for Concurrent Objects,\u201d ACM Transactions on Programming Languages and\nSystems (TOPLAS), volume 12, number 3, pages 463\u2013492, July 1990. doi:\n10.1145/78969.78972\n[7] Leslie Lamport: \u201cOn interprocess communication,\u201d Distributed Computing, vol\u2010\nume 1, number 2, pages 77\u2013101, June 1986. doi:10.1007/BF01786228\n[8] David K. Gifford: \u201cInformation Storage in a Decentralized Computer System,\u201d\nXerox Palo Alto Research Centers, CSL-81-8, June 1981.\n[9] Martin Kleppmann: \u201cPlease Stop Calling Databases CP or AP,\u201d martin.klepp\u2010\nmann.com, May 11, 2015.\n[10] Kyle Kingsbury: \u201cCall Me Maybe: MongoDB Stale Reads,\u201d aphyr.com, April 20,\n2015.\n[11] Kyle Kingsbury: \u201cComputational Techniques in Knossos,\u201d aphyr.com, May 17,\n2014.\n[12] Peter Bailis: \u201cLinearizability Versus Serializability,\u201d bailis.org, September 24,\n2014.\n[13] Philip A. Bernstein, Vassos Hadzilacos, and Nathan Goodman: Concurrency\nControl and Recovery in Database Systems. Addison-Wesley, 1987. ISBN:\n978-0-201-10715-9, available online at research.microsoft.com.\n[14] Mike Burrows: \u201cThe Chubby Lock Service for Loosely-Coupled Distributed Sys\u2010\ntems,\u201d at 7th USENIX Symposium on Operating System Design and Implementation\n(OSDI), November 2006."}
{"399": "[17] \u201cApache Curator,\u201d Apache Software Foundation, curator.apache.org, 2015.\n[18] Morali Vallath: Oracle 10g RAC Grid, Services & Clustering. Elsevier Digital\nPress, 2006. ISBN: 978-1-555-58321-7\n[19] Peter Bailis, Alan Fekete, Michael J Franklin, et al.: \u201cCoordination-Avoiding\nDatabase Systems,\u201d Proceedings of the VLDB Endowment, volume 8, number 3, pages\n185\u2013196, November 2014.\n[20] Kyle Kingsbury: \u201cCall Me Maybe: etcd and Consul,\u201d aphyr.com, June 9, 2014.\n[21] Flavio P. Junqueira, Benjamin C. Reed, and Marco Serafini: \u201cZab: High-\nPerformance Broadcast for Primary-Backup Systems,\u201d at 41st IEEE International\nConference on Dependable Systems and Networks (DSN), June 2011. doi:10.1109/\nDSN.2011.5958223\n[22] Diego Ongaro and John K. Ousterhout: \u201cIn Search of an Understandable Con\u2010\nsensus Algorithm (Extended Version),\u201d at USENIX Annual Technical Conference\n(ATC), June 2014.\n[23] Hagit Attiya, Amotz Bar-Noy, and Danny Dolev: \u201cSharing Memory Robustly in\nMessage-Passing Systems,\u201d Journal of the ACM, volume 42, number 1, pages 124\u2013\n142, January 1995. doi:10.1145/200836.200869\n[24] Nancy Lynch and Alex Shvartsman: \u201cRobust Emulation of Shared Memory\nUsing Dynamic Quorum-Acknowledged Broadcasts,\u201d at 27th Annual International\nSymposium on Fault-Tolerant Computing (FTCS), June 1997. doi:10.1109/FTCS.\n1997.614100\n[25] Christian Cachin, Rachid Guerraoui, and Lu\u00eds Rodrigues: Introduction to Relia\u2010\nble and Secure Distributed Programming, 2nd edition. Springer, 2011. ISBN:\n978-3-642-15259-7, doi:10.1007/978-3-642-15260-3\n[26] Sam Elliott, Mark Allen, and Martin Kleppmann: personal communication,\nthread on twitter.com, October 15, 2015.\n[27] Niklas Ekstr\u00f6m, Mikhail Panchenko, and Jonathan Ellis: \u201cPossible Issue with\nRead Repair?,\u201d email thread on cassandra-dev mailing list, October 2012.\n[28] Maurice P. Herlihy: \u201cWait-Free Synchronization,\u201d ACM Transactions on Pro\u2010\ngramming Languages and Systems (TOPLAS), volume 13, number 1, pages 124\u2013149,\nJanuary 1991. doi:10.1145/114005.102808\n[29] Armando Fox and Eric A. Brewer: \u201cHarvest, Yield, and Scalable Tolerant Sys\u2010\ntems,\u201d at 7th Workshop on Hot Topics in Operating Systems (HotOS), March 1999."}
{"400": "[30] Seth Gilbert and Nancy Lynch: \u201cBrewer\u2019s Conjecture and the Feasibility of Con\u2010\nsistent, Available, Partition-Tolerant Web Services,\u201d ACM SIGACT News, volume 33,\nnumber 2, pages 51\u201359, June 2002. doi:10.1145/564585.564601\n[31] Seth Gilbert and Nancy Lynch: \u201cPerspectives on the CAP Theorem,\u201d IEEE Com\u2010\nputer Magazine, volume 45, number 2, pages 30\u201336, February 2012. doi:10.1109/MC.\n2011.389\n[32] Eric A. Brewer: \u201cCAP Twelve Years Later: How the \u2018Rules\u2019 Have Changed,\u201d IEEE\nComputer Magazine, volume 45, number 2, pages 23\u201329, February 2012. doi:\n10.1109/MC.2012.37\n[33] Susan B. Davidson, Hector Garcia-Molina, and Dale Skeen: \u201cConsistency in Par\u2010\ntitioned Networks,\u201d ACM Computing Surveys, volume 17, number 3, pages 341\u2013370,\nSeptember 1985. doi:10.1145/5505.5508\n[34] Paul R. Johnson and Robert H. Thomas: \u201cRFC 677: The Maintenance of Dupli\u2010\ncate Databases,\u201d Network Working Group, January 27, 1975.\n[35] Bruce G. Lindsay, Patricia Griffiths Selinger, C. Galtieri, et al.: \u201cNotes on Dis\u2010\ntributed Databases,\u201d IBM Research, Research Report RJ2571(33471), July 1979.\n[36] Michael J. Fischer and Alan Michael: \u201cSacrificing Serializability to Attain High\nAvailability of Data in an Unreliable Network,\u201d at 1st ACM Symposium on Principles\nof Database Systems (PODS), March 1982. doi:10.1145/588111.588124\n[37] Eric A. Brewer: \u201cNoSQL: Past, Present, Future,\u201d at QCon San Francisco, Novem\u2010\nber 2012.\n[38] Henry Robinson: \u201cCAP Confusion: Problems with \u2018Partition Tolerance,\u2019\u201d\nblog.cloudera.com, April 26, 2010.\n[39] Adrian Cockcroft: \u201cMigrating to Microservices,\u201d at QCon London, March 2014.\n[40] Martin Kleppmann: \u201cA Critique of the CAP Theorem,\u201d arXiv:1509.05393, Sep\u2010\ntember 17, 2015.\n[41] Nancy A. Lynch: \u201cA Hundred Impossibility Proofs for Distributed Computing,\u201d\nat 8th ACM Symposium on Principles of Distributed Computing (PODC), August\n1989. doi:10.1145/72981.72982\n[42] Hagit Attiya, Faith Ellen, and Adam Morrison: \u201cLimitations of Highly-Available\nEventually-Consistent Data Stores,\u201d at ACM Symposium on Principles of Distributed\nComputing (PODC), July 2015. doi:10.1145/2767386.2767419\n[43] Peter Sewell, Susmit Sarkar, Scott Owens, et al.: \u201cx86-TSO: A Rigorous and Usa\u2010"}
{"401": "[44] Martin Thompson: \u201cMemory Barriers/Fences,\u201d mechanical-\nsympathy.blogspot.co.uk, July 24, 2011.\n[45] Ulrich Drepper: \u201cWhat Every Programmer Should Know About Memory,\u201d\nakkadia.org, November 21, 2007.\n[46] Daniel J. Abadi: \u201cConsistency Tradeoffs in Modern Distributed Database System\nDesign,\u201d IEEE Computer Magazine, volume 45, number 2, pages 37\u201342, February\n2012. doi:10.1109/MC.2012.33\n[47] Hagit Attiya and Jennifer L. Welch: \u201cSequential Consistency Versus Linearizabil\u2010\nity,\u201d ACM Transactions on Computer Systems (TOCS), volume 12, number 2, pages\n91\u2013122, May 1994. doi:10.1145/176575.176576\n[48] Mustaque Ahamad, Gil Neiger, James E. Burns, et al.: \u201cCausal Memory: Defini\u2010\ntions, Implementation, and Programming,\u201d Distributed Computing, volume 9, num\u2010\nber 1, pages 37\u201349, March 1995. doi:10.1007/BF01784241\n[49] Wyatt Lloyd, Michael J. Freedman, Michael Kaminsky, and David G. Andersen:\n\u201cStronger Semantics for Low-Latency Geo-Replicated Storage,\u201d at 10th USENIX Sym\u2010\nposium on Networked Systems Design and Implementation (NSDI), April 2013.\n[50] Marek Zawirski, Annette Bieniusa, Valter Balegas, et al.: \u201cSwiftCloud: Fault-\nTolerant Geo-Replication Integrated All the Way to the Client Machine,\u201d INRIA\nResearch Report 8347, August 2013.\n[51] Peter Bailis, Ali Ghodsi, Joseph M Hellerstein, and Ion Stoica: \u201cBolt-on Causal\nConsistency,\u201d at ACM International Conference on Management of Data (SIGMOD),\nJune 2013.\n[52] Philippe Ajoux, Nathan Bronson, Sanjeev Kumar, et al.: \u201cChallenges to Adopting\nStronger Consistency at Scale,\u201d at 15th USENIX Workshop on Hot Topics in Operat\u2010\ning Systems (HotOS), May 2015.\n[53] Peter Bailis: \u201cCausality Is Expensive (and What to Do About It),\u201d bailis.org, Feb\u2010\nruary 5, 2014.\n[54] Ricardo Gon\u00e7alves, Paulo S\u00e9rgio Almeida, Carlos Baquero, and Victor Fonte:\n\u201cConcise Server-Wide Causality Management for Eventually Consistent Data Stores,\u201d\nat 15th IFIP International Conference on Distributed Applications and Interoperable\nSystems (DAIS), June 2015. doi:10.1007/978-3-319-19129-4_6\n[55] Rob Conery: \u201cA Better ID Generator for PostgreSQL,\u201d rob.conery.io, May 29,\n2014."}
{"402": "[57] Xavier D\u00e9fago, Andr\u00e9 Schiper, and P\u00e9ter Urb\u00e1n: \u201cTotal Order Broadcast and\nMulticast Algorithms: Taxonomy and Survey,\u201d ACM Computing Surveys, volume 36,\nnumber 4, pages 372\u2013421, December 2004. doi:10.1145/1041680.1041682\n[58] Hagit Attiya and Jennifer Welch: Distributed Computing: Fundamentals, Simula\u2010\ntions and Advanced Topics, 2nd edition. John Wiley & Sons, 2004. ISBN:\n978-0-471-45324-6, doi:10.1002/0471478210\n[59] Mahesh Balakrishnan, Dahlia Malkhi, Vijayan Prabhakaran, et al.: \u201cCORFU: A\nShared Log Design for Flash Clusters,\u201d at 9th USENIX Symposium on Networked Sys\u2010\ntems Design and Implementation (NSDI), April 2012.\n[60] Fred B. Schneider: \u201cImplementing Fault-Tolerant Services Using the State\nMachine Approach: A Tutorial,\u201d ACM Computing Surveys, volume 22, number 4,\npages 299\u2013319, December 1990.\n[61] Alexander Thomson, Thaddeus Diamond, Shu-Chun Weng, et al.: \u201cCalvin: Fast\nDistributed Transactions for Partitioned Database Systems,\u201d at ACM International\nConference on Management of Data (SIGMOD), May 2012.\n[62] Mahesh Balakrishnan, Dahlia Malkhi, Ted Wobber, et al.: \u201cTango: Distributed\nData Structures over a Shared Log,\u201d at 24th ACM Symposium on Operating Systems\nPrinciples (SOSP), November 2013. doi:10.1145/2517349.2522732\n[63] Robbert van Renesse and Fred B. Schneider: \u201cChain Replication for Supporting\nHigh Throughput and Availability,\u201d at 6th USENIX Symposium on Operating System\nDesign and Implementation (OSDI), December 2004.\n[64] Leslie Lamport: \u201cHow to Make a Multiprocessor Computer That Correctly Exe\u2010\ncutes Multiprocess Programs,\u201d IEEE Transactions on Computers, volume 28, number\n9, pages 690\u2013691, September 1979. doi:10.1109/TC.1979.1675439\n[65] Enis S\u00f6ztutar, Devaraj Das, and Carter Shanklin: \u201cApache HBase High Availabil\u2010\nity at the Next Level,\u201d hortonworks.com, January 22, 2015.\n[66] Brian F Cooper, Raghu Ramakrishnan, Utkarsh Srivastava, et al.: \u201cPNUTS:\nYahoo!\u2019s Hosted Data Serving Platform,\u201d at 34th International Conference on Very\nLarge Data Bases (VLDB), August 2008. doi:10.14778/1454159.1454167\n[67] Tushar Deepak Chandra and Sam Toueg: \u201cUnreliable Failure Detectors for Reli\u2010\nable Distributed Systems,\u201d Journal of the ACM, volume 43, number 2, pages 225\u2013267,\nMarch 1996. doi:10.1145/226643.226647\n[68] Michael J. Fischer, Nancy Lynch, and Michael S. Paterson: \u201cImpossibility of Dis\u2010\ntributed Consensus with One Faulty Process,\u201d Journal of the ACM, volume 32, num\u2010"}
{"403": "[69] Michael Ben-Or: \u201cAnother Advantage of Free Choice: Completely Asynchro\u2010\nnous Agreement Protocols,\u201d at 2nd ACM Symposium on Principles of Distributed\nComputing (PODC), August 1983. doi:10.1145/800221.806707\n[70] Jim N. Gray and Leslie Lamport: \u201cConsensus on Transaction Commit,\u201d ACM\nTransactions on Database Systems (TODS), volume 31, number 1, pages 133\u2013160,\nMarch 2006. doi:10.1145/1132863.1132867\n[71] Rachid Guerraoui: \u201cRevisiting the Relationship Between Non-Blocking Atomic\nCommitment and Consensus,\u201d at 9th International Workshop on Distributed Algo\u2010\nrithms (WDAG), September 1995. doi:10.1007/BFb0022140\n[72] Thanumalayan Sankaranarayana Pillai, Vijay Chidambaram, Ramnatthan Ala\u2010\ngappan, et al.: \u201cAll File Systems Are Not Created Equal: On the Complexity of Craft\u2010\ning Crash-Consistent Applications,\u201d at 11th USENIX Symposium on Operating\nSystems Design and Implementation (OSDI), October 2014.\n[73] Jim Gray: \u201cThe Transaction Concept: Virtues and Limitations,\u201d at 7th Interna\u2010\ntional Conference on Very Large Data Bases (VLDB), September 1981.\n[74] Hector Garcia-Molina and Kenneth Salem: \u201cSagas,\u201d at ACM International Con\u2010\nference on Management of Data (SIGMOD), May 1987. doi:10.1145/38713.38742\n[75] C. Mohan, Bruce G. Lindsay, and Ron Obermarck: \u201cTransaction Management in\nthe R* Distributed Database Management System,\u201d ACM Transactions on Database\nSystems, volume 11, number 4, pages 378\u2013396, December 1986. doi:\n10.1145/7239.7266\n[76] \u201cDistributed Transaction Processing: The XA Specification,\u201d X/Open Company\nLtd., Technical Standard XO/CAE/91/300, December 1991. ISBN: 978-1-872-63024-3\n[77] Mike Spille: \u201cXA Exposed, Part II,\u201d jroller.com, April 3, 2004.\n[78] Ivan Silva Neto and Francisco Reverbel: \u201cLessons Learned from Implementing\nWS-Coordination and WS-AtomicTransaction,\u201d at 7th IEEE/ACIS International\nConference on Computer and Information Science (ICIS), May 2008. doi:10.1109/\nICIS.2008.75\n[79] James E. Johnson, David E. Langworthy, Leslie Lamport, and Friedrich H. Vogt:\n\u201cFormal Specification of a Web Services Protocol,\u201d at 1st International Workshop on\nWeb Services and Formal Methods (WS-FM), February 2004. doi:10.1016/j.entcs.\n2004.02.022\n[80] Dale Skeen: \u201cNonblocking Commit Protocols,\u201d at ACM International Conference\non Management of Data (SIGMOD), April 1981. doi:10.1145/582318.582339"}
{"404": "[82] Pat Helland: \u201cLife Beyond Distributed Transactions: An Apostate\u2019s Opinion,\u201d at\n3rd Biennial Conference on Innovative Data Systems Research (CIDR), January 2007.\n[83] Jonathan Oliver: \u201cMy Beef with MSDTC and Two-Phase Commits,\u201d blog.jona\u2010\nthanoliver.com, April 4, 2011.\n[84] Oren Eini (Ahende Rahien): \u201cThe Fallacy of Distributed Transactions,\u201d\nayende.com, July 17, 2014.\n[85] Clemens Vasters: \u201cTransactions in Windows Azure (with Service Bus) \u2013 An\nEmail Discussion,\u201d vasters.com, July 30, 2012.\n[86] \u201cUnderstanding Transactionality in Azure,\u201d NServiceBus Documentation, Par\u2010\nticular Software, 2015.\n[87] Randy Wigginton, Ryan Lowe, Marcos Albe, and Fernando Ipar: \u201cDistributed\nTransactions in MySQL,\u201d at MySQL Conference and Expo, April 2013.\n[88] Mike Spille: \u201cXA Exposed, Part I,\u201d jroller.com, April 3, 2004.\n[89] Ajmer Dhariwal: \u201cOrphaned MSDTC Transactions (-2 spids),\u201d eraofdata.com,\nDecember 12, 2008.\n[90] Paul Randal: \u201cReal World Story of DBCC PAGE Saving the Day,\u201d sqlskills.com,\nJune 19, 2013.\n[91] \u201cin-doubt xact resolution Server Configuration Option,\u201d SQL Server 2016 docu\u2010\nmentation, Microsoft, Inc., 2016.\n[92] Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer: \u201cConsensus in the Pres\u2010\nence of Partial Synchrony,\u201d Journal of the ACM, volume 35, number 2, pages 288\u2013\n323, April 1988. doi:10.1145/42282.42283\n[93] Miguel Castro and Barbara H. Liskov: \u201cPractical Byzantine Fault Tolerance and\nProactive Recovery,\u201d ACM Transactions on Computer Systems, volume 20, number 4,\npages 396\u2013461, November 2002. doi:10.1145/571637.571640\n[94] Brian M. Oki and Barbara H. Liskov: \u201cViewstamped Replication: A New Primary\nCopy Method to Support Highly-Available Distributed Systems,\u201d at 7th ACM Sympo\u2010\nsium on Principles of Distributed Computing (PODC), August 1988. doi:\n10.1145/62546.62549\n[95] Barbara H. Liskov and James Cowling: \u201cViewstamped Replication Revisited,\u201d\nMassachusetts Institute of Technology, Tech Report MIT-CSAIL-TR-2012-021, July\n2012."}
{"405": "[97] Leslie Lamport: \u201cPaxos Made Simple,\u201d ACM SIGACT News, volume 32, number\n4, pages 51\u201358, December 2001.\n[98] Tushar Deepak Chandra, Robert Griesemer, and Joshua Redstone: \u201cPaxos Made\nLive \u2013 An Engineering Perspective,\u201d at 26th ACM Symposium on Principles of Dis\u2010\ntributed Computing (PODC), June 2007.\n[99] Robbert van Renesse: \u201cPaxos Made Moderately Complex,\u201d cs.cornell.edu, March\n2011.\n[100] Diego Ongaro: \u201cConsensus: Bridging Theory and Practice,\u201d PhD Thesis, Stan\u2010\nford University, August 2014.\n[101] Heidi Howard, Malte Schwarzkopf, Anil Madhavapeddy, and Jon Crowcroft:\n\u201cRaft Refloated: Do We Have Consensus?,\u201d ACM SIGOPS Operating Systems Review,\nvolume 49, number 1, pages 12\u201321, January 2015. doi:10.1145/2723872.2723876\n[102] Andr\u00e9 Medeiros: \u201cZooKeeper\u2019s Atomic Broadcast Protocol: Theory and Prac\u2010\ntice,\u201d Aalto University School of Science, March 20, 2012.\n[103] Robbert van Renesse, Nicolas Schiper, and Fred B. Schneider: \u201cVive La Diff\u00e9r\u2010\nence: Paxos vs. Viewstamped Replication vs. Zab,\u201d IEEE Transactions on Dependable\nand Secure Computing, volume 12, number 4, pages 472\u2013484, September 2014. doi:\n10.1109/TDSC.2014.2355848\n[104] Will Portnoy: \u201cLessons Learned from Implementing Paxos,\u201d blog.willport\u2010\nnoy.com, June 14, 2012.\n[105] Heidi Howard, Dahlia Malkhi, and Alexander Spiegelman: \u201cFlexible Paxos:\nQuorum Intersection Revisited,\u201d arXiv:1608.06696, August 24, 2016.\n[106] Heidi Howard and Jon Crowcroft: \u201cCoracle: Evaluating Consensus at the Inter\u2010\nnet Edge,\u201d at Annual Conference of the ACM Special Interest Group on Data Commu\u2010\nnication (SIGCOMM), August 2015. doi:10.1145/2829988.2790010\n[107] Kyle Kingsbury: \u201cCall Me Maybe: Elasticsearch 1.5.0,\u201d aphyr.com, April 27,\n2015.\n[108] Ivan Kelly: \u201cBookKeeper Tutorial,\u201d github.com, October 2014.\n[109] Camille Fournier: \u201cConsensus Systems for the Skeptical Architect,\u201d at Craft\nConference, Budapest, Hungary, April 2015.\n[110] Kenneth P. Birman: \u201cA History of the Virtual Synchrony Replication Model,\u201d\nin Replication: Theory and Practice, Springer LNCS volume 5959, chapter 6, pages\n91\u2013120, 2010. ISBN: 978-3-642-11293-5, doi:10.1007/978-3-642-11294-2_6"}
{"406": ""}
{"407": "PART III\nDerived Data\nIn Parts I and II of this book, we assembled from the ground up all the major consid\u2010\nerations that go into a distributed database, from the layout of data on disk all the\nway to the limits of distributed consistency in the presence of faults. However, this\ndiscussion assumed that there was only one database in the application.\nIn reality, data systems are often more complex. In a large application you often need\nto be able to access and process data in many different ways, and there is no one data\u2010\nbase that can satisfy all those different needs simultaneously. Applications thus com\u2010\nmonly use a combination of several different datastores, indexes, caches, analytics\nsystems, etc. and implement mechanisms for moving data from one store to another.\nIn this final part of the book, we will examine the issues around integrating multiple\ndifferent data systems, potentially with different data models and optimized for dif\u2010\nferent access patterns, into one coherent application architecture. This aspect of\nsystem-building is often overlooked by vendors who claim that their product can sat\u2010\nisfy all your needs. In reality, integrating disparate systems is one of the most impor\u2010\ntant things that needs to be done in a nontrivial application."}
{"408": "Systems of Record and Derived Data\nOn a high level, systems that store and process data can be grouped into two broad\ncategories:\nSystems of record\nA system of record, also known as source of truth, holds the authoritative version\nof your data. When new data comes in, e.g., as user input, it is first written here.\nEach fact is represented exactly once (the representation is typically normalized).\nIf there is any discrepancy between another system and the system of record,\nthen the value in the system of record is (by definition) the correct one.\nDerived data systems\nData in a derived system is the result of taking some existing data from another\nsystem and transforming or processing it in some way. If you lose derived data,\nyou can recreate it from the original source. A classic example is a cache: data can\nbe served from the cache if present, but if the cache doesn\u2019t contain what you\nneed, you can fall back to the underlying database. Denormalized values, indexes,\nand materialized views also fall into this category. In recommendation systems,\npredictive summary data is often derived from usage logs.\nTechnically speaking, derived data is redundant, in the sense that it duplicates exist\u2010\ning information. However, it is often essential for getting good performance on read\nqueries. It is commonly denormalized. You can derive several different datasets from\na single source, enabling you to look at the data from different \u201cpoints of view.\u201d\nNot all systems make a clear distinction between systems of record and derived data\nin their architecture, but it\u2019s a very helpful distinction to make, because it clarifies the\ndataflow through your system: it makes explicit which parts of the system have which\ninputs and which outputs, and how they depend on each other.\nMost databases, storage engines, and query languages are not inherently either a sys\u2010\ntem of record or a derived system. A database is just a tool: how you use it is up to\nyou. The distinction between system of record and derived data system depends not\non the tool, but on how you use it in your application.\nBy being clear about which data is derived from which other data, you can bring\nclarity to an otherwise confusing system architecture. This point will be a running\ntheme throughout this part of the book."}
{"409": "Overview of Chapters\nWe will start in Chapter 10 by examining batch-oriented dataflow systems such as\nMapReduce, and see how they give us good tools and principles for building large-\nscale data systems. In Chapter 11 we will take those ideas and apply them to data\nstreams, which allow us to do the same kinds of things with lower delays. Chapter 12\nconcludes the book by exploring ideas about how we might use these tools to build\nreliable, scalable, and maintainable applications in the future."}
{"410": ""}
{"411": "CHAPTER 10\nBatch Processing\nA system cannot be successful if it is too strongly influenced by a single person. Once the\ninitial design is complete and fairly robust, the real test begins as people with many different\nviewpoints undertake their own experiments.\n\u2014Donald Knuth\nIn the first two parts of this book we talked a lot about requests and queries, and the\ncorresponding responses or results. This style of data processing is assumed in many\nmodern data systems: you ask for something, or you send an instruction, and some\ntime later the system (hopefully) gives you an answer. Databases, caches, search\nindexes, web servers, and many other systems work this way.\nIn such online systems, whether it\u2019s a web browser requesting a page or a service call\u2010\ning a remote API, we generally assume that the request is triggered by a human user,\nand that the user is waiting for the response. They shouldn\u2019t have to wait too long, so\nwe pay a lot of attention to the response time of these systems (see \u201cDescribing Perfor\u2010\nmance\u201d on page 13).\nThe web, and increasing numbers of HTTP/REST-based APIs, has made the request/\nresponse style of interaction so common that it\u2019s easy to take it for granted. But we\nshould remember that it\u2019s not the only way of building systems, and that other\napproaches have their merits too. Let\u2019s distinguish three different types of systems:\nServices (online systems)\nA service waits for a request or instruction from a client to arrive. When one is\nreceived, the service tries to handle it as quickly as possible and sends a response\nback. Response time is usually the primary measure of performance of a service,"}
{"412": "Batch processing systems (offline systems)\nA batch processing system takes a large amount of input data, runs a job to pro\u2010\ncess it, and produces some output data. Jobs often take a while (from a few\nminutes to several days), so there normally isn\u2019t a user waiting for the job to fin\u2010\nish. Instead, batch jobs are often scheduled to run periodically (for example, once\na day). The primary performance measure of a batch job is usually throughput\n(the time it takes to crunch through an input dataset of a certain size). We dis\u2010\ncuss batch processing in this chapter.\nStream processing systems (near-real-time systems)\nStream processing is somewhere between online and offline/batch processing (so\nit is sometimes called near-real-time or nearline processing). Like a batch pro\u2010\ncessing system, a stream processor consumes inputs and produces outputs\n(rather than responding to requests). However, a stream job operates on events\nshortly after they happen, whereas a batch job operates on a fixed set of input\ndata. This difference allows stream processing systems to have lower latency than\nthe equivalent batch systems. As stream processing builds upon batch process\u2010\ning, we discuss it in Chapter 11.\nAs we shall see in this chapter, batch processing is an important building block in our\nquest to build reliable, scalable, and maintainable applications. For example, Map\u2010\nReduce, a batch processing algorithm published in 2004 [1], was (perhaps over-\nenthusiastically) called \u201cthe algorithm that makes Google so massively scalable\u201d [2]. It\nwas subsequently implemented in various open source data systems, including\nHadoop, CouchDB, and MongoDB.\nMapReduce is a fairly low-level programming model compared to the parallel pro\u2010\ncessing systems that were developed for data warehouses many years previously [3,\n4], but it was a major step forward in terms of the scale of processing that could be\nachieved on commodity hardware. Although the importance of MapReduce is now\ndeclining [5], it is still worth understanding, because it provides a clear picture of\nwhy and how batch processing is useful.\nIn fact, batch processing is a very old form of computing. Long before programmable\ndigital computers were invented, punch card tabulating machines\u2014such as the Hol\u2010\nlerith machines used in the 1890 US Census [6]\u2014implemented a semi-mechanized\nform of batch processing to compute aggregate statistics from large inputs. And Map\u2010\nReduce bears an uncanny resemblance to the electromechanical IBM card-sorting\nmachines that were widely used for business data processing in the 1940s and 1950s\n[7]. As usual, history has a tendency of repeating itself."}
{"413": "while because the ideas and lessons from Unix carry over to large-scale, heterogene\u2010\nous distributed data systems.\nBatch Processing with Unix Tools\nLet\u2019s start with a simple example. Say you have a web server that appends a line to a\nlog file every time it serves a request. For example, using the nginx default access log\nformat, one line of the log might look like this:\n216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] \"GET /css/typography.css HTTP/1.1\"\n200 3377 \"http://martin.kleppmann.com/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X\n10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115\nSafari/537.36\"\n(That is actually one line; it\u2019s only broken onto multiple lines here for readability.)\nThere\u2019s a lot of information in that line. In order to interpret it, you need to look at\nthe definition of the log format, which is as follows:\n$remote_addr - $remote_user [$time_local] \"$request\"\n$status $body_bytes_sent \"$http_referer\" \"$http_user_agent\"\nSo, this one line of the log indicates that on February 27, 2015, at 17:55:11 UTC, the\nserver received a request for the file /css/typography.css from the client IP address\n216.58.210.78. The user was not authenticated, so $remote_user is set to a hyphen\n(-). The response status was 200 (i.e., the request was successful), and the response\nwas 3,377 bytes in size. The web browser was Chrome 40, and it loaded the file\nbecause it was referenced in the page at the URL http://martin.kleppmann.com/.\nSimple Log Analysis\nVarious tools can take these log files and produce pretty reports about your website\ntraffic, but for the sake of exercise, let\u2019s build our own, using basic Unix tools. For\nexample, say you want to find the five most popular pages on your website. You can\ndo this in a Unix shell as follows:i\ncat /var/log/nginx/access.log |\nawk '{print $7}' |\nsort |\nuniq -c |\nsort -r -n |\nhead -n 5\nRead the log file."}
{"414": "Split each line into fields by whitespace, and output only the seventh such field\nfrom each line, which happens to be the requested URL. In our example line, this\nrequest URL is /css/typography.css.\nAlphabetically sort the list of requested URLs. If some URL has been requested\nn times, then after sorting, the file contains the same URL repeated n times in a\nrow.\nThe uniq command filters out repeated lines in its input by checking whether\ntwo adjacent lines are the same. The -c option tells it to also output a counter: for\nevery distinct URL, it reports how many times that URL appeared in the input.\nThe second sort sorts by the number (-n) at the start of each line, which is the\nnumber of times the URL was requested. It then returns the results in reverse\n(-r) order, i.e. with the largest number first.\nFinally, head outputs just the first five lines (-n 5) of input, and discards the rest.\nThe output of that series of commands looks something like this:\n4189 /favicon.ico\n3631 /2013/05/24/improving-security-of-ssh-private-keys.html\n2124 /2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html\n1369 /\n915 /css/typography.css\nAlthough the preceding command line likely looks a bit obscure if you\u2019re unfamiliar\nwith Unix tools, it is incredibly powerful. It will process gigabytes of log files in a\nmatter of seconds, and you can easily modify the analysis to suit your needs. For\nexample, if you want to omit CSS files from the report, change the awk argument to\n'$7 !~ /\\.css$/ {print $7}'. If you want to count top client IP addresses instead\nof top pages, change the awk argument to '{print $1}'. And so on.\nWe don\u2019t have space in this book to explore Unix tools in detail, but they are very\nmuch worth learning about. Surprisingly many data analyses can be done in a few\nminutes using some combination of awk, sed, grep, sort, uniq, and xargs, and they\nperform surprisingly well [8].\nChain of commands versus custom program\nInstead of the chain of Unix commands, you could write a simple program to do the\nsame thing. For example, in Ruby, it might look something like this:"}
{"415": "counts = Hash.new(0)\nFile.open('/var/log/nginx/access.log') do |file|\nfile.each do |line|\nurl = line.split[6]\ncounts[url] += 1\nend\nend\ntop5 = counts.map{|url, count| [count, url] }.sort.reverse[0...5]\ntop5.each{|count, url| puts \"#{count} #{url}\" }\ncounts is a hash table that keeps a counter for the number of times we\u2019ve seen\neach URL. A counter is zero by default.\nFrom each line of the log, we take the URL to be the seventh whitespace-\nseparated field (the array index here is 6 because Ruby\u2019s arrays are zero-indexed).\nIncrement the counter for the URL in the current line of the log.\nSort the hash table contents by counter value (descending), and take the top five\nentries.\nPrint out those top five entries.\nThis program is not as concise as the chain of Unix pipes, but it\u2019s fairly readable, and\nwhich of the two you prefer is partly a matter of taste. However, besides the superfi\u2010\ncial syntactic differences between the two, there is a big difference in the execution\nflow, which becomes apparent if you run this analysis on a large file.\nSorting versus in-memory aggregation\nThe Ruby script keeps an in-memory hash table of URLs, where each URL is mapped\nto the number of times it has been seen. The Unix pipeline example does not have\nsuch a hash table, but instead relies on sorting a list of URLs in which multiple occur\u2010\nrences of the same URL are simply repeated.\nWhich approach is better? It depends how many different URLs you have. For most\nsmall to mid-sized websites, you can probably fit all distinct URLs, and a counter for\neach URL, in (say) 1 GB of memory. In this example, the working set of the job (the\namount of memory to which the job needs random access) depends only on the\nnumber of distinct URLs: if there are a million log entries for a single URL, the space\nrequired in the hash table is still just one URL plus the size of the counter. If this"}
{"416": "same principle as we discussed in \u201cSSTables and LSM-Trees\u201d on page 76: chunks of\ndata can be sorted in memory and written out to disk as segment files, and then mul\u2010\ntiple sorted segments can be merged into a larger sorted file. Mergesort has sequential\naccess patterns that perform well on disks. (Remember that optimizing for sequential\nI/O was a recurring theme in Chapter 3. The same pattern reappears here.)\nThe sort utility in GNU Coreutils (Linux) automatically handles larger-than-\nmemory datasets by spilling to disk, and automatically parallelizes sorting across\nmultiple CPU cores [9]. This means that the simple chain of Unix commands we saw\nearlier easily scales to large datasets, without running out of memory. The bottleneck\nis likely to be the rate at which the input file can be read from disk.\nThe Unix Philosophy\nIt\u2019s no coincidence that we were able to analyze a log file quite easily, using a chain of\ncommands like in the previous example: this was in fact one of the key design ideas of\nUnix, and it remains astonishingly relevant today. Let\u2019s look at it in some more depth\nso that we can borrow some ideas from Unix [10].\nDoug McIlroy, the inventor of Unix pipes, first described them like this in 1964 [11]:\n\u201cWe should have some ways of connecting programs like [a] garden hose\u2014screw in\nanother segment when it becomes necessary to massage data in another way. This is\nthe way of I/O also.\u201d The plumbing analogy stuck, and the idea of connecting pro\u2010\ngrams with pipes became part of what is now known as the Unix philosophy\u2014a set of\ndesign principles that became popular among the developers and users of Unix. The\nphilosophy was described in 1978 as follows [12, 13]:\n1. Make each program do one thing well. To do a new job, build afresh rather than\ncomplicate old programs by adding new \u201cfeatures\u201d.\n2. Expect the output of every program to become the input to another, as yet\nunknown, program. Don\u2019t clutter output with extraneous information. Avoid\nstringently columnar or binary input formats. Don\u2019t insist on interactive input.\n3. Design and build software, even operating systems, to be tried early, ideally within\nweeks. Don\u2019t hesitate to throw away the clumsy parts and rebuild them.\n4. Use tools in preference to unskilled help to lighten a programming task, even if\nyou have to detour to build the tools and expect to throw some of them out after\nyou\u2019ve finished using them.\nThis approach\u2014automation, rapid prototyping, incremental iteration, being friendly\nto experimentation, and breaking down large projects into manageable chunks\u2014\nsounds remarkably like the Agile and DevOps movements of today. Surprisingly little"}
{"417": "The sort tool is a great example of a program that does one thing well. It is arguably\na better sorting implementation than most programming languages have in their\nstandard libraries (which do not spill to disk and do not use multiple threads, even\nwhen that would be beneficial). And yet, sort is barely useful in isolation. It only\nbecomes powerful in combination with the other Unix tools, such as uniq.\nA Unix shell like bash lets us easily compose these small programs into surprisingly\npowerful data processing jobs. Even though many of these programs are written by\ndifferent groups of people, they can be joined together in flexible ways. What does\nUnix do to enable this composability?\nA uniform interface\nIf you expect the output of one program to become the input to another program,\nthat means those programs must use the same data format\u2014in other words, a com\u2010\npatible interface. If you want to be able to connect any program\u2019s output to any pro\u2010\ngram\u2019s input, that means that all programs must use the same input/output interface.\nIn Unix, that interface is a file (or, more precisely, a file descriptor). A file is just an\nordered sequence of bytes. Because that is such a simple interface, many different\nthings can be represented using the same interface: an actual file on the filesystem, a\ncommunication channel to another process (Unix socket, stdin, stdout), a device\ndriver (say /dev/audio or /dev/lp0), a socket representing a TCP connection, and so\non. It\u2019s easy to take this for granted, but it\u2019s actually quite remarkable that these very\ndifferent things can share a uniform interface, so they can easily be plugged together.ii\nBy convention, many (but not all) Unix programs treat this sequence of bytes as\nASCII text. Our log analysis example used this fact: awk, sort, uniq, and head all treat\ntheir input file as a list of records separated by the \\n (newline, ASCII 0x0A) charac\u2010\nter. The choice of \\n is arbitrary\u2014arguably, the ASCII record separator 0x1E would\nhave been a better choice, since it\u2019s intended for this purpose [14]\u2014but in any case,\nthe fact that all these programs have standardized on using the same record separator\nallows them to interoperate.\nii. Another example of a uniform interface is URLs and HTTP, the foundations of the web. A URL identifies\na particular thing (resource) on a website, and you can link to any URL from any other website. A user with a\nweb browser can thus seamlessly jump between websites by following links, even though the servers may be\noperated by entirely unrelated organizations. This principle seems obvious today, but it was a key insight in"}
{"418": "The parsing of each record (i.e., a line of input) is more vague. Unix tools commonly\nsplit a line into fields by whitespace or tab characters, but CSV (comma-separated),\npipe-separated, and other encodings are also used. Even a fairly simple tool like\nxargs has half a dozen command-line options for specifying how its input should be\nparsed.\nThe uniform interface of ASCII text mostly works, but it\u2019s not exactly beautiful: our\nlog analysis example used {print $7} to extract the URL, which is not very readable.\nIn an ideal world this could have perhaps been {print $request_url} or something\nof that sort. We will return to this idea later.\nAlthough it\u2019s not perfect, even decades later, the uniform interface of Unix is still\nsomething remarkable. Not many pieces of software interoperate and compose as\nwell as Unix tools do: you can\u2019t easily pipe the contents of your email account and\nyour online shopping history through a custom analysis tool into a spreadsheet and\npost the results to a social network or a wiki. Today it\u2019s an exception, not the norm,\nto have programs that work together as smoothly as Unix tools do.\nEven databases with the same data model often don\u2019t make it easy to get data out of\none and into the other. This lack of integration leads to Balkanization of data.\nSeparation of logic and wiring\nAnother characteristic feature of Unix tools is their use of standard input (stdin) and\nstandard output (stdout). If you run a program and don\u2019t specify anything else,\nstdin comes from the keyboard and stdout goes to the screen. However, you can\nalso take input from a file and/or redirect output to a file. Pipes let you attach the\nstdout of one process to the stdin of another process (with a small in-memory\nbuffer, and without writing the entire intermediate data stream to disk).\nA program can still read and write files directly if it needs to, but the Unix approach\nworks best if a program doesn\u2019t worry about particular file paths and simply uses\nstdin and stdout. This allows a shell user to wire up the input and output in what\u2010\never way they want; the program doesn\u2019t know or care where the input is coming\nfrom and where the output is going to. (One could say this is a form of loose coupling,\nlate binding [15], or inversion of control [16].) Separating the input/output wiring\nfrom the program logic makes it easier to compose small tools into bigger systems.\nYou can even write your own programs and combine them with the tools provided\nby the operating system. Your program just needs to read input from stdin and write\noutput to stdout, and it can participate in data processing pipelines. In the log analy\u2010\nsis example, you could write a tool that translates user-agent strings into more sensi\u2010"}
{"419": "However, there are limits to what you can do with stdin and stdout. Programs that\nneed multiple inputs or outputs are possible but tricky. You can\u2019t pipe a program\u2019s\noutput into a network connection [17, 18].iii If a program directly opens files for read\u2010\ning and writing, or starts another program as a subprocess, or opens a network con\u2010\nnection, then that I/O is wired up by the program itself. It can still be configurable\n(through command-line options, for example), but the flexibility of wiring up inputs\nand outputs in a shell is reduced.\nTransparency and experimentation\nPart of what makes Unix tools so successful is that they make it quite easy to see what\nis going on:\n\u2022 The input files to Unix commands are normally treated as immutable. This\nmeans you can run the commands as often as you want, trying various\ncommand-line options, without damaging the input files.\n\u2022 You can end the pipeline at any point, pipe the output into less, and look at it to\nsee if it has the expected form. This ability to inspect is great for debugging.\n\u2022 You can write the output of one pipeline stage to a file and use that file as input\nto the next stage. This allows you to restart the later stage without rerunning the\nentire pipeline.\nThus, even though Unix tools are quite blunt, simple tools compared to a query opti\u2010\nmizer of a relational database, they remain amazingly useful, especially for experi\u2010\nmentation.\nHowever, the biggest limitation of Unix tools is that they run only on a single\nmachine\u2014and that\u2019s where tools like Hadoop come in.\nMapReduce and Distributed Filesystems\nMapReduce is a bit like Unix tools, but distributed across potentially thousands of\nmachines. Like Unix tools, it is a fairly blunt, brute-force, but surprisingly effective\ntool. A single MapReduce job is comparable to a single Unix process: it takes one or\nmore inputs and produces one or more outputs.\nAs with most Unix tools, running a MapReduce job normally does not modify the\ninput and does not have any side effects other than producing the output. The output"}
{"420": "files are written once, in a sequential fashion (not modifying any existing part of a file\nonce it has been written).\nWhile Unix tools use stdin and stdout as input and output, MapReduce jobs read\nand write files on a distributed filesystem. In Hadoop\u2019s implementation of Map\u2010\nReduce, that filesystem is called HDFS (Hadoop Distributed File System), an open\nsource reimplementation of the Google File System (GFS) [19].\nVarious other distributed filesystems besides HDFS exist, such as GlusterFS and the\nQuantcast File System (QFS) [20]. Object storage services such as Amazon S3, Azure\nBlob Storage, and OpenStack Swift [21] are similar in many ways.iv In this chapter we\nwill mostly use HDFS as a running example, but the principles apply to any dis\u2010\ntributed filesystem.\nHDFS is based on the shared-nothing principle (see the introduction to Part II), in\ncontrast to the shared-disk approach of Network Attached Storage (NAS) and Storage\nArea Network (SAN) architectures. Shared-disk storage is implemented by a central\u2010\nized storage appliance, often using custom hardware and special network infrastruc\u2010\nture such as Fibre Channel. On the other hand, the shared-nothing approach requires\nno special hardware, only computers connected by a conventional datacenter net\u2010\nwork.\nHDFS consists of a daemon process running on each machine, exposing a network\nservice that allows other nodes to access files stored on that machine (assuming that\nevery general-purpose machine in a datacenter has some disks attached to it). A cen\u2010\ntral server called the NameNode keeps track of which file blocks are stored on which\nmachine. Thus, HDFS conceptually creates one big filesystem that can use the space\non the disks of all machines running the daemon.\nIn order to tolerate machine and disk failures, file blocks are replicated on multiple\nmachines. Replication may mean simply several copies of the same data on multiple\nmachines, as in Chapter 5, or an erasure coding scheme such as Reed\u2013Solomon codes,\nwhich allows lost data to be recovered with lower storage overhead than full replica\u2010\ntion [20, 22]. The techniques are similar to RAID, which provides redundancy across\nseveral disks attached to the same machine; the difference is that in a distributed file\u2010\nsystem, file access and replication are done over a conventional datacenter network\nwithout special hardware."}
{"421": "HDFS has scaled well: at the time of writing, the biggest HDFS deployments run on\ntens of thousands of machines, with combined storage capacity of hundreds of peta\u2010\nbytes [23]. Such large scale has become viable because the cost of data storage and\naccess on HDFS, using commodity hardware and open source software, is much\nlower than that of the equivalent capacity on a dedicated storage appliance [24].\nMapReduce Job Execution\nMapReduce is a programming framework with which you can write code to process\nlarge datasets in a distributed filesystem like HDFS. The easiest way of understanding\nit is by referring back to the web server log analysis example in \u201cSimple Log Analysis\u201d\non page 391. The pattern of data processing in MapReduce is very similar to this\nexample:\n1. Read a set of input files, and break it up into records. In the web server log exam\u2010\nple, each record is one line in the log (that is, \\n is the record separator).\n2. Call the mapper function to extract a key and value from each input record. In\nthe preceding example, the mapper function is awk '{print $7}': it extracts the\nURL ($7) as the key, and leaves the value empty.\n3. Sort all of the key-value pairs by key. In the log example, this is done by the first\nsort command.\n4. Call the reducer function to iterate over the sorted key-value pairs. If there are\nmultiple occurrences of the same key, the sorting has made them adjacent in the\nlist, so it is easy to combine those values without having to keep a lot of state in\nmemory. In the preceding example, the reducer is implemented by the command\nuniq -c, which counts the number of adjacent records with the same key.\nThose four steps can be performed by one MapReduce job. Steps 2 (map) and 4\n(reduce) are where you write your custom data processing code. Step 1 (breaking files\ninto records) is handled by the input format parser. Step 3, the sort step, is implicit\nin MapReduce\u2014you don\u2019t have to write it, because the output from the mapper is\nalways sorted before it is given to the reducer.\nTo create a MapReduce job, you need to implement two callback functions, the map\u2010\nper and reducer, which behave as follows (see also \u201cMapReduce Querying\u201d on page\n46):\nMapper\nThe mapper is called once for every input record, and its job is to extract the key"}
{"422": "Reducer\nThe MapReduce framework takes the key-value pairs produced by the mappers,\ncollects all the values belonging to the same key, and calls the reducer with an\niterator over that collection of values. The reducer can produce output records\n(such as the number of occurrences of the same URL).\nIn the web server log example, we had a second sort command in step 5, which\nranked URLs by number of requests. In MapReduce, if you need a second sorting\nstage, you can implement it by writing a second MapReduce job and using the output\nof the first job as input to the second job. Viewed like this, the role of the mapper is to\nprepare the data by putting it into a form that is suitable for sorting, and the role of\nthe reducer is to process the data that has been sorted.\nDistributed execution of MapReduce\nThe main difference from pipelines of Unix commands is that MapReduce can paral\u2010\nlelize a computation across many machines, without you having to write code to\nexplicitly handle the parallelism. The mapper and reducer only operate on one record\nat a time; they don\u2019t need to know where their input is coming from or their output is\ngoing to, so the framework can handle the complexities of moving data between\nmachines.\nIt is possible to use standard Unix tools as mappers and reducers in a distributed\ncomputation [25], but more commonly they are implemented as functions in a con\u2010\nventional programming language. In Hadoop MapReduce, the mapper and reducer\nare each a Java class that implements a particular interface. In MongoDB and\nCouchDB, mappers and reducers are JavaScript functions (see \u201cMapReduce Query\u2010\ning\u201d on page 46).\nFigure 10-1 shows the dataflow in a Hadoop MapReduce job. Its parallelization is\nbased on partitioning (see Chapter 6): the input to a job is typically a directory in\nHDFS, and each file or file block within the input directory is considered to be a sepa\u2010\nrate partition that can be processed by a separate map task (marked by m 1, m 2, and\nm 3 in Figure 10-1).\nEach input file is typically hundreds of megabytes in size. The MapReduce scheduler\n(not shown in the diagram) tries to run each mapper on one of the machines that\nstores a replica of the input file, provided that machine has enough spare RAM and\nCPU resources to run the map task [26]. This principle is known as putting the com\u2010\nputation near the data [27]: it saves copying the input file over the network, reducing\nnetwork load and increasing locality."}
{"423": "Figure 10-1. A MapReduce job with three mappers and three reducers.\nIn most cases, the application code that should run in the map task is not yet present\non the machine that is assigned the task of running it, so the MapReduce framework\nfirst copies the code (e.g., JAR files in the case of a Java program) to the appropriate\nmachines. It then starts the map task and begins reading the input file, passing one\nrecord at a time to the mapper callback. The output of the mapper consists of key-\nvalue pairs.\nThe reduce side of the computation is also partitioned. While the number of map\ntasks is determined by the number of input file blocks, the number of reduce tasks is\nconfigured by the job author (it can be different from the number of map tasks). To\nensure that all key-value pairs with the same key end up at the same reducer, the\nframework uses a hash of the key to determine which reduce task should receive a\nparticular key-value pair (see \u201cPartitioning by Hash of Key\u201d on page 203).\nThe key-value pairs must be sorted, but the dataset is likely too large to be sorted with\na conventional sorting algorithm on a single machine. Instead, the sorting is per\u2010\nformed in stages. First, each map task partitions its output by reducer, based on the\nhash of the key. Each of these partitions is written to a sorted file on the mapper\u2019s\nlocal disk, using a technique similar to what we discussed in \u201cSSTables and LSM-"}
{"424": "Whenever a mapper finishes reading its input file and writing its sorted output files,\nthe MapReduce scheduler notifies the reducers that they can start fetching the output\nfiles from that mapper. The reducers connect to each of the mappers and download\nthe files of sorted key-value pairs for their partition. The process of partitioning by\nreducer, sorting, and copying data partitions from mappers to reducers is known as\nthe shuffle [26] (a confusing term\u2014unlike shuffling a deck of cards, there is no ran\u2010\ndomness in MapReduce).\nThe reduce task takes the files from the mappers and merges them together, preserv\u2010\ning the sort order. Thus, if different mappers produced records with the same key,\nthey will be adjacent in the merged reducer input.\nThe reducer is called with a key and an iterator that incrementally scans over all\nrecords with the same key (which may in some cases not all fit in memory). The\nreducer can use arbitrary logic to process these records, and can generate any number\nof output records. These output records are written to a file on the distributed filesys\u2010\ntem (usually, one copy on the local disk of the machine running the reducer, with\nreplicas on other machines).\nMapReduce workflows\nThe range of problems you can solve with a single MapReduce job is limited. Refer\u2010\nring back to the log analysis example, a single MapReduce job could determine the\nnumber of page views per URL, but not the most popular URLs, since that requires a\nsecond round of sorting.\nThus, it is very common for MapReduce jobs to be chained together into workflows,\nsuch that the output of one job becomes the input to the next job. The Hadoop Map\u2010\nReduce framework does not have any particular support for workflows, so this chain\u2010\ning is done implicitly by directory name: the first job must be configured to write its\noutput to a designated directory in HDFS, and the second job must be configured to\nread that same directory name as its input. From the MapReduce framework\u2019s point\nof view, they are two independent jobs.\nChained MapReduce jobs are therefore less like pipelines of Unix commands (which\npass the output of one process as input to another process directly, using only a small\nin-memory buffer) and more like a sequence of commands where each command\u2019s\noutput is written to a temporary file, and the next command reads from the tempo\u2010\nrary file. This design has advantages and disadvantages, which we will discuss in\n\u201cMaterialization of Intermediate State\u201d on page 419.\nA batch job\u2019s output is only considered valid when the job has completed successfully"}
{"425": "executions, various workflow schedulers for Hadoop have been developed, including\nOozie, Azkaban, Luigi, Airflow, and Pinball [28].\nThese schedulers also have management features that are useful when maintaining a\nlarge collection of batch jobs. Workflows consisting of 50 to 100 MapReduce jobs are\ncommon when building recommendation systems [29], and in a large organization,\nmany different teams may be running different jobs that read each other\u2019s output.\nTool support is important for managing such complex dataflows.\nVarious higher-level tools for Hadoop, such as Pig [30], Hive [31], Cascading [32],\nCrunch [33], and FlumeJava [34], also set up workflows of multiple MapReduce\nstages that are automatically wired together appropriately.\nReduce-Side Joins and Grouping\nWe discussed joins in Chapter 2 in the context of data models and query languages,\nbut we have not delved into how joins are actually implemented. It is time that we\npick up that thread again.\nIn many datasets it is common for one record to have an association with another\nrecord: a foreign key in a relational model, a document reference in a document\nmodel, or an edge in a graph model. A join is necessary whenever you have some\ncode that needs to access records on both sides of that association (both the record\nthat holds the reference and the record being referenced). As discussed in Chapter 2,\ndenormalization can reduce the need for joins but generally not remove it entirely.v\nIn a database, if you execute a query that involves only a small number of records, the\ndatabase will typically use an index to quickly locate the records of interest (see Chap\u2010\nter 3). If the query involves joins, it may require multiple index lookups. However,\nMapReduce has no concept of indexes\u2014at least not in the usual sense.\nWhen a MapReduce job is given a set of files as input, it reads the entire content of all\nof those files; a database would call this operation a full table scan. If you only want to\nread a small number of records, a full table scan is outrageously expensive compared\nto an index lookup. However, in analytic queries (see \u201cTransaction Processing or\nAnalytics?\u201d on page 90) it is common to want to calculate aggregates over a large\nnumber of records. In this case, scanning the entire input might be quite a reasonable\nthing to do, especially if you can parallelize the processing across multiple machines."}
{"426": "When we talk about joins in the context of batch processing, we mean resolving all\noccurrences of some association within a dataset. For example, we assume that a job\nis processing the data for all users simultaneously, not merely looking up the data for\none particular user (which would be done far more efficiently with an index).\nExample: analysis of user activity events\nA typical example of a join in a batch job is illustrated in Figure 10-2. On the left is a\nlog of events describing the things that logged-in users did on a website (known as\nactivity events or clickstream data), and on the right is a database of users. You can\nthink of this example as being part of a star schema (see \u201cStars and Snowflakes: Sche\u2010\nmas for Analytics\u201d on page 93): the log of events is the fact table, and the user data\u2010\nbase is one of the dimensions.\nFigure 10-2. A join between a log of user activity events and a database of user profiles.\nAn analytics task may need to correlate user activity with user profile information:\nfor example, if the profile contains the user\u2019s age or date of birth, the system could\ndetermine which pages are most popular with which age groups. However, the activ\u2010\nity events contain only the user ID, not the full user profile information. Embedding\nthat profile information in every single activity event would most likely be too waste\u2010\nful. Therefore, the activity events need to be joined with the user profile database.\nThe simplest implementation of this join would go over the activity events one by\none and query the user database (on a remote server) for every user ID it encounters.\nThis is possible, but it would most likely suffer from very poor performance: the pro\u2010\ncessing throughput would be limited by the round-trip time to the database server,\nthe effectiveness of a local cache would depend very much on the distribution of data,"}
{"427": "In order to achieve good throughput in a batch process, the computation must be (as\nmuch as possible) local to one machine. Making random-access requests over the\nnetwork for every record you want to process is too slow. Moreover, querying a\nremote database would mean that the batch job becomes nondeterministic, because\nthe data in the remote database might change.\nThus, a better approach would be to take a copy of the user database (for example,\nextracted from a database backup using an ETL process\u2014see \u201cData Warehousing\u201d on\npage 91) and to put it in the same distributed filesystem as the log of user activity\nevents. You would then have the user database in one set of files in HDFS and the\nuser activity records in another set of files, and could use MapReduce to bring\ntogether all of the relevant records in the same place and process them efficiently.\nSort-merge joins\nRecall that the purpose of the mapper is to extract a key and value from each input\nrecord. In the case of Figure 10-2, this key would be the user ID: one set of mappers\nwould go over the activity events (extracting the user ID as the key and the activity\nevent as the value), while another set of mappers would go over the user database\n(extracting the user ID as the key and the user\u2019s date of birth as the value). This pro\u2010\ncess is illustrated in Figure 10-3.\nFigure 10-3. A reduce-side sort-merge join on user ID. If the input datasets are parti\u2010\ntioned into multiple files, each could be processed with multiple mappers in parallel.\nWhen the MapReduce framework partitions the mapper output by key and then sorts\nthe key-value pairs, the effect is that all the activity events and the user record with"}
{"428": "sees the record from the user database first, followed by the activity events in time\u2010\nstamp order\u2014this technique is known as a secondary sort [26].\nThe reducer can then perform the actual join logic easily: the reducer function is\ncalled once for every user ID, and thanks to the secondary sort, the first value is\nexpected to be the date-of-birth record from the user database. The reducer stores the\ndate of birth in a local variable and then iterates over the activity events with the same\nuser ID, outputting pairs of viewed-url and viewer-age-in-years. Subsequent Map\u2010\nReduce jobs could then calculate the distribution of viewer ages for each URL, and\ncluster by age group.\nSince the reducer processes all of the records for a particular user ID in one go, it only\nneeds to keep one user record in memory at any one time, and it never needs to make\nany requests over the network. This algorithm is known as a sort-merge join, since\nmapper output is sorted by key, and the reducers then merge together the sorted lists\nof records from both sides of the join.\nBringing related data together in the same place\nIn a sort-merge join, the mappers and the sorting process make sure that all the nec\u2010\nessary data to perform the join operation for a particular user ID is brought together\nin the same place: a single call to the reducer. Having lined up all the required data in\nadvance, the reducer can be a fairly simple, single-threaded piece of code that can\nchurn through records with high throughput and low memory overhead.\nOne way of looking at this architecture is that mappers \u201csend messages\u201d to the reduc\u2010\ners. When a mapper emits a key-value pair, the key acts like the destination address\nto which the value should be delivered. Even though the key is just an arbitrary string\n(not an actual network address like an IP address and port number), it behaves like\nan address: all key-value pairs with the same key will be delivered to the same desti\u2010\nnation (a call to the reducer).\nUsing the MapReduce programming model has separated the physical network com\u2010\nmunication aspects of the computation (getting the data to the right machine) from\nthe application logic (processing the data once you have it). This separation contrasts\nwith the typical use of databases, where a request to fetch data from a database often\noccurs somewhere deep inside a piece of application code [36]. Since MapReduce\nhandles all network communication, it also shields the application code from having\nto worry about partial failures, such as the crash of another node: MapReduce trans\u2010\nparently retries failed tasks without affecting the application logic.\nGROUP BY"}
{"429": "records with the same key form a group, and the next step is often to perform some\nkind of aggregation within each group\u2014for example:\n\u2022 Counting the number of records in each group (like in our example of counting\npage views, which you would express as a COUNT(*) aggregation in SQL)\n\u2022 Adding up the values in one particular field (SUM(fieldname)) in SQL\n\u2022 Picking the top k records according to some ranking function\nThe simplest way of implementing such a grouping operation with MapReduce is to\nset up the mappers so that the key-value pairs they produce use the desired grouping\nkey. The partitioning and sorting process then brings together all the records with the\nsame key in the same reducer. Thus, grouping and joining look quite similar when\nimplemented on top of MapReduce.\nAnother common use for grouping is collating all the activity events for a particular\nuser session, in order to find out the sequence of actions that the user took\u2014a pro\u2010\ncess called sessionization [37]. For example, such analysis could be used to work out\nwhether users who were shown a new version of your website are more likely to make\na purchase than those who were shown the old version (A/B testing), or to calculate\nwhether some marketing activity is worthwhile.\nIf you have multiple web servers handling user requests, the activity events for a par\u2010\nticular user are most likely scattered across various different servers\u2019 log files. You can\nimplement sessionization by using a session cookie, user ID, or similar identifier as\nthe grouping key and bringing all the activity events for a particular user together in\none place, while distributing different users\u2019 events across different partitions.\nHandling skew\nThe pattern of \u201cbringing all records with the same key to the same place\u201d breaks\ndown if there is a very large amount of data related to a single key. For example, in a\nsocial network, most users might be connected to a few hundred people, but a small\nnumber of celebrities may have many millions of followers. Such disproportionately\nactive database records are known as linchpin objects [38] or hot keys.\nCollecting all activity related to a celebrity (e.g., replies to something they posted) in a\nsingle reducer can lead to significant skew (also known as hot spots)\u2014that is, one\nreducer that must process significantly more records than the others (see \u201cSkewed\nWorkloads and Relieving Hot Spots\u201d on page 205). Since a MapReduce job is only\ncomplete when all of its mappers and reducers have completed, any subsequent jobs\nmust wait for the slowest reducer to complete before they can start."}
{"430": "records relating to a hot key to one of several reducers, chosen at random (in contrast\nto conventional MapReduce, which chooses a reducer deterministically based on a\nhash of the key). For the other input to the join, records relating to the hot key need\nto be replicated to all reducers handling that key [40].\nThis technique spreads the work of handling the hot key over several reducers, which\nallows it to be parallelized better, at the cost of having to replicate the other join input\nto multiple reducers. The sharded join method in Crunch is similar, but requires the\nhot keys to be specified explicitly rather than using a sampling job. This technique is\nalso very similar to one we discussed in \u201cSkewed Workloads and Relieving Hot\nSpots\u201d on page 205, using randomization to alleviate hot spots in a partitioned data\u2010\nbase.\nHive\u2019s skewed join optimization takes an alternative approach. It requires hot keys to\nbe specified explicitly in the table metadata, and it stores records related to those keys\nin separate files from the rest. When performing a join on that table, it uses a map-\nside join (see the next section) for the hot keys.\nWhen grouping records by a hot key and aggregating them, you can perform the\ngrouping in two stages. The first MapReduce stage sends records to a random\nreducer, so that each reducer performs the grouping on a subset of records for the\nhot key and outputs a more compact aggregated value per key. The second Map\u2010\nReduce job then combines the values from all of the first-stage reducers into a single\nvalue per key.\nMap-Side Joins\nThe join algorithms described in the last section perform the actual join logic in the\nreducers, and are hence known as reduce-side joins. The mappers take the role of pre\u2010\nparing the input data: extracting the key and value from each input record, assigning\nthe key-value pairs to a reducer partition, and sorting by key.\nThe reduce-side approach has the advantage that you do not need to make any\nassumptions about the input data: whatever its properties and structure, the mappers\ncan prepare the data to be ready for joining. However, the downside is that all that\nsorting, copying to reducers, and merging of reducer inputs can be quite expensive.\nDepending on the available memory buffers, data may be written to disk several\ntimes as it passes through the stages of MapReduce [37].\nOn the other hand, if you can make certain assumptions about your input data, it is\npossible to make joins faster by using a so-called map-side join. This approach uses a\ncut-down MapReduce job in which there are no reducers and no sorting. Instead,"}
{"431": "Broadcast hash joins\nThe simplest way of performing a map-side join applies in the case where a large\ndataset is joined with a small dataset. In particular, the small dataset needs to be small\nenough that it can be loaded entirely into memory in each of the mappers.\nFor example, imagine in the case of Figure 10-2 that the user database is small\nenough to fit in memory. In this case, when a mapper starts up, it can first read the\nuser database from the distributed filesystem into an in-memory hash table. Once\nthis is done, the mapper can scan over the user activity events and simply look up the\nuser ID for each event in the hash table.vi\nThere can still be several map tasks: one for each file block of the large input to the\njoin (in the example of Figure 10-2, the activity events are the large input). Each of\nthese mappers loads the small input entirely into memory.\nThis simple but effective algorithm is called a broadcast hash join: the word broadcast\nreflects the fact that each mapper for a partition of the large input reads the entirety\nof the small input (so the small input is effectively \u201cbroadcast\u201d to all partitions of the\nlarge input), and the word hash reflects its use of a hash table. This join method is\nsupported by Pig (under the name \u201creplicated join\u201d), Hive (\u201cMapJoin\u201d), Cascading,\nand Crunch. It is also used in data warehouse query engines such as Impala [41].\nInstead of loading the small join input into an in-memory hash table, an alternative is\nto store the small join input in a read-only index on the local disk [42]. The fre\u2010\nquently used parts of this index will remain in the operating system\u2019s page cache, so\nthis approach can provide random-access lookups almost as fast as an in-memory\nhash table, but without actually requiring the dataset to fit in memory.\nPartitioned hash joins\nIf the inputs to the map-side join are partitioned in the same way, then the hash join\napproach can be applied to each partition independently. In the case of Figure 10-2,\nyou might arrange for the activity events and the user database to each be partitioned\nbased on the last decimal digit of the user ID (so there are 10 partitions on either\nside). For example, mapper 3 first loads all users with an ID ending in 3 into a hash\ntable, and then scans over all the activity events for each user whose ID ends in 3.\nIf the partitioning is done correctly, you can be sure that all the records you might\nwant to join are located in the same numbered partition, and so it is sufficient for\neach mapper to only read one partition from each of the input datasets. This has the\nadvantage that each mapper can load a smaller amount of data into its hash table."}
{"432": "This approach only works if both of the join\u2019s inputs have the same number of parti\u2010\ntions, with records assigned to partitions based on the same key and the same hash\nfunction. If the inputs are generated by prior MapReduce jobs that already perform\nthis grouping, then this can be a reasonable assumption to make.\nPartitioned hash joins are known as bucketed map joins in Hive [37].\nMap-side merge joins\nAnother variant of a map-side join applies if the input datasets are not only parti\u2010\ntioned in the same way, but also sorted based on the same key. In this case, it does not\nmatter whether the inputs are small enough to fit in memory, because a mapper can\nperform the same merging operation that would normally be done by a reducer:\nreading both input files incrementally, in order of ascending key, and matching\nrecords with the same key.\nIf a map-side merge join is possible, it probably means that prior MapReduce jobs\nbrought the input datasets into this partitioned and sorted form in the first place. In\nprinciple, this join could have been performed in the reduce stage of the prior job.\nHowever, it may still be appropriate to perform the merge join in a separate map-\nonly job, for example if the partitioned and sorted datasets are also needed for other\npurposes besides this particular join.\nMapReduce workflows with map-side joins\nWhen the output of a MapReduce join is consumed by downstream jobs, the choice\nof map-side or reduce-side join affects the structure of the output. The output of a\nreduce-side join is partitioned and sorted by the join key, whereas the output of a\nmap-side join is partitioned and sorted in the same way as the large input (since one\nmap task is started for each file block of the join\u2019s large input, regardless of whether a\npartitioned or broadcast join is used).\nAs discussed, map-side joins also make more assumptions about the size, sorting, and\npartitioning of their input datasets. Knowing about the physical layout of datasets in\nthe distributed filesystem becomes important when optimizing join strategies: it is\nnot sufficient to just know the encoding format and the name of the directory in\nwhich the data is stored; you must also know the number of partitions and the keys\nby which the data is partitioned and sorted.\nIn the Hadoop ecosystem, this kind of metadata about the partitioning of datasets is\noften maintained in HCatalog and the Hive metastore [37]."}
{"433": "The Output of Batch Workflows\nWe have talked a lot about the various algorithms for implementing workflows of\nMapReduce jobs, but we neglected an important question: what is the result of all of\nthat processing, once it is done? Why are we running all these jobs in the first place?\nIn the case of database queries, we distinguished transaction processing (OLTP) pur\u2010\nposes from analytic purposes (see \u201cTransaction Processing or Analytics?\u201d on page\n90). We saw that OLTP queries generally look up a small number of records by key,\nusing indexes, in order to present them to a user (for example, on a web page). On\nthe other hand, analytic queries often scan over a large number of records, perform\u2010\ning groupings and aggregations, and the output often has the form of a report: a\ngraph showing the change in a metric over time, or the top 10 items according to\nsome ranking, or a breakdown of some quantity into subcategories. The consumer of\nsuch a report is often an analyst or a manager who needs to make business decisions.\nWhere does batch processing fit in? It is not transaction processing, nor is it analyt\u2010\nics. It is closer to analytics, in that a batch process typically scans over large portions\nof an input dataset. However, a workflow of MapReduce jobs is not the same as a\nSQL query used for analytic purposes (see \u201cComparing Hadoop to Distributed Data\u2010\nbases\u201d on page 414). The output of a batch process is often not a report, but some\nother kind of structure.\nBuilding search indexes\nGoogle\u2019s original use of MapReduce was to build indexes for its search engine, which\nwas implemented as a workflow of 5 to 10 MapReduce jobs [1]. Although Google\nlater moved away from using MapReduce for this purpose [43], it helps to under\u2010\nstand MapReduce if you look at it through the lens of building a search index. (Even\ntoday, Hadoop MapReduce remains a good way of building indexes for Lucene/Solr\n[44].)\nWe saw briefly in \u201cFull-text search and fuzzy indexes\u201d on page 88 how a full-text\nsearch index such as Lucene works: it is a file (the term dictionary) in which you can\nefficiently look up a particular keyword and find the list of all the document IDs con\u2010\ntaining that keyword (the postings list). This is a very simplified view of a search\nindex\u2014in reality it requires various additional data, in order to rank search results by\nrelevance, correct misspellings, resolve synonyms, and so on\u2014but the principle\nholds.\nIf you need to perform a full-text search over a fixed set of documents, then a batch\nprocess is a very effective way of building the indexes: the mappers partition the set of"}
{"434": "Since querying a search index by keyword is a read-only operation, these index files\nare immutable once they have been created.\nIf the indexed set of documents changes, one option is to periodically rerun the entire\nindexing workflow for the entire set of documents, and replace the previous index\nfiles wholesale with the new index files when it is done. This approach can be compu\u2010\ntationally expensive if only a small number of documents have changed, but it has the\nadvantage that the indexing process is very easy to reason about: documents in,\nindexes out.\nAlternatively, it is possible to build indexes incrementally. As discussed in Chapter 3,\nif you want to add, remove, or update documents in an index, Lucene writes out new\nsegment files and asynchronously merges and compacts segment files in the back\u2010\nground. We will see more on such incremental processing in Chapter 11.\nKey-value stores as batch process output\nSearch indexes are just one example of the possible outputs of a batch processing\nworkflow. Another common use for batch processing is to build machine learning\nsystems such as classifiers (e.g., spam filters, anomaly detection, image recognition)\nand recommendation systems (e.g., people you may know, products you may be\ninterested in, or related searches [29]).\nThe output of those batch jobs is often some kind of database: for example, a data\u2010\nbase that can be queried by user ID to obtain suggested friends for that user, or a\ndatabase that can be queried by product ID to get a list of related products [45].\nThese databases need to be queried from the web application that handles user\nrequests, which is usually separate from the Hadoop infrastructure. So how does the\noutput from the batch process get back into a database where the web application can\nquery it?\nThe most obvious choice might be to use the client library for your favorite database\ndirectly within a mapper or reducer, and to write from the batch job directly to the\ndatabase server, one record at a time. This will work (assuming your firewall rules\nallow direct access from your Hadoop environment to your production databases),\nbut it is a bad idea for several reasons:\n\u2022 As discussed previously in the context of joins, making a network request for\nevery single record is orders of magnitude slower than the normal throughput of\na batch task. Even if the client library supports batching, performance is likely to\nbe poor."}
{"435": "ies is likely to suffer. This can in turn cause operational problems in other parts\nof the system [35].\n\u2022 Normally, MapReduce provides a clean all-or-nothing guarantee for job output:\nif a job succeeds, the result is the output of running every task exactly once, even\nif some tasks failed and had to be retried along the way; if the entire job fails, no\noutput is produced. However, writing to an external system from inside a job\nproduces externally visible side effects that cannot be hidden in this way. Thus,\nyou have to worry about the results from partially completed jobs being visible to\nother systems, and the complexities of Hadoop task attempts and speculative\nexecution.\nA much better solution is to build a brand-new database inside the batch job and\nwrite it as files to the job\u2019s output directory in the distributed filesystem, just like the\nsearch indexes in the last section. Those data files are then immutable once written,\nand can be loaded in bulk into servers that handle read-only queries. Various key-\nvalue stores support building database files in MapReduce jobs, including Voldemort\n[46], Terrapin [47], ElephantDB [48], and HBase bulk loading [49].\nBuilding these database files is a good use of MapReduce: using a mapper to extract a\nkey and then sorting by that key is already a lot of the work required to build an\nindex. Since most of these key-value stores are read-only (the files can only be written\nonce by a batch job and are then immutable), the data structures are quite simple. For\nexample, they do not require a WAL (see \u201cMaking B-trees reliable\u201d on page 82).\nWhen loading data into Voldemort, the server continues serving requests to the old\ndata files while the new data files are copied from the distributed filesystem to the\nserver\u2019s local disk. Once the copying is complete, the server atomically switches over\nto querying the new files. If anything goes wrong in this process, it can easily switch\nback to the old files again, since they are still there and immutable [46].\nPhilosophy of batch process outputs\nThe Unix philosophy that we discussed earlier in this chapter (\u201cThe Unix Philoso\u2010\nphy\u201d on page 394) encourages experimentation by being very explicit about dataflow:\na program reads its input and writes its output. In the process, the input is left\nunchanged, any previous output is completely replaced with the new output, and\nthere are no other side effects. This means that you can rerun a command as often as\nyou like, tweaking or debugging it, without messing up the state of your system.\nThe handling of output from MapReduce jobs follows the same philosophy. By treat\u2010\ning inputs as immutable and avoiding side effects (such as writing to external data\u2010"}
{"436": "\u2022 If you introduce a bug into the code and the output is wrong or corrupted, you\ncan simply roll back to a previous version of the code and rerun the job, and the\noutput will be correct again. Or, even simpler, you can keep the old output in a\ndifferent directory and simply switch back to it. Databases with read-write trans\u2010\nactions do not have this property: if you deploy buggy code that writes bad data\nto the database, then rolling back the code will do nothing to fix the data in the\ndatabase. (The idea of being able to recover from buggy code has been called\nhuman fault tolerance [50].)\n\u2022 As a consequence of this ease of rolling back, feature development can proceed\nmore quickly than in an environment where mistakes could mean irreversible\ndamage. This principle of minimizing irreversibility is beneficial for Agile soft\u2010\nware development [51].\n\u2022 If a map or reduce task fails, the MapReduce framework automatically re-\nschedules it and runs it again on the same input. If the failure is due to a bug in\nthe code, it will keep crashing and eventually cause the job to fail after a few\nattempts; but if the failure is due to a transient issue, the fault is tolerated. This\nautomatic retry is only safe because inputs are immutable and outputs from\nfailed tasks are discarded by the MapReduce framework.\n\u2022 The same set of files can be used as input for various different jobs, including\nmonitoring jobs that calculate metrics and evaluate whether a job\u2019s output has\nthe expected characteristics (for example, by comparing it to the output from the\nprevious run and measuring discrepancies).\n\u2022 Like Unix tools, MapReduce jobs separate logic from wiring (configuring the\ninput and output directories), which provides a separation of concerns and ena\u2010\nbles potential reuse of code: one team can focus on implementing a job that does\none thing well, while other teams can decide where and when to run that job.\nIn these areas, the design principles that worked well for Unix also seem to be work\u2010\ning well for Hadoop\u2014but Unix and Hadoop also differ in some ways. For example,\nbecause most Unix tools assume untyped text files, they have to do a lot of input\nparsing (our log analysis example at the beginning of the chapter used {print $7} to\nextract the URL). On Hadoop, some of those low-value syntactic conversions are\neliminated by using more structured file formats: Avro (see \u201cAvro\u201d on page 122) and\nParquet (see \u201cColumn-Oriented Storage\u201d on page 95) are often used, as they provide\nefficient schema-based encoding and allow evolution of their schemas over time (see\nChapter 4).\nComparing Hadoop to Distributed Databases"}
{"437": "(which happens to always run the sort utility between the map phase and the reduce\nphase). We saw how you can implement various join and grouping operations on top\nof these primitives.\nWhen the MapReduce paper [1] was published, it was\u2014in some sense\u2014not at all\nnew. All of the processing and parallel join algorithms that we discussed in the last\nfew sections had already been implemented in so-called massively parallel processing\n(MPP) databases more than a decade previously [3, 40]. For example, the Gamma\ndatabase machine, Teradata, and Tandem NonStop SQL were pioneers in this area\n[52].\nThe biggest difference is that MPP databases focus on parallel execution of analytic\nSQL queries on a cluster of machines, while the combination of MapReduce and a\ndistributed filesystem [19] provides something much more like a general-purpose\noperating system that can run arbitrary programs.\nDiversity of storage\nDatabases require you to structure data according to a particular model (e.g., rela\u2010\ntional or documents), whereas files in a distributed filesystem are just byte sequences,\nwhich can be written using any data model and encoding. They might be collections\nof database records, but they can equally well be text, images, videos, sensor readings,\nsparse matrices, feature vectors, genome sequences, or any other kind of data.\nTo put it bluntly, Hadoop opened up the possibility of indiscriminately dumping data\ninto HDFS, and only later figuring out how to process it further [53]. By contrast,\nMPP databases typically require careful up-front modeling of the data and query pat\u2010\nterns before importing the data into the database\u2019s proprietary storage format.\nFrom a purist\u2019s point of view, it may seem that this careful modeling and import is\ndesirable, because it means users of the database have better-quality data to work\nwith. However, in practice, it appears that simply making data available quickly\u2014\neven if it is in a quirky, difficult-to-use, raw format\u2014is often more valuable than try\u2010\ning to decide on the ideal data model up front [54].\nThe idea is similar to a data warehouse (see \u201cData Warehousing\u201d on page 91): simply\nbringing data from various parts of a large organization together in one place is val\u2010\nuable, because it enables joins across datasets that were previously disparate. The\ncareful schema design required by an MPP database slows down that centralized data\ncollection; collecting data in its raw form, and worrying about schema design later,\nallows the data collection to be speeded up (a concept sometimes known as a \u201cdata\nlake\u201d or \u201centerprise data hub\u201d [55])."}
{"438": "[56]; see \u201cSchema flexibility in the document model\u201d on page 39). This can be an\nadvantage if the producer and consumers are different teams with different priorities.\nThere may not even be one ideal data model, but rather different views onto the data\nthat are suitable for different purposes. Simply dumping data in its raw form allows\nfor several such transformations. This approach has been dubbed the sushi principle:\n\u201craw data is better\u201d [57].\nThus, Hadoop has often been used for implementing ETL processes (see \u201cData Ware\u2010\nhousing\u201d on page 91): data from transaction processing systems is dumped into the\ndistributed filesystem in some raw form, and then MapReduce jobs are written to\nclean up that data, transform it into a relational form, and import it into an MPP data\nwarehouse for analytic purposes. Data modeling still happens, but it is in a separate\nstep, decoupled from the data collection. This decoupling is possible because a dis\u2010\ntributed filesystem supports data encoded in any format.\nDiversity of processing models\nMPP databases are monolithic, tightly integrated pieces of software that take care of\nstorage layout on disk, query planning, scheduling, and execution. Since these com\u2010\nponents can all be tuned and optimized for the specific needs of the database, the sys\u2010\ntem as a whole can achieve very good performance on the types of queries for which\nit is designed. Moreover, the SQL query language allows expressive queries and ele\u2010\ngant semantics without the need to write code, making it accessible to graphical tools\nused by business analysts (such as Tableau).\nOn the other hand, not all kinds of processing can be sensibly expressed as SQL quer\u2010\nies. For example, if you are building machine learning and recommendation systems,\nor full-text search indexes with relevance ranking models, or performing image anal\u2010\nysis, you most likely need a more general model of data processing. These kinds of\nprocessing are often very specific to a particular application (e.g., feature engineering\nfor machine learning, natural language models for machine translation, risk estima\u2010\ntion functions for fraud prediction), so they inevitably require writing code, not just\nqueries.\nMapReduce gave engineers the ability to easily run their own code over large data\u2010\nsets. If you have HDFS and MapReduce, you can build a SQL query execution engine\non top of it, and indeed this is what the Hive project did [31]. However, you can also\nwrite many other forms of batch processes that do not lend themselves to being\nexpressed as a SQL query.\nSubsequently, people found that MapReduce was too limiting and performed too\nbadly for some types of processing, so various other processing models were devel\u2010"}
{"439": "form, it was feasible to implement a whole range of approaches, which would not\nhave been possible within the confines of a monolithic MPP database [58].\nCrucially, those various processing models can all be run on a single shared-use clus\u2010\nter of machines, all accessing the same files on the distributed filesystem. In the\nHadoop approach, there is no need to import the data into several different special\u2010\nized systems for different kinds of processing: the system is flexible enough to sup\u2010\nport a diverse set of workloads within the same cluster. Not having to move data\naround makes it a lot easier to derive value from the data, and a lot easier to experi\u2010\nment with new processing models.\nThe Hadoop ecosystem includes both random-access OLTP databases such as HBase\n(see \u201cSSTables and LSM-Trees\u201d on page 76) and MPP-style analytic databases such as\nImpala [41]. Neither HBase nor Impala uses MapReduce, but both use HDFS for\nstorage. They are very different approaches to accessing and processing data, but they\ncan nevertheless coexist and be integrated in the same system.\nDesigning for frequent faults\nWhen comparing MapReduce to MPP databases, two more differences in design\napproach stand out: the handling of faults and the use of memory and disk. Batch\nprocesses are less sensitive to faults than online systems, because they do not immedi\u2010\nately affect users if they fail and they can always be run again.\nIf a node crashes while a query is executing, most MPP databases abort the entire\nquery, and either let the user resubmit the query or automatically run it again [3]. As\nqueries normally run for a few seconds or a few minutes at most, this way of handling\nerrors is acceptable, since the cost of retrying is not too great. MPP databases also\nprefer to keep as much data as possible in memory (e.g., using hash joins) to avoid\nthe cost of reading from disk.\nOn the other hand, MapReduce can tolerate the failure of a map or reduce task\nwithout it affecting the job as a whole by retrying work at the granularity of an indi\u2010\nvidual task. It is also very eager to write data to disk, partly for fault tolerance, and\npartly on the assumption that the dataset will be too big to fit in memory anyway.\nThe MapReduce approach is more appropriate for larger jobs: jobs that process so\nmuch data and run for such a long time that they are likely to experience at least one\ntask failure along the way. In that case, rerunning the entire job due to a single task\nfailure would be wasteful. Even if recovery at the granularity of an individual task\nintroduces overheads that make fault-free processing slower, it can still be a reason\u2010\nable trade-off if the rate of task failures is high enough."}
{"440": "ence a machine failure. Is it really worth incurring significant overheads for the sake\nof fault tolerance?\nTo understand the reasons for MapReduce\u2019s sparing use of memory and task-level\nrecovery, it is helpful to look at the environment for which MapReduce was originally\ndesigned. Google has mixed-use datacenters, in which online production services and\noffline batch jobs run on the same machines. Every task has a resource allocation\n(CPU cores, RAM, disk space, etc.) that is enforced using containers. Every task also\nhas a priority, and if a higher-priority task needs more resources, lower-priority tasks\non the same machine can be terminated (preempted) in order to free up resources.\nPriority also determines pricing of the computing resources: teams must pay for the\nresources they use, and higher-priority processes cost more [59].\nThis architecture allows non-production (low-priority) computing resources to be\novercommitted, because the system knows that it can reclaim the resources if neces\u2010\nsary. Overcommitting resources in turn allows better utilization of machines and\ngreater efficiency compared to systems that segregate production and non-\nproduction tasks. However, as MapReduce jobs run at low priority, they run the risk\nof being preempted at any time because a higher-priority process requires their\nresources. Batch jobs effectively \u201cpick up the scraps under the table,\u201d using any com\u2010\nputing resources that remain after the high-priority processes have taken what they\nneed.\nAt Google, a MapReduce task that runs for an hour has an approximately 5% risk of\nbeing terminated to make space for a higher-priority process. This rate is more than\nan order of magnitude higher than the rate of failures due to hardware issues,\nmachine reboot, or other reasons [59]. At this rate of preemptions, if a job has 100\ntasks that each run for 10 minutes, there is a risk greater than 50% that at least one\ntask will be terminated before it is finished.\nAnd this is why MapReduce is designed to tolerate frequent unexpected task termina\u2010\ntion: it\u2019s not because the hardware is particularly unreliable, it\u2019s because the freedom\nto arbitrarily terminate processes enables better resource utilization in a computing\ncluster.\nAmong open source cluster schedulers, preemption is less widely used. YARN\u2019s\nCapacityScheduler supports preemption for balancing the resource allocation of dif\u2010\nferent queues [58], but general priority preemption is not supported in YARN,\nMesos, or Kubernetes at the time of writing [60]. In an environment where tasks are\nnot so often terminated, the design decisions of MapReduce make less sense. In the\nnext section, we will look at some alternatives to MapReduce that make different\ndesign decisions."}
{"441": "Beyond MapReduce\nAlthough MapReduce became very popular and received a lot of hype in the late\n2000s, it is just one among many possible programming models for distributed sys\u2010\ntems. Depending on the volume of data, the structure of the data, and the type of pro\u2010\ncessing being done with it, other tools may be more appropriate for expressing a\ncomputation.\nWe nevertheless spent a lot of time in this chapter discussing MapReduce because it\nis a useful learning tool, as it is a fairly clear and simple abstraction on top of a dis\u2010\ntributed filesystem. That is, simple in the sense of being able to understand what it is\ndoing, not in the sense of being easy to use. Quite the opposite: implementing a com\u2010\nplex processing job using the raw MapReduce APIs is actually quite hard and labori\u2010\nous\u2014for instance, you would need to implement any join algorithms from scratch\n[37].\nIn response to the difficulty of using MapReduce directly, various higher-level pro\u2010\ngramming models (Pig, Hive, Cascading, Crunch) were created as abstractions on top\nof MapReduce. If you understand how MapReduce works, they are fairly easy to\nlearn, and their higher-level constructs make many common batch processing tasks\nsignificantly easier to implement.\nHowever, there are also problems with the MapReduce execution model itself, which\nare not fixed by adding another level of abstraction and which manifest themselves as\npoor performance for some kinds of processing. On the one hand, MapReduce is\nvery robust: you can use it to process almost arbitrarily large quantities of data on an\nunreliable multi-tenant system with frequent task terminations, and it will still get the\njob done (albeit slowly). On the other hand, other tools are sometimes orders of mag\u2010\nnitude faster for some kinds of processing.\nIn the rest of this chapter, we will look at some of those alternatives for batch process\u2010\ning. In Chapter 11 we will move to stream processing, which can be regarded as\nanother way of speeding up batch processing.\nMaterialization of Intermediate State\nAs discussed previously, every MapReduce job is independent from every other job.\nThe main contact points of a job with the rest of the world are its input and output\ndirectories on the distributed filesystem. If you want the output of one job to become\nthe input to a second job, you need to configure the second job\u2019s input directory to be\nthe same as the first job\u2019s output directory, and an external workflow scheduler must"}
{"442": "by name and reuse it as input to several different jobs (including jobs developed by\nother teams). Publishing data to a well-known location in the distributed filesystem\nallows loose coupling so that jobs don\u2019t need to know who is producing their input or\nconsuming their output (see \u201cSeparation of logic and wiring\u201d on page 396).\nHowever, in many cases, you know that the output of one job is only ever used as\ninput to one other job, which is maintained by the same team. In this case, the files\non the distributed filesystem are simply intermediate state: a means of passing data\nfrom one job to the next. In the complex workflows used to build recommendation\nsystems consisting of 50 or 100 MapReduce jobs [29], there is a lot of such intermedi\u2010\nate state.\nThe process of writing out this intermediate state to files is called materialization.\n(We came across the term previously in the context of materialized views, in \u201cAggre\u2010\ngation: Data Cubes and Materialized Views\u201d on page 101. It means to eagerly com\u2010\npute the result of some operation and write it out, rather than computing it on\ndemand when requested.)\nBy contrast, the log analysis example at the beginning of the chapter used Unix pipes\nto connect the output of one command with the input of another. Pipes do not fully\nmaterialize the intermediate state, but instead stream the output to the input incre\u2010\nmentally, using only a small in-memory buffer.\nMapReduce\u2019s approach of fully materializing intermediate state has downsides com\u2010\npared to Unix pipes:\n\u2022 A MapReduce job can only start when all tasks in the preceding jobs (that gener\u2010\nate its inputs) have completed, whereas processes connected by a Unix pipe are\nstarted at the same time, with output being consumed as soon as it is produced.\nSkew or varying load on different machines means that a job often has a few\nstraggler tasks that take much longer to complete than the others. Having to wait\nuntil all of the preceding job\u2019s tasks have completed slows down the execution of\nthe workflow as a whole.\n\u2022 Mappers are often redundant: they just read back the same file that was just writ\u2010\nten by a reducer, and prepare it for the next stage of partitioning and sorting. In\nmany cases, the mapper code could be part of the previous reducer: if the reducer\noutput was partitioned and sorted in the same way as mapper output, then\nreducers could be chained together directly, without interleaving with mapper\nstages.\n\u2022 Storing intermediate state in a distributed filesystem means those files are repli\u2010\ncated across several nodes, which is often overkill for such temporary data."}
{"443": "Dataflow engines\nIn order to fix these problems with MapReduce, several new execution engines for\ndistributed batch computations were developed, the most well known of which are\nSpark [61, 62], Tez [63, 64], and Flink [65, 66]. There are various differences in the\nway they are designed, but they have one thing in common: they handle an entire\nworkflow as one job, rather than breaking it up into independent subjobs.\nSince they explicitly model the flow of data through several processing stages, these\nsystems are known as dataflow engines. Like MapReduce, they work by repeatedly\ncalling a user-defined function to process one record at a time on a single thread.\nThey parallelize work by partitioning inputs, and they copy the output of one func\u2010\ntion over the network to become the input to another function.\nUnlike in MapReduce, these functions need not take the strict roles of alternating\nmap and reduce, but instead can be assembled in more flexible ways. We call these\nfunctions operators, and the dataflow engine provides several different options for\nconnecting one operator\u2019s output to another\u2019s input:\n\u2022 One option is to repartition and sort records by key, like in the shuffle stage of\nMapReduce (see \u201cDistributed execution of MapReduce\u201d on page 400). This fea\u2010\nture enables sort-merge joins and grouping in the same way as in MapReduce.\n\u2022 Another possibility is to take several inputs and to partition them in the same\nway, but skip the sorting. This saves effort on partitioned hash joins, where the\npartitioning of records is important but the order is irrelevant because building\nthe hash table randomizes the order anyway.\n\u2022 For broadcast hash joins, the same output from one operator can be sent to all\npartitions of the join operator.\nThis style of processing engine is based on research systems like Dryad [67] and\nNephele [68], and it offers several advantages compared to the MapReduce model:\n\u2022 Expensive work such as sorting need only be performed in places where it is\nactually required, rather than always happening by default between every map\nand reduce stage.\n\u2022 There are no unnecessary map tasks, since the work done by a mapper can often\nbe incorporated into the preceding reduce operator (because a mapper does not\nchange the partitioning of a dataset).\n\u2022 Because all joins and data dependencies in a workflow are explicitly declared, the\nscheduler has an overview of what data is required where, so it can make locality"}
{"444": "exchanged through a shared memory buffer rather than having to copy it over\nthe network.\n\u2022 It is usually sufficient for intermediate state between operators to be kept in\nmemory or written to local disk, which requires less I/O than writing it to HDFS\n(where it must be replicated to several machines and written to disk on each rep\u2010\nlica). MapReduce already uses this optimization for mapper output, but dataflow\nengines generalize the idea to all intermediate state.\n\u2022 Operators can start executing as soon as their input is ready; there is no need to\nwait for the entire preceding stage to finish before the next one starts.\n\u2022 Existing Java Virtual Machine (JVM) processes can be reused to run new opera\u2010\ntors, reducing startup overheads compared to MapReduce (which launches a\nnew JVM for each task).\nYou can use dataflow engines to implement the same computations as MapReduce\nworkflows, and they usually execute significantly faster due to the optimizations\ndescribed here. Since operators are a generalization of map and reduce, the same pro\u2010\ncessing code can run on either execution engine: workflows implemented in Pig,\nHive, or Cascading can be switched from MapReduce to Tez or Spark with a simple\nconfiguration change, without modifying code [64].\nTez is a fairly thin library that relies on the YARN shuffle service for the actual copy\u2010\ning of data between nodes [58], whereas Spark and Flink are big frameworks that\ninclude their own network communication layer, scheduler, and user-facing APIs.\nWe will discuss those high-level APIs shortly.\nFault tolerance\nAn advantage of fully materializing intermediate state to a distributed filesystem is\nthat it is durable, which makes fault tolerance fairly easy in MapReduce: if a task fails,\nit can just be restarted on another machine and read the same input again from the\nfilesystem.\nSpark, Flink, and Tez avoid writing intermediate state to HDFS, so they take a differ\u2010\nent approach to tolerating faults: if a machine fails and the intermediate state on that\nmachine is lost, it is recomputed from other data that is still available (a prior inter\u2010\nmediary stage if possible, or otherwise the original input data, which is normally on\nHDFS).\nTo enable this recomputation, the framework must keep track of how a given piece of\ndata was computed\u2014which input partitions it used, and which operators were"}
{"445": "When recomputing data, it is important to know whether the computation is deter\u2010\nministic: that is, given the same input data, do the operators always produce the same\noutput? This question matters if some of the lost data has already been sent to down\u2010\nstream operators. If the operator is restarted and the recomputed data is not the same\nas the original lost data, it becomes very hard for downstream operators to resolve the\ncontradictions between the old and new data. The solution in the case of nondeter\u2010\nministic operators is normally to kill the downstream operators as well, and run them\nagain on the new data.\nIn order to avoid such cascading faults, it is better to make operators deterministic.\nNote however that it is easy for nondeterministic behavior to accidentally creep in:\nfor example, many programming languages do not guarantee any particular order\nwhen iterating over elements of a hash table, many probabilistic and statistical\nalgorithms explicitly rely on using random numbers, and any use of the system clock\nor external data sources is nondeterministic. Such causes of nondeterminism need to\nbe removed in order to reliably recover from faults, for example by generating\npseudorandom numbers using a fixed seed.\nRecovering from faults by recomputing data is not always the right answer: if the\nintermediate data is much smaller than the source data, or if the computation is very\nCPU-intensive, it is probably cheaper to materialize the intermediate data to files\nthan to recompute it.\nDiscussion of materialization\nReturning to the Unix analogy, we saw that MapReduce is like writing the output of\neach command to a temporary file, whereas dataflow engines look much more like\nUnix pipes. Flink especially is built around the idea of pipelined execution: that is,\nincrementally passing the output of an operator to other operators, and not waiting\nfor the input to be complete before starting to process it.\nA sorting operation inevitably needs to consume its entire input before it can pro\u2010\nduce any output, because it\u2019s possible that the very last input record is the one with\nthe lowest key and thus needs to be the very first output record. Any operator that\nrequires sorting will thus need to accumulate state, at least temporarily. But many\nother parts of a workflow can be executed in a pipelined manner.\nWhen the job completes, its output needs to go somewhere durable so that users can\nfind it and use it\u2014most likely, it is written to the distributed filesystem again. Thus,\nwhen using a dataflow engine, materialized datasets on HDFS are still usually the\ninputs and the final outputs of a job. Like with MapReduce, the inputs are immutable\nand the output is completely replaced. The improvement over MapReduce is that you"}
{"446": "Graphs and Iterative Processing\nIn \u201cGraph-Like Data Models\u201d on page 49 we discussed using graphs for modeling\ndata, and using graph query languages to traverse the edges and vertices in a graph.\nThe discussion in Chapter 2 was focused around OLTP-style use: quickly executing\nqueries to find a small number of vertices matching certain criteria.\nIt is also interesting to look at graphs in a batch processing context, where the goal is\nto perform some kind of offline processing or analysis on an entire graph. This need\noften arises in machine learning applications such as recommendation engines, or in\nranking systems. For example, one of the most famous graph analysis algorithms is\nPageRank [69], which tries to estimate the popularity of a web page based on what\nother web pages link to it. It is used as part of the formula that determines the order\nin which web search engines present their results.\nDataflow engines like Spark, Flink, and Tez (see \u201cMaterialization of\nIntermediate State\u201d on page 419) typically arrange the operators in\na job as a directed acyclic graph (DAG). This is not the same as\ngraph processing: in dataflow engines, the flow of data from one\noperator to another is structured as a graph, while the data itself\ntypically consists of relational-style tuples. In graph processing, the\ndata itself has the form of a graph. Another unfortunate naming\nconfusion!\nMany graph algorithms are expressed by traversing one edge at a time, joining one\nvertex with an adjacent vertex in order to propagate some information, and repeating\nuntil some condition is met\u2014for example, until there are no more edges to follow, or\nuntil some metric converges. We saw an example in Figure 2-6, which made a list of\nall the locations in North America contained in a database by repeatedly following\nedges indicating which location is within which other location (this kind of algorithm\nis called a transitive closure).\nIt is possible to store a graph in a distributed filesystem (in files containing lists of\nvertices and edges), but this idea of \u201crepeating until done\u201d cannot be expressed in\nplain MapReduce, since it only performs a single pass over the data. This kind of\nalgorithm is thus often implemented in an iterative style:\n1. An external scheduler runs a batch process to calculate one step of the algorithm.\n2. When the batch process completes, the scheduler checks whether it has finished\n(based on the completion condition\u2014e.g., there are no more edges to follow, or"}
{"447": "This approach works, but implementing it with MapReduce is often very inefficient,\nbecause MapReduce does not account for the iterative nature of the algorithm: it will\nalways read the entire input dataset and produce a completely new output dataset,\neven if only a small part of the graph has changed compared to the last iteration.\nThe Pregel processing model\nAs an optimization for batch processing graphs, the bulk synchronous parallel (BSP)\nmodel of computation [70] has become popular. Among others, it is implemented by\nApache Giraph [37], Spark\u2019s GraphX API, and Flink\u2019s Gelly API [71]. It is also\nknown as the Pregel model, as Google\u2019s Pregel paper popularized this approach for\nprocessing graphs [72].\nRecall that in MapReduce, mappers conceptually \u201csend a message\u201d to a particular call\nof the reducer because the framework collects together all the mapper outputs with\nthe same key. A similar idea is behind Pregel: one vertex can \u201csend a message\u201d to\nanother vertex, and typically those messages are sent along the edges in a graph.\nIn each iteration, a function is called for each vertex, passing it all the messages that\nwere sent to it\u2014much like a call to the reducer. The difference from MapReduce is\nthat in the Pregel model, a vertex remembers its state in memory from one iteration\nto the next, so the function only needs to process new incoming messages. If no mes\u2010\nsages are being sent in some part of the graph, no work needs to be done.\nIt\u2019s a bit similar to the actor model (see \u201cDistributed actor frameworks\u201d on page 138),\nif you think of each vertex as an actor, except that vertex state and messages between\nvertices are fault-tolerant and durable, and communication proceeds in fixed rounds:\nat every iteration, the framework delivers all messages sent in the previous iteration.\nActors normally have no such timing guarantee.\nFault tolerance\nThe fact that vertices can only communicate by message passing (not by querying\neach other directly) helps improve the performance of Pregel jobs, since messages can\nbe batched and there is less waiting for communication. The only waiting is between\niterations: since the Pregel model guarantees that all messages sent in one iteration\nare delivered in the next iteration, the prior iteration must completely finish, and all\nof its messages must be copied over the network, before the next one can start.\nEven though the underlying network may drop, duplicate, or arbitrarily delay mes\u2010\nsages (see \u201cUnreliable Networks\u201d on page 277), Pregel implementations guarantee\nthat messages are processed exactly once at their destination vertex in the following"}
{"448": "This fault tolerance is achieved by periodically checkpointing the state of all vertices\nat the end of an iteration\u2014i.e., writing their full state to durable storage. If a node\nfails and its in-memory state is lost, the simplest solution is to roll back the entire\ngraph computation to the last checkpoint and restart the computation. If the algo\u2010\nrithm is deterministic and messages are logged, it is also possible to selectively\nrecover only the partition that was lost (like we previously discussed for dataflow\nengines) [72].\nParallel execution\nA vertex does not need to know on which physical machine it is executing; when it\nsends messages to other vertices, it simply sends them to a vertex ID. It is up to the\nframework to partition the graph\u2014i.e., to decide which vertex runs on which\nmachine, and how to route messages over the network so that they end up in the\nright place.\nBecause the programming model deals with just one vertex at a time (sometimes\ncalled \u201cthinking like a vertex\u201d), the framework may partition the graph in arbitrary\nways. Ideally it would be partitioned such that vertices are colocated on the same\nmachine if they need to communicate a lot. However, finding such an optimized par\u2010\ntitioning is hard\u2014in practice, the graph is often simply partitioned by an arbitrarily\nassigned vertex ID, making no attempt to group related vertices together.\nAs a result, graph algorithms often have a lot of cross-machine communication over\u2010\nhead, and the intermediate state (messages sent between nodes) is often bigger than\nthe original graph. The overhead of sending messages over the network can signifi\u2010\ncantly slow down distributed graph algorithms.\nFor this reason, if your graph can fit in memory on a single computer, it\u2019s quite likely\nthat a single-machine (maybe even single-threaded) algorithm will outperform a dis\u2010\ntributed batch process [73, 74]. Even if the graph is bigger than memory, it can fit on\nthe disks of a single computer, single-machine processing using a framework such as\nGraphChi is a viable option [75]. If the graph is too big to fit on a single machine, a\ndistributed approach such as Pregel is unavoidable; efficiently parallelizing graph\nalgorithms is an area of ongoing research [76].\nHigh-Level APIs and Languages\nOver the years since MapReduce first became popular, the execution engines for dis\u2010\ntributed batch processing have matured. By now, the infrastructure has become\nrobust enough to store and process many petabytes of data on clusters of over 10,000\nmachines. As the problem of physically operating batch processes at such scale has"}
{"449": "As discussed previously, higher-level languages and APIs such as Hive, Pig, Cascad\u2010\ning, and Crunch became popular because programming MapReduce jobs by hand is\nquite laborious. As Tez emerged, these high-level languages had the additional bene\u2010\nfit of being able to move to the new dataflow execution engine without the need to\nrewrite job code. Spark and Flink also include their own high-level dataflow APIs,\noften taking inspiration from FlumeJava [34].\nThese dataflow APIs generally use relational-style building blocks to express a com\u2010\nputation: joining datasets on the value of some field; grouping tuples by key; filtering\nby some condition; and aggregating tuples by counting, summing, or other functions.\nInternally, these operations are implemented using the various join and grouping\nalgorithms that we discussed earlier in this chapter.\nBesides the obvious advantage of requiring less code, these high-level interfaces also\nallow interactive use, in which you write analysis code incrementally in a shell and\nrun it frequently to observe what it is doing. This style of development is very helpful\nwhen exploring a dataset and experimenting with approaches for processing it. It is\nalso reminiscent of the Unix philosophy, which we discussed in \u201cThe Unix Philoso\u2010\nphy\u201d on page 394.\nMoreover, these high-level interfaces not only make the humans using the system\nmore productive, but they also improve the job execution efficiency at a machine\nlevel.\nThe move toward declarative query languages\nAn advantage of specifying joins as relational operators, compared to spelling out the\ncode that performs the join, is that the framework can analyze the properties of the\njoin inputs and automatically decide which of the aforementioned join algorithms\nwould be most suitable for the task at hand. Hive, Spark, and Flink have cost-based\nquery optimizers that can do this, and even change the order of joins so that the\namount of intermediate state is minimized [66, 77, 78, 79].\nThe choice of join algorithm can make a big difference to the performance of a batch\njob, and it is nice not to have to understand and remember all the various join algo\u2010\nrithms we discussed in this chapter. This is possible if joins are specified in a declara\u2010\ntive way: the application simply states which joins are required, and the query\noptimizer decides how they can best be executed. We previously came across this idea\nin \u201cQuery Languages for Data\u201d on page 42.\nHowever, in other ways, MapReduce and its dataflow successors are very different\nfrom the fully declarative query model of SQL. MapReduce was built around the idea"}
{"450": "upon a large ecosystem of existing libraries to do things like parsing, natural language\nanalysis, image analysis, and running numerical or statistical algorithms.\nThe freedom to easily run arbitrary code is what has long distinguished batch pro\u2010\ncessing systems of MapReduce heritage from MPP databases (see \u201cComparing\nHadoop to Distributed Databases\u201d on page 414); although databases have facilities\nfor writing user-defined functions, they are often cumbersome to use and not well\nintegrated with the package managers and dependency management systems that are\nwidely used in most programming languages (such as Maven for Java, npm for Java\u2010\nScript, and Rubygems for Ruby).\nHowever, dataflow engines have found that there are also advantages to incorporat\u2010\ning more declarative features in areas besides joins. For example, if a callback func\u2010\ntion contains only a simple filtering condition, or it just selects some fields from a\nrecord, then there is significant CPU overhead in calling the function on every\nrecord. If such simple filtering and mapping operations are expressed in a declarative\nway, the query optimizer can take advantage of column-oriented storage layouts (see\n\u201cColumn-Oriented Storage\u201d on page 95) and read only the required columns from\ndisk. Hive, Spark DataFrames, and Impala also use vectorized execution (see \u201cMem\u2010\nory bandwidth and vectorized processing\u201d on page 99): iterating over data in a tight\ninner loop that is friendly to CPU caches, and avoiding function calls. Spark gener\u2010\nates JVM bytecode [79] and Impala uses LLVM to generate native code for these\ninner loops [41].\nBy incorporating declarative aspects in their high-level APIs, and having query opti\u2010\nmizers that can take advantage of them during execution, batch processing frame\u2010\nworks begin to look more like MPP databases (and can achieve comparable\nperformance). At the same time, by having the extensibility of being able to run arbi\u2010\ntrary code and read data in arbitrary formats, they retain their flexibility advantage.\nSpecialization for different domains\nWhile the extensibility of being able to run arbitrary code is useful, there are also\nmany common cases where standard processing patterns keep reoccurring, and so it\nis worth having reusable implementations of the common building blocks. Tradition\u2010\nally, MPP databases have served the needs of business intelligence analysts and busi\u2010\nness reporting, but that is just one among many domains in which batch processing\nis used.\nAnother domain of increasing importance is statistical and numerical algorithms,\nwhich are needed for machine learning applications such as classification and recom\u2010\nmendation systems. Reusable implementations are emerging: for example, Mahout"}
{"451": "Also useful are spatial algorithms such as k-nearest neighbors [80], which searches for\nitems that are close to a given item in some multi-dimensional space\u2014a kind of simi\u2010\nlarity search. Approximate search is also important for genome analysis algorithms,\nwhich need to find strings that are similar but not identical [81].\nBatch processing engines are being used for distributed execution of algorithms from\nan increasingly wide range of domains. As batch processing systems gain built-in\nfunctionality and high-level declarative operators, and as MPP databases become\nmore programmable and flexible, the two are beginning to look more alike: in the\nend, they are all just systems for storing and processing data.\nSummary\nIn this chapter we explored the topic of batch processing. We started by looking at\nUnix tools such as awk, grep, and sort, and we saw how the design philosophy of\nthose tools is carried forward into MapReduce and more recent dataflow engines.\nSome of those design principles are that inputs are immutable, outputs are intended\nto become the input to another (as yet unknown) program, and complex problems\nare solved by composing small tools that \u201cdo one thing well.\u201d\nIn the Unix world, the uniform interface that allows one program to be composed\nwith another is files and pipes; in MapReduce, that interface is a distributed filesys\u2010\ntem. We saw that dataflow engines add their own pipe-like data transport mecha\u2010\nnisms to avoid materializing intermediate state to the distributed filesystem, but the\ninitial input and final output of a job is still usually HDFS.\nThe two main problems that distributed batch processing frameworks need to solve\nare:\nPartitioning\nIn MapReduce, mappers are partitioned according to input file blocks. The out\u2010\nput of mappers is repartitioned, sorted, and merged into a configurable number\nof reducer partitions. The purpose of this process is to bring all the related data\u2014\ne.g., all the records with the same key\u2014together in the same place.\nPost-MapReduce dataflow engines try to avoid sorting unless it is required, but\nthey otherwise take a broadly similar approach to partitioning.\nFault tolerance\nMapReduce frequently writes to disk, which makes it easy to recover from an\nindividual failed task without restarting the entire job but slows down execution\nin the failure-free case. Dataflow engines perform less materialization of inter\u2010"}
{"452": "We discussed several join algorithms for MapReduce, most of which are also inter\u2010\nnally used in MPP databases and dataflow engines. They also provide a good illustra\u2010\ntion of how partitioned algorithms work:\nSort-merge joins\nEach of the inputs being joined goes through a mapper that extracts the join key.\nBy partitioning, sorting, and merging, all the records with the same key end up\ngoing to the same call of the reducer. This function can then output the joined\nrecords.\nBroadcast hash joins\nOne of the two join inputs is small, so it is not partitioned and it can be entirely\nloaded into a hash table. Thus, you can start a mapper for each partition of the\nlarge join input, load the hash table for the small input into each mapper, and\nthen scan over the large input one record at a time, querying the hash table for\neach record.\nPartitioned hash joins\nIf the two join inputs are partitioned in the same way (using the same key, same\nhash function, and same number of partitions), then the hash table approach can\nbe used independently for each partition.\nDistributed batch processing engines have a deliberately restricted programming\nmodel: callback functions (such as mappers and reducers) are assumed to be stateless\nand to have no externally visible side effects besides their designated output. This\nrestriction allows the framework to hide some of the hard distributed systems prob\u2010\nlems behind its abstraction: in the face of crashes and network issues, tasks can be\nretried safely, and the output from any failed tasks is discarded. If several tasks for a\npartition succeed, only one of them actually makes its output visible.\nThanks to the framework, your code in a batch processing job does not need to worry\nabout implementing fault-tolerance mechanisms: the framework can guarantee that\nthe final output of a job is the same as if no faults had occurred, even though in real\u2010\nity various tasks perhaps had to be retried. These reliable semantics are much stron\u2010\nger than what you usually have in online services that handle user requests and that\nwrite to databases as a side effect of processing a request.\nThe distinguishing feature of a batch processing job is that it reads some input data\nand produces some output data, without modifying the input\u2014in other words, the\noutput is derived from the input. Crucially, the input data is bounded: it has a known,\nfixed size (for example, it consists of a set of log files at some point in time, or a snap\u2010\nshot of a database\u2019s contents). Because it is bounded, a job knows when it has finished"}
{"453": "this case, a job is never complete, because at any time there may still be more work\ncoming in. We shall see that stream and batch processing are similar in some\nrespects, but the assumption of unbounded streams also changes a lot about how we\nbuild systems.\nReferences\n[1] Jeffrey Dean and Sanjay Ghemawat: \u201cMapReduce: Simplified Data Processing on\nLarge Clusters,\u201d at 6th USENIX Symposium on Operating System Design and Imple\u2010\nmentation (OSDI), December 2004.\n[2] Joel Spolsky: \u201cThe Perils of JavaSchools,\u201d joelonsoftware.com, December 25, 2005.\n[3] Shivnath Babu and Herodotos Herodotou: \u201cMassively Parallel Databases and\nMapReduce Systems,\u201d Foundations and Trends in Databases, volume 5, number 1,\npages 1\u2013104, November 2013. doi:10.1561/1900000036\n[4] David J. DeWitt and Michael Stonebraker: \u201cMapReduce: A Major Step Back\u2010\nwards,\u201d originally published at databasecolumn.vertica.com, January 17, 2008.\n[5] Henry Robinson: \u201cThe Elephant Was a Trojan Horse: On the Death of Map-\nReduce at Google,\u201d the-paper-trail.org, June 25, 2014.\n[6] \u201cThe Hollerith Machine,\u201d United States Census Bureau, census.gov.\n[7] \u201cIBM 82, 83, and 84 Sorters Reference Manual,\u201d Edition A24-1034-1, Interna\u2010\ntional Business Machines Corporation, July 1962.\n[8] Adam Drake: \u201cCommand-Line Tools Can Be 235x Faster than Your Hadoop\nCluster,\u201d aadrake.com, January 25, 2014.\n[9] \u201cGNU Coreutils 8.23 Documentation,\u201d Free Software Foundation, Inc., 2014.\n[10] Martin Kleppmann: \u201cKafka, Samza, and the Unix Philosophy of Distributed\nData,\u201d martin.kleppmann.com, August 5, 2015.\n[11] Doug McIlroy: Internal Bell Labs memo, October 1964. Cited in: Dennis M.\nRichie: \u201cAdvice from Doug McIlroy,\u201d cm.bell-labs.com.\n[12] M. D. McIlroy, E. N. Pinson, and B. A. Tague: \u201cUNIX Time-Sharing System:\nForeword,\u201d The Bell System Technical Journal, volume 57, number 6, pages 1899\u2013\n1904, July 1978.\n[13] Eric S. Raymond: The Art of UNIX Programming. Addison-Wesley, 2003. ISBN:\n978-0-13-142901-7"}
{"454": "[16] Martin Fowler: \u201cInversionOfControl,\u201d martinfowler.com, June 26, 2005.\n[17] Daniel J. Bernstein: \u201cTwo File Descriptors for Sockets,\u201d cr.yp.to.\n[18] Rob Pike and Dennis M. Ritchie: \u201cThe Styx Architecture for Distributed Sys\u2010\ntems,\u201d Bell Labs Technical Journal, volume 4, number 2, pages 146\u2013152, April 1999.\n[19] Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung: \u201cThe Google File Sys\u2010\ntem,\u201d at 19th ACM Symposium on Operating Systems Principles (SOSP), October\n2003. doi:10.1145/945445.945450\n[20] Michael Ovsiannikov, Silvius Rus, Damian Reeves, et al.: \u201cThe Quantcast File\nSystem,\u201d Proceedings of the VLDB Endowment, volume 6, number 11, pages 1092\u2013\n1101, August 2013. doi:10.14778/2536222.2536234\n[21] \u201cOpenStack Swift 2.6.1 Developer Documentation,\u201d OpenStack Foundation,\ndocs.openstack.org, March 2016.\n[22] Zhe Zhang, Andrew Wang, Kai Zheng, et al.: \u201cIntroduction to HDFS Erasure\nCoding in Apache Hadoop,\u201d blog.cloudera.com, September 23, 2015.\n[23] Peter Cnudde: \u201cHadoop Turns 10,\u201d yahoohadoop.tumblr.com, February 5, 2016.\n[24] Eric Baldeschwieler: \u201cThinking About the HDFS vs. Other Storage Technolo\u2010\ngies,\u201d hortonworks.com, July 25, 2012.\n[25] Brendan Gregg: \u201cManta: Unix Meets Map Reduce,\u201d dtrace.org, June 25, 2013.\n[26] Tom White: Hadoop: The Definitive Guide, 4th edition. O\u2019Reilly Media, 2015.\nISBN: 978-1-491-90163-2\n[27] Jim N. Gray: \u201cDistributed Computing Economics,\u201d Microsoft Research Tech\nReport MSR-TR-2003-24, March 2003.\n[28] M\u00e1rton Trencs\u00e9ni: \u201cLuigi vs Airflow vs Pinball,\u201d bytepawn.com, February 6,\n2016.\n[29] Roshan Sumbaly, Jay Kreps, and Sam Shah: \u201cThe \u2018Big Data\u2019 Ecosystem at\nLinkedIn,\u201d at ACM International Conference on Management of Data (SIGMOD),\nJuly 2013. doi:10.1145/2463676.2463707\n[30] Alan F. Gates, Olga Natkovich, Shubham Chopra, et al.: \u201cBuilding a High-Level\nDataflow System on Top of Map-Reduce: The Pig Experience,\u201d at 35th International\nConference on Very Large Data Bases (VLDB), August 2009.\n[31] Ashish Thusoo, Joydeep Sen Sarma, Namit Jain, et al.: \u201cHive \u2013 A Petabyte Scale\nData Warehouse Using Hadoop,\u201d at 26th IEEE International Conference on Data"}
{"455": "[33] \u201cApache Crunch User Guide,\u201d Apache Software Foundation, crunch.apache.org.\n[34] Craig Chambers, Ashish Raniwala, Frances Perry, et al.: \u201cFlumeJava: Easy, Effi\u2010\ncient Data-Parallel Pipelines,\u201d at 31st ACM SIGPLAN Conference on Programming\nLanguage Design and Implementation (PLDI), June 2010. doi:\n10.1145/1806596.1806638\n[35] Jay Kreps: \u201cWhy Local State is a Fundamental Primitive in Stream Processing,\u201d\noreilly.com, July 31, 2014.\n[36] Martin Kleppmann: \u201cRethinking Caching in Web Apps,\u201d martin.klepp\u2010\nmann.com, October 1, 2012.\n[37] Mark Grover, Ted Malaska, Jonathan Seidman, and Gwen Shapira: Hadoop\nApplication Architectures. O\u2019Reilly Media, 2015. ISBN: 978-1-491-90004-8\n[38] Philippe Ajoux, Nathan Bronson, Sanjeev Kumar, et al.: \u201cChallenges to Adopting\nStronger Consistency at Scale,\u201d at 15th USENIX Workshop on Hot Topics in Operat\u2010\ning Systems (HotOS), May 2015.\n[39] Sriranjan Manjunath: \u201cSkewed Join,\u201d wiki.apache.org, 2009.\n[40] David J. DeWitt, Jeffrey F. Naughton, Donovan A. Schneider, and S. Seshadri:\n\u201cPractical Skew Handling in Parallel Joins,\u201d at 18th International Conference on Very\nLarge Data Bases (VLDB), August 1992.\n[41] Marcel Kornacker, Alexander Behm, Victor Bittorf, et al.: \u201cImpala: A Modern,\nOpen-Source SQL Engine for Hadoop,\u201d at 7th Biennial Conference on Innovative\nData Systems Research (CIDR), January 2015.\n[42] Matthieu Monsch: \u201cOpen-Sourcing PalDB, a Lightweight Companion for Stor\u2010\ning Side Data,\u201d engineering.linkedin.com, October 26, 2015.\n[43] Daniel Peng and Frank Dabek: \u201cLarge-Scale Incremental Processing Using Dis\u2010\ntributed Transactions and Notifications,\u201d at 9th USENIX conference on Operating Sys\u2010\ntems Design and Implementation (OSDI), October 2010.\n[44] \u201c\u201cCloudera Search User Guide,\u201d Cloudera, Inc., September 2015.\n[45] Lili Wu, Sam Shah, Sean Choi, et al.: \u201cThe Browsemaps: Collaborative Filtering\nat LinkedIn,\u201d at 6th Workshop on Recommender Systems and the Social Web\n(RSWeb), October 2014.\n[46] Roshan Sumbaly, Jay Kreps, Lei Gao, et al.: \u201cServing Large-Scale Batch Compu\u2010\nted Data with Project Voldemort,\u201d at 10th USENIX Conference on File and Storage\nTechnologies (FAST), February 2012."}
{"456": "[48] Nathan Marz: \u201cElephantDB,\u201d slideshare.net, May 30, 2011.\n[49] Jean-Daniel (JD) Cryans: \u201cHow-to: Use HBase Bulk Loading, and Why,\u201d\nblog.cloudera.com, September 27, 2013.\n[50] Nathan Marz: \u201cHow to Beat the CAP Theorem,\u201d nathanmarz.com, October 13,\n2011.\n[51] Molly Bartlett Dishman and Martin Fowler: \u201cAgile Architecture,\u201d at O\u2019Reilly\nSoftware Architecture Conference, March 2015.\n[52] David J. DeWitt and Jim N. Gray: \u201cParallel Database Systems: The Future of\nHigh Performance Database Systems,\u201d Communications of the ACM, volume 35,\nnumber 6, pages 85\u201398, June 1992. doi:10.1145/129888.129894\n[53] Jay Kreps: \u201cBut the multi-tenancy thing is actually really really hard,\u201d tweet\u2010\nstorm, twitter.com, October 31, 2014.\n[54] Jeffrey Cohen, Brian Dolan, Mark Dunlap, et al.: \u201cMAD Skills: New Analysis\nPractices for Big Data,\u201d Proceedings of the VLDB Endowment, volume 2, number 2,\npages 1481\u20131492, August 2009. doi:10.14778/1687553.1687576\n[55] Ignacio Terrizzano, Peter Schwarz, Mary Roth, and John E. Colino: \u201cData Wran\u2010\ngling: The Challenging Journey from the Wild to the Lake,\u201d at 7th Biennial Confer\u2010\nence on Innovative Data Systems Research (CIDR), January 2015.\n[56] Paige Roberts: \u201cTo Schema on Read or to Schema on Write, That Is the Hadoop\nData Lake Question,\u201d adaptivesystemsinc.com, July 2, 2015.\n[57] Bobby Johnson and Joseph Adler: \u201cThe Sushi Principle: Raw Data Is Better,\u201d at\nStrata+Hadoop World, February 2015.\n[58] Vinod Kumar Vavilapalli, Arun C. Murthy, Chris Douglas, et al.: \u201cApache\nHadoop YARN: Yet Another Resource Negotiator,\u201d at 4th ACM Symposium on\nCloud Computing (SoCC), October 2013. doi:10.1145/2523616.2523633\n[59] Abhishek Verma, Luis Pedrosa, Madhukar Korupolu, et al.: \u201cLarge-Scale Cluster\nManagement at Google with Borg,\u201d at 10th European Conference on Computer Sys\u2010\ntems (EuroSys), April 2015. doi:10.1145/2741948.2741964\n[60] Malte Schwarzkopf: \u201cThe Evolution of Cluster Scheduler Architectures,\u201d firma\u2010\nment.io, March 9, 2016.\n[61] Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, et al.: \u201cResilient Dis\u2010\ntributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing,\u201d\nat 9th USENIX Symposium on Networked Systems Design and Implementation"}
{"457": "[63] Bikas Saha and Hitesh Shah: \u201cApache Tez: Accelerating Hadoop Query Process\u2010\ning,\u201d at Hadoop Summit, June 2014.\n[64] Bikas Saha, Hitesh Shah, Siddharth Seth, et al.: \u201cApache Tez: A Unifying Frame\u2010\nwork for Modeling and Building Data Processing Applications,\u201d at ACM Interna\u2010\ntional Conference on Management of Data (SIGMOD), June 2015. doi:\n10.1145/2723372.2742790\n[65] Kostas Tzoumas: \u201cApache Flink: API, Runtime, and Project Roadmap,\u201d slide\u2010\nshare.net, January 14, 2015.\n[66] Alexander Alexandrov, Rico Bergmann, Stephan Ewen, et al.: \u201cThe Stratosphere\nPlatform for Big Data Analytics,\u201d The VLDB Journal, volume 23, number 6, pages\n939\u2013964, May 2014. doi:10.1007/s00778-014-0357-y\n[67] Michael Isard, Mihai Budiu, Yuan Yu, et al.: \u201cDryad: Distributed Data-Parallel\nPrograms from Sequential Building Blocks,\u201d at European Conference on Computer\nSystems (EuroSys), March 2007. doi:10.1145/1272996.1273005\n[68] Daniel Warneke and Odej Kao: \u201cNephele: Efficient Parallel Data Processing in\nthe Cloud,\u201d at 2nd Workshop on Many-Task Computing on Grids and Supercomputers\n(MTAGS), November 2009. doi:10.1145/1646468.1646476\n[69] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd: \u201cThe\nPageRank Citation Ranking: Bringing Order to the Web,\u201d Stanford InfoLab Techni\u2010\ncal Report 422, 1999.\n[70] Leslie G. Valiant: \u201cA Bridging Model for Parallel Computation,\u201d Communica\u2010\ntions of the ACM, volume 33, number 8, pages 103\u2013111, August 1990. doi:\n10.1145/79173.79181\n[71] Stephan Ewen, Kostas Tzoumas, Moritz Kaufmann, and Volker Markl: \u201cSpin\u2010\nning Fast Iterative Data Flows,\u201d Proceedings of the VLDB Endowment, volume 5,\nnumber 11, pages 1268-1279, July 2012. doi:10.14778/2350229.2350245\n[72] Grzegorz Malewicz, Matthew H. Austern, Aart J. C. Bik, et al.: \u201cPregel: A System\nfor Large-Scale Graph Processing,\u201d at ACM International Conference on Management\nof Data (SIGMOD), June 2010. doi:10.1145/1807167.1807184\n[73] Frank McSherry, Michael Isard, and Derek G. Murray: \u201cScalability! But at What\nCOST?,\u201d at 15th USENIX Workshop on Hot Topics in Operating Systems (HotOS),\nMay 2015.\n[74] Ionel Gog, Malte Schwarzkopf, Natacha Crooks, et al.: \u201cMusketeer: All for One,\nOne for All in Data Processing Systems,\u201d at 10th European Conference on Computer"}
{"458": "[75] Aapo Kyrola, Guy Blelloch, and Carlos Guestrin: \u201cGraphChi: Large-Scale Graph\nComputation on Just a PC,\u201d at 10th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI), October 2012.\n[76] Andrew Lenharth, Donald Nguyen, and Keshav Pingali: \u201cParallel Graph Analyt\u2010\nics,\u201d Communications of the ACM, volume 59, number 5, pages 78\u201387, May 2016. doi:\n10.1145/2901919\n[77] Fabian H\u00fcske: \u201cPeeking into Apache Flink\u2019s Engine Room,\u201d flink.apache.org,\nMarch 13, 2015.\n[78] Mostafa Mokhtar: \u201cHive 0.14 Cost Based Optimizer (CBO) Technical Over\u2010\nview,\u201d hortonworks.com, March 2, 2015.\n[79] Michael Armbrust, Reynold S Xin, Cheng Lian, et al.: \u201cSpark SQL: Relational\nData Processing in Spark,\u201d at ACM International Conference on Management of Data\n(SIGMOD), June 2015. doi:10.1145/2723372.2742797\n[80] Daniel Blazevski: \u201cPlanting Quadtrees for Apache Flink,\u201d insightdataengineer\u2010\ning.com, March 25, 2016.\n[81] Tom White: \u201cGenome Analysis Toolkit: Now Using Apache Spark for Data Pro\u2010\ncessing,\u201d blog.cloudera.com, April 6, 2016."}
{"459": ""}
{"460": ""}
{"461": "CHAPTER 11\nStream Processing\nA complex system that works is invariably found to have evolved from a simple system that\nworks. The inverse proposition also appears to be true: A complex system designed from\nscratch never works and cannot be made to work.\n\u2014John Gall, Systemantics (1975)\nIn Chapter 10 we discussed batch processing\u2014techniques that read a set of files as\ninput and produce a new set of output files. The output is a form of derived data; that\nis, a dataset that can be recreated by running the batch process again if necessary. We\nsaw how this simple but powerful idea can be used to create search indexes, recom\u2010\nmendation systems, analytics, and more.\nHowever, one big assumption remained throughout Chapter 10: namely, that the\ninput is bounded\u2014i.e., of a known and finite size\u2014so the batch process knows when\nit has finished reading its input. For example, the sorting operation that is central to\nMapReduce must read its entire input before it can start producing output: it could\nhappen that the very last input record is the one with the lowest key, and thus needs\nto be the very first output record, so starting the output early is not an option.\nIn reality, a lot of data is unbounded because it arrives gradually over time: your users\nproduced data yesterday and today, and they will continue to produce more data\ntomorrow. Unless you go out of business, this process never ends, and so the dataset\nis never \u201ccomplete\u201d in any meaningful way [1]. Thus, batch processors must artifi\u2010\ncially divide the data into chunks of fixed duration: for example, processing a day\u2019s\nworth of data at the end of every day, or processing an hour\u2019s worth of data at the end\nof every hour."}
{"462": "slices entirely and simply processing every event as it happens. That is the idea\nbehind stream processing.\nIn general, a \u201cstream\u201d refers to data that is incrementally made available over time.\nThe concept appears in many places: in the stdin and stdout of Unix, programming\nlanguages (lazy lists) [2], filesystem APIs (such as Java\u2019s FileInputStream), TCP con\u2010\nnections, delivering audio and video over the internet, and so on.\nIn this chapter we will look at event streams as a data management mechanism: the\nunbounded, incrementally processed counterpart to the batch data we saw in the\nlast chapter. We will first discuss how streams are represented, stored, and transmit\u2010\nted over a network. In \u201cDatabases and Streams\u201d on page 451 we will investigate\nthe relationship between streams and databases. And finally, in \u201cProcessing Streams\u201d\non page 464 we will explore approaches and tools for processing those streams\ncontinually, and ways that they can be used to build applications.\nTransmitting Event Streams\nIn the batch processing world, the inputs and outputs of a job are files (perhaps on a\ndistributed filesystem). What does the streaming equivalent look like?\nWhen the input is a file (a sequence of bytes), the first processing step is usually to\nparse it into a sequence of records. In a stream processing context, a record is more\ncommonly known as an event, but it is essentially the same thing: a small, self-\ncontained, immutable object containing the details of something that happened at\nsome point in time. An event usually contains a timestamp indicating when it hap\u2010\npened according to a time-of-day clock (see \u201cMonotonic Versus Time-of-Day\nClocks\u201d on page 288).\nFor example, the thing that happened might be an action that a user took, such as\nviewing a page or making a purchase. It might also originate from a machine, such as\na periodic measurement from a temperature sensor, or a CPU utilization metric. In\nthe example of \u201cBatch Processing with Unix Tools\u201d on page 391, each line of the web\nserver log is an event.\nAn event may be encoded as a text string, or JSON, or perhaps in some binary form,\nas discussed in Chapter 4. This encoding allows you to store an event, for example by\nappending it to a file, inserting it into a relational table, or writing it to a document\ndatabase. It also allows you to send the event over the network to another node in\norder to process it.\nIn batch processing, a file is written once and then potentially read by multiple jobs."}
{"463": "related records; in a streaming system, related events are usually grouped together\ninto a topic or stream.\nIn principle, a file or database is sufficient to connect producers and consumers: a\nproducer writes every event that it generates to the datastore, and each consumer\nperiodically polls the datastore to check for events that have appeared since it last ran.\nThis is essentially what a batch process does when it processes a day\u2019s worth of data at\nthe end of every day.\nHowever, when moving toward continual processing with low delays, polling\nbecomes expensive if the datastore is not designed for this kind of usage. The more\noften you poll, the lower the percentage of requests that return new events, and thus\nthe higher the overheads become. Instead, it is better for consumers to be notified\nwhen new events appear.\nDatabases have traditionally not supported this kind of notification mechanism very\nwell: relational databases commonly have triggers, which can react to a change (e.g., a\nrow being inserted into a table), but they are very limited in what they can do and\nhave been somewhat of an afterthought in database design [4, 5]. Instead, specialized\ntools have been developed for the purpose of delivering event notifications.\nMessaging Systems\nA common approach for notifying consumers about new events is to use a messaging\nsystem: a producer sends a message containing the event, which is then pushed to\nconsumers. We touched on these systems previously in \u201cMessage-Passing Dataflow\u201d\non page 136, but we will now go into more detail.\nA direct communication channel like a Unix pipe or TCP connection between pro\u2010\nducer and consumer would be a simple way of implementing a messaging system.\nHowever, most messaging systems expand on this basic model. In particular, Unix\npipes and TCP connect exactly one sender with one recipient, whereas a messaging\nsystem allows multiple producer nodes to send messages to the same topic and allows\nmultiple consumer nodes to receive messages in a topic.\nWithin this publish/subscribe model, different systems take a wide range of\napproaches, and there is no one right answer for all purposes. To differentiate the\nsystems, it is particularly helpful to ask the following two questions:\n1. What happens if the producers send messages faster than the consumers can pro\u2010\ncess them? Broadly speaking, there are three options: the system can drop mes\u2010\nsages, buffer messages in a queue, or apply backpressure (also known as flow"}
{"464": "it fills up, the sender is blocked until the recipient takes data out of the buffer (see\n\u201cNetwork congestion and queueing\u201d on page 282).\nIf messages are buffered in a queue, it is important to understand what happens\nas that queue grows. Does the system crash if the queue no longer fits in mem\u2010\nory, or does it write messages to disk? If so, how does the disk access affect the\nperformance of the messaging system [6]?\n2. What happens if nodes crash or temporarily go offline\u2014are any messages lost? As\nwith databases, durability may require some combination of writing to disk\nand/or replication (see the sidebar \u201cReplication and Durability\u201d on page 227),\nwhich has a cost. If you can afford to sometimes lose messages, you can probably\nget higher throughput and lower latency on the same hardware.\nWhether message loss is acceptable depends very much on the application. For exam\u2010\nple, with sensor readings and metrics that are transmitted periodically, an occasional\nmissing data point is perhaps not important, since an updated value will be sent a\nshort time later anyway. However, beware that if a large number of messages are\ndropped, it may not be immediately apparent that the metrics are incorrect [7]. If you\nare counting events, it is more important that they are delivered reliably, since every\nlost message means incorrect counters.\nA nice property of the batch processing systems we explored in Chapter 10 is that\nthey provide a strong reliability guarantee: failed tasks are automatically retried, and\npartial output from failed tasks is automatically discarded. This means the output is\nthe same as if no failures had occurred, which helps simplify the programming\nmodel. Later in this chapter we will examine how we can provide similar guarantees\nin a streaming context.\nDirect messaging from producers to consumers\nA number of messaging systems use direct network communication between produc\u2010\ners and consumers without going via intermediary nodes:\n\u2022 UDP multicast is widely used in the financial industry for streams such as stock\nmarket feeds, where low latency is important [8]. Although UDP itself is unrelia\u2010\nble, application-level protocols can recover lost packets (the producer must\nremember packets it has sent so that it can retransmit them on demand).\n\u2022 Brokerless messaging libraries such as ZeroMQ [9] and nanomsg take a similar\napproach, implementing publish/subscribe messaging over TCP or IP multicast.\n\u2022 StatsD [10] and Brubeck [7] use unreliable UDP messaging for collecting metrics"}
{"465": "\u2022 If the consumer exposes a service on the network, producers can make a direct\nHTTP or RPC request (see \u201cDataflow Through Services: REST and RPC\u201d on page\n131) to push messages to the consumer. This is the idea behind webhooks [12], a\npattern in which a callback URL of one service is registered with another service,\nand it makes a request to that URL whenever an event occurs.\nAlthough these direct messaging systems work well in the situations for which they\nare designed, they generally require the application code to be aware of the possibility\nof message loss. The faults they can tolerate are quite limited: even if the protocols\ndetect and retransmit packets that are lost in the network, they generally assume that\nproducers and consumers are constantly online.\nIf a consumer is offline, it may miss messages that were sent while it is unreachable.\nSome protocols allow the producer to retry failed message deliveries, but this\napproach may break down if the producer crashes, losing the buffer of messages that\nit was supposed to retry.\nMessage brokers\nA widely used alternative is to send messages via a message broker (also known as a\nmessage queue), which is essentially a kind of database that is optimized for handling\nmessage streams [13]. It runs as a server, with producers and consumers connecting\nto it as clients. Producers write messages to the broker, and consumers receive them\nby reading them from the broker.\nBy centralizing the data in the broker, these systems can more easily tolerate clients\nthat come and go (connect, disconnect, and crash), and the question of durability is\nmoved to the broker instead. Some message brokers only keep messages in memory,\nwhile others (depending on configuration) write them to disk so that they are not lost\nin case of a broker crash. Faced with slow consumers, they generally allow unboun\u2010\nded queueing (as opposed to dropping messages or backpressure), although this\nchoice may also depend on the configuration.\nA consequence of queueing is also that consumers are generally asynchronous: when\na producer sends a message, it normally only waits for the broker to confirm that it\nhas buffered the message and does not wait for the message to be processed by con\u2010\nsumers. The delivery to consumers will happen at some undetermined future point in\ntime\u2014often within a fraction of a second, but sometimes significantly later if there is\na queue backlog.\nMessage brokers compared to databases"}
{"466": "them quite similar in nature to databases, although there are still important practical\ndifferences between message brokers and databases:\n\u2022 Databases usually keep data until it is explicitly deleted, whereas most message\nbrokers automatically delete a message when it has been successfully delivered to\nits consumers. Such message brokers are not suitable for long-term data storage.\n\u2022 Since they quickly delete messages, most message brokers assume that their\nworking set is fairly small\u2014i.e., the queues are short. If the broker needs to buffer\na lot of messages because the consumers are slow (perhaps spilling messages to\ndisk if they no longer fit in memory), each individual message takes longer to\nprocess, and the overall throughput may degrade [6].\n\u2022 Databases often support secondary indexes and various ways of searching for\ndata, while message brokers often support some way of subscribing to a subset of\ntopics matching some pattern. The mechanisms are different, but both are essen\u2010\ntially ways for a client to select the portion of the data that it wants to know\nabout.\n\u2022 When querying a database, the result is typically based on a point-in-time snap\u2010\nshot of the data; if another client subsequently writes something to the database\nthat changes the query result, the first client does not find out that its prior result\nis now outdated (unless it repeats the query, or polls for changes). By contrast,\nmessage brokers do not support arbitrary queries, but they do notify clients when\ndata changes (i.e., when new messages become available).\nThis is the traditional view of message brokers, which is encapsulated in standards\nlike JMS [14] and AMQP [15] and implemented in software like RabbitMQ,\nActiveMQ, HornetQ, Qpid, TIBCO Enterprise Message Service, IBM MQ, Azure Ser\u2010\nvice Bus, and Google Cloud Pub/Sub [16].\nMultiple consumers\nWhen multiple consumers read messages in the same topic, two main patterns of\nmessaging are used, as illustrated in Figure 11-1:\nLoad balancing\nEach message is delivered to one of the consumers, so the consumers can share\nthe work of processing the messages in the topic. The broker may assign mes\u2010\nsages to consumers arbitrarily. This pattern is useful when the messages are\nexpensive to process, and so you want to be able to add consumers to parallelize\nthe processing. (In AMQP, you can implement load balancing by having multi\u2010"}
{"467": "Fan-out\nEach message is delivered to all of the consumers. Fan-out allows several inde\u2010\npendent consumers to each \u201ctune in\u201d to the same broadcast of messages, without\naffecting each other\u2014the streaming equivalent of having several different batch\njobs that read the same input file. (This feature is provided by topic subscriptions\nin JMS, and exchange bindings in AMQP.)\nFigure 11-1. (a) Load balancing: sharing the work of consuming a topic among con\u2010\nsumers; (b) fan-out: delivering each message to multiple consumers.\nThe two patterns can be combined: for example, two separate groups of consumers\nmay each subscribe to a topic, such that each group collectively receives all messages,\nbut within each group only one of the nodes receives each message.\nAcknowledgments and redelivery\nConsumers may crash at any time, so it could happen that a broker delivers a mes\u2010\nsage to a consumer but the consumer never processes it, or only partially processes it\nbefore crashing. In order to ensure that the message is not lost, message brokers use\nacknowledgments: a client must explicitly tell the broker when it has finished process\u2010\ning a message so that the broker can remove it from the queue.\nIf the connection to a client is closed or times out without the broker receiving an\nacknowledgment, it assumes that the message was not processed, and therefore it\ndelivers the message again to another consumer. (Note that it could happen that the\nmessage actually was fully processed, but the acknowledgment was lost in the net\u2010"}
{"468": "When combined with load balancing, this redelivery behavior has an interesting\neffect on the ordering of messages. In Figure 11-2, the consumers generally process\nmessages in the order they were sent by producers. However, consumer 2 crashes\nwhile processing message m3, at the same time as consumer 1 is processing message\nm4. The unacknowledged message m3 is subsequently redelivered to consumer 1,\nwith the result that consumer 1 processes messages in the order m4, m3, m5. Thus,\nm3 and m4 are not delivered in the same order as they were sent by producer 1.\nFigure 11-2. Consumer 2 crashes while processing m3, so it is redelivered to consumer 1\nat a later time.\nEven if the message broker otherwise tries to preserve the order of messages (as\nrequired by both the JMS and AMQP standards), the combination of load balancing\nwith redelivery inevitably leads to messages being reordered. To avoid this issue, you\ncan use a separate queue per consumer (i.e., not use the load balancing feature). Mes\u2010\nsage reordering is not a problem if messages are completely independent of each\nother, but it can be important if there are causal dependencies between messages, as\nwe shall see later in the chapter.\nPartitioned Logs\nSending a packet over a network or making a request to a network service is normally\na transient operation that leaves no permanent trace. Although it is possible to record\nit permanently (using packet capture and logging), we normally don\u2019t think of it that\nway. Even message brokers that durably write messages to disk quickly delete them"}
{"469": "Databases and filesystems take the opposite approach: everything that is written to a\ndatabase or file is normally expected to be permanently recorded, at least until some\u2010\none explicitly chooses to delete it again.\nThis difference in mindset has a big impact on how derived data is created. A key\nfeature of batch processes, as discussed in Chapter 10, is that you can run them\nrepeatedly, experimenting with the processing steps, without risk of damaging the\ninput (since the input is read-only). This is not the case with AMQP/JMS-style mes\u2010\nsaging: receiving a message is destructive if the acknowledgment causes it to be\ndeleted from the broker, so you cannot run the same consumer again and expect to\nget the same result.\nIf you add a new consumer to a messaging system, it typically only starts receiving\nmessages sent after the time it was registered; any prior messages are already gone\nand cannot be recovered. Contrast this with files and databases, where you can add a\nnew client at any time, and it can read data written arbitrarily far in the past (as long\nas it has not been explicitly overwritten or deleted by the application).\nWhy can we not have a hybrid, combining the durable storage approach of databases\nwith the low-latency notification facilities of messaging? This is the idea behind log-\nbased message brokers.\nUsing logs for message storage\nA log is simply an append-only sequence of records on disk. We previously discussed\nlogs in the context of log-structured storage engines and write-ahead logs in Chap\u2010\nter 3, and in the context of replication in Chapter 5.\nThe same structure can be used to implement a message broker: a producer sends a\nmessage by appending it to the end of the log, and a consumer receives messages by\nreading the log sequentially. If a consumer reaches the end of the log, it waits for a\nnotification that a new message has been appended. The Unix tool tail -f, which\nwatches a file for data being appended, essentially works like this.\nIn order to scale to higher throughput than a single disk can offer, the log can be\npartitioned (in the sense of Chapter 6). Different partitions can then be hosted on dif\u2010\nferent machines, making each partition a separate log that can be read and written\nindependently from other partitions. A topic can then be defined as a group of parti\u2010\ntions that all carry messages of the same type. This approach is illustrated in\nFigure 11-3.\nWithin each partition, the broker assigns a monotonically increasing sequence num\u2010\nber, or offset, to every message (in Figure 11-3, the numbers in boxes are message off\u2010"}
{"470": "Figure 11-3. Producers send messages by appending them to a topic-partition file, and\nconsumers read these files sequentially.\nApache Kafka [17, 18], Amazon Kinesis Streams [19], and Twitter\u2019s DistributedLog\n[20, 21] are log-based message brokers that work like this. Google Cloud Pub/Sub is\narchitecturally similar but exposes a JMS-style API rather than a log abstraction [16].\nEven though these message brokers write all messages to disk, they are able to achieve\nthroughput of millions of messages per second by partitioning across multiple\nmachines, and fault tolerance by replicating messages [22, 23].\nLogs compared to traditional messaging\nThe log-based approach trivially supports fan-out messaging, because several con\u2010\nsumers can independently read the log without affecting each other\u2014reading a mes\u2010\nsage does not delete it from the log. To achieve load balancing across a group of\nconsumers, instead of assigning individual messages to consumer clients, the broker\ncan assign entire partitions to nodes in the consumer group.\nEach client then consumes all the messages in the partitions it has been assigned.\nTypically, when a consumer has been assigned a log partition, it reads the messages in\nthe partition sequentially, in a straightforward single-threaded manner. This coarse-\ngrained load balancing approach has some downsides:"}
{"471": "\u2022 The number of nodes sharing the work of consuming a topic can be at most the\nnumber of log partitions in that topic, because messages within the same parti\u2010\ntion are delivered to the same node.i\n\u2022 If a single message is slow to process, it holds up the processing of subsequent\nmessages in that partition (a form of head-of-line blocking; see \u201cDescribing Per\u2010\nformance\u201d on page 13).\nThus, in situations where messages may be expensive to process and you want to par\u2010\nallelize processing on a message-by-message basis, and where message ordering is not\nso important, the JMS/AMQP style of message broker is preferable. On the other\nhand, in situations with high message throughput, where each message is fast to pro\u2010\ncess and where message ordering is important, the log-based approach works very\nwell.\nConsumer offsets\nConsuming a partition sequentially makes it easy to tell which messages have been\nprocessed: all messages with an offset less than a consumer\u2019s current offset have\nalready been processed, and all messages with a greater offset have not yet been seen.\nThus, the broker does not need to track acknowledgments for every single message\u2014\nit only needs to periodically record the consumer offsets. The reduced bookkeeping\noverhead and the opportunities for batching and pipelining in this approach help\nincrease the throughput of log-based systems.\nThis offset is in fact very similar to the log sequence number that is commonly found\nin single-leader database replication, and which we discussed in \u201cSetting Up New\nFollowers\u201d on page 155. In database replication, the log sequence number allows a\nfollower to reconnect to a leader after it has become disconnected, and resume repli\u2010\ncation without skipping any writes. Exactly the same principle is used here: the mes\u2010\nsage broker behaves like a leader database, and the consumer like a follower.\nIf a consumer node fails, another node in the consumer group is assigned the failed\nconsumer\u2019s partitions, and it starts consuming messages at the last recorded offset. If\nthe consumer had processed subsequent messages but not yet recorded their offset,\nthose messages will be processed a second time upon restart. We will discuss ways of\ndealing with this issue later in the chapter.\ni. It\u2019s possible to create a load balancing scheme in which two consumers share the work of processing a par\u2010"}
{"472": "Disk space usage\nIf you only ever append to the log, you will eventually run out of disk space. To\nreclaim disk space, the log is actually divided into segments, and from time to time\nold segments are deleted or moved to archive storage. (We\u2019ll discuss a more sophisti\u2010\ncated way of freeing disk space later.)\nThis means that if a slow consumer cannot keep up with the rate of messages, and it\nfalls so far behind that its consumer offset points to a deleted segment, it will miss\nsome of the messages. Effectively, the log implements a bounded-size buffer that dis\u2010\ncards old messages when it gets full, also known as a circular buffer or ring buffer.\nHowever, since that buffer is on disk, it can be quite large.\nLet\u2019s do a back-of-the-envelope calculation. At the time of writing, a typical large\nhard drive has a capacity of 6 TB and a sequential write throughput of 150 MB/s. If\nyou are writing messages at the fastest possible rate, it takes about 11 hours to fill the\ndrive. Thus, the disk can buffer 11 hours\u2019 worth of messages, after which it will start\noverwriting old messages. This ratio remains the same, even if you use many hard\ndrives and machines. In practice, deployments rarely use the full write bandwidth of\nthe disk, so the log can typically keep a buffer of several days\u2019 or even weeks\u2019 worth of\nmessages.\nRegardless of how long you retain messages, the throughput of a log remains more or\nless constant, since every message is written to disk anyway [18]. This behavior is in\ncontrast to messaging systems that keep messages in memory by default and only\nwrite them to disk if the queue grows too large: such systems are fast when queues are\nshort and become much slower when they start writing to disk, so the throughput\ndepends on the amount of history retained.\nWhen consumers cannot keep up with producers\nAt the beginning of \u201cMessaging Systems\u201d on page 441 we discussed three choices of\nwhat to do if a consumer cannot keep up with the rate at which producers are send\u2010\ning messages: dropping messages, buffering, or applying backpressure. In this taxon\u2010\nomy, the log-based approach is a form of buffering with a large but fixed-size buffer\n(limited by the available disk space).\nIf a consumer falls so far behind that the messages it requires are older than what is\nretained on disk, it will not be able to read those messages\u2014so the broker effectively\ndrops old messages that go back further than the size of the buffer can accommodate.\nYou can monitor how far a consumer is behind the head of the log, and raise an alert\nif it falls behind significantly. As the buffer is large, there is enough time for a human"}
{"473": "Even if a consumer does fall too far behind and starts missing messages, only that\nconsumer is affected; it does not disrupt the service for other consumers. This fact is\na big operational advantage: you can experimentally consume a production log for\ndevelopment, testing, or debugging purposes, without having to worry much about\ndisrupting production services. When a consumer is shut down or crashes, it stops\nconsuming resources\u2014the only thing that remains is its consumer offset.\nThis behavior also contrasts with traditional message brokers, where you need to be\ncareful to delete any queues whose consumers have been shut down\u2014otherwise they\ncontinue unnecessarily accumulating messages and taking away memory from con\u2010\nsumers that are still active.\nReplaying old messages\nWe noted previously that with AMQP- and JMS-style message brokers, processing\nand acknowledging messages is a destructive operation, since it causes the messages\nto be deleted on the broker. On the other hand, in a log-based message broker, con\u2010\nsuming messages is more like reading from a file: it is a read-only operation that does\nnot change the log.\nThe only side effect of processing, besides any output of the consumer, is that the\nconsumer offset moves forward. But the offset is under the consumer\u2019s control, so it\ncan easily be manipulated if necessary: for example, you can start a copy of a con\u2010\nsumer with yesterday\u2019s offsets and write the output to a different location, in order to\nreprocess the last day\u2019s worth of messages. You can repeat this any number of times,\nvarying the processing code.\nThis aspect makes log-based messaging more like the batch processes of the last\nchapter, where derived data is clearly separated from input data through a repeatable\ntransformation process. It allows more experimentation and easier recovery from\nerrors and bugs, making it a good tool for integrating dataflows within an organiza\u2010\ntion [24].\nDatabases and Streams\nWe have drawn some comparisons between message brokers and databases. Even\nthough they have traditionally been considered separate categories of tools, we saw\nthat log-based message brokers have been successful in taking ideas from databases\nand applying them to messaging. We can also go in reverse: take ideas from messag\u2010\ning and streams, and apply them to databases.\nWe said previously that an event is a record of something that happened at some"}
{"474": "cessed. This observation suggests that the connection between databases and streams\nruns deeper than just the physical storage of logs on disk\u2014it is quite fundamental.\nIn fact, a replication log (see \u201cImplementation of Replication Logs\u201d on page 158) is a\nstream of database write events, produced by the leader as it processes transactions.\nThe followers apply that stream of writes to their own copy of the database and thus\nend up with an accurate copy of the same data. The events in the replication log\ndescribe the data changes that occurred.\nWe also came across the state machine replication principle in \u201cTotal Order Broad\u2010\ncast\u201d on page 348, which states: if every event represents a write to the database, and\nevery replica processes the same events in the same order, then the replicas will all\nend up in the same final state. (Processing an event is assumed to be a deterministic\noperation.) It\u2019s just another case of event streams!\nIn this section we will first look at a problem that arises in heterogeneous data sys\u2010\ntems, and then explore how we can solve it by bringing ideas from event streams to\ndatabases.\nKeeping Systems in Sync\nAs we have seen throughout this book, there is no single system that can satisfy all\ndata storage, querying, and processing needs. In practice, most nontrivial applica\u2010\ntions need to combine several different technologies in order to satisfy their require\u2010\nments: for example, using an OLTP database to serve user requests, a cache to speed\nup common requests, a full-text index to handle search queries, and a data warehouse\nfor analytics. Each of these has its own copy of the data, stored in its own representa\u2010\ntion that is optimized for its own purposes.\nAs the same or related data appears in several different places, they need to be kept in\nsync with one another: if an item is updated in the database, it also needs to be upda\u2010\nted in the cache, search indexes, and data warehouse. With data warehouses this syn\u2010\nchronization is usually performed by ETL processes (see \u201cData Warehousing\u201d on\npage 91), often by taking a full copy of a database, transforming it, and bulk-loading\nit into the data warehouse\u2014in other words, a batch process. Similarly, we saw in\n\u201cThe Output of Batch Workflows\u201d on page 411 how search indexes, recommendation\nsystems, and other derived data systems might be created using batch processes.\nIf periodic full database dumps are too slow, an alternative that is sometimes used is\ndual writes, in which the application code explicitly writes to each of the systems\nwhen data changes: for example, first writing to the database, then updating the\nsearch index, then invalidating the cache entries (or even performing those writes"}
{"475": "item X: client 1 wants to set the value to A, and client 2 wants to set it to B. Both\nclients first write the new value to the database, then write it to the search index. Due\nto unlucky timing, the requests are interleaved: the database first sees the write from\nclient 1 setting the value to A, then the write from client 2 setting the value to B, so\nthe final value in the database is B. The search index first sees the write from client 2,\nthen client 1, so the final value in the search index is A. The two systems are now\npermanently inconsistent with each other, even though no error occurred.\nFigure 11-4. In the database, X is first set to A and then to B, while at the search index\nthe writes arrive in the opposite order.\nUnless you have some additional concurrency detection mechanism, such as the ver\u2010\nsion vectors we discussed in \u201cDetecting Concurrent Writes\u201d on page 184, you will not\neven notice that concurrent writes occurred\u2014one value will simply silently overwrite\nanother value.\nAnother problem with dual writes is that one of the writes may fail while the other\nsucceeds. This is a fault-tolerance problem rather than a concurrency problem, but it\nalso has the effect of the two systems becoming inconsistent with each other. Ensur\u2010\ning that they either both succeed or both fail is a case of the atomic commit problem,\nwhich is expensive to solve (see \u201cAtomic Commit and Two-Phase Commit (2PC)\u201d on\npage 354).\nIf you only have one replicated database with a single leader, then that leader deter\u2010\nmines the order of writes, so the state machine replication approach works among\nreplicas of the database. However, in Figure 11-4 there isn\u2019t a single leader: the data\u2010\nbase may have a leader and the search index may have a leader, but neither follows\nthe other, and so conflicts can occur (see \u201cMulti-Leader Replication\u201d on page 168)."}
{"476": "Change Data Capture\nThe problem with most databases\u2019 replication logs is that they have long been consid\u2010\nered to be an internal implementation detail of the database, not a public API. Clients\nare supposed to query the database through its data model and query language, not\nparse the replication logs and try to extract data from them.\nFor decades, many databases simply did not have a documented way of getting the\nlog of changes written to them. For this reason it was difficult to take all the changes\nmade in a database and replicate them to a different storage technology such as a\nsearch index, cache, or data warehouse.\nMore recently, there has been growing interest in change data capture (CDC), which\nis the process of observing all data changes written to a database and extracting them\nin a form in which they can be replicated to other systems. CDC is especially interest\u2010\ning if changes are made available as a stream, immediately as they are written.\nFor example, you can capture the changes in a database and continually apply the\nsame changes to a search index. If the log of changes is applied in the same order, you\ncan expect the data in the search index to match the data in the database. The search\nindex and any other derived data systems are just consumers of the change stream, as\nillustrated in Figure 11-5.\nFigure 11-5. Taking data in the order it was written to one database, and applying the\nchanges to other systems in the same order.\nImplementing change data capture\nWe can call the log consumers derived data systems, as discussed in the introduction"}
{"477": "Essentially, change data capture makes one database the leader (the one from which\nthe changes are captured), and turns the others into followers. A log-based message\nbroker is well suited for transporting the change events from the source database,\nsince it preserves the ordering of messages (avoiding the reordering issue of\nFigure 11-2).\nDatabase triggers can be used to implement change data capture (see \u201cTrigger-based\nreplication\u201d on page 161) by registering triggers that observe all changes to data\ntables and add corresponding entries to a changelog table. However, they tend to be\nfragile and have significant performance overheads. Parsing the replication log can be\na more robust approach, although it also comes with challenges, such as handling\nschema changes.\nLinkedIn\u2019s Databus [25], Facebook\u2019s Wormhole [26], and Yahoo!\u2019s Sherpa [27] use\nthis idea at large scale. Bottled Water implements CDC for PostgreSQL using an API\nthat decodes the write-ahead log [28], Maxwell and Debezium do something similar\nfor MySQL by parsing the binlog [29, 30, 31], Mongoriver reads the MongoDB oplog\n[32, 33], and GoldenGate provides similar facilities for Oracle [34, 35].\nLike message brokers, change data capture is usually asynchronous: the system of\nrecord database does not wait for the change to be applied to consumers before com\u2010\nmitting it. This design has the operational advantage that adding a slow consumer\ndoes not affect the system of record too much, but it has the downside that all the\nissues of replication lag apply (see \u201cProblems with Replication Lag\u201d on page 161).\nInitial snapshot\nIf you have the log of all changes that were ever made to a database, you can recon\u2010\nstruct the entire state of the database by replaying the log. However, in many cases,\nkeeping all changes forever would require too much disk space, and replaying it\nwould take too long, so the log needs to be truncated.\nBuilding a new full-text index, for example, requires a full copy of the entire database\n\u2014it is not sufficient to only apply a log of recent changes, since it would be missing\nitems that were not recently updated. Thus, if you don\u2019t have the entire log history,\nyou need to start with a consistent snapshot, as previously discussed in \u201cSetting Up\nNew Followers\u201d on page 155.\nThe snapshot of the database must correspond to a known position or offset in the\nchange log, so that you know at which point to start applying changes after the snap\u2010\nshot has been processed. Some CDC tools integrate this snapshot facility, while oth\u2010\ners leave it as a manual operation."}
{"478": "Log compaction\nIf you can only keep a limited amount of log history, you need to go through the\nsnapshot process every time you want to add a new derived data system. However,\nlog compaction provides a good alternative.\nWe discussed log compaction previously in \u201cHash Indexes\u201d on page 72, in the con\u2010\ntext of log-structured storage engines (see Figure 3-2 for an example). The principle\nis simple: the storage engine periodically looks for log records with the same key,\nthrows away any duplicates, and keeps only the most recent update for each key. This\ncompaction and merging process runs in the background.\nIn a log-structured storage engine, an update with a special null value (a tombstone)\nindicates that a key was deleted, and causes it to be removed during log compaction.\nBut as long as a key is not overwritten or deleted, it stays in the log forever. The disk\nspace required for such a compacted log depends only on the current contents of the\ndatabase, not the number of writes that have ever occurred in the database. If the\nsame key is frequently overwritten, previous values will eventually be garbage-\ncollected, and only the latest value will be retained.\nThe same idea works in the context of log-based message brokers and change data\ncapture. If the CDC system is set up such that every change has a primary key, and\nevery update for a key replaces the previous value for that key, then it\u2019s sufficient to\nkeep just the most recent write for a particular key.\nNow, whenever you want to rebuild a derived data system such as a search index, you\ncan start a new consumer from offset 0 of the log-compacted topic, and sequentially\nscan over all messages in the log. The log is guaranteed to contain the most recent\nvalue for every key in the database (and maybe some older values)\u2014in other words,\nyou can use it to obtain a full copy of the database contents without having to take\nanother snapshot of the CDC source database.\nThis log compaction feature is supported by Apache Kafka. As we shall see later in\nthis chapter, it allows the message broker to be used for durable storage, not just for\ntransient messaging.\nAPI support for change streams\nIncreasingly, databases are beginning to support change streams as a first-class inter\u2010\nface, rather than the typical retrofitted and reverse-engineered CDC efforts. For\nexample, RethinkDB allows queries to subscribe to notifications when the results of a\nquery change [36], Firebase [37] and CouchDB [38] provide data synchronization\nbased on a change feed that is also made available to applications, and Meteor uses"}
{"479": "model as a table into which transactions can insert tuples, but which cannot be quer\u2010\nied. The stream then consists of the log of tuples that committed transactions have\nwritten to this special table, in the order they were committed. External consumers\ncan asynchronously consume this log and use it to update derived data systems.\nKafka Connect [41] is an effort to integrate change data capture tools for a wide\nrange of database systems with Kafka. Once the stream of change events is in Kafka, it\ncan be used to update derived data systems such as search indexes, and also feed into\nstream processing systems as discussed later in this chapter.\nEvent Sourcing\nThere are some parallels between the ideas we\u2019ve discussed here and event sourcing, a\ntechnique that was developed in the domain-driven design (DDD) community [42,\n43, 44]. We will discuss event sourcing briefly, because it incorporates some useful\nand relevant ideas for streaming systems.\nSimilarly to change data capture, event sourcing involves storing all changes to the\napplication state as a log of change events. The biggest difference is that event sourc\u2010\ning applies the idea at a different level of abstraction:\n\u2022 In change data capture, the application uses the database in a mutable way,\nupdating and deleting records at will. The log of changes is extracted from the\ndatabase at a low level (e.g., by parsing the replication log), which ensures that\nthe order of writes extracted from the database matches the order in which they\nwere actually written, avoiding the race condition in Figure 11-4. The application\nwriting to the database does not need to be aware that CDC is occurring.\n\u2022 In event sourcing, the application logic is explicitly built on the basis of immuta\u2010\nble events that are written to an event log. In this case, the event store is append-\nonly, and updates or deletes are discouraged or prohibited. Events are designed\nto reflect things that happened at the application level, rather than low-level state\nchanges.\nEvent sourcing is a powerful technique for data modeling: from an application point\nof view it is more meaningful to record the user\u2019s actions as immutable events, rather\nthan recording the effect of those actions on a mutable database. Event sourcing\nmakes it easier to evolve applications over time, helps with debugging by making it\neasier to understand after the fact why something happened, and guards against\napplication bugs (see \u201cAdvantages of immutable events\u201d on page 460).\nFor example, storing the event \u201cstudent cancelled their course enrollment\u201d clearly"}
{"480": "data is later going to be used. If a new application feature is introduced\u2014for example,\n\u201cthe place is offered to the next person on the waiting list\u201d\u2014the event sourcing\napproach allows that new side effect to easily be chained off the existing event.\nEvent sourcing is similar to the chronicle data model [45], and there are also similari\u2010\nties between an event log and the fact table that you find in a star schema (see \u201cStars\nand Snowflakes: Schemas for Analytics\u201d on page 93).\nSpecialized databases such as Event Store [46] have been developed to support appli\u2010\ncations using event sourcing, but in general the approach is independent of any par\u2010\nticular tool. A conventional database or a log-based message broker can also be used\nto build applications in this style.\nDeriving current state from the event log\nAn event log by itself is not very useful, because users generally expect to see the cur\u2010\nrent state of a system, not the history of modifications. For example, on a shopping\nwebsite, users expect to be able to see the current contents of their cart, not an\nappend-only list of all the changes they have ever made to their cart.\nThus, applications that use event sourcing need to take the log of events (representing\nthe data written to the system) and transform it into application state that is suitable\nfor showing to a user (the way in which data is read from the system [47]). This\ntransformation can use arbitrary logic, but it should be deterministic so that you can\nrun it again and derive the same application state from the event log.\nLike with change data capture, replaying the event log allows you to reconstruct the\ncurrent state of the system. However, log compaction needs to be handled differently:\n\u2022 A CDC event for the update of a record typically contains the entire new version\nof the record, so the current value for a primary key is entirely determined by the\nmost recent event for that primary key, and log compaction can discard previous\nevents for the same key.\n\u2022 On the other hand, with event sourcing, events are modeled at a higher level: an\nevent typically expresses the intent of a user action, not the mechanics of the state\nupdate that occurred as a result of the action. In this case, later events typically\ndo not override prior events, and so you need the full history of events to recon\u2010\nstruct the final state. Log compaction is not possible in the same way.\nApplications that use event sourcing typically have some mechanism for storing\nsnapshots of the current state that is derived from the log of events, so they don\u2019t\nneed to repeatedly reprocess the full log. However, this is only a performance optimi\u2010"}
{"481": "Commands and events\nThe event sourcing philosophy is careful to distinguish between events and com\u2010\nmands [48]. When a request from a user first arrives, it is initially a command: at this\npoint it may still fail, for example because some integrity condition is violated. The\napplication must first validate that it can execute the command. If the validation is\nsuccessful and the command is accepted, it becomes an event, which is durable and\nimmutable.\nFor example, if a user tries to register a particular username, or reserve a seat on an\nairplane or in a theater, then the application needs to check that the username or seat\nis not already taken. (We previously discussed this example in \u201cFault-Tolerant Con\u2010\nsensus\u201d on page 364.) When that check has succeeded, the application can generate\nan event to indicate that a particular username was registered by a particular user ID,\nor that a particular seat has been reserved for a particular customer.\nAt the point when the event is generated, it becomes a fact. Even if the customer later\ndecides to change or cancel the reservation, the fact remains true that they formerly\nheld a reservation for a particular seat, and the change or cancellation is a separate\nevent that is added later.\nA consumer of the event stream is not allowed to reject an event: by the time the con\u2010\nsumer sees the event, it is already an immutable part of the log, and it may have\nalready been seen by other consumers. Thus, any validation of a command needs to\nhappen synchronously, before it becomes an event\u2014for example, by using a serializa\u2010\nble transaction that atomically validates the command and publishes the event.\nAlternatively, the user request to reserve a seat could be split into two events: first a\ntentative reservation, and then a separate confirmation event once the reservation has\nbeen validated (as discussed in \u201cImplementing linearizable storage using total order\nbroadcast\u201d on page 350). This split allows the validation to take place in an asynchro\u2010\nnous process.\nState, Streams, and Immutability\nWe saw in Chapter 10 that batch processing benefits from the immutability of its\ninput files, so you can run experimental processing jobs on existing input files\nwithout fear of damaging them. This principle of immutability is also what makes\nevent sourcing and change data capture so powerful.\nWe normally think of databases as storing the current state of the application\u2014this\nrepresentation is optimized for reads, and it is usually the most convenient for serv\u2010\ning queries. The nature of state is that it changes, so databases support updating and"}
{"482": "Whenever you have state that changes, that state is the result of the events that muta\u2010\nted it over time. For example, your list of currently available seats is the result of the\nreservations you have processed, the current account balance is the result of the cred\u2010\nits and debits on the account, and the response time graph for your web server is an\naggregation of the individual response times of all web requests that have occurred.\nNo matter how the state changes, there was always a sequence of events that caused\nthose changes. Even as things are done and undone, the fact remains true that those\nevents occurred. The key idea is that mutable state and an append-only log of immut\u2010\nable events do not contradict each other: they are two sides of the same coin. The log\nof all changes, the changelog, represents the evolution of state over time.\nIf you are mathematically inclined, you might say that the application state is what\nyou get when you integrate an event stream over time, and a change stream is what\nyou get when you differentiate the state by time, as shown in Figure 11-6 [49, 50, 51].\nThe analogy has limitations (for example, the second derivative of state does not\nseem to be meaningful), but it\u2019s a useful starting point for thinking about data.\nFigure 11-6. The relationship between the current application state and an event\nstream.\nIf you store the changelog durably, that simply has the effect of making the state\nreproducible. If you consider the log of events to be your system of record, and any\nmutable state as being derived from it, it becomes easier to reason about the flow of\ndata through a system. As Pat Helland puts it [52]:\nTransaction logs record all the changes made to the database. High-speed appends are\nthe only way to change the log. From this perspective, the contents of the database\nhold a caching of the latest record values in the logs. The truth is the log. The database\nis a cache of a subset of the log. That cached subset happens to be the latest value of\neach record and index value from the log.\nLog compaction, as discussed in \u201cLog compaction\u201d on page 456, is one way of bridg\u2010\ning the distinction between log and database state: it retains only the latest version of\neach record, and discards overwritten versions."}
{"483": "recorded in an append-only ledger, which is essentially a log of events describing\nmoney, goods, or services that have changed hands. The accounts, such as profit and\nloss or the balance sheet, are derived from the transactions in the ledger by adding\nthem up [53].\nIf a mistake is made, accountants don\u2019t erase or change the incorrect transaction in\nthe ledger\u2014instead, they add another transaction that compensates for the mistake,\nfor example refunding an incorrect charge. The incorrect transaction still remains in\nthe ledger forever, because it might be important for auditing reasons. If incorrect\nfigures, derived from the incorrect ledger, have already been published, then the fig\u2010\nures for the next accounting period include a correction. This process is entirely nor\u2010\nmal in accounting [54].\nAlthough such auditability is particularly important in financial systems, it is also\nbeneficial for many other systems that are not subject to such strict regulation. As\ndiscussed in \u201cPhilosophy of batch process outputs\u201d on page 413, if you accidentally\ndeploy buggy code that writes bad data to a database, recovery is much harder if the\ncode is able to destructively overwrite data. With an append-only log of immutable\nevents, it is much easier to diagnose what happened and recover from the problem.\nImmutable events also capture more information than just the current state. For\nexample, on a shopping website, a customer may add an item to their cart and then\nremove it again. Although the second event cancels out the first event from the point\nof view of order fulfillment, it may be useful to know for analytics purposes that the\ncustomer was considering a particular item but then decided against it. Perhaps they\nwill choose to buy it in the future, or perhaps they found a substitute. This informa\u2010\ntion is recorded in an event log, but would be lost in a database that deletes items\nwhen they are removed from the cart [42].\nDeriving several views from the same event log\nMoreover, by separating mutable state from the immutable event log, you can derive\nseveral different read-oriented representations from the same log of events. This\nworks just like having multiple consumers of a stream (Figure 11-5): for example, the\nanalytic database Druid ingests directly from Kafka using this approach [55], Pista\u2010\nchio is a distributed key-value store that uses Kafka as a commit log [56], and Kafka\nConnect sinks can export data from Kafka to various different databases and indexes\n[41]. It would make sense for many other storage and indexing systems, such as\nsearch servers, to similarly take their input from a distributed log (see \u201cKeeping Sys\u2010\ntems in Sync\u201d on page 452).\nHaving an explicit translation step from an event log to a database makes it easier to"}
{"484": "systems without having to modify them. Running old and new systems side by side is\noften easier than performing a complicated schema migration in an existing system.\nOnce the old system is no longer needed, you can simply shut it down and reclaim its\nresources [47, 57].\nStoring data is normally quite straightforward if you don\u2019t have to worry about how it\nis going to be queried and accessed; many of the complexities of schema design,\nindexing, and storage engines are the result of wanting to support certain query and\naccess patterns (see Chapter 3). For this reason, you gain a lot of flexibility by sepa\u2010\nrating the form in which data is written from the form it is read, and by allowing sev\u2010\neral different read views. This idea is sometimes known as command query\nresponsibility segregation (CQRS) [42, 58, 59].\nThe traditional approach to database and schema design is based on the fallacy that\ndata must be written in the same form as it will be queried. Debates about normaliza\u2010\ntion and denormalization (see \u201cMany-to-One and Many-to-Many Relationships\u201d on\npage 33) become largely irrelevant if you can translate data from a write-optimized\nevent log to read-optimized application state: it is entirely reasonable to denormalize\ndata in the read-optimized views, as the translation process gives you a mechanism\nfor keeping it consistent with the event log.\nIn \u201cDescribing Load\u201d on page 11 we discussed Twitter\u2019s home timelines, a cache of\nrecently written tweets by the people a particular user is following (like a mailbox).\nThis is another example of read-optimized state: home timelines are highly denor\u2010\nmalized, since your tweets are duplicated in all of the timelines of the people follow\u2010\ning you. However, the fan-out service keeps this duplicated state in sync with new\ntweets and new following relationships, which keeps the duplication manageable.\nConcurrency control\nThe biggest downside of event sourcing and change data capture is that the consum\u2010\ners of the event log are usually asynchronous, so there is a possibility that a user may\nmake a write to the log, then read from a log-derived view and find that their write\nhas not yet been reflected in the read view. We discussed this problem and potential\nsolutions previously in \u201cReading Your Own Writes\u201d on page 162.\nOne solution would be to perform the updates of the read view synchronously with\nappending the event to the log. This requires a transaction to combine the writes into\nan atomic unit, so either you need to keep the event log and the read view in the same\nstorage system, or you need a distributed transaction across the different systems.\nAlternatively, you could use the approach discussed in \u201cImplementing linearizable\nstorage using total order broadcast\u201d on page 350."}
{"485": "action requiring data to be changed in several different places. With event sourcing,\nyou can design an event such that it is a self-contained description of a user action.\nThe user action then requires only a single write in one place\u2014namely appending the\nevents to the log\u2014which is easy to make atomic.\nIf the event log and the application state are partitioned in the same way (for exam\u2010\nple, processing an event for a customer in partition 3 only requires updating partition\n3 of the application state), then a straightforward single-threaded log consumer needs\nno concurrency control for writes\u2014by construction, it only processes a single event\nat a time (see also \u201cActual Serial Execution\u201d on page 252). The log removes the non\u2010\ndeterminism of concurrency by defining a serial order of events in a partition [24]. If\nan event touches multiple state partitions, a bit more work is required, which we will\ndiscuss in Chapter 12.\nLimitations of immutability\nMany systems that don\u2019t use an event-sourced model nevertheless rely on immutabil\u2010\nity: various databases internally use immutable data structures or multi-version data\nto support point-in-time snapshots (see \u201cIndexes and snapshot isolation\u201d on page\n241). Version control systems such as Git, Mercurial, and Fossil also rely on immuta\u2010\nble data to preserve version history of files.\nTo what extent is it feasible to keep an immutable history of all changes forever? The\nanswer depends on the amount of churn in the dataset. Some workloads mostly add\ndata and rarely update or delete; they are easy to make immutable. Other workloads\nhave a high rate of updates and deletes on a comparatively small dataset; in these\ncases, the immutable history may grow prohibitively large, fragmentation may\nbecome an issue, and the performance of compaction and garbage collection\nbecomes crucial for operational robustness [60, 61].\nBesides the performance reasons, there may also be circumstances in which you need\ndata to be deleted for administrative reasons, in spite of all immutability. For exam\u2010\nple, privacy regulations may require deleting a user\u2019s personal information after they\nclose their account, data protection legislation may require erroneous information to\nbe removed, or an accidental leak of sensitive information may need to be contained.\nIn these circumstances, it\u2019s not sufficient to just append another event to the log to\nindicate that the prior data should be considered deleted\u2014you actually want to\nrewrite history and pretend that the data was never written in the first place. For\nexample, Datomic calls this feature excision [62], and the Fossil version control sys\u2010\ntem has a similar concept called shunning [63]."}
{"486": "to retrieve the data\u201d than actually \u201cmaking it impossible to retrieve the data.\u201d Never\u2010\ntheless, you sometimes have to try, as we shall see in \u201cLegislation and self-regulation\u201d\non page 542.\nProcessing Streams\nSo far in this chapter we have talked about where streams come from (user activity\nevents, sensors, and writes to databases), and we have talked about how streams are\ntransported (through direct messaging, via message brokers, and in event logs).\nWhat remains is to discuss what you can do with the stream once you have it\u2014\nnamely, you can process it. Broadly, there are three options:\n1. You can take the data in the events and write it to a database, cache, search index,\nor similar storage system, from where it can then be queried by other clients. As\nshown in Figure 11-5, this is a good way of keeping a database in sync with\nchanges happening in other parts of the system\u2014especially if the stream con\u2010\nsumer is the only client writing to the database. Writing to a storage system is the\nstreaming equivalent of what we discussed in \u201cThe Output of Batch Workflows\u201d\non page 411.\n2. You can push the events to users in some way, for example by sending email\nalerts or push notifications, or by streaming the events to a real-time dashboard\nwhere they are visualized. In this case, a human is the ultimate consumer of the\nstream.\n3. You can process one or more input streams to produce one or more output\nstreams. Streams may go through a pipeline consisting of several such processing\nstages before they eventually end up at an output (option 1 or 2).\nIn the rest of this chapter, we will discuss option 3: processing streams to produce\nother, derived streams. A piece of code that processes streams like this is known as an\noperator or a job. It is closely related to the Unix processes and MapReduce jobs we\ndiscussed in Chapter 10, and the pattern of dataflow is similar: a stream processor\nconsumes input streams in a read-only fashion and writes its output to a different\nlocation in an append-only fashion.\nThe patterns for partitioning and parallelization in stream processors are also very\nsimilar to those in MapReduce and the dataflow engines we saw in Chapter 10, so we\nwon\u2019t repeat those topics here. Basic mapping operations such as transforming and\nfiltering records also work the same."}
{"487": "change: with a batch job that has been running for a few minutes, a failed task can\nsimply be restarted from the beginning, but with a stream job that has been running\nfor several years, restarting from the beginning after a crash may not be a viable\noption.\nUses of Stream Processing\nStream processing has long been used for monitoring purposes, where an organiza\u2010\ntion wants to be alerted if certain things happen. For example:\n\u2022 Fraud detection systems need to determine if the usage patterns of a credit card\nhave unexpectedly changed, and block the card if it is likely to have been stolen.\n\u2022 Trading systems need to examine price changes in a financial market and execute\ntrades according to specified rules.\n\u2022 Manufacturing systems need to monitor the status of machines in a factory, and\nquickly identify the problem if there is a malfunction.\n\u2022 Military and intelligence systems need to track the activities of a potential aggres\u2010\nsor, and raise the alarm if there are signs of an attack.\nThese kinds of applications require quite sophisticated pattern matching and correla\u2010\ntions. However, other uses of stream processing have also emerged over time. In this\nsection we will briefly compare and contrast some of these applications.\nComplex event processing\nComplex event processing (CEP) is an approach developed in the 1990s for analyzing\nevent streams, especially geared toward the kind of application that requires search\u2010\ning for certain event patterns [65, 66]. Similarly to the way that a regular expression\nallows you to search for certain patterns of characters in a string, CEP allows you to\nspecify rules to search for certain patterns of events in a stream.\nCEP systems often use a high-level declarative query language like SQL, or a graphi\u2010\ncal user interface, to describe the patterns of events that should be detected. These\nqueries are submitted to a processing engine that consumes the input streams and\ninternally maintains a state machine that performs the required matching. When a\nmatch is found, the engine emits a complex event (hence the name) with the details of\nthe event pattern that was detected [67].\nIn these systems, the relationship between queries and data is reversed compared to\nnormal databases. Usually, a database stores data persistently and treats queries as"}
{"488": "Implementations of CEP include Esper [69], IBM InfoSphere Streams [70], Apama,\nTIBCO StreamBase, and SQLstream. Distributed stream processors like Samza are\nalso gaining SQL support for declarative queries on streams [71].\nStream analytics\nAnother area in which stream processing is used is for analytics on streams. The\nboundary between CEP and stream analytics is blurry, but as a general rule, analytics\ntends to be less interested in finding specific event sequences and is more oriented\ntoward aggregations and statistical metrics over a large number of events\u2014for exam\u2010\nple:\n\u2022 Measuring the rate of some type of event (how often it occurs per time interval)\n\u2022 Calculating the rolling average of a value over some time period\n\u2022 Comparing current statistics to previous time intervals (e.g., to detect trends or\nto alert on metrics that are unusually high or low compared to the same time last\nweek)\nSuch statistics are usually computed over fixed time intervals\u2014for example, you\nmight want to know the average number of queries per second to a service over the\nlast 5 minutes, and their 99th percentile response time during that period. Averaging\nover a few minutes smoothes out irrelevant fluctuations from one second to the next,\nwhile still giving you a timely picture of any changes in traffic pattern. The time\ninterval over which you aggregate is known as a window, and we will look into win\u2010\ndowing in more detail in \u201cReasoning About Time\u201d on page 468.\nStream analytics systems sometimes use probabilistic algorithms, such as Bloom fil\u2010\nters (which we encountered in \u201cPerformance optimizations\u201d on page 79) for set\nmembership, HyperLogLog [72] for cardinality estimation, and various percentile\nestimation algorithms (see \u201cPercentiles in Practice\u201d on page 16). Probabilistic algo\u2010\nrithms produce approximate results, but have the advantage of requiring significantly\nless memory in the stream processor than exact algorithms. This use of approxima\u2010\ntion algorithms sometimes leads people to believe that stream processing systems are\nalways lossy and inexact, but that is wrong: there is nothing inherently approximate\nabout stream processing, and probabilistic algorithms are merely an optimization\n[73].\nMany open source distributed stream processing frameworks are designed with ana\u2010\nlytics in mind: for example, Apache Storm, Spark Streaming, Flink, Concord, Samza,\nand Kafka Streams [74]. Hosted services include Google Cloud Dataflow and Azure"}
{"489": "Maintaining materialized views\nWe saw in \u201cDatabases and Streams\u201d on page 451 that a stream of changes to a data\u2010\nbase can be used to keep derived data systems, such as caches, search indexes, and\ndata warehouses, up to date with a source database. We can regard these examples as\nspecific cases of maintaining materialized views (see \u201cAggregation: Data Cubes and\nMaterialized Views\u201d on page 101): deriving an alternative view onto some dataset so\nthat you can query it efficiently, and updating that view whenever the underlying\ndata changes [50].\nSimilarly, in event sourcing, application state is maintained by applying a log of\nevents; here the application state is also a kind of materialized view. Unlike stream\nanalytics scenarios, it is usually not sufficient to consider only events within some\ntime window: building the materialized view potentially requires all events over an\narbitrary time period, apart from any obsolete events that may be discarded by log\ncompaction (see \u201cLog compaction\u201d on page 456). In effect, you need a window that\nstretches all the way back to the beginning of time.\nIn principle, any stream processor could be used for materialized view maintenance,\nalthough the need to maintain events forever runs counter to the assumptions of\nsome analytics-oriented frameworks that mostly operate on windows of a limited\nduration. Samza and Kafka Streams support this kind of usage, building upon Kafka\u2019s\nsupport for log compaction [75].\nSearch on streams\nBesides CEP, which allows searching for patterns consisting of multiple events, there\nis also sometimes a need to search for individual events based on complex criteria,\nsuch as full-text search queries.\nFor example, media monitoring services subscribe to feeds of news articles and\nbroadcasts from media outlets, and search for any news mentioning companies,\nproducts, or topics of interest. This is done by formulating a search query in advance,\nand then continually matching the stream of news items against this query. Similar\nfeatures exist on some websites: for example, users of real estate websites can ask to\nbe notified when a new property matching their search criteria appears on the mar\u2010\nket. The percolator feature of Elasticsearch [76] is one option for implementing this\nkind of stream search.\nConventional search engines first index the documents and then run queries over the\nindex. By contrast, searching a stream turns the processing on its head: the queries\nare stored, and the documents run past the queries, like in CEP. In the simplest case,"}
{"490": "Message passing and RPC\nIn \u201cMessage-Passing Dataflow\u201d on page 136 we discussed message-passing systems as\nan alternative to RPC\u2014i.e., as a mechanism for services to communicate, as used for\nexample in the actor model. Although these systems are also based on messages and\nevents, we normally don\u2019t think of them as stream processors:\n\u2022 Actor frameworks are primarily a mechanism for managing concurrency and\ndistributed execution of communicating modules, whereas stream processing is\nprimarily a data management technique.\n\u2022 Communication between actors is often ephemeral and one-to-one, whereas\nevent logs are durable and multi-subscriber.\n\u2022 Actors can communicate in arbitrary ways (including cyclic request/response\npatterns), but stream processors are usually set up in acyclic pipelines where\nevery stream is the output of one particular job, and derived from a well-defined\nset of input streams.\nThat said, there is some crossover area between RPC-like systems and stream pro\u2010\ncessing. For example, Apache Storm has a feature called distributed RPC, which\nallows user queries to be farmed out to a set of nodes that also process event streams;\nthese queries are then interleaved with events from the input streams, and results can\nbe aggregated and sent back to the user [78]. (See also \u201cMulti-partition data process\u2010\ning\u201d on page 514.)\nIt is also possible to process streams using actor frameworks. However, many such\nframeworks do not guarantee message delivery in the case of crashes, so the process\u2010\ning is not fault-tolerant unless you implement additional retry logic.\nReasoning About Time\nStream processors often need to deal with time, especially when used for analytics\npurposes, which frequently use time windows such as \u201cthe average over the last five\nminutes.\u201d It might seem that the meaning of \u201cthe last five minutes\u201d should be unam\u2010\nbiguous and clear, but unfortunately the notion is surprisingly tricky.\nIn a batch process, the processing tasks rapidly crunch through a large collection of\nhistorical events. If some kind of breakdown by time needs to happen, the batch pro\u2010\ncess needs to look at the timestamp embedded in each event. There is no point in\nlooking at the system clock of the machine running the batch process, because the\ntime at which the process is run has nothing to do with the time at which the events\nactually occurred."}
{"491": "deterministic: running the same process again on the same input yields the same\nresult (see \u201cFault tolerance\u201d on page 422).\nOn the other hand, many stream processing frameworks use the local system clock\non the processing machine (the processing time) to determine windowing [79]. This\napproach has the advantage of being simple, and it is reasonable if the delay between\nevent creation and event processing is negligibly short. However, it breaks down if\nthere is any significant processing lag\u2014i.e., if the processing may happen noticeably\nlater than the time at which the event actually occurred.\nEvent time versus processing time\nThere are many reasons why processing may be delayed: queueing, network faults\n(see \u201cUnreliable Networks\u201d on page 277), a performance issue leading to contention\nin the message broker or processor, a restart of the stream consumer, or reprocessing\nof past events (see \u201cReplaying old messages\u201d on page 451) while recovering from a\nfault or after fixing a bug in the code.\nMoreover, message delays can also lead to unpredictable ordering of messages. For\nexample, say a user first makes one web request (which is handled by web server A),\nand then a second request (which is handled by server B). A and B emit events\ndescribing the requests they handled, but B\u2019s event reaches the message broker before\nA\u2019s event does. Now stream processors will first see the B event and then the A event,\neven though they actually occurred in the opposite order.\nIf it helps to have an analogy, consider the Star Wars movies: Episode IV was released\nin 1977, Episode V in 1980, and Episode VI in 1983, followed by Episodes I, II, and\nIII in 1999, 2002, and 2005, respectively, and Episode VII in 2015 [80].ii If you\nwatched the movies in the order they came out, the order in which you processed the\nmovies is inconsistent with the order of their narrative. (The episode number is like\nthe event timestamp, and the date when you watched the movie is the processing\ntime.) As humans, we are able to cope with such discontinuities, but stream process\u2010\ning algorithms need to be specifically written to accommodate such timing and\nordering issues.\nConfusing event time and processing time leads to bad data. For example, say you\nhave a stream processor that measures the rate of requests (counting the number of\nrequests per second). If you redeploy the stream processor, it may be shut down for a\nminute and process the backlog of events when it comes back up. If you measure the\nrate based on the processing time, it will look as if there was a sudden anomalous\nspike of requests while processing the backlog, when in fact the real rate of requests\nwas steady (Figure 11-7)."}
{"492": "Figure 11-7. Windowing by processing time introduces artifacts due to variations in\nprocessing rate.\nKnowing when you\u2019re ready\nA tricky problem when defining windows in terms of event time is that you can never\nbe sure when you have received all of the events for a particular window, or whether\nthere are some events still to come.\nFor example, say you\u2019re grouping events into one-minute windows so that you can\ncount the number of requests per minute. You have counted some number of events\nwith timestamps that fall in the 37th minute of the hour, and time has moved on;\nnow most of the incoming events fall within the 38th and 39th minutes of the hour.\nWhen do you declare that you have finished the window for the 37th minute, and\noutput its counter value?\nYou can time out and declare a window ready after you have not seen any new events\nfor a while, but it could still happen that some events were buffered on another\nmachine somewhere, delayed due to a network interruption. You need to be able to\nhandle such straggler events that arrive after the window has already been declared\ncomplete. Broadly, you have two options [1]:\n1. Ignore the straggler events, as they are probably a small percentage of events in\nnormal circumstances. You can track the number of dropped events as a metric,"}
{"493": "In some cases it is possible to use a special message to indicate, \u201cFrom now on there\nwill be no more messages with a timestamp earlier than t,\u201d which can be used by con\u2010\nsumers to trigger windows [81]. However, if several producers on different machines\nare generating events, each with their own minimum timestamp thresholds, the con\u2010\nsumers need to keep track of each producer individually. Adding and removing pro\u2010\nducers is trickier in this case.\nWhose clock are you using, anyway?\nAssigning timestamps to events is even more difficult when events can be buffered at\nseveral points in the system. For example, consider a mobile app that reports events\nfor usage metrics to a server. The app may be used while the device is offline, in\nwhich case it will buffer events locally on the device and send them to a server when\nan internet connection is next available (which may be hours or even days later). To\nany consumers of this stream, the events will appear as extremely delayed stragglers.\nIn this context, the timestamp on the events should really be the time at which the\nuser interaction occurred, according to the mobile device\u2019s local clock. However, the\nclock on a user-controlled device often cannot be trusted, as it may be accidentally or\ndeliberately set to the wrong time (see \u201cClock Synchronization and Accuracy\u201d on\npage 289). The time at which the event was received by the server (according to the\nserver\u2019s clock) is more likely to be accurate, since the server is under your control, but\nless meaningful in terms of describing the user interaction.\nTo adjust for incorrect device clocks, one approach is to log three timestamps [82]:\n\u2022 The time at which the event occurred, according to the device clock\n\u2022 The time at which the event was sent to the server, according to the device clock\n\u2022 The time at which the event was received by the server, according to the server\nclock\nBy subtracting the second timestamp from the third, you can estimate the offset\nbetween the device clock and the server clock (assuming the network delay is negligi\u2010\nble compared to the required timestamp accuracy). You can then apply that offset to\nthe event timestamp, and thus estimate the true time at which the event actually\noccurred (assuming the device clock offset did not change between the time the event\noccurred and the time it was sent to the server).\nThis problem is not unique to stream processing\u2014batch processing suffers from\nexactly the same issues of reasoning about time. It is just more noticeable in a stream\u2010\ning context, where we are more aware of the passage of time."}
{"494": "Types of windows\nOnce you know how the timestamp of an event should be determined, the next step\nis to decide how windows over time periods should be defined. The window can then\nbe used for aggregations, for example to count events, or to calculate the average of\nvalues within the window. Several types of windows are in common use [79, 83]:\nTumbling window\nA tumbling window has a fixed length, and every event belongs to exactly one\nwindow. For example, if you have a 1-minute tumbling window, all the events\nwith timestamps between 10:03:00 and 10:03:59 are grouped into one window,\nevents between 10:04:00 and 10:04:59 into the next window, and so on. You\ncould implement a 1-minute tumbling window by taking each event timestamp\nand rounding it down to the nearest minute to determine the window that it\nbelongs to.\nHopping window\nA hopping window also has a fixed length, but allows windows to overlap in\norder to provide some smoothing. For example, a 5-minute window with a hop\nsize of 1 minute would contain the events between 10:03:00 and 10:07:59, then\nthe next window would cover events between 10:04:00 and 10:08:59, and so on.\nYou can implement this hopping window by first calculating 1-minute tumbling\nwindows, and then aggregating over several adjacent windows.\nSliding window\nA sliding window contains all the events that occur within some interval of each\nother. For example, a 5-minute sliding window would cover events at 10:03:39\nand 10:08:12, because they are less than 5 minutes apart (note that tumbling and\nhopping 5-minute windows would not have put these two events in the same\nwindow, as they use fixed boundaries). A sliding window can be implemented by\nkeeping a buffer of events sorted by time and removing old events when they\nexpire from the window.\nSession window\nUnlike the other window types, a session window has no fixed duration. Instead,\nit is defined by grouping together all events for the same user that occur closely\ntogether in time, and the window ends when the user has been inactive for some\ntime (for example, if there have been no events for 30 minutes). Sessionization is\na common requirement for website analytics (see \u201cGROUP BY\u201d on page 406).\nStream Joins"}
{"495": "data pipelines to incremental processing of unbounded datasets, there is exactly the\nsame need for joins on streams.\nHowever, the fact that new events can appear anytime on a stream makes joins on\nstreams more challenging than in batch jobs. To understand the situation better, let\u2019s\ndistinguish three different types of joins: stream-stream joins, stream-table joins, and\ntable-table joins [84]. In the following sections we\u2019ll illustrate each by example.\nStream-stream join (window join)\nSay you have a search feature on your website, and you want to detect recent trends\nin searched-for URLs. Every time someone types a search query, you log an event\ncontaining the query and the results returned. Every time someone clicks one of the\nsearch results, you log another event recording the click. In order to calculate the\nclick-through rate for each URL in the search results, you need to bring together the\nevents for the search action and the click action, which are connected by having the\nsame session ID. Similar analyses are needed in advertising systems [85].\nThe click may never come if the user abandons their search, and even if it comes, the\ntime between the search and the click may be highly variable: in many cases it might\nbe a few seconds, but it could be as long as days or weeks (if a user runs a search,\nforgets about that browser tab, and then returns to the tab and clicks a result some\u2010\ntime later). Due to variable network delays, the click event may even arrive before the\nsearch event. You can choose a suitable window for the join\u2014for example, you may\nchoose to join a click with a search if they occur at most one hour apart.\nNote that embedding the details of the search in the click event is not equivalent to\njoining the events: doing so would only tell you about the cases where the user\nclicked a search result, not about the searches where the user did not click any of the\nresults. In order to measure search quality, you need accurate click-through rates, for\nwhich you need both the search events and the click events.\nTo implement this type of join, a stream processor needs to maintain state: for exam\u2010\nple, all the events that occurred in the last hour, indexed by session ID. Whenever a\nsearch event or click event occurs, it is added to the appropriate index, and the\nstream processor also checks the other index to see if another event for the same ses\u2010\nsion ID has already arrived. If there is a matching event, you emit an event saying\nwhich search result was clicked. If the search event expires without you seeing a\nmatching click event, you emit an event saying which search results were not clicked.\nStream-table join (stream enrichment)"}
{"496": "stream of activity events containing a user ID, and the output is a stream of activity\nevents in which the user ID has been augmented with profile information about the\nuser. This process is sometimes known as enriching the activity events with informa\u2010\ntion from the database.\nTo perform this join, the stream process needs to look at one activity event at a time,\nlook up the event\u2019s user ID in the database, and add the profile information to the\nactivity event. The database lookup could be implemented by querying a remote\ndatabase; however, as discussed in \u201cExample: analysis of user activity events\u201d on page\n404, such remote queries are likely to be slow and risk overloading the database [75].\nAnother approach is to load a copy of the database into the stream processor so that\nit can be queried locally without a network round-trip. This technique is very similar\nto the hash joins we discussed in \u201cMap-Side Joins\u201d on page 408: the local copy of the\ndatabase might be an in-memory hash table if it is small enough, or an index on the\nlocal disk.\nThe difference to batch jobs is that a batch job uses a point-in-time snapshot of the\ndatabase as input, whereas a stream processor is long-running, and the contents of\nthe database are likely to change over time, so the stream processor\u2019s local copy of the\ndatabase needs to be kept up to date. This issue can be solved by change data capture:\nthe stream processor can subscribe to a changelog of the user profile database as well\nas the stream of activity events. When a profile is created or modified, the stream\nprocessor updates its local copy. Thus, we obtain a join between two streams: the\nactivity events and the profile updates.\nA stream-table join is actually very similar to a stream-stream join; the biggest differ\u2010\nence is that for the table changelog stream, the join uses a window that reaches back\nto the \u201cbeginning of time\u201d (a conceptually infinite window), with newer versions of\nrecords overwriting older ones. For the stream input, the join might not maintain a\nwindow at all.\nTable-table join (materialized view maintenance)\nConsider the Twitter timeline example that we discussed in \u201cDescribing Load\u201d on\npage 11. We said that when a user wants to view their home timeline, it is too expen\u2010\nsive to iterate over all the people the user is following, find their recent tweets, and\nmerge them.\nInstead, we want a timeline cache: a kind of per-user \u201cinbox\u201d to which tweets are\nwritten as they are sent, so that reading the timeline is a single lookup. Materializing\nand maintaining this cache requires the following event processing:"}
{"497": "\u2022 When a user deletes a tweet, it is removed from all users\u2019 timelines.\n\u2022 When user u starts following user u , recent tweets by u are added to u \u2019s\n1 2 2 1\ntimeline.\n\u2022 When user u unfollows user u , tweets by u are removed from u \u2019s timeline.\n1 2 2 1\nTo implement this cache maintenance in a stream processor, you need streams of\nevents for tweets (sending and deleting) and for follow relationships (following and\nunfollowing). The stream process needs to maintain a database containing the set of\nfollowers for each user so that it knows which timelines need to be updated when a\nnew tweet arrives [86].\nAnother way of looking at this stream process is that it maintains a materialized view\nfor a query that joins two tables (tweets and follows), something like the following:\nSELECT follows.follower_id AS timeline_id,\narray_agg(tweets.* ORDER BY tweets.timestamp DESC)\nFROM tweets\nJOIN follows ON follows.followee_id = tweets.sender_id\nGROUP BY follows.follower_id\nThe join of the streams corresponds directly to the join of the tables in that query.\nThe timelines are effectively a cache of the result of this query, updated every time the\nunderlying tables change.iii\nTime-dependence of joins\nThe three types of joins described here (stream-stream, stream-table, and table-table)\nhave a lot in common: they all require the stream processor to maintain some state\n(search and click events, user profiles, or follower list) based on one join input, and\nquery that state on messages from the other join input.\nThe order of the events that maintain the state is important (it matters whether you\nfirst follow and then unfollow, or the other way round). In a partitioned log, the\nordering of events within a single partition is preserved, but there is typically no\nordering guarantee across different streams or partitions.\nThis raises a question: if events on different streams happen around a similar time, in\nwhich order are they processed? In the stream-table join example, if a user updates\ntheir profile, which activity events are joined with the old profile (processed before\nthe profile update), and which are joined with the new profile (processed after the"}
{"498": "profile update)? Put another way: if state changes over time, and you join with some\nstate, what point in time do you use for the join [45]?\nSuch time dependence can occur in many places. For example, if you sell things, you\nneed to apply the right tax rate to invoices, which depends on the country or state,\nthe type of product, and the date of sale (since tax rates change from time to time).\nWhen joining sales to a table of tax rates, you probably want to join with the tax rate\nat the time of the sale, which may be different from the current tax rate if you are\nreprocessing historical data.\nIf the ordering of events across streams is undetermined, the join becomes nondeter\u2010\nministic [87], which means you cannot rerun the same job on the same input and\nnecessarily get the same result: the events on the input streams may be interleaved in\na different way when you run the job again.\nIn data warehouses, this issue is known as a slowly changing dimension (SCD), and it\nis often addressed by using a unique identifier for a particular version of the joined\nrecord: for example, every time the tax rate changes, it is given a new identifier, and\nthe invoice includes the identifier for the tax rate at the time of sale [88, 89]. This\nchange makes the join deterministic, but has the consequence that log compaction is\nnot possible, since all versions of the records in the table need to be retained.\nFault Tolerance\nIn the final section of this chapter, let\u2019s consider how stream processors can tolerate\nfaults. We saw in Chapter 10 that batch processing frameworks can tolerate faults\nfairly easily: if a task in a MapReduce job fails, it can simply be started again on\nanother machine, and the output of the failed task is discarded. This transparent retry\nis possible because input files are immutable, each task writes its output to a separate\nfile on HDFS, and output is only made visible when a task completes successfully.\nIn particular, the batch approach to fault tolerance ensures that the output of the\nbatch job is the same as if nothing had gone wrong, even if in fact some tasks did fail.\nIt appears as though every input record was processed exactly once\u2014no records are\nskipped, and none are processed twice. Although restarting tasks means that records\nmay in fact be processed multiple times, the visible effect in the output is as if they\nhad only been processed once. This principle is known as exactly-once semantics,\nalthough effectively-once would be a more descriptive term [90].\nThe same issue of fault tolerance arises in stream processing, but it is less straightfor\u2010\nward to handle: waiting until a task is finished before making its output visible is not\nan option, because a stream is infinite and so you can never finish processing it."}
{"499": "Microbatching and checkpointing\nOne solution is to break the stream into small blocks, and treat each block like a min\u2010\niature batch process. This approach is called microbatching, and it is used in Spark\nStreaming [91]. The batch size is typically around one second, which is the result of a\nperformance compromise: smaller batches incur greater scheduling and coordination\noverhead, while larger batches mean a longer delay before results of the stream pro\u2010\ncessor become visible.\nMicrobatching also implicitly provides a tumbling window equal to the batch size\n(windowed by processing time, not event timestamps); any jobs that require larger\nwindows need to explicitly carry over state from one microbatch to the next.\nA variant approach, used in Apache Flink, is to periodically generate rolling check\u2010\npoints of state and write them to durable storage [92, 93]. If a stream operator\ncrashes, it can restart from its most recent checkpoint and discard any output gener\u2010\nated between the last checkpoint and the crash. The checkpoints are triggered by bar\u2010\nriers in the message stream, similar to the boundaries between microbatches, but\nwithout forcing a particular window size.\nWithin the confines of the stream processing framework, the microbatching and\ncheckpointing approaches provide the same exactly-once semantics as batch process\u2010\ning. However, as soon as output leaves the stream processor (for example, by writing\nto a database, sending messages to an external message broker, or sending emails),\nthe framework is no longer able to discard the output of a failed batch. In this case,\nrestarting a failed task causes the external side effect to happen twice, and micro\u2010\nbatching or checkpointing alone is not sufficient to prevent this problem.\nAtomic commit revisited\nIn order to give the appearance of exactly-once processing in the presence of faults,\nwe need to ensure that all outputs and side effects of processing an event take effect if\nand only if the processing is successful. Those effects include any messages sent to\ndownstream operators or external messaging systems (including email or push notifi\u2010\ncations), any database writes, any changes to operator state, and any acknowledg\u2010\nment of input messages (including moving the consumer offset forward in a log-\nbased message broker).\nThose things either all need to happen atomically, or none of them must happen, but\nthey should not go out of sync with each other. If this approach sounds familiar, it is\nbecause we discussed it in \u201cExactly-once message processing\u201d on page 360 in the con\u2010\ntext of distributed transactions and two-phase commit."}
{"500": "used in Google Cloud Dataflow [81, 92] and VoltDB [94], and there are plans to add\nsimilar features to Apache Kafka [95, 96]. Unlike XA, these implementations do not\nattempt to provide transactions across heterogeneous technologies, but instead keep\nthem internal by managing both state changes and messaging within the stream pro\u2010\ncessing framework. The overhead of the transaction protocol can be amortized by\nprocessing several input messages within a single transaction.\nIdempotence\nOur goal is to discard the partial output of any failed tasks so that they can be safely\nretried without taking effect twice. Distributed transactions are one way of achieving\nthat goal, but another way is to rely on idempotence [97].\nAn idempotent operation is one that you can perform multiple times, and it has the\nsame effect as if you performed it only once. For example, setting a key in a key-value\nstore to some fixed value is idempotent (writing the value again simply overwrites the\nvalue with an identical value), whereas incrementing a counter is not idempotent\n(performing the increment again means the value is incremented twice).\nEven if an operation is not naturally idempotent, it can often be made idempotent\nwith a bit of extra metadata. For example, when consuming messages from Kafka,\nevery message has a persistent, monotonically increasing offset. When writing a value\nto an external database, you can include the offset of the message that triggered the\nlast write with the value. Thus, you can tell whether an update has already been\napplied, and avoid performing the same update again.\nThe state handling in Storm\u2019s Trident is based on a similar idea [78]. Relying on\nidempotence implies several assumptions: restarting a failed task must replay the\nsame messages in the same order (a log-based message broker does this), the process\u2010\ning must be deterministic, and no other node may concurrently update the same\nvalue [98, 99].\nWhen failing over from one processing node to another, fencing may be required (see\n\u201cThe leader and the lock\u201d on page 301) to prevent interference from a node that is\nthought to be dead but is actually alive. Despite all those caveats, idempotent opera\u2010\ntions can be an effective way of achieving exactly-once semantics with only a small\noverhead.\nRebuilding state after a failure\nAny stream process that requires state\u2014for example, any windowed aggregations\n(such as counters, averages, and histograms) and any tables and indexes used for"}
{"501": "\u201cStream-table join (stream enrichment)\u201d on page 473. An alternative is to keep state\nlocal to the stream processor, and replicate it periodically. Then, when the stream\nprocessor is recovering from a failure, the new task can read the replicated state and\nresume processing without data loss.\nFor example, Flink periodically captures snapshots of operator state and writes them\nto durable storage such as HDFS [92, 93]; Samza and Kafka Streams replicate state\nchanges by sending them to a dedicated Kafka topic with log compaction, similar to\nchange data capture [84, 100]. VoltDB replicates state by redundantly processing\neach input message on several nodes (see \u201cActual Serial Execution\u201d on page 252).\nIn some cases, it may not even be necessary to replicate the state, because it can be\nrebuilt from the input streams. For example, if the state consists of aggregations over\na fairly short window, it may be fast enough to simply replay the input events corre\u2010\nsponding to that window. If the state is a local replica of a database, maintained by\nchange data capture, the database can also be rebuilt from the log-compacted change\nstream (see \u201cLog compaction\u201d on page 456).\nHowever, all of these trade-offs depend on the performance characteristics of the\nunderlying infrastructure: in some systems, network delay may be lower than disk\naccess latency, and network bandwidth may be comparable to disk bandwidth. There\nis no universally ideal trade-off for all situations, and the merits of local versus\nremote state may also shift as storage and networking technologies evolve.\nSummary\nIn this chapter we have discussed event streams, what purposes they serve, and how\nto process them. In some ways, stream processing is very much like the batch pro\u2010\ncessing we discussed in Chapter 10, but done continuously on unbounded (never-\nending) streams rather than on a fixed-size input. From this perspective, message\nbrokers and event logs serve as the streaming equivalent of a filesystem.\nWe spent some time comparing two types of message brokers:\nAMQP/JMS-style message broker\nThe broker assigns individual messages to consumers, and consumers acknowl\u2010\nedge individual messages when they have been successfully processed. Messages\nare deleted from the broker once they have been acknowledged. This approach is\nappropriate as an asynchronous form of RPC (see also \u201cMessage-Passing Data\u2010\nflow\u201d on page 136), for example in a task queue, where the exact order of mes\u2010\nsage processing is not important and where there is no need to go back and read"}
{"502": "Log-based message broker\nThe broker assigns all messages in a partition to the same consumer node, and\nalways delivers messages in the same order. Parallelism is achieved through par\u2010\ntitioning, and consumers track their progress by checkpointing the offset of the\nlast message they have processed. The broker retains messages on disk, so it is\npossible to jump back and reread old messages if necessary.\nThe log-based approach has similarities to the replication logs found in databases\n(see Chapter 5) and log-structured storage engines (see Chapter 3). We saw that this\napproach is especially appropriate for stream processing systems that consume input\nstreams and generate derived state or derived output streams.\nIn terms of where streams come from, we discussed several possibilities: user activity\nevents, sensors providing periodic readings, and data feeds (e.g., market data in\nfinance) are naturally represented as streams. We saw that it can also be useful to\nthink of the writes to a database as a stream: we can capture the changelog\u2014i.e., the\nhistory of all changes made to a database\u2014either implicitly through change data cap\u2010\nture or explicitly through event sourcing. Log compaction allows the stream to retain\na full copy of the contents of a database.\nRepresenting databases as streams opens up powerful opportunities for integrating\nsystems. You can keep derived data systems such as search indexes, caches, and ana\u2010\nlytics systems continually up to date by consuming the log of changes and applying\nthem to the derived system. You can even build fresh views onto existing data by\nstarting from scratch and consuming the log of changes from the beginning all the\nway to the present.\nThe facilities for maintaining state as streams and replaying messages are also the\nbasis for the techniques that enable stream joins and fault tolerance in various stream\nprocessing frameworks. We discussed several purposes of stream processing, includ\u2010\ning searching for event patterns (complex event processing), computing windowed\naggregations (stream analytics), and keeping derived data systems up to date (materi\u2010\nalized views).\nWe then discussed the difficulties of reasoning about time in a stream processor,\nincluding the distinction between processing time and event timestamps, and the\nproblem of dealing with straggler events that arrive after you thought your window\nwas complete.\nWe distinguished three types of joins that may appear in stream processes:\nStream-stream joins\nBoth input streams consist of activity events, and the join operator searches for"}
{"503": "two join inputs may in fact be the same stream (a self-join) if you want to find\nrelated events within that one stream.\nStream-table joins\nOne input stream consists of activity events, while the other is a database change\u2010\nlog. The changelog keeps a local copy of the database up to date. For each activity\nevent, the join operator queries the database and outputs an enriched activity\nevent.\nTable-table joins\nBoth input streams are database changelogs. In this case, every change on one\nside is joined with the latest state of the other side. The result is a stream of\nchanges to the materialized view of the join between the two tables.\nFinally, we discussed techniques for achieving fault tolerance and exactly-once\nsemantics in a stream processor. As with batch processing, we need to discard the\npartial output of any failed tasks. However, since a stream process is long-running\nand produces output continuously, we can\u2019t simply discard all output. Instead, a\nfiner-grained recovery mechanism can be used, based on microbatching, checkpoint\u2010\ning, transactions, or idempotent writes.\nReferences\n[1] Tyler Akidau, Robert Bradshaw, Craig Chambers, et al.: \u201cThe Dataflow Model: A\nPractical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale,\nUnbounded, Out-of-Order Data Processing,\u201d Proceedings of the VLDB Endowment,\nvolume 8, number 12, pages 1792\u20131803, August 2015. doi:10.14778/2824032.2824076\n[2] Harold Abelson, Gerald Jay Sussman, and Julie Sussman: Structure and Interpre\u2010\ntation of Computer Programs, 2nd edition. MIT Press, 1996. ISBN:\n978-0-262-51087-5, available online at mitpress.mit.edu\n[3] Patrick Th. Eugster, Pascal A. Felber, Rachid Guerraoui, and Anne-Marie Ker\u2010\nmarrec: \u201cThe Many Faces of Publish/Subscribe,\u201d ACM Computing Surveys, volume\n35, number 2, pages 114\u2013131, June 2003. doi:10.1145/857076.857078\n[4] Joseph M. Hellerstein and Michael Stonebraker: Readings in Database Systems,\n4th edition. MIT Press, 2005. ISBN: 978-0-262-69314-1, available online at red\u2010\nbook.cs.berkeley.edu\n[5] Don Carney, U\u011fur \u00c7etintemel, Mitch Cherniack, et al.: \u201cMonitoring Streams \u2013 A\nNew Class of Data Management Applications,\u201d at 28th International Conference on"}
{"504": "[7] Vicent Mart\u00ed: \u201cBrubeck, a statsd-Compatible Metrics Aggregator,\u201d githubengin\u2010\neering.com, June 15, 2015.\n[8] Seth Lowenberger: \u201cMoldUDP64 Protocol Specification V 1.00,\u201d nasdaq\u2010\ntrader.com, July 2009.\n[9] Pieter Hintjens: ZeroMQ \u2013 The Guide. O\u2019Reilly Media, 2013. ISBN:\n978-1-449-33404-8\n[10] Ian Malpass: \u201cMeasure Anything, Measure Everything,\u201d codeascraft.com, Febru\u2010\nary 15, 2011.\n[11] Dieter Plaetinck: \u201c25 Graphite, Grafana and statsd Gotchas,\u201d blog.raintank.io,\nMarch 3, 2016.\n[12] Jeff Lindsay: \u201cWeb Hooks to Revolutionize the Web,\u201d progrium.com, May 3,\n2007.\n[13] Jim N. Gray: \u201cQueues Are Databases,\u201d Microsoft Research Technical Report\nMSR-TR-95-56, December 1995.\n[14] Mark Hapner, Rich Burridge, Rahul Sharma, et al.: \u201cJSR-343 Java Message Ser\u2010\nvice (JMS) 2.0 Specification,\u201d jms-spec.java.net, March 2013.\n[15] Sanjay Aiyagari, Matthew Arrott, Mark Atwell, et al.: \u201cAMQP: Advanced Mes\u2010\nsage Queuing Protocol Specification,\u201d Version 0-9-1, November 2008.\n[16] \u201cGoogle Cloud Pub/Sub: A Google-Scale Messaging Service,\u201d cloud.google.com,\n2016.\n[17] \u201cApache Kafka 0.9 Documentation,\u201d kafka.apache.org, November 2015.\n[18] Jay Kreps, Neha Narkhede, and Jun Rao: \u201cKafka: A Distributed Messaging Sys\u2010\ntem for Log Processing,\u201d at 6th International Workshop on Networking Meets Data\u2010\nbases (NetDB), June 2011.\n[19] \u201cAmazon Kinesis Streams Developer Guide,\u201d docs.aws.amazon.com, April 2016.\n[20] Leigh Stewart and Sijie Guo: \u201cBuilding DistributedLog: Twitter\u2019s High-\nPerformance Replicated Log Service,\u201d blog.twitter.com, September 16, 2015.\n[21] \u201cDistributedLog Documentation,\u201d Twitter, Inc., distributedlog.io, May 2016.\n[22] Jay Kreps: \u201cBenchmarking Apache Kafka: 2 Million Writes Per Second (On\nThree Cheap Machines),\u201d engineering.linkedin.com, April 27, 2014.\n[23] Kartik Paramasivam: \u201cHow We\u2019re Improving and Advancing Kafka at"}
{"505": "[25] Shirshanka Das, Chavdar Botev, Kapil Surlaker, et al.: \u201cAll Aboard the Data\u2010\nbus!,\u201d at 3rd ACM Symposium on Cloud Computing (SoCC), October 2012.\n[26] Yogeshwer Sharma, Philippe Ajoux, Petchean Ang, et al.: \u201cWormhole: Reliable\nPub-Sub to Support Geo-Replicated Internet Services,\u201d at 12th USENIX Symposium\non Networked Systems Design and Implementation (NSDI), May 2015.\n[27] P. P. S. Narayan: \u201cSherpa Update,\u201d developer.yahoo.com, June 8, .\n[28] Martin Kleppmann: \u201cBottled Water: Real-Time Integration of PostgreSQL and\nKafka,\u201d martin.kleppmann.com, April 23, 2015.\n[29] Ben Osheroff: \u201cIntroducing Maxwell, a mysql-to-kafka Binlog Processor,\u201d devel\u2010\noper.zendesk.com, August 20, 2015.\n[30] Randall Hauch: \u201cDebezium 0.2.1 Released,\u201d debezium.io, June 10, 2016.\n[31] Prem Santosh Udaya Shankar: \u201cStreaming MySQL Tables in Real-Time to\nKafka,\u201d engineeringblog.yelp.com, August 1, 2016.\n[32] \u201cMongoriver,\u201d Stripe, Inc., github.com, September 2014.\n[33] Dan Harvey: \u201cChange Data Capture with Mongo + Kafka,\u201d at Hadoop Users\nGroup UK, August 2015.\n[34] \u201cOracle GoldenGate 12c: Real-Time Access to Real-Time Information,\u201d Oracle\nWhite Paper, March 2015.\n[35] \u201cOracle GoldenGate Fundamentals: How Oracle GoldenGate Works,\u201d Oracle\nCorporation, youtube.com, November 2012.\n[36] Slava Akhmechet: \u201cAdvancing the Realtime Web,\u201d rethinkdb.com, January 27,\n2015.\n[37] \u201cFirebase Realtime Database Documentation,\u201d Google, Inc., firebase.google.com,\nMay 2016.\n[38] \u201cApache CouchDB 1.6 Documentation,\u201d docs.couchdb.org, 2014.\n[39] Matt DeBergalis: \u201cMeteor 0.7.0: Scalable Database Queries Using MongoDB\nOplog Instead of Poll-and-Diff,\u201d info.meteor.com, December 17, 2013.\n[40] \u201cChapter 15. Importing and Exporting Live Data,\u201d VoltDB 6.4 User Manual,\ndocs.voltdb.com, June 2016.\n[41] Neha Narkhede: \u201cAnnouncing Kafka Connect: Building Large-Scale Low-\nLatency Data Pipelines,\u201d confluent.io, February 18, 2016."}
{"506": "[44] Vaughn Vernon: Implementing Domain-Driven Design. Addison-Wesley Profes\u2010\nsional, 2013. ISBN: 978-0-321-83457-7\n[45] H. V. Jagadish, Inderpal Singh Mumick, and Abraham Silberschatz: \u201cView\nMaintenance Issues for the Chronicle Data Model,\u201d at 14th ACM SIGACT-SIGMOD-\nSIGART Symposium on Principles of Database Systems (PODS), May 1995. doi:\n10.1145/212433.220201\n[46] \u201cEvent Store 3.5.0 Documentation,\u201d Event Store LLP, docs.geteventstore.com,\nFebruary 2016.\n[47] Martin Kleppmann: Making Sense of Stream Processing. Report, O\u2019Reilly Media,\nMay 2016.\n[48] Sander Mak: \u201cEvent-Sourced Architectures with Akka,\u201d at JavaOne, September\n2014.\n[49] Julian Hyde: personal communication, June 2016.\n[50] Ashish Gupta and Inderpal Singh Mumick: Materialized Views: Techniques,\nImplementations, and Applications. MIT Press, 1999. ISBN: 978-0-262-57122-7\n[51] Timothy Griffin and Leonid Libkin: \u201cIncremental Maintenance of Views with\nDuplicates,\u201d at ACM International Conference on Management of Data (SIGMOD),\nMay 1995. doi:10.1145/223784.223849\n[52] Pat Helland: \u201cImmutability Changes Everything,\u201d at 7th Biennial Conference on\nInnovative Data Systems Research (CIDR), January 2015.\n[53] Martin Kleppmann: \u201cAccounting for Computer Scientists,\u201d martin.klepp\u2010\nmann.com, March 7, 2011.\n[54] Pat Helland: \u201cAccountants Don\u2019t Use Erasers,\u201d blogs.msdn.com, June 14, 2007.\n[55] Fangjin Yang: \u201cDogfooding with Druid, Samza, and Kafka: Metametrics at Met\u2010\namarkets,\u201d metamarkets.com, June 3, 2015.\n[56] Gavin Li, Jianqiu Lv, and Hang Qi: \u201cPistachio: Co-Locate the Data and Compute\nfor Fastest Cloud Compute,\u201d yahoohadoop.tumblr.com, April 13, 2015.\n[57] Kartik Paramasivam: \u201cStream Processing Hard Problems \u2013 Part 1: Killing\nLambda,\u201d engineering.linkedin.com, June 27, 2016.\n[58] Martin Fowler: \u201cCQRS,\u201d martinfowler.com, July 14, 2011.\n[59] Greg Young: \u201cCQRS Documents,\u201d cqrs.files.wordpress.com, November 2010."}
{"507": "[61] Daniel Eloff, Slava Akhmechet, Jay Kreps, et al.: \u201cRe: Turning the Database\nInside-out with Apache Samza,\u201d Hacker News discussion, news.ycombinator.com,\nMarch 4, 2015.\n[62] \u201cDatomic Development Resources: Excision,\u201d Cognitect, Inc., docs.datomic.com.\n[63] \u201cFossil Documentation: Deleting Content from Fossil,\u201d fossil-scm.org, 2016.\n[64] Jay Kreps: \u201cThe irony of distributed systems is that data loss is really easy but\ndeleting data is surprisingly hard,\u201d twitter.com, March 30, 2015.\n[65] David C. Luckham: \u201cWhat\u2019s the Difference Between ESP and CEP?,\u201d complexe\u2010\nvents.com, August 1, 2006.\n[66] Srinath Perera: \u201cHow Is Stream Processing and Complex Event Processing\n(CEP) Different?,\u201d quora.com, December 3, 2015.\n[67] Arvind Arasu, Shivnath Babu, and Jennifer Widom: \u201cThe CQL Continuous\nQuery Language: Semantic Foundations and Query Execution,\u201d The VLDB Journal,\nvolume 15, number 2, pages 121\u2013142, June 2006. doi:10.1007/s00778-004-0147-z\n[68] Julian Hyde: \u201cData in Flight: How Streaming SQL Technology Can Help Solve\nthe Web 2.0 Data Crunch,\u201d ACM Queue, volume 7, number 11, December 2009. doi:\n10.1145/1661785.1667562\n[69] \u201cEsper Reference, Version 5.4.0,\u201d EsperTech, Inc., espertech.com, April 2016.\n[70] Zubair Nabi, Eric Bouillet, Andrew Bainbridge, and Chris Thomas: \u201cOf Streams\nand Storms,\u201d IBM technical report, developer.ibm.com, April 2014.\n[71] Milinda Pathirage, Julian Hyde, Yi Pan, and Beth Plale: \u201cSamzaSQL: Scalable\nFast Data Management with Streaming SQL,\u201d at IEEE International Workshop on\nHigh-Performance Big Data Computing (HPBDC), May 2016. doi:10.1109/IPDPSW.\n2016.141\n[72] Philippe Flajolet, \u00c9ric Fusy, Olivier Gandouet, and Fr\u00e9d\u00e9ric Meunier: \u201cHyperLog\nLog: The Analysis of a Near-Optimal Cardinality Estimation Algorithm,\u201d at Confer\u2010\nence on Analysis of Algorithms (AofA), June 2007.\n[73] Jay Kreps: \u201cQuestioning the Lambda Architecture,\u201d oreilly.com, July 2, 2014.\n[74] Ian Hellstr\u00f6m: \u201cAn Overview of Apache Streaming Technologies,\u201d database\u2010\nline.wordpress.com, March 12, 2016.\n[75] Jay Kreps: \u201cWhy Local State Is a Fundamental Primitive in Stream Processing,\u201d\noreilly.com, July 31, 2014."}
{"508": "[78] \u201cApache Storm 1.0.1 Documentation,\u201d storm.apache.org, May 2016.\n[79] Tyler Akidau: \u201cThe World Beyond Batch: Streaming 102,\u201d oreilly.com, January\n20, 2016.\n[80] Stephan Ewen: \u201cStreaming Analytics with Apache Flink,\u201d at Kafka Summit, April\n2016.\n[81] Tyler Akidau, Alex Balikov, Kaya Bekiro\u011flu, et al.: \u201cMillWheel: Fault-Tolerant\nStream Processing at Internet Scale,\u201d at 39th International Conference on Very Large\nData Bases (VLDB), August 2013.\n[82] Alex Dean: \u201cImproving Snowplow\u2019s Understanding of Time,\u201d snowplowanalyt\u2010\nics.com, September 15, 2015.\n[83] \u201cWindowing (Azure Stream Analytics),\u201d Microsoft Azure Reference,\nmsdn.microsoft.com, April 2016.\n[84] \u201cState Management,\u201d Apache Samza 0.10 Documentation, samza.apache.org,\nDecember 2015.\n[85] Rajagopal Ananthanarayanan, Venkatesh Basker, Sumit Das, et al.: \u201cPhoton:\nFault-Tolerant and Scalable Joining of Continuous Data Streams,\u201d at ACM Interna\u2010\ntional Conference on Management of Data (SIGMOD), June 2013. doi:\n10.1145/2463676.2465272\n[86] Martin Kleppmann: \u201cSamza Newsfeed Demo,\u201d github.com, September 2014.\n[87] Ben Kirwin: \u201cDoing the Impossible: Exactly-Once Messaging Patterns in Kafka,\u201d\nben.kirw.in, November 28, 2014.\n[88] Pat Helland: \u201cData on the Outside Versus Data on the Inside,\u201d at 2nd Biennial\nConference on Innovative Data Systems Research (CIDR), January 2005.\n[89] Ralph Kimball and Margy Ross: The Data Warehouse Toolkit: The Definitive\nGuide to Dimensional Modeling, 3rd edition. John Wiley & Sons, 2013. ISBN:\n978-1-118-53080-1\n[90] Viktor Klang: \u201cI\u2019m coining the phrase \u2018effectively-once\u2019 for message processing\nwith at-least-once + idempotent operations,\u201d twitter.com, October 20, 2016.\n[91] Matei Zaharia, Tathagata Das, Haoyuan Li, et al.: \u201cDiscretized Streams: An Effi\u2010\ncient and Fault-Tolerant Model for Stream Processing on Large Clusters,\u201d at 4th\nUSENIX Conference in Hot Topics in Cloud Computing (HotCloud), June 2012.\n[92] Kostas Tzoumas, Stephan Ewen, and Robert Metzger: \u201cHigh-Throughput, Low-"}
{"509": "[93] Paris Carbone, Gyula F\u00f3ra, Stephan Ewen, et al.: \u201cLightweight Asynchronous\nSnapshots for Distributed Dataflows,\u201d arXiv:1506.08603 [cs.DC], June 29, 2015.\n[94] Ryan Betts and John Hugg: Fast Data: Smart and at Scale. Report, O\u2019Reilly\nMedia, October 2015.\n[95] Flavio Junqueira: \u201cMaking Sense of Exactly-Once Semantics,\u201d at Strata+Hadoop\nWorld London, June 2016.\n[96] Jason Gustafson, Flavio Junqueira, Apurva Mehta, Sriram Subramanian, and\nGuozhang Wang: \u201cKIP-98 \u2013 Exactly Once Delivery and Transactional Messaging,\u201d\ncwiki.apache.org, November 2016.\n[97] Pat Helland: \u201cIdempotence Is Not a Medical Condition,\u201d Communications of the\nACM, volume 55, number 5, page 56, May 2012. doi:10.1145/2160718.2160734\n[98] Jay Kreps: \u201cRe: Trying to Achieve Deterministic Behavior on Recovery/Rewind,\u201d\nemail to samza-dev mailing list, September 9, 2014.\n[99] E. N. (Mootaz) Elnozahy, Lorenzo Alvisi, Yi-Min Wang, and David B. Johnson:\n\u201cA Survey of Rollback-Recovery Protocols in Message-Passing Systems,\u201d ACM Com\u2010\nputing Surveys, volume 34, number 3, pages 375\u2013408, September 2002. doi:\n10.1145/568522.568525\n[100] Adam Warski: \u201cKafka Streams \u2013 How Does It Fit the Stream Processing Land\u2010\nscape?,\u201d softwaremill.com, June 1, 2016."}
{"510": ""}
{"511": "CHAPTER 12\nThe Future of Data Systems\nIf a thing be ordained to another as to its end, its last end cannot consist in the preservation\nof its being. Hence a captain does not intend as a last end, the preservation of the ship\nentrusted to him, since a ship is ordained to something else as its end, viz. to navigation.\n(Often quoted as: If the highest aim of a captain was the preserve his ship, he would keep it\nin port forever.)\n\u2014St. Thomas Aquinas, Summa Theologica (1265\u20131274)\nSo far, this book has been mostly about describing things as they are at present. In\nthis final chapter, we will shift our perspective toward the future and discuss how\nthings should be: I will propose some ideas and approaches that, I believe, may funda\u2010\nmentally improve the ways we design and build applications.\nOpinions and speculation about the future are of course subjective, and so I will use\nthe first person in this chapter when writing about my personal opinions. You are\nwelcome to disagree with them and form your own opinions, but I hope that the\nideas in this chapter will at least be a starting point for a productive discussion and\nbring some clarity to concepts that are often confused.\nThe goal of this book was outlined in Chapter 1: to explore how to create applications\nand systems that are reliable, scalable, and maintainable. These themes have run\nthrough all of the chapters: for example, we discussed many fault-tolerance algo\u2010\nrithms that help improve reliability, partitioning to improve scalability, and mecha\u2010\nnisms for evolution and abstraction that improve maintainability. In this chapter we\nwill bring all of these ideas together, and build on them to envisage the future. Our\ngoal is to discover how to design applications that are better than the ones of today\u2014\nrobust, correct, evolvable, and ultimately beneficial to humanity."}
{"512": "Data Integration\nA recurring theme in this book has been that for any given problem, there are several\nsolutions, all of which have different pros, cons, and trade-offs. For example, when\ndiscussing storage engines in Chapter 3, we saw log-structured storage, B-trees, and\ncolumn-oriented storage. When discussing replication in Chapter 5, we saw single-\nleader, multi-leader, and leaderless approaches.\nIf you have a problem such as \u201cI want to store some data and look it up again later,\u201d\nthere is no one right solution, but many different approaches that are each appropri\u2010\nate in different circumstances. A software implementation typically has to pick one\nparticular approach. It\u2019s hard enough to get one code path robust and performing\nwell\u2014trying to do everything in one piece of software almost guarantees that the\nimplementation will be poor.\nThus, the most appropriate choice of software tool also depends on the circumstan\u2010\nces. Every piece of software, even a so-called \u201cgeneral-purpose\u201d database, is designed\nfor a particular usage pattern.\nFaced with this profusion of alternatives, the first challenge is then to figure out the\nmapping between the software products and the circumstances in which they are a\ngood fit. Vendors are understandably reluctant to tell you about the kinds of work\u2010\nloads for which their software is poorly suited, but hopefully the previous chapters\nhave equipped you with some questions to ask in order to read between the lines and\nbetter understand the trade-offs.\nHowever, even if you perfectly understand the mapping between tools and circum\u2010\nstances for their use, there is another challenge: in complex applications, data is often\nused in several different ways. There is unlikely to be one piece of software that is\nsuitable for all the different circumstances in which the data is used, so you inevitably\nend up having to cobble together several different pieces of software in order to pro\u2010\nvide your application\u2019s functionality.\nCombining Specialized Tools by Deriving Data\nFor example, it is common to need to integrate an OLTP database with a full-text\nsearch index in order to handle queries for arbitrary keywords. Although some data\u2010\nbases (such as PostgreSQL) include a full-text indexing feature, which can be suffi\u2010\ncient for simple applications [1], more sophisticated search facilities require specialist\ninformation retrieval tools. Conversely, search indexes are generally not very suitable\nas a durable system of record, and so many applications need to combine two differ\u2010"}
{"513": "gration problem becomes harder. Besides the database and the search index, perhaps\nyou need to keep copies of the data in analytics systems (data warehouses, or batch\nand stream processing systems); maintain caches or denormalized versions of objects\nthat were derived from the original data; pass the data through machine learning,\nclassification, ranking, or recommendation systems; or send notifications based on\nchanges to the data.\nSurprisingly often I see software engineers make statements like, \u201cIn my experience,\n99% of people only need X\u201d or \u201c\u2026don\u2019t need X\u201d (for various values of X). I think that\nsuch statements say more about the experience of the speaker than about the actual\nusefulness of a technology. The range of different things you might want to do with\ndata is dizzyingly wide. What one person considers to be an obscure and pointless\nfeature may well be a central requirement for someone else. The need for data inte\u2010\ngration often only becomes apparent if you zoom out and consider the dataflows\nacross an entire organization.\nReasoning about dataflows\nWhen copies of the same data need to be maintained in several storage systems in\norder to satisfy different access patterns, you need to be very clear about the inputs\nand outputs: where is data written first, and which representations are derived from\nwhich sources? How do you get data into all the right places, in the right formats?\nFor example, you might arrange for data to first be written to a system of record data\u2010\nbase, capturing the changes made to that database (see \u201cChange Data Capture\u201d on\npage 454) and then applying the changes to the search index in the same order. If\nchange data capture (CDC) is the only way of updating the index, you can be confi\u2010\ndent that the index is entirely derived from the system of record, and therefore con\u2010\nsistent with it (barring bugs in the software). Writing to the database is the only way\nof supplying new input into this system.\nAllowing the application to directly write to both the search index and the database\nintroduces the problem shown in Figure 11-4, in which two clients concurrently send\nconflicting writes, and the two storage systems process them in a different order. In\nthis case, neither the database nor the search index is \u201cin charge\u201d of determining the\norder of writes, and so they may make contradictory decisions and become perma\u2010\nnently inconsistent with each other.\nIf it is possible for you to funnel all user input through a single system that decides on\nan ordering for all writes, it becomes much easier to derive other representations of\nthe data by processing the writes in the same order. This is an application of the state\nmachine replication approach that we saw in \u201cTotal Order Broadcast\u201d on page 348."}
{"514": "Updating a derived data system based on an event log can often be made determinis\u2010\ntic and idempotent (see \u201cIdempotence\u201d on page 478), making it quite easy to recover\nfrom faults.\nDerived data versus distributed transactions\nThe classic approach for keeping different data systems consistent with each other\ninvolves distributed transactions, as discussed in \u201cAtomic Commit and Two-Phase\nCommit (2PC)\u201d on page 354. How does the approach of using derived data systems\nfare in comparison to distributed transactions?\nAt an abstract level, they achieve a similar goal by different means. Distributed trans\u2010\nactions decide on an ordering of writes by using locks for mutual exclusion (see\n\u201cTwo-Phase Locking (2PL)\u201d on page 257), while CDC and event sourcing use a log\nfor ordering. Distributed transactions use atomic commit to ensure that changes take\neffect exactly once, while log-based systems are often based on deterministic retry\nand idempotence.\nThe biggest difference is that transaction systems usually provide linearizability (see\n\u201cLinearizability\u201d on page 324), which implies useful guarantees such as reading your\nown writes (see \u201cReading Your Own Writes\u201d on page 162). On the other hand,\nderived data systems are often updated asynchronously, and so they do not by default\noffer the same timing guarantees.\nWithin limited environments that are willing to pay the cost of distributed transac\u2010\ntions, they have been used successfully. However, I think that XA has poor fault toler\u2010\nance and performance characteristics (see \u201cDistributed Transactions in Practice\u201d on\npage 360), which severely limit its usefulness. I believe that it might be possible to\ncreate a better protocol for distributed transactions, but getting such a protocol\nwidely adopted and integrated with existing tools would be challenging, and unlikely\nto happen soon.\nIn the absence of widespread support for a good distributed transaction protocol, I\nbelieve that log-based derived data is the most promising approach for integrating\ndifferent data systems. However, guarantees such as reading your own writes are use\u2010\nful, and I don\u2019t think that it is productive to tell everyone \u201ceventual consistency is\ninevitable\u2014suck it up and learn to deal with it\u201d (at least not without good guidance\non how to deal with it).\nIn \u201cAiming for Correctness\u201d on page 515 we will discuss some approaches for imple\u2010\nmenting stronger guarantees on top of asynchronously derived systems, and work\ntoward a middle ground between distributed transactions and asynchronous log-"}
{"515": "The limits of total ordering\nWith systems that are small enough, constructing a totally ordered event log is\nentirely feasible (as demonstrated by the popularity of databases with single-leader\nreplication, which construct precisely such a log). However, as systems are scaled\ntoward bigger and more complex workloads, limitations begin to emerge:\n\u2022 In most cases, constructing a totally ordered log requires all events to pass\nthrough a single leader node that decides on the ordering. If the throughput of\nevents is greater than a single machine can handle, you need to partition it across\nmultiple machines (see \u201cPartitioned Logs\u201d on page 446). The order of events in\ntwo different partitions is then ambiguous.\n\u2022 If the servers are spread across multiple geographically distributed datacenters,\nfor example in order to tolerate an entire datacenter going offline, you typically\nhave a separate leader in each datacenter, because network delays make synchro\u2010\nnous cross-datacenter coordination inefficient (see \u201cMulti-Leader Replication\u201d\non page 168). This implies an undefined ordering of events that originate in two\ndifferent datacenters.\n\u2022 When applications are deployed as microservices (see \u201cDataflow Through Serv\u2010\nices: REST and RPC\u201d on page 131), a common design choice is to deploy each\nservice and its durable state as an independent unit, with no durable state shared\nbetween services. When two events originate in different services, there is no\ndefined order for those events.\n\u2022 Some applications maintain client-side state that is updated immediately on user\ninput (without waiting for confirmation from a server), and even continue to\nwork offline (see \u201cClients with offline operation\u201d on page 170). With such appli\u2010\ncations, clients and servers are very likely to see events in different orders.\nIn formal terms, deciding on a total order of events is known as total order broadcast,\nwhich is equivalent to consensus (see \u201cConsensus algorithms and total order broad\u2010\ncast\u201d on page 366). Most consensus algorithms are designed for situations in which\nthe throughput of a single node is sufficient to process the entire stream of events,\nand these algorithms do not provide a mechanism for multiple nodes to share the\nwork of ordering the events. It is still an open research problem to design consensus\nalgorithms that can scale beyond the throughput of a single node and that work well\nin a geographically distributed setting.\nOrdering events to capture causality\nIn cases where there is no causal link between events, the lack of a total order is not a"}
{"516": "partition. However, causal dependencies sometimes arise in more subtle ways (see\nalso \u201cOrdering and Causality\u201d on page 339).\nFor example, consider a social networking service, and two users who were in a rela\u2010\ntionship but have just broken up. One of the users removes the other as a friend, and\nthen sends a message to their remaining friends complaining about their ex-partner.\nThe user\u2019s intention is that their ex-partner should not see the rude message, since\nthe message was sent after the friend status was revoked.\nHowever, in a system that stores friendship status in one place and messages in\nanother place, that ordering dependency between the unfriend event and the message-\nsend event may be lost. If the causal dependency is not captured, a service that sends\nnotifications about new messages may process the message-send event before the\nunfriend event, and thus incorrectly send a notification to the ex-partner.\nIn this example, the notifications are effectively a join between the messages and the\nfriend list, making it related to the timing issues of joins that we discussed previously\n(see \u201cTime-dependence of joins\u201d on page 475). Unfortunately, there does not seem to\nbe a simple answer to this problem [2, 3]. Starting points include:\n\u2022 Logical timestamps can provide total ordering without coordination (see\n\u201cSequence Number Ordering\u201d on page 343), so they may help in cases where\ntotal order broadcast is not feasible. However, they still require recipients to han\u2010\ndle events that are delivered out of order, and they require additional metadata to\nbe passed around.\n\u2022 If you can log an event to record the state of the system that the user saw before\nmaking a decision, and give that event a unique identifier, then any later events\ncan reference that event identifier in order to record the causal dependency [4].\nWe will return to this idea in \u201cReads are events too\u201d on page 513.\n\u2022 Conflict resolution algorithms (see \u201cAutomatic Conflict Resolution\u201d on page\n174) help with processing events that are delivered in an unexpected order. They\nare useful for maintaining state, but they do not help if actions have external side\neffects (such as sending a notification to a user).\nPerhaps, over time, patterns for application development will emerge that allow\ncausal dependencies to be captured efficiently, and derived state to be maintained\ncorrectly, without forcing all events to go through the bottleneck of total order\nbroadcast.\nBatch and Stream Processing"}
{"517": "the appropriate outputs. Batch and stream processors are the tools for achieving this\ngoal.\nThe outputs of batch and stream processes are derived datasets such as search\nindexes, materialized views, recommendations to show to users, aggregate metrics,\nand so on (see \u201cThe Output of Batch Workflows\u201d on page 411 and \u201cUses of Stream\nProcessing\u201d on page 465).\nAs we saw in Chapter 10 and Chapter 11, batch and stream processing have a lot of\nprinciples in common, and the main fundamental difference is that stream process\u2010\nors operate on unbounded datasets whereas batch process inputs are of a known,\nfinite size. There are also many detailed differences in the ways the processing\nengines are implemented, but these distinctions are beginning to blur.\nSpark performs stream processing on top of a batch processing engine by breaking\nthe stream into microbatches, whereas Apache Flink performs batch processing on\ntop of a stream processing engine [5]. In principle, one type of processing can be\nemulated on top of the other, although the performance characteristics vary: for\nexample, microbatching may perform poorly on hopping or sliding windows [6].\nMaintaining derived state\nBatch processing has a quite strong functional flavor (even if the code is not written\nin a functional programming language): it encourages deterministic, pure functions\nwhose output depends only on the input and which have no side effects other than\nthe explicit outputs, treating inputs as immutable and outputs as append-only.\nStream processing is similar, but it extends operators to allow managed, fault-tolerant\nstate (see \u201cRebuilding state after a failure\u201d on page 478).\nThe principle of deterministic functions with well-defined inputs and outputs is not\nonly good for fault tolerance (see \u201cIdempotence\u201d on page 478), but also simplifies\nreasoning about the dataflows in an organization [7]. No matter whether the derived\ndata is a search index, a statistical model, or a cache, it is helpful to think in terms of\ndata pipelines that derive one thing from another, pushing state changes in one sys\u2010\ntem through functional application code and applying the effects to derived systems.\nIn principle, derived data systems could be maintained synchronously, just like a\nrelational database updates secondary indexes synchronously within the same trans\u2010\naction as writes to the table being indexed. However, asynchrony is what makes sys\u2010\ntems based on event logs robust: it allows a fault in one part of the system to be\ncontained locally, whereas distributed transactions abort if any one participant fails,\nso they tend to amplify failures by spreading them to the rest of the system (see \u201cLim\u2010"}
{"518": "needs to send writes to multiple partitions (if the index is term-partitioned) or send\nreads to all partitions (if the index is document-partitioned). Such cross-partition\ncommunication is also most reliable and scalable if the index is maintained asynchro\u2010\nnously [8] (see also \u201cMulti-partition data processing\u201d on page 514).\nReprocessing data for application evolution\nWhen maintaining derived data, batch and stream processing are both useful. Stream\nprocessing allows changes in the input to be reflected in derived views with low delay,\nwhereas batch processing allows large amounts of accumulated historical data to be\nreprocessed in order to derive new views onto an existing dataset.\nIn particular, reprocessing existing data provides a good mechanism for maintaining\na system, evolving it to support new features and changed requirements (see Chap\u2010\nter 4). Without reprocessing, schema evolution is limited to simple changes like\nadding a new optional field to a record, or adding a new type of record. This is the\ncase both in a schema-on-write and in a schema-on-read context (see \u201cSchema flexi\u2010\nbility in the document model\u201d on page 39). On the other hand, with reprocessing it is\npossible to restructure a dataset into a completely different model in order to better\nserve new requirements.\nSchema Migrations on Railways\nLarge-scale \u201cschema migrations\u201d occur in noncomputer systems as well. For example,\nin the early days of railway building in 19th-century England there were various com\u2010\npeting standards for the gauge (the distance between the two rails). Trains built for\none gauge couldn\u2019t run on tracks of another gauge, which restricted the possible\ninterconnections in the train network [9].\nAfter a single standard gauge was finally decided upon in 1846, tracks with other\ngauges had to be converted\u2014but how do you do this without shutting down the train\nline for months or years? The solution is to first convert the track to dual gauge or\nmixed gauge by adding a third rail. This conversion can be done gradually, and when\nit is done, trains of both gauges can run on the line, using two of the three rails. Even\u2010\ntually, once all trains have been converted to the standard gauge, the rail providing\nthe nonstandard gauge can be removed.\n\u201cReprocessing\u201d the existing tracks in this way, and allowing the old and new versions\nto exist side by side, makes it possible to change the gauge gradually over the course\nof years. Nevertheless, it is an expensive undertaking, which is why nonstandard\ngauges still exist today. For example, the BART system in the San Francisco Bay Area\nuses a different gauge from the majority of the US."}
{"519": "Derived views allow gradual evolution. If you want to restructure a dataset, you do\nnot need to perform the migration as a sudden switch. Instead, you can maintain the\nold schema and the new schema side by side as two independently derived views onto\nthe same underlying data. You can then start shifting a small number of users to the\nnew view in order to test its performance and find any bugs, while most users con\u2010\ntinue to be routed to the old view. Gradually, you can increase the proportion of\nusers accessing the new view, and eventually you can drop the old view [10].\nThe beauty of such a gradual migration is that every stage of the process is easily\nreversible if something goes wrong: you always have a working system to go back to.\nBy reducing the risk of irreversible damage, you can be more confident about going\nahead, and thus move faster to improve your system [11].\nThe lambda architecture\nIf batch processing is used to reprocess historical data, and stream processing is used\nto process recent updates, then how do you combine the two? The lambda architec\u2010\nture [12] is a proposal in this area that has gained a lot of attention.\nThe core idea of the lambda architecture is that incoming data should be recorded by\nappending immutable events to an always-growing dataset, similarly to event sourc\u2010\ning (see \u201cEvent Sourcing\u201d on page 457). From these events, read-optimized views are\nderived. The lambda architecture proposes running two different systems in parallel:\na batch processing system such as Hadoop MapReduce, and a separate stream-\nprocessing system such as Storm.\nIn the lambda approach, the stream processor consumes the events and quickly pro\u2010\nduces an approximate update to the view; the batch processor later consumes the\nsame set of events and produces a corrected version of the derived view. The reason\u2010\ning behind this design is that batch processing is simpler and thus less prone to bugs,\nwhile stream processors are thought to be less reliable and harder to make fault-\ntolerant (see \u201cFault Tolerance\u201d on page 476). Moreover, the stream process can use\nfast approximate algorithms while the batch process uses slower exact algorithms.\nThe lambda architecture was an influential idea that shaped the design of data sys\u2010\ntems for the better, particularly by popularizing the principle of deriving views onto\nstreams of immutable events and reprocessing events when needed. However, I also\nthink that it has a number of practical problems:\n\u2022 Having to maintain the same logic to run both in a batch and in a stream pro\u2010\ncessing framework is significant additional effort. Although libraries such as\nSummingbird [13] provide an abstraction for computations that can be run in"}
{"520": "\u2022 Since the stream pipeline and the batch pipeline produce separate outputs, they\nneed to be merged in order to respond to user requests. This merge is fairly easy\nif the computation is a simple aggregation over a tumbling window, but it\nbecomes significantly harder if the view is derived using more complex opera\u2010\ntions such as joins and sessionization, or if the output is not a time series.\n\u2022 Although it is great to have the ability to reprocess the entire historical dataset,\ndoing so frequently is expensive on large datasets. Thus, the batch pipeline often\nneeds to be set up to process incremental batches (e.g., an hour\u2019s worth of data at\nthe end of every hour) rather than reprocessing everything. This raises the prob\u2010\nlems discussed in \u201cReasoning About Time\u201d on page 468, such as handling strag\u2010\nglers and handling windows that cross boundaries between batches.\nIncrementalizing a batch computation adds complexity, making it more akin to\nthe streaming layer, which runs counter to the goal of keeping the batch layer as\nsimple as possible.\nUnifying batch and stream processing\nMore recent work has enabled the benefits of the lambda architecture to be enjoyed\nwithout its downsides, by allowing both batch computations (reprocessing historical\ndata) and stream computations (processing events as they arrive) to be implemented\nin the same system [15].\nUnifying batch and stream processing in one system requires the following features,\nwhich are becoming increasingly widely available:\n\u2022 The ability to replay historical events through the same processing engine that\nhandles the stream of recent events. For example, log-based message brokers\nhave the ability to replay messages (see \u201cReplaying old messages\u201d on page 451),\nand some stream processors can read input from a distributed filesystem like\nHDFS.\n\u2022 Exactly-once semantics for stream processors\u2014that is, ensuring that the output\nis the same as if no faults had occurred, even if faults did in fact occur (see \u201cFault\nTolerance\u201d on page 476). Like with batch processing, this requires discarding the\npartial output of any failed tasks.\n\u2022 Tools for windowing by event time, not by processing time, since processing\ntime is meaningless when reprocessing historical events (see \u201cReasoning About\nTime\u201d on page 468). For example, Apache Beam provides an API for expressing\nsuch computations, which can then be run using Apache Flink or Google Cloud\nDataflow."}
{"521": "Unbundling Databases\nAt a most abstract level, databases, Hadoop, and operating systems all perform the\nsame functions: they store some data, and they allow you to process and query that\ndata [16]. A database stores data in records of some data model (rows in tables, docu\u2010\nments, vertices in a graph, etc.) while an operating system\u2019s filesystem stores data in\nfiles\u2014but at their core, both are \u201cinformation management\u201d systems [17]. As we saw\nin Chapter 10, the Hadoop ecosystem is somewhat like a distributed version of Unix.\nOf course, there are many practical differences. For example, many filesystems do not\ncope very well with a directory containing 10 million small files, whereas a database\ncontaining 10 million small records is completely normal and unremarkable. Never\u2010\ntheless, the similarities and differences between operating systems and databases are\nworth exploring.\nUnix and relational databases have approached the information management prob\u2010\nlem with very different philosophies. Unix viewed its purpose as presenting program\u2010\nmers with a logical but fairly low-level hardware abstraction, whereas relational\ndatabases wanted to give application programmers a high-level abstraction that\nwould hide the complexities of data structures on disk, concurrency, crash recovery,\nand so on. Unix developed pipes and files that are just sequences of bytes, whereas\ndatabases developed SQL and transactions.\nWhich approach is better? Of course, it depends what you want. Unix is \u201csimpler\u201d in\nthe sense that it is a fairly thin wrapper around hardware resources; relational data\u2010\nbases are \u201csimpler\u201d in the sense that a short declarative query can draw on a lot of\npowerful infrastructure (query optimization, indexes, join methods, concurrency\ncontrol, replication, etc.) without the author of the query needing to understand the\nimplementation details.\nThe tension between these philosophies has lasted for decades (both Unix and the\nrelational model emerged in the early 1970s) and still isn\u2019t resolved. For example, I\nwould interpret the NoSQL movement as wanting to apply a Unix-esque approach of\nlow-level abstractions to the domain of distributed OLTP data storage.\nIn this section I will attempt to reconcile the two philosophies, in the hope that we\ncan combine the best of both worlds.\nComposing Data Storage Technologies\nOver the course of this book we have discussed various features provided by data\u2010\nbases and how they work, including:"}
{"522": "\u2022 Materialized views, which are a kind of precomputed cache of query results (see\n\u201cAggregation: Data Cubes and Materialized Views\u201d on page 101)\n\u2022 Replication logs, which keep copies of the data on other nodes up to date (see\n\u201cImplementation of Replication Logs\u201d on page 158)\n\u2022 Full-text search indexes, which allow keyword search in text (see \u201cFull-text\nsearch and fuzzy indexes\u201d on page 88) and which are built into some relational\ndatabases [1]\nIn Chapters 10 and 11, similar themes emerged. We talked about building full-text\nsearch indexes (see \u201cThe Output of Batch Workflows\u201d on page 411), about material\u2010\nized view maintenance (see \u201cMaintaining materialized views\u201d on page 467), and\nabout replicating changes from a database to derived data systems (see \u201cChange Data\nCapture\u201d on page 454).\nIt seems that there are parallels between the features that are built into databases and\nthe derived data systems that people are building with batch and stream processors.\nCreating an index\nThink about what happens when you run CREATE INDEX to create a new index in a\nrelational database. The database has to scan over a consistent snapshot of a table,\npick out all of the field values being indexed, sort them, and write out the index. Then\nit must process the backlog of writes that have been made since the consistent snap\u2010\nshot was taken (assuming the table was not locked while creating the index, so writes\ncould continue). Once that is done, the database must continue to keep the index up\nto date whenever a transaction writes to the table.\nThis process is remarkably similar to setting up a new follower replica (see \u201cSetting\nUp New Followers\u201d on page 155), and also very similar to bootstrapping change data\ncapture in a streaming system (see \u201cInitial snapshot\u201d on page 455).\nWhenever you run CREATE INDEX, the database essentially reprocesses the existing\ndataset (as discussed in \u201cReprocessing data for application evolution\u201d on page 496)\nand derives the index as a new view onto the existing data. The existing data may be a\nsnapshot of the state rather than a log of all changes that ever happened, but the two\nare closely related (see \u201cState, Streams, and Immutability\u201d on page 459).\nThe meta-database of everything\nIn this light, I think that the dataflow across an entire organization starts looking like\none huge database [7]. Whenever a batch, stream, or ETL process transports data"}
{"523": "Viewed like this, batch and stream processors are like elaborate implementations of\ntriggers, stored procedures, and materialized view maintenance routines. The derived\ndata systems they maintain are like different index types. For example, a relational\ndatabase may support B-tree indexes, hash indexes, spatial indexes (see \u201cMulti-\ncolumn indexes\u201d on page 87), and other types of indexes. In the emerging architec\u2010\nture of derived data systems, instead of implementing those facilities as features of a\nsingle integrated database product, they are provided by various different pieces of\nsoftware, running on different machines, administered by different teams.\nWhere will these developments take us in the future? If we start from the premise\nthat there is no single data model or storage format that is suitable for all access pat\u2010\nterns, I speculate that there are two avenues by which different storage and process\u2010\ning tools can nevertheless be composed into a cohesive system:\nFederated databases: unifying reads\nIt is possible to provide a unified query interface to a wide variety of underlying\nstorage engines and processing methods\u2014an approach known as a federated\ndatabase or polystore [18, 19]. For example, PostgreSQL\u2019s foreign data wrapper\nfeature fits this pattern [20]. Applications that need a specialized data model or\nquery interface can still access the underlying storage engines directly, while\nusers who want to combine data from disparate places can do so easily through\nthe federated interface.\nA federated query interface follows the relational tradition of a single integrated\nsystem with a high-level query language and elegant semantics, but a compli\u2010\ncated implementation.\nUnbundled databases: unifying writes\nWhile federation addresses read-only querying across several different systems, it\ndoes not have a good answer to synchronizing writes across those systems. We\nsaid that within a single database, creating a consistent index is a built-in feature.\nWhen we compose several storage systems, we similarly need to ensure that all\ndata changes end up in all the right places, even in the face of faults. Making it\neasier to reliably plug together storage systems (e.g., through change data capture\nand event logs) is like unbundling a database\u2019s index-maintenance features in a\nway that can synchronize writes across disparate technologies [7, 21].\nThe unbundled approach follows the Unix tradition of small tools that do one\nthing well [22], that communicate through a uniform low-level API (pipes), and\nthat can be composed using a higher-level language (the shell) [16].\nMaking unbundling work"}
{"524": "querying requires mapping one data model into another, which takes some thought\nbut is ultimately quite a manageable problem. I think that keeping the writes to sev\u2010\neral storage systems in sync is the harder engineering problem, and so I will focus\non it.\nThe traditional approach to synchronizing writes requires distributed transactions\nacross heterogeneous storage systems [18], which I think is the wrong solution (see\n\u201cDerived data versus distributed transactions\u201d on page 492). Transactions within a\nsingle storage or stream processing system are feasible, but when data crosses the\nboundary between different technologies, I believe that an asynchronous event log\nwith idempotent writes is a much more robust and practical approach.\nFor example, distributed transactions are used within some stream processors to ach\u2010\nieve exactly-once semantics (see \u201cAtomic commit revisited\u201d on page 477), and this\ncan work quite well. However, when a transaction would need to involve systems\nwritten by different groups of people (e.g., when data is written from a stream pro\u2010\ncessor to a distributed key-value store or search index), the lack of a standardized\ntransaction protocol makes integration much harder. An ordered log of events with\nidempotent consumers (see \u201cIdempotence\u201d on page 478) is a much simpler abstrac\u2010\ntion, and thus much more feasible to implement across heterogeneous systems [7].\nThe big advantage of log-based integration is loose coupling between the various com\u2010\nponents, which manifests itself in two ways:\n1. At a system level, asynchronous event streams make the system as a whole more\nrobust to outages or performance degradation of individual components. If a\nconsumer runs slow or fails, the event log can buffer messages (see \u201cDisk space\nusage\u201d on page 450), allowing the producer and any other consumers to continue\nrunning unaffected. The faulty consumer can catch up when it is fixed, so it\ndoesn\u2019t miss any data, and the fault is contained. By contrast, the synchronous\ninteraction of distributed transactions tends to escalate local faults into large-\nscale failures (see \u201cLimitations of distributed transactions\u201d on page 363).\n2. At a human level, unbundling data systems allows different software components\nand services to be developed, improved, and maintained independently from\neach other by different teams. Specialization allows each team to focus on doing\none thing well, with well-defined interfaces to other teams\u2019 systems. Event logs\nprovide an interface that is powerful enough to capture fairly strong consistency\nproperties (due to durability and ordering of events), but also general enough to\nbe applicable to almost any kind of data."}
{"525": "required for maintaining state in stream processors, and in order to serve queries for\nthe output of batch and stream processors (see \u201cThe Output of Batch Workflows\u201d on\npage 411 and \u201cProcessing Streams\u201d on page 464). Specialized query engines will con\u2010\ntinue to be important for particular workloads: for example, query engines in MPP\ndata warehouses are optimized for exploratory analytic queries and handle this kind\nof workload very well (see \u201cComparing Hadoop to Distributed Databases\u201d on page\n414).\nThe complexity of running several different pieces of infrastructure can be a problem:\neach piece of software has a learning curve, configuration issues, and operational\nquirks, and so it is worth deploying as few moving parts as possible. A single integra\u2010\nted software product may also be able to achieve better and more predictable perfor\u2010\nmance on the kinds of workloads for which it is designed, compared to a system\nconsisting of several tools that you have composed with application code [23]. As I\nsaid in the Preface, building for scale that you don\u2019t need is wasted effort and may\nlock you into an inflexible design. In effect, it is a form of premature optimization.\nThe goal of unbundling is not to compete with individual databases on performance\nfor particular workloads; the goal is to allow you to combine several different data\u2010\nbases in order to achieve good performance for a much wider range of workloads\nthan is possible with a single piece of software. It\u2019s about breadth, not depth\u2014in the\nsame vein as the diversity of storage and processing models that we discussed in\n\u201cComparing Hadoop to Distributed Databases\u201d on page 414.\nThus, if there is a single technology that does everything you need, you\u2019re most likely\nbest off simply using that product rather than trying to reimplement it yourself from\nlower-level components. The advantages of unbundling and composition only come\ninto the picture when there is no single piece of software that satisfies all your\nrequirements.\nWhat\u2019s missing?\nThe tools for composing data systems are getting better, but I think one major part is\nmissing: we don\u2019t yet have the unbundled-database equivalent of the Unix shell (i.e., a\nhigh-level language for composing storage and processing systems in a simple and\ndeclarative way).\nFor example, I would love it if we could simply declare mysql | elasticsearch, by\nanalogy to Unix pipes [22], which would be the unbundled equivalent of CREATE\nINDEX: it would take all the documents in a MySQL database and index them in an\nElasticsearch cluster. It would then continually capture all the changes made to the\ndatabase and automatically apply them to the search index, without us having to"}
{"526": "Similarly, it would be great to be able to precompute and update caches more easily.\nRecall that a materialized view is essentially a precomputed cache, so you could imag\u2010\nine creating a cache by declaratively specifying materialized views for complex quer\u2010\nies, including recursive queries on graphs (see \u201cGraph-Like Data Models\u201d on page\n49) and application logic. There is interesting early-stage research in this area, such as\ndifferential dataflow [24, 25], and I hope that these ideas will find their way into pro\u2010\nduction systems.\nDesigning Applications Around Dataflow\nThe approach of unbundling databases by composing specialized storage and pro\u2010\ncessing systems with application code is also becoming known as the \u201cdatabase\ninside-out\u201d approach [26], after the title of a conference talk I gave in 2014 [27].\nHowever, calling it a \u201cnew architecture\u201d is too grandiose. I see it more as a design\npattern, a starting point for discussion, and we give it a name simply so that we can\nbetter talk about it.\nThese ideas are not mine; they are simply an amalgamation of other people\u2019s ideas\nfrom which I think we should learn. In particular, there is a lot of overlap with data\u2010\nflow languages such as Oz [28] and Juttle [29], functional reactive programming (FRP)\nlanguages such as Elm [30, 31], and logic programming languages such as Bloom [32].\nThe term unbundling in this context was proposed by Jay Kreps [7].\nEven spreadsheets have dataflow programming capabilities that are miles ahead of\nmost mainstream programming languages [33]. In a spreadsheet, you can put a for\u2010\nmula in one cell (for example, the sum of cells in another column), and whenever any\ninput to the formula changes, the result of the formula is automatically recalculated.\nThis is exactly what we want at a data system level: when a record in a database\nchanges, we want any index for that record to be automatically updated, and any\ncached views or aggregations that depend on the record to be automatically\nrefreshed. You should not have to worry about the technical details of how this\nrefresh happens, but be able to simply trust that it works correctly.\nThus, I think that most data systems still have something to learn from the features\nthat VisiCalc already had in 1979 [34]. The difference from spreadsheets is that\ntoday\u2019s data systems need to be fault-tolerant, scalable, and store data durably. They\nalso need to be able to integrate disparate technologies written by different groups of\npeople over time, and reuse existing libraries and services: it is unrealistic to expect all\nsoftware to be developed using one particular language, framework, or tool.\nIn this section I will expand on these ideas and explore some ways of building appli\u2010"}
{"527": "Application code as a derivation function\nWhen one dataset is derived from another, it goes through some kind of transforma\u2010\ntion function. For example:\n\u2022 A secondary index is a kind of derived dataset with a straightforward transforma\u2010\ntion function: for each row or document in the base table, it picks out the values\nin the columns or fields being indexed, and sorts by those values (assuming a B-\ntree or SSTable index, which are sorted by key, as discussed in Chapter 3).\n\u2022 A full-text search index is created by applying various natural language process\u2010\ning functions such as language detection, word segmentation, stemming or lem\u2010\nmatization, spelling correction, and synonym identification, followed by building\na data structure for efficient lookups (such as an inverted index).\n\u2022 In a machine learning system, we can consider the model as being derived from\nthe training data by applying various feature extraction and statistical analysis\nfunctions. When the model is applied to new input data, the output of the model\nis derived from the input and the model (and hence, indirectly, from the training\ndata).\n\u2022 A cache often contains an aggregation of data in the form in which it is going to\nbe displayed in a user interface (UI). Populating the cache thus requires knowl\u2010\nedge of what fields are referenced in the UI; changes in the UI may require\nupdating the definition of how the cache is populated and rebuilding the cache.\nThe derivation function for a secondary index is so commonly required that it is built\ninto many databases as a core feature, and you can invoke it by merely saying CREATE\nINDEX. For full-text indexing, basic linguistic features for common languages may be\nbuilt into a database, but the more sophisticated features often require domain-\nspecific tuning. In machine learning, feature engineering is notoriously application-\nspecific, and often has to incorporate detailed knowledge about the user interaction\nand deployment of an application [35].\nWhen the function that creates a derived dataset is not a standard cookie-cutter func\u2010\ntion like creating a secondary index, custom code is required to handle the\napplication-specific aspects. And this custom code is where many databases struggle.\nAlthough relational databases commonly support triggers, stored procedures, and\nuser-defined functions, which can be used to execute application code within the\ndatabase, they have been somewhat of an afterthought in database design (see\n\u201cTransmitting Event Streams\u201d on page 440).\nSeparation of application code and state"}
{"528": "poorly suited for this purpose. They do not fit well with the requirements of modern\napplication development, such as dependency and package management, version\ncontrol, rolling upgrades, evolvability, monitoring, metrics, calls to network services,\nand integration with external systems.\nOn the other hand, deployment and cluster management tools such as Mesos, YARN,\nDocker, Kubernetes, and others are designed specifically for the purpose of running\napplication code. By focusing on doing one thing well, they are able to do it much\nbetter than a database that provides execution of user-defined functions as one of its\nmany features.\nI think it makes sense to have some parts of a system that specialize in durable data\nstorage, and other parts that specialize in running application code. The two can\ninteract while still remaining independent.\nMost web applications today are deployed as stateless services, in which any user\nrequest can be routed to any application server, and the server forgets everything\nabout the request once it has sent the response. This style of deployment is conve\u2010\nnient, as servers can be added or removed at will, but the state has to go somewhere:\ntypically, a database. The trend has been to keep stateless application logic separate\nfrom state management (databases): not putting application logic in the database and\nnot putting persistent state in the application [36]. As people in the functional pro\u2010\ngramming community like to joke, \u201cWe believe in the separation of Church and\nstate\u201d [37].i\nIn this typical web application model, the database acts as a kind of mutable shared\nvariable that can be accessed synchronously over the network. The application can\nread and update the variable, and the database takes care of making it durable, pro\u2010\nviding some concurrency control and fault tolerance.\nHowever, in most programming languages you cannot subscribe to changes in a\nmutable variable\u2014you can only read it periodically. Unlike in a spreadsheet, readers\nof the variable don\u2019t get notified if the value of the variable changes. (You can imple\u2010\nment such notifications in your own code\u2014this is known as the observer pattern\u2014\nbut most languages do not have this pattern as a built-in feature.)\nDatabases have inherited this passive approach to mutable data: if you want to find\nout whether the content of the database has changed, often your only option is to poll\n(i.e., to repeat your query periodically). Subscribing to changes is only just beginning\nto emerge as a feature (see \u201cAPI support for change streams\u201d on page 456)."}
{"529": "Dataflow: Interplay between state changes and application code\nThinking about applications in terms of dataflow implies renegotiating the relation\u2010\nship between application code and state management. Instead of treating a database\nas a passive variable that is manipulated by the application, we think much more\nabout the interplay and collaboration between state, state changes, and code that pro\u2010\ncesses them. Application code responds to state changes in one place by triggering\nstate changes in another place.\nWe saw this line of thinking in \u201cDatabases and Streams\u201d on page 451, where we dis\u2010\ncussed treating the log of changes to a database as a stream of events that we can sub\u2010\nscribe to. Message-passing systems such as actors (see \u201cMessage-Passing Dataflow\u201d\non page 136) also have this concept of responding to events. Already in the 1980s, the\ntuple spaces model explored expressing distributed computations in terms of pro\u2010\ncesses that observe state changes and react to them [38, 39].\nAs discussed, similar things happen inside a database when a trigger fires due to a\ndata change, or when a secondary index is updated to reflect a change in the table\nbeing indexed. Unbundling the database means taking this idea and applying it to the\ncreation of derived datasets outside of the primary database: caches, full-text search\nindexes, machine learning, or analytics systems. We can use stream processing and\nmessaging systems for this purpose.\nThe important thing to keep in mind is that maintaining derived data is not the same\nas asynchronous job execution, for which messaging systems are traditionally\ndesigned (see \u201cLogs compared to traditional messaging\u201d on page 448):\n\u2022 When maintaining derived data, the order of state changes is often important (if\nseveral views are derived from an event log, they need to process the events in the\nsame order so that they remain consistent with each other). As discussed in\n\u201cAcknowledgments and redelivery\u201d on page 445, many message brokers do not\nhave this property when redelivering unacknowledged messages. Dual writes are\nalso ruled out (see \u201cKeeping Systems in Sync\u201d on page 452).\n\u2022 Fault tolerance is key for derived data: losing just a single message causes the\nderived dataset to go permanently out of sync with its data source. Both message\ndelivery and derived state updates must be reliable. For example, many actor sys\u2010\ntems by default maintain actor state and messages in memory, so they are lost if\nthe machine running the actor crashes.\nStable message ordering and fault-tolerant message processing are quite stringent\ndemands, but they are much less expensive and more operationally robust than dis\u2010"}
{"530": "This application code can do the arbitrary processing that built-in derivation func\u2010\ntions in databases generally don\u2019t provide. Like Unix tools chained by pipes, stream\noperators can be composed to build large systems around dataflow. Each operator\ntakes streams of state changes as input, and produces other streams of state changes\nas output.\nStream processors and services\nThe currently trendy style of application development involves breaking down func\u2010\ntionality into a set of services that communicate via synchronous network requests\nsuch as REST APIs (see \u201cDataflow Through Services: REST and RPC\u201d on page 131).\nThe advantage of such a service-oriented architecture over a single monolithic appli\u2010\ncation is primarily organizational scalability through loose coupling: different teams\ncan work on different services, which reduces coordination effort between teams (as\nlong as the services can be deployed and updated independently).\nComposing stream operators into dataflow systems has a lot of similar characteristics\nto the microservices approach [40]. However, the underlying communication mecha\u2010\nnism is very different: one-directional, asynchronous message streams rather than\nsynchronous request/response interactions.\nBesides the advantages listed in \u201cMessage-Passing Dataflow\u201d on page 136, such as\nbetter fault tolerance, dataflow systems can also achieve better performance. For\nexample, say a customer is purchasing an item that is priced in one currency but paid\nfor in another currency. In order to perform the currency conversion, you need to\nknow the current exchange rate. This operation could be implemented in two ways\n[40, 41]:\n1. In the microservices approach, the code that processes the purchase would prob\u2010\nably query an exchange-rate service or database in order to obtain the current\nrate for a particular currency.\n2. In the dataflow approach, the code that processes purchases would subscribe to a\nstream of exchange rate updates ahead of time, and record the current rate in a\nlocal database whenever it changes. When it comes to processing the purchase, it\nonly needs to query the local database.\nThe second approach has replaced a synchronous network request to another service\nwith a query to a local database (which may be on the same machine, even in the\nsame process).ii Not only is the dataflow approach faster, but it is also more robust to"}
{"531": "the failure of another service. The fastest and most reliable network request is no net\u2010\nwork request at all! Instead of RPC, we now have a stream join between purchase\nevents and exchange rate update events (see \u201cStream-table join (stream enrichment)\u201d\non page 473).\nThe join is time-dependent: if the purchase events are reprocessed at a later point in\ntime, the exchange rate will have changed. If you want to reconstruct the original out\u2010\nput, you will need to obtain the historical exchange rate at the original time of pur\u2010\nchase. No matter whether you query a service or subscribe to a stream of exchange\nrate updates, you will need to handle this time dependence (see \u201cTime-dependence of\njoins\u201d on page 475).\nSubscribing to a stream of changes, rather than querying the current state when\nneeded, brings us closer to a spreadsheet-like model of computation: when some\npiece of data changes, any derived data that depends on it can swiftly be updated.\nThere are still many open questions, for example around issues like time-dependent\njoins, but I believe that building applications around dataflow ideas is a very promis\u2010\ning direction to go in.\nObserving Derived State\nAt an abstract level, the dataflow systems discussed in the last section give you a pro\u2010\ncess for creating derived datasets (such as search indexes, materialized views, and\npredictive models) and keeping them up to date. Let\u2019s call that process the write path:\nwhenever some piece of information is written to the system, it may go through mul\u2010\ntiple stages of batch and stream processing, and eventually every derived dataset is\nupdated to incorporate the data that was written. Figure 12-1 shows an example of\nupdating a search index."}
{"532": "request you read from the derived dataset, perhaps perform some more processing\non the results, and construct the response to the user.\nTaken together, the write path and the read path encompass the whole journey of the\ndata, from the point where it is collected to the point where it is consumed (probably\nby another human). The write path is the portion of the journey that is precomputed\n\u2014i.e., that is done eagerly as soon as the data comes in, regardless of whether anyone\nhas asked to see it. The read path is the portion of the journey that only happens\nwhen someone asks for it. If you are familiar with functional programming lan\u2010\nguages, you might notice that the write path is similar to eager evaluation, and the\nread path is similar to lazy evaluation.\nThe derived dataset is the place where the write path and the read path meet, as illus\u2010\ntrated in Figure 12-1. It represents a trade-off between the amount of work that needs\nto be done at write time and the amount that needs to be done at read time.\nMaterialized views and caching\nA full-text search index is a good example: the write path updates the index, and the\nread path searches the index for keywords. Both reads and writes need to do some\nwork. Writes need to update the index entries for all terms that appear in the docu\u2010\nment. Reads need to search for each of the words in the query, and apply Boolean\nlogic to find documents that contain all of the words in the query (an AND operator),\nor any synonym of each of the words (an OR operator).\nIf you didn\u2019t have an index, a search query would have to scan over all documents\n(like grep), which would get very expensive if you had a large number of documents.\nNo index means less work on the write path (no index to update), but a lot more\nwork on the read path.\nOn the other hand, you could imagine precomputing the search results for all possi\u2010\nble queries. In that case, you would have less work to do on the read path: no Boolean\nlogic, just find the results for your query and return them. However, the write path\nwould be a lot more expensive: the set of possible search queries that could be asked\nis infinite, and thus precomputing all possible search results would require infinite\ntime and storage space. That wouldn\u2019t work so well.iii\nAnother option would be to precompute the search results for only a fixed set of the\nmost common queries, so that they can be served quickly without having to go to the\nindex. The uncommon queries can still be served from the index. This would gener\u2010\nally be called a cache of common queries, although we could also call it a materialized"}
{"533": "view, as it would need to be updated when new documents appear that should be\nincluded in the results of one of the common queries.\nFrom this example we can see that an index is not the only possible boundary\nbetween the write path and the read path. Caching of common search results is possi\u2010\nble, and grep-like scanning without the index is also possible on a small number of\ndocuments. Viewed like this, the role of caches, indexes, and materialized views is\nsimple: they shift the boundary between the read path and the write path. They allow\nus to do more work on the write path, by precomputing results, in order to save effort\non the read path.\nShifting the boundary between work done on the write path and the read path was in\nfact the topic of the Twitter example at the beginning of this book, in \u201cDescribing\nLoad\u201d on page 11. In that example, we also saw how the boundary between write path\nand read path might be drawn differently for celebrities compared to ordinary users.\nAfter 500 pages we have come full circle!\nStateful, offline-capable clients\nI find the idea of a boundary between write and read paths interesting because we can\ndiscuss shifting that boundary and explore what that shift means in practical terms.\nLet\u2019s look at the idea in a different context.\nThe huge popularity of web applications in the last two decades has led us to certain\nassumptions about application development that are easy to take for granted. In par\u2010\nticular, the client/server model\u2014in which clients are largely stateless and servers have\nthe authority over data\u2014is so common that we almost forget that anything else\nexists. However, technology keeps moving on, and I think it is important to question\nthe status quo from time to time.\nTraditionally, web browsers have been stateless clients that can only do useful things\nwhen you have an internet connection (just about the only thing you could do offline\nwas to scroll up and down in a page that you had previously loaded while online).\nHowever, recent \u201csingle-page\u201d JavaScript web apps have gained a lot of stateful capa\u2010\nbilities, including client-side user interface interaction and persistent local storage in\nthe web browser. Mobile apps can similarly store a lot of state on the device and don\u2019t\nrequire a round-trip to the server for most user interactions.\nThese changing capabilities have led to a renewed interest in offline-first applications\nthat do as much as possible using a local database on the same device, without requir\u2010\ning an internet connection, and sync with remote servers in the background when a\nnetwork connection is available [42]. Since mobile devices often have slow and unre\u2010"}
{"534": "When we move away from the assumption of stateless clients talking to a central\ndatabase and toward state that is maintained on end-user devices, a world of new\nopportunities opens up. In particular, we can think of the on-device state as a cache of\nstate on the server. The pixels on the screen are a materialized view onto model\nobjects in the client app; the model objects are a local replica of state in a remote\ndatacenter [27].\nPushing state changes to clients\nIn a typical web page, if you load the page in a web browser and the data subse\u2010\nquently changes on the server, the browser does not find out about the change until\nyou reload the page. The browser only reads the data at one point in time, assuming\nthat it is static\u2014it does not subscribe to updates from the server. Thus, the state on\nthe device is a stale cache that is not updated unless you explicitly poll for changes.\n(HTTP-based feed subscription protocols like RSS are really just a basic form of poll\u2010\ning.)\nMore recent protocols have moved beyond the basic request/response pattern of\nHTTP: server-sent events (the EventSource API) and WebSockets provide communi\u2010\ncation channels by which a web browser can keep an open TCP connection to a\nserver, and the server can actively push messages to the browser as long as it remains\nconnected. This provides an opportunity for the server to actively inform the end-\nuser client about any changes to the state it has stored locally, reducing the staleness\nof the client-side state.\nIn terms of our model of write path and read path, actively pushing state changes all\nthe way to client devices means extending the write path all the way to the end user.\nWhen a client is first initialized, it would still need to use a read path to get its initial\nstate, but thereafter it could rely on a stream of state changes sent by the server. The\nideas we discussed around stream processing and messaging are not restricted to run\u2010\nning only in a datacenter: we can take the ideas further, and extend them all the way\nto end-user devices [43].\nThe devices will be offline some of the time, and unable to receive any notifications of\nstate changes from the server during that time. But we already solved that problem: in\n\u201cConsumer offsets\u201d on page 449 we discussed how a consumer of a log-based mes\u2010\nsage broker can reconnect after failing or becoming disconnected, and ensure that it\ndoesn\u2019t miss any messages that arrived while it was disconnected. The same techni\u2010\nque works for individual users, where each device is a small subscriber to a small\nstream of events."}
{"535": "internal client-side state by subscribing to a stream of events representing user input\nor responses from a server, structured similarly to event sourcing (see \u201cEvent Sourc\u2010\ning\u201d on page 457).\nIt would be very natural to extend this programming model to also allow a server to\npush state-change events into this client-side event pipeline. Thus, state changes\ncould flow through an end-to-end write path: from the interaction on one device that\ntriggers a state change, via event logs and through several derived data systems and\nstream processors, all the way to the user interface of a person observing the state on\nanother device. These state changes could be propagated with fairly low delay\u2014say,\nunder one second end to end.\nSome applications, such as instant messaging and online games, already have such a\n\u201creal-time\u201d architecture (in the sense of interactions with low delay, not in the sense\nof \u201cResponse time guarantees\u201d on page 298). But why don\u2019t we build all applications\nthis way?\nThe challenge is that the assumption of stateless clients and request/response interac\u2010\ntions is very deeply ingrained in our databases, libraries, frameworks, and protocols.\nMany datastores support read and write operations where a request returns one\nresponse, but much fewer provide an ability to subscribe to changes\u2014i.e., a request\nthat returns a stream of responses over time (see \u201cAPI support for change streams\u201d\non page 456).\nIn order to extend the write path all the way to the end user, we would need to funda\u2010\nmentally rethink the way we build many of these systems: moving away from request/\nresponse interaction and toward publish/subscribe dataflow [27]. I think that the\nadvantages of more responsive user interfaces and better offline support would make\nit worth the effort. If you are designing data systems, I hope that you will keep in\nmind the option of subscribing to changes, not just querying the current state.\nReads are events too\nWe discussed that when a stream processor writes derived data to a store (database,\ncache, or index), and when user requests query that store, the store acts as the bound\u2010\nary between the write path and the read path. The store allows random-access read\nqueries to the data that would otherwise require scanning the whole event log.\nIn many cases, the data storage is separate from the streaming system. But recall that\nstream processors also need to maintain state to perform aggregations and joins (see\n\u201cStream Joins\u201d on page 472). This state is normally hidden inside the stream pro\u2010\ncessor, but some frameworks allow it to also be queried by outside clients [45], turn\u2010"}
{"536": "the nodes that store the data being queried. This is a reasonable design, but not the\nonly possible one. It is also possible to represent read requests as streams of events,\nand send both the read events and the write events through a stream processor; the\nprocessor responds to read events by emitting the result of the read to an output\nstream [46].\nWhen both the writes and the reads are represented as events, and routed to the same\nstream operator in order to be handled, we are in fact performing a stream-table join\nbetween the stream of read queries and the database. The read event needs to be sent\nto the database partition holding the data (see \u201cRequest Routing\u201d on page 214), just\nlike batch and stream processors need to copartition inputs on the same key when\njoining (see \u201cReduce-Side Joins and Grouping\u201d on page 403).\nThis correspondence between serving requests and performing joins is quite funda\u2010\nmental [47]. A one-off read request just passes the request through the join operator\nand then immediately forgets it; a subscribe request is a persistent join with past and\nfuture events on the other side of the join.\nRecording a log of read events potentially also has benefits with regard to tracking\ncausal dependencies and data provenance across a system: it would allow you to\nreconstruct what the user saw before they made a particular decision. For example, in\nan online shop, it is likely that the predicted shipping date and the inventory status\nshown to a customer affect whether they choose to buy an item [4]. To analyze this\nconnection, you need to record the result of the user\u2019s query of the shipping and\ninventory status.\nWriting read events to durable storage thus enables better tracking of causal depen\u2010\ndencies (see \u201cOrdering events to capture causality\u201d on page 493), but it incurs addi\u2010\ntional storage and I/O cost. Optimizing such systems to reduce the overhead is still\nan open research problem [2]. But if you already log read requests for operational\npurposes, as a side effect of request processing, it is not such a great change to make\nthe log the source of the requests instead.\nMulti-partition data processing\nFor queries that only touch a single partition, the effort of sending queries through a\nstream and collecting a stream of responses is perhaps overkill. However, this idea\nopens the possibility of distributed execution of complex queries that need to com\u2010\nbine data from several partitions, taking advantage of the infrastructure for message\nrouting, partitioning, and joining that is already provided by stream processors.\nStorm\u2019s distributed RPC feature supports this usage pattern (see \u201cMessage passing"}
{"537": "Another example of this pattern occurs in fraud prevention: in order to assess the risk\nof whether a particular purchase event is fraudulent, you can examine the reputation\nscores of the user\u2019s IP address, email address, billing address, shipping address, and\nso on. Each of these reputation databases is itself partitioned, and so collecting the\nscores for a particular purchase event requires a sequence of joins with differently\npartitioned datasets [49].\nThe internal query execution graphs of MPP databases have similar characteristics\n(see \u201cComparing Hadoop to Distributed Databases\u201d on page 414). If you need to per\u2010\nform this kind of multi-partition join, it is probably simpler to use a database that\nprovides this feature than to implement it using a stream processor. However, treat\u2010\ning queries as streams provides an option for implementing large-scale applications\nthat run against the limits of conventional off-the-shelf solutions.\nAiming for Correctness\nWith stateless services that only read data, it is not a big deal if something goes\nwrong: you can fix the bug and restart the service, and everything returns to normal.\nStateful systems such as databases are not so simple: they are designed to remember\nthings forever (more or less), so if something goes wrong, the effects also potentially\nlast forever\u2014which means they require more careful thought [50].\nWe want to build applications that are reliable and correct (i.e., programs whose\nsemantics are well defined and understood, even in the face of various faults). For\napproximately four decades, the transaction properties of atomicity, isolation, and\ndurability (Chapter 7) have been the tools of choice for building correct applications.\nHowever, those foundations are weaker than they seem: witness for example the con\u2010\nfusion of weak isolation levels (see \u201cWeak Isolation Levels\u201d on page 233).\nIn some areas, transactions are being abandoned entirely and replaced with models\nthat offer better performance and scalability, but much messier semantics (see for\nexample \u201cLeaderless Replication\u201d on page 177). Consistency is often talked about, but\npoorly defined (see \u201cConsistency\u201d on page 224 and Chapter 9). Some people assert\nthat we should \u201cembrace weak consistency\u201d for the sake of better availability, while\nlacking a clear idea of what that actually means in practice.\nFor a topic that is so important, our understanding and our engineering methods are\nsurprisingly flaky. For example, it is very difficult to determine whether it is safe to\nrun a particular application at a particular transaction isolation level or replication\nconfiguration [51, 52]. Often simple solutions appear to work correctly when concur\u2010\nrency is low and there are no faults, but turn out to have many subtle bugs in more"}
{"538": "behavior in the presence of network problems and crashes. Even if infrastructure\nproducts like databases were free from problems, application code would still need to\ncorrectly use the features they provide, which is error-prone if the configuration is\nhard to understand (which is the case with weak isolation levels, quorum configura\u2010\ntions, and so on).\nIf your application can tolerate occasionally corrupting or losing data in unpredicta\u2010\nble ways, life is a lot simpler, and you might be able to get away with simply crossing\nyour fingers and hoping for the best. On the other hand, if you need stronger assur\u2010\nances of correctness, then serializability and atomic commit are established\napproaches, but they come at a cost: they typically only work in a single datacenter\n(ruling out geographically distributed architectures), and they limit the scale and\nfault-tolerance properties you can achieve.\nWhile the traditional transaction approach is not going away, I also believe it is not\nthe last word in making applications correct and resilient to faults. In this section I\nwill suggest some ways of thinking about correctness in the context of dataflow archi\u2010\ntectures.\nThe End-to-End Argument for Databases\nJust because an application uses a data system that provides comparatively strong\nsafety properties, such as serializable transactions, that does not mean the application\nis guaranteed to be free from data loss or corruption. For example, if an application\nhas a bug that causes it to write incorrect data, or delete data from a database, serial\u2010\nizable transactions aren\u2019t going to save you.\nThis example may seem frivolous, but it is worth taking seriously: application bugs\noccur, and people make mistakes. I used this example in \u201cState, Streams, and Immut\u2010\nability\u201d on page 459 to argue in favor of immutable and append-only data, because it\nis easier to recover from such mistakes if you remove the ability of faulty code to\ndestroy good data.\nAlthough immutability is useful, it is not a cure-all by itself. Let\u2019s look at a more sub\u2010\ntle example of data corruption that can occur.\nExactly-once execution of an operation\nIn \u201cFault Tolerance\u201d on page 476 we encountered an idea called exactly-once (or\neffectively-once) semantics. If something goes wrong while processing a message, you\ncan either give up (drop the message\u2014i.e., incur data loss) or try again. If you try\nagain, there is the risk that it actually succeeded the first time, but you just didn\u2019t find"}
{"539": "(overstating some metric). In this context, exactly-once means arranging the compu\u2010\ntation such that the final effect is the same as if no faults had occurred, even if the\noperation actually was retried due to some fault. We previously discussed a few\napproaches for achieving this goal.\nOne of the most effective approaches is to make the operation idempotent (see\n\u201cIdempotence\u201d on page 478); that is, to ensure that it has the same effect, no matter\nwhether it is executed once or multiple times. However, taking an operation that is\nnot naturally idempotent and making it idempotent requires some effort and care:\nyou may need to maintain some additional metadata (such as the set of operation IDs\nthat have updated a value), and ensure fencing when failing over from one node to\nanother (see \u201cThe leader and the lock\u201d on page 301).\nDuplicate suppression\nThe same pattern of needing to suppress duplicates occurs in many other places\nbesides stream processing. For example, TCP uses sequence numbers on packets to\nput them in the correct order at the recipient, and to determine whether any packets\nwere lost or duplicated on the network. Any lost packets are retransmitted and any\nduplicates are removed by the TCP stack before it hands the data to an application.\nHowever, this duplicate suppression only works within the context of a single TCP\nconnection. Imagine the TCP connection is a client\u2019s connection to a database, and it\nis currently executing the transaction in Example 12-1. In many databases, a transac\u2010\ntion is tied to a client connection (if the client sends several queries, the database\nknows that they belong to the same transaction because they are sent on the same\nTCP connection). If the client suffers a network interruption and connection timeout\nafter sending the COMMIT, but before hearing back from the database server, it does\nnot know whether the transaction has been committed or aborted (Figure 8-1).\nExample 12-1. A nonidempotent transfer of money from one account to another\nBEGIN TRANSACTION;\nUPDATE accounts SET balance = balance + 11.00 WHERE account_id = 1234;\nUPDATE accounts SET balance = balance - 11.00 WHERE account_id = 4321;\nCOMMIT;\nThe client can reconnect to the database and retry the transaction, but now it is out\u2010\nside of the scope of TCP duplicate suppression. Since the transaction in Example 12-1\nis not idempotent, it could happen that $22 is transferred instead of the desired $11.\nThus, even though Example 12-1 is a standard example for transaction atomicity, it is\nactually not correct, and real banks do not work like this [3]."}
{"540": "work fault, and tell it whether to commit or abort an in-doubt transaction. Is this suf\u2010\nficient to ensure that the transaction will only be executed once? Unfortunately not.\nEven if we can suppress duplicate transactions between the database client and\nserver, we still need to worry about the network between the end-user device and the\napplication server. For example, if the end-user client is a web browser, it probably\nuses an HTTP POST request to submit an instruction to the server. Perhaps the user\nis on a weak cellular data connection, and they succeed in sending the POST, but the\nsignal becomes too weak before they are able to receive the response from the server.\nIn this case, the user will probably be shown an error message, and they may retry\nmanually. Web browsers warn, \u201cAre you sure you want to submit this form again?\u201d\u2014\nand the user says yes, because they wanted the operation to happen. (The Post/Redi\u2010\nrect/Get pattern [54] avoids this warning message in normal operation, but it doesn\u2019t\nhelp if the POST request times out.) From the web server\u2019s point of view the retry is a\nseparate request, and from the database\u2019s point of view it is a separate transaction.\nThe usual deduplication mechanisms don\u2019t help.\nOperation identifiers\nTo make the operation idempotent through several hops of network communication,\nit is not sufficient to rely just on a transaction mechanism provided by a database\u2014\nyou need to consider the end-to-end flow of the request.\nFor example, you could generate a unique identifier for an operation (such as a\nUUID) and include it as a hidden form field in the client application, or calculate a\nhash of all the relevant form fields to derive the operation ID [3]. If the web browser\nsubmits the POST request twice, the two requests will have the same operation ID.\nYou can then pass that operation ID all the way through to the database and check\nthat you only ever execute one operation with a given ID, as shown in Example 12-2.\nExample 12-2. Suppressing duplicate requests using a unique ID\nALTER TABLE requests ADD UNIQUE (request_id);\nBEGIN TRANSACTION;\nINSERT INTO requests\n(request_id, from_account, to_account, amount)\nVALUES('0286FDB8-D7E1-423F-B40B-792B3608036C', 4321, 1234, 11.00);\nUPDATE accounts SET balance = balance + 11.00 WHERE account_id = 1234;\nUPDATE accounts SET balance = balance - 11.00 WHERE account_id = 4321;"}
{"541": "Example 12-2 relies on a uniqueness constraint on the request_id column. If a\ntransaction attempts to insert an ID that already exists, the INSERT fails and the trans\u2010\naction is aborted, preventing it from taking effect twice. Relational databases can gen\u2010\nerally maintain a uniqueness constraint correctly, even at weak isolation levels\n(whereas an application-level check-then-insert may fail under nonserializable isola\u2010\ntion, as discussed in \u201cWrite Skew and Phantoms\u201d on page 246).\nBesides suppressing duplicate requests, the requests table in Example 12-2 acts as a\nkind of event log, hinting in the direction of event sourcing (see \u201cEvent Sourcing\u201d on\npage 457). The updates to the account balances don\u2019t actually have to happen in the\nsame transaction as the insertion of the event, since they are redundant and could be\nderived from the request event in a downstream consumer\u2014as long as the event is\nprocessed exactly once, which can again be enforced using the request ID.\nThe end-to-end argument\nThis scenario of suppressing duplicate transactions is just one example of a more\ngeneral principle called the end-to-end argument, which was articulated by Saltzer,\nReed, and Clark in 1984 [55]:\nThe function in question can completely and correctly be implemented only with the\nknowledge and help of the application standing at the endpoints of the communica\u2010\ntion system. Therefore, providing that questioned function as a feature of the commu\u2010\nnication system itself is not possible. (Sometimes an incomplete version of the function\nprovided by the communication system may be useful as a performance enhance\u2010\nment.)\nIn our example, the function in question was duplicate suppression. We saw that TCP\nsuppresses duplicate packets at the TCP connection level, and some stream process\u2010\nors provide so-called exactly-once semantics at the message processing level, but that\nis not enough to prevent a user from submitting a duplicate request if the first one\ntimes out. By themselves, TCP, database transactions, and stream processors cannot\nentirely rule out these duplicates. Solving the problem requires an end-to-end solu\u2010\ntion: a transaction identifier that is passed all the way from the end-user client to the\ndatabase.\nThe end-to-end argument also applies to checking the integrity of data: checksums\nbuilt into Ethernet, TCP, and TLS can detect corruption of packets in the network,\nbut they cannot detect corruption due to bugs in the software at the sending and\nreceiving ends of the network connection, or corruption on the disks where the data\nis stored. If you want to catch all possible sources of data corruption, you also need\nend-to-end checksums."}
{"542": "against network attackers, but not against compromises of the server. Only end-to-\nend encryption and authentication can protect against all of these things.\nAlthough the low-level features (TCP duplicate suppression, Ethernet checksums,\nWiFi encryption) cannot provide the desired end-to-end features by themselves, they\nare still useful, since they reduce the probability of problems at the higher levels. For\nexample, HTTP requests would often get mangled if we didn\u2019t have TCP putting the\npackets back in the right order. We just need to remember that the low-level reliabil\u2010\nity features are not by themselves sufficient to ensure end-to-end correctness.\nApplying end-to-end thinking in data systems\nThis brings me back to my original thesis: just because an application uses a data sys\u2010\ntem that provides comparatively strong safety properties, such as serializable transac\u2010\ntions, that does not mean the application is guaranteed to be free from data loss or\ncorruption. The application itself needs to take end-to-end measures, such as dupli\u2010\ncate suppression, as well.\nThat is a shame, because fault-tolerance mechanisms are hard to get right. Low-level\nreliability mechanisms, such as those in TCP, work quite well, and so the remaining\nhigher-level faults occur fairly rarely. It would be really nice to wrap up the remain\u2010\ning high-level fault-tolerance machinery in an abstraction so that application code\nneedn\u2019t worry about it\u2014but I fear that we have not yet found the right abstraction.\nTransactions have long been seen as a good abstraction, and I do believe that they are\nuseful. As discussed in the introduction to Chapter 7, they take a wide range of possi\u2010\nble issues (concurrent writes, constraint violations, crashes, network interruptions,\ndisk failures) and collapse them down to two possible outcomes: commit or abort.\nThat is a huge simplification of the programming model, but I fear that it is not\nenough.\nTransactions are expensive, especially when they involve heterogeneous storage tech\u2010\nnologies (see \u201cDistributed Transactions in Practice\u201d on page 360). When we refuse to\nuse distributed transactions because they are too expensive, we end up having to\nreimplement fault-tolerance mechanisms in application code. As numerous examples\nthroughout this book have shown, reasoning about concurrency and partial failure is\ndifficult and counterintuitive, and so I suspect that most application-level mecha\u2010\nnisms do not work correctly. The consequence is lost or corrupted data.\nFor these reasons, I think it is worth exploring fault-tolerance abstractions that make\nit easy to provide application-specific end-to-end correctness properties, but also\nmaintain good performance and good operational characteristics in a large-scale dis\u2010"}
{"543": "Enforcing Constraints\nLet\u2019s think about correctness in the context of the ideas around unbundling databases\n(\u201cUnbundling Databases\u201d on page 499). We saw that end-to-end duplicate suppres\u2010\nsion can be achieved with a request ID that is passed all the way from the client to the\ndatabase that records the write. What about other kinds of constraints?\nIn particular, let\u2019s focus on uniqueness constraints\u2014such as the one we relied on in\nExample 12-2. In \u201cConstraints and uniqueness guarantees\u201d on page 330 we saw sev\u2010\neral other examples of application features that need to enforce uniqueness: a user\u2010\nname or email address must uniquely identify a user, a file storage service cannot\nhave more than one file with the same name, and two people cannot book the same\nseat on a flight or in a theater.\nOther kinds of constraints are very similar: for example, ensuring that an account\nbalance never goes negative, that you don\u2019t sell more items than you have in stock in\nthe warehouse, or that a meeting room does not have overlapping bookings. Techni\u2010\nques that enforce uniqueness can often be used for these kinds of constraints as well.\nUniqueness constraints require consensus\nIn Chapter 9 we saw that in a distributed setting, enforcing a uniqueness constraint\nrequires consensus: if there are several concurrent requests with the same value, the\nsystem somehow needs to decide which one of the conflicting operations is accepted,\nand reject the others as violations of the constraint.\nThe most common way of achieving this consensus is to make a single node the\nleader, and put it in charge of making all the decisions. That works fine as long as you\ndon\u2019t mind funneling all requests through a single node (even if the client is on the\nother side of the world), and as long as that node doesn\u2019t fail. If you need to tolerate\nthe leader failing, you\u2019re back at the consensus problem again (see \u201cSingle-leader rep\u2010\nlication and consensus\u201d on page 367).\nUniqueness checking can be scaled out by partitioning based on the value that needs\nto be unique. For example, if you need to ensure uniqueness by request ID, as in\nExample 12-2, you can ensure all requests with the same request ID are routed to the\nsame partition (see Chapter 6). If you need usernames to be unique, you can partition\nby hash of username.\nHowever, asynchronous multi-master replication is ruled out, because it could hap\u2010\npen that different masters concurrently accept conflicting writes, and thus the values\nare no longer unique (see \u201cImplementing Linearizable Systems\u201d on page 332). If you\nwant to be able to immediately reject any writes that would violate the constraint,"}
{"544": "Uniqueness in log-based messaging\nThe log ensures that all consumers see messages in the same order\u2014a guarantee that\nis formally known as total order broadcast and is equivalent to consensus (see \u201cTotal\nOrder Broadcast\u201d on page 348). In the unbundled database approach with log-based\nmessaging, we can use a very similar approach to enforce uniqueness constraints.\nA stream processor consumes all the messages in a log partition sequentially on a sin\u2010\ngle thread (see \u201cLogs compared to traditional messaging\u201d on page 448). Thus, if the\nlog is partitioned based on the value that needs to be unique, a stream processor can\nunambiguously and deterministically decide which one of several conflicting opera\u2010\ntions came first. For example, in the case of several users trying to claim the same\nusername [57]:\n1. Every request for a username is encoded as a message, and appended to a parti\u2010\ntion determined by the hash of the username.\n2. A stream processor sequentially reads the requests in the log, using a local data\u2010\nbase to keep track of which usernames are taken. For every request for a user\u2010\nname that is available, it records the name as taken and emits a success message\nto an output stream. For every request for a username that is already taken, it\nemits a rejection message to an output stream.\n3. The client that requested the username watches the output stream and waits for a\nsuccess or rejection message corresponding to its request.\nThis algorithm is basically the same as in \u201cImplementing linearizable storage using\ntotal order broadcast\u201d on page 350. It scales easily to a large request throughput by\nincreasing the number of partitions, as each partition can be processed independ\u2010\nently.\nThe approach works not only for uniqueness constraints, but also for many other\nkinds of constraints. Its fundamental principle is that any writes that may conflict are\nrouted to the same partition and processed sequentially. As discussed in \u201cWhat is a\nconflict?\u201d on page 174 and \u201cWrite Skew and Phantoms\u201d on page 246, the definition of\na conflict may depend on the application, but the stream processor can use arbitrary\nlogic to validate a request. This idea is similar to the approach pioneered by Bayou in\nthe 1990s [58].\nMulti-partition request processing\nEnsuring that an operation is executed atomically, while satisfying constraints,\nbecomes more interesting when several partitions are involved. In Example 12-2,"}
{"545": "son why those three things should be in the same partition, since they are all\nindependent from each other.\nIn the traditional approach to databases, executing this transaction would require an\natomic commit across all three partitions, which essentially forces it into a total order\nwith respect to all other transactions on any of those partitions. Since there is now\ncross-partition coordination, different partitions can no longer be processed inde\u2010\npendently, so throughput is likely to suffer.\nHowever, it turns out that equivalent correctness can be achieved with partitioned\nlogs, and without an atomic commit:\n1. The request to transfer money from account A to account B is given a unique\nrequest ID by the client, and appended to a log partition based on the request ID.\n2. A stream processor reads the log of requests. For each request message it emits\ntwo messages to output streams: a debit instruction to the payer account A (par\u2010\ntitioned by A), and a credit instruction to the payee account B (partitioned by B).\nThe original request ID is included in those emitted messages.\n3. Further processors consume the streams of credit and debit instructions, dedu\u2010\nplicate by request ID, and apply the changes to the account balances.\nSteps 1 and 2 are necessary because if the client directly sent the credit and debit\ninstructions, it would require an atomic commit across those two partitions to ensure\nthat either both or neither happen. To avoid the need for a distributed transaction,\nwe first durably log the request as a single message, and then derive the credit and\ndebit instructions from that first message. Single-object writes are atomic in almost\nall data systems (see \u201cSingle-object writes\u201d on page 230), and so the request either\nappears in the log or it doesn\u2019t, without any need for a multi-partition atomic com\u2010\nmit.\nIf the stream processor in step 2 crashes, it resumes processing from its last check\u2010\npoint. In doing so, it does not skip any request messages, but it may process requests\nmultiple times and produce duplicate credit and debit instructions. However, since it\nis deterministic, it will just produce the same instructions again, and the processors in\nstep 3 can easily deduplicate them using the end-to-end request ID.\nIf you want to ensure that the payer account is not overdrawn by this transfer, you\ncan additionally have a stream processor (partitioned by payer account number) that\nmaintains account balances and validates transactions. Only valid transactions would\nthen be placed in the request log in step 1."}
{"546": "idea of using multiple differently partitioned stages is similar to what we discussed in\n\u201cMulti-partition data processing\u201d on page 514 (see also \u201cConcurrency control\u201d on\npage 462).\nTimeliness and Integrity\nA convenient property of transactions is that they are typically linearizable (see \u201cLin\u2010\nearizability\u201d on page 324): that is, a writer waits until a transaction is committed, and\nthereafter its writes are immediately visible to all readers.\nThis is not the case when unbundling an operation across multiple stages of stream\nprocessors: consumers of a log are asynchronous by design, so a sender does not wait\nuntil its message has been processed by consumers. However, it is possible for a client\nto wait for a message to appear on an output stream. This is what we did in \u201cUnique\u2010\nness in log-based messaging\u201d on page 522 when checking whether a uniqueness con\u2010\nstraint was satisfied.\nIn this example, the correctness of the uniqueness check does not depend on whether\nthe sender of the message waits for the outcome. The waiting only has the purpose of\nsynchronously informing the sender whether or not the uniqueness check succeeded,\nbut this notification can be decoupled from the effects of processing the message.\nMore generally, I think the term consistency conflates two different requirements that\nare worth considering separately:\nTimeliness\nTimeliness means ensuring that users observe the system in an up-to-date state.\nWe saw previously that if a user reads from a stale copy of the data, they may\nobserve it in an inconsistent state (see \u201cProblems with Replication Lag\u201d on page\n161). However, that inconsistency is temporary, and will eventually be resolved\nsimply by waiting and trying again.\nThe CAP theorem (see \u201cThe Cost of Linearizability\u201d on page 335) uses consis\u2010\ntency in the sense of linearizability, which is a strong way of achieving timeliness.\nWeaker timeliness properties like read-after-write consistency (see \u201cReading\nYour Own Writes\u201d on page 162) can also be useful.\nIntegrity\nIntegrity means absence of corruption; i.e., no data loss, and no contradictory or\nfalse data. In particular, if some derived dataset is maintained as a view onto\nsome underlying data (see \u201cDeriving current state from the event log\u201d on page\n458), the derivation must be correct. For example, a database index must cor\u2010"}
{"547": "If integrity is violated, the inconsistency is permanent: waiting and trying again is\nnot going to fix database corruption in most cases. Instead, explicit checking and\nrepair is needed. In the context of ACID transactions (see \u201cThe Meaning of\nACID\u201d on page 223), consistency is usually understood as some kind of\napplication-specific notion of integrity. Atomicity and durability are important\ntools for preserving integrity.\nIn slogan form: violations of timeliness are \u201ceventual consistency,\u201d whereas violations\nof integrity are \u201cperpetual inconsistency.\u201d\nI am going to assert that in most applications, integrity is much more important than\ntimeliness. Violations of timeliness can be annoying and confusing, but violations of\nintegrity can be catastrophic.\nFor example, on your credit card statement, it is not surprising if a transaction that\nyou made within the last 24 hours does not yet appear\u2014it is normal that these sys\u2010\ntems have a certain lag. We know that banks reconcile and settle transactions asyn\u2010\nchronously, and timeliness is not very important here [3]. However, it would be very\nbad if the statement balance was not equal to the sum of the transactions plus the\nprevious statement balance (an error in the sums), or if a transaction was charged to\nyou but not paid to the merchant (disappearing money). Such problems would be\nviolations of the integrity of the system.\nCorrectness of dataflow systems\nACID transactions usually provide both timeliness (e.g., linearizability) and integrity\n(e.g., atomic commit) guarantees. Thus, if you approach application correctness from\nthe point of view of ACID transactions, the distinction between timeliness and integ\u2010\nrity is fairly inconsequential.\nOn the other hand, an interesting property of the event-based dataflow systems that\nwe have discussed in this chapter is that they decouple timeliness and integrity. When\nprocessing event streams asynchronously, there is no guarantee of timeliness, unless\nyou explicitly build consumers that wait for a message to arrive before returning. But\nintegrity is in fact central to streaming systems.\nExactly-once or effectively-once semantics (see \u201cFault Tolerance\u201d on page 476) is a\nmechanism for preserving integrity. If an event is lost, or if an event takes effect\ntwice, the integrity of a data system could be violated. Thus, fault-tolerant message\ndelivery and duplicate suppression (e.g., idempotent operations) are important for\nmaintaining the integrity of a data system in the face of faults.\nAs we saw in the last section, reliable stream processing systems can preserve integ\u2010"}
{"548": "performance and operational robustness. We achieved this integrity through a com\u2010\nbination of mechanisms:\n\u2022 Representing the content of the write operation as a single message, which can\neasily be written atomically\u2014an approach that fits very well with event sourcing\n(see \u201cEvent Sourcing\u201d on page 457)\n\u2022 Deriving all other state updates from that single message using deterministic der\u2010\nivation functions, similarly to stored procedures (see \u201cActual Serial Execution\u201d\non page 252 and \u201cApplication code as a derivation function\u201d on page 505)\n\u2022 Passing a client-generated request ID through all these levels of processing, ena\u2010\nbling end-to-end duplicate suppression and idempotence\n\u2022 Making messages immutable and allowing derived data to be reprocessed from\ntime to time, which makes it easier to recover from bugs (see \u201cAdvantages of\nimmutable events\u201d on page 460)\nThis combination of mechanisms seems to me a very promising direction for build\u2010\ning fault-tolerant applications in the future.\nLoosely interpreted constraints\nAs discussed previously, enforcing a uniqueness constraint requires consensus, typi\u2010\ncally implemented by funneling all events in a particular partition through a single\nnode. This limitation is unavoidable if we want the traditional form of uniqueness\nconstraint, and stream processing cannot avoid it.\nHowever, another thing to realize is that many real applications can actually get away\nwith much weaker notions of uniqueness:\n\u2022 If two people concurrently register the same username or book the same seat,\nyou can send one of them a message to apologize, and ask them to choose a dif\u2010\nferent one. This kind of change to correct a mistake is called a compensating\ntransaction [59, 60].\n\u2022 If customers order more items than you have in your warehouse, you can order\nin more stock, apologize to customers for the delay, and offer them a discount.\nThis is actually the same as what you\u2019d have to do if, say, a forklift truck ran over\nsome of the items in your warehouse, leaving you with fewer items in stock than\nyou thought you had [61]. Thus, the apology workflow already needs to be part\nof your business processes anyway, and so it might be unnecessary to require a\nlinearizable constraint on the number of items in stock."}
{"549": "erately violated for business reasons, and compensation processes (refunds,\nupgrades, providing a complimentary room at a neighboring hotel) are put in\nplace to handle situations in which demand exceeds supply. Even if there was no\noverbooking, apology and compensation processes would be needed in order to\ndeal with flights being cancelled due to bad weather or staff on strike\u2014recover\u2010\ning from such issues is just a normal part of business [3].\n\u2022 If someone withdraws more money than they have in their account, the bank can\ncharge them an overdraft fee and ask them to pay back what they owe. By limit\u2010\ning the total withdrawals per day, the risk to the bank is bounded.\nIn many business contexts, it is actually acceptable to temporarily violate a constraint\nand fix it up later by apologizing. The cost of the apology (in terms of money or repu\u2010\ntation) varies, but it is often quite low: you can\u2019t unsend an email, but you can send a\nfollow-up email with a correction. If you accidentally charge a credit card twice, you\ncan refund one of the charges, and the cost to you is just the processing fees and per\u2010\nhaps a customer complaint. Once money has been paid out of an ATM, you can\u2019t\ndirectly get it back, although in principle you can send debt collectors to recover the\nmoney if the account was overdrawn and the customer won\u2019t pay it back.\nWhether the cost of the apology is acceptable is a business decision. If it is acceptable,\nthe traditional model of checking all constraints before even writing the data is\nunnecessarily restrictive, and a linearizable constraint is not needed. It may well be a\nreasonable choice to go ahead with a write optimistically, and to check the constraint\nafter the fact. You can still ensure that the validation occurs before doing things that\nwould be expensive to recover from, but that doesn\u2019t imply you must do the valida\u2010\ntion before you even write the data.\nThese applications do require integrity: you would not want to lose a reservation, or\nhave money disappear due to mismatched credits and debits. But they don\u2019t require\ntimeliness on the enforcement of the constraint: if you have sold more items than you\nhave in the warehouse, you can patch up the problem after the fact by apologizing.\nDoing so is similar to the conflict resolution approaches we discussed in \u201cHandling\nWrite Conflicts\u201d on page 171.\nCoordination-avoiding data systems\nWe have now made two interesting observations:\n1. Dataflow systems can maintain integrity guarantees on derived data without\natomic commit, linearizability, or synchronous cross-partition coordination."}
{"550": "Taken together, these observations mean that dataflow systems can provide the data\nmanagement services for many applications without requiring coordination, while\nstill giving strong integrity guarantees. Such coordination-avoiding data systems have\na lot of appeal: they can achieve better performance and fault tolerance than systems\nthat need to perform synchronous coordination [56].\nFor example, such a system could operate distributed across multiple datacenters in a\nmulti-leader configuration, asynchronously replicating between regions. Any one\ndatacenter can continue operating independently from the others, because no syn\u2010\nchronous cross-region coordination is required. Such a system would have weak\ntimeliness guarantees\u2014it could not be linearizable without introducing coordination\n\u2014but it can still have strong integrity guarantees.\nIn this context, serializable transactions are still useful as part of maintaining derived\nstate, but they can be run at a small scope where they work well [8]. Heterogeneous\ndistributed transactions such as XA transactions (see \u201cDistributed Transactions in\nPractice\u201d on page 360) are not required. Synchronous coordination can still be intro\u2010\nduced in places where it is needed (for example, to enforce strict constraints before\nan operation from which recovery is not possible), but there is no need for everything\nto pay the cost of coordination if only a small part of an application needs it [43].\nAnother way of looking at coordination and constraints: they reduce the number of\napologies you have to make due to inconsistencies, but potentially also reduce the\nperformance and availability of your system, and thus potentially increase the num\u2010\nber of apologies you have to make due to outages. You cannot reduce the number of\napologies to zero, but you can aim to find the best trade-off for your needs\u2014the\nsweet spot where there are neither too many inconsistencies nor too many availability\nproblems.\nTrust, but Verify\nAll of our discussion of correctness, integrity, and fault-tolerance has been under the\nassumption that certain things might go wrong, but other things won\u2019t. We call these\nassumptions our system model (see \u201cMapping system models to the real world\u201d on\npage 309): for example, we should assume that processes can crash, machines can\nsuddenly lose power, and the network can arbitrarily delay or drop messages. But we\nmight also assume that data written to disk is not lost after fsync, that data in mem\u2010\nory is not corrupted, and that the multiplication instruction of our CPU always\nreturns the correct result.\nThese assumptions are quite reasonable, as they are true most of the time, and it"}
{"551": "things less likely. The question is whether violations of our assumptions happen often\nenough that we may encounter them in practice.\nWe have seen that data can become corrupted while it is sitting untouched on disks\n(see \u201cReplication and Durability\u201d on page 227), and data corruption on the network\ncan sometimes evade the TCP checksums (see \u201cWeak forms of lying\u201d on page 306).\nMaybe this is something we should be paying more attention to?\nOne application that I worked on in the past collected crash reports from clients, and\nsome of the reports we received could only be explained by random bit-flips in the\nmemory of those devices. It seems unlikely, but if you have enough devices running\nyour software, even very unlikely things do happen. Besides random memory corrup\u2010\ntion due to hardware faults or radiation, certain pathological memory access patterns\ncan flip bits even in memory that has no faults [62]\u2014an effect that can be used to\nbreak security mechanisms in operating systems [63] (this technique is known as\nrowhammer). Once you look closely, hardware isn\u2019t quite the perfect abstraction that\nit may seem.\nTo be clear, random bit-flips are still very rare on modern hardware [64]. I just want\nto point out that they are not beyond the realm of possibility, and so they deserve\nsome attention.\nMaintaining integrity in the face of software bugs\nBesides such hardware issues, there is always the risk of software bugs, which would\nnot be caught by lower-level network, memory, or filesystem checksums. Even widely\nused database software has bugs: I have personally seen cases of MySQL failing to\ncorrectly maintain a uniqueness constraint [65] and PostgreSQL\u2019s serializable isola\u2010\ntion level exhibiting write skew anomalies [66], even though MySQL and PostgreSQL\nare robust and well-regarded databases that have been battle-tested by many people\nfor many years. In less mature software, the situation is likely to be much worse.\nDespite considerable efforts in careful design, testing, and review, bugs still creep in.\nAlthough they are rare, and they eventually get found and fixed, there is still a period\nduring which such bugs can corrupt data.\nWhen it comes to application code, we have to assume many more bugs, since most\napplications don\u2019t receive anywhere near the amount of review and testing that data\u2010\nbase code does. Many applications don\u2019t even correctly use the features that databases\noffer for preserving integrity, such as foreign key or uniqueness constraints [36].\nConsistency in the sense of ACID (see \u201cConsistency\u201d on page 224) is based on the\nidea that the database starts off in a consistent state, and a transaction transforms it"}
{"552": "in some way, for example using a weak isolation level unsafely, the integrity of the\ndatabase cannot be guaranteed.\nDon\u2019t just blindly trust what they promise\nWith both hardware and software not always living up to the ideal that we would like\nthem to be, it seems that data corruption is inevitable sooner or later. Thus, we\nshould at least have a way of finding out if data has been corrupted so that we can fix\nit and try to track down the source of the error. Checking the integrity of data is\nknown as auditing.\nAs discussed in \u201cAdvantages of immutable events\u201d on page 460, auditing is not just\nfor financial applications. However, auditability is highly important in finance pre\u2010\ncisely because everyone knows that mistakes happen, and we all recognize the need to\nbe able to detect and fix problems.\nMature systems similarly tend to consider the possibility of unlikely things going\nwrong, and manage that risk. For example, large-scale storage systems such as HDFS\nand Amazon S3 do not fully trust disks: they run background processes that continu\u2010\nally read back files, compare them to other replicas, and move files from one disk to\nanother, in order to mitigate the risk of silent corruption [67].\nIf you want to be sure that your data is still there, you have to actually read it and\ncheck. Most of the time it will still be there, but if it isn\u2019t, you really want to find out\nsooner rather than later. By the same argument, it is important to try restoring from\nyour backups from time to time\u2014otherwise you may only find out that your backup\nis broken when it is too late and you have already lost data. Don\u2019t just blindly trust\nthat it is all working.\nA culture of verification\nSystems like HDFS and S3 still have to assume that disks work correctly most of the\ntime\u2014which is a reasonable assumption, but not the same as assuming that they\nalways work correctly. However, not many systems currently have this kind of \u201ctrust,\nbut verify\u201d approach of continually auditing themselves. Many assume that correct\u2010\nness guarantees are absolute and make no provision for the possibility of rare data\ncorruption. I hope that in the future we will see more self-validating or self-auditing\nsystems that continually check their own integrity, rather than relying on blind trust\n[68].\nI fear that the culture of ACID databases has led us toward developing applications\non the basis of blindly trusting technology (such as a transaction mechanism), and"}
{"553": "But then the database landscape changed: weaker consistency guarantees became the\nnorm under the banner of NoSQL, and less mature storage technologies became\nwidely used. Yet, because the audit mechanisms had not been developed, we contin\u2010\nued building applications on the basis of blind trust, even though this approach had\nnow become more dangerous. Let\u2019s think for a moment about designing for audita\u2010\nbility.\nDesigning for auditability\nIf a transaction mutates several objects in a database, it is difficult to tell after the fact\nwhat that transaction means. Even if you capture the transaction logs (see \u201cChange\nData Capture\u201d on page 454), the insertions, updates, and deletions in various tables\ndo not necessarily give a clear picture of why those mutations were performed. The\ninvocation of the application logic that decided on those mutations is transient and\ncannot be reproduced.\nBy contrast, event-based systems can provide better auditability. In the event sourc\u2010\ning approach, user input to the system is represented as a single immutable event,\nand any resulting state updates are derived from that event. The derivation can be\nmade deterministic and repeatable, so that running the same log of events through\nthe same version of the derivation code will result in the same state updates.\nBeing explicit about dataflow (see \u201cPhilosophy of batch process outputs\u201d on page\n413) makes the provenance of data much clearer, which makes integrity checking\nmuch more feasible. For the event log, we can use hashes to check that the event stor\u2010\nage has not been corrupted. For any derived state, we can rerun the batch and stream\nprocessors that derived it from the event log in order to check whether we get the\nsame result, or even run a redundant derivation in parallel.\nA deterministic and well-defined dataflow also makes it easier to debug and trace the\nexecution of a system in order to determine why it did something [4, 69]. If some\u2010\nthing unexpected occurred, it is valuable to have the diagnostic capability to repro\u2010\nduce the exact circumstances that led to the unexpected event\u2014a kind of time-travel\ndebugging capability.\nThe end-to-end argument again\nIf we cannot fully trust that every individual component of the system will be free\nfrom corruption\u2014that every piece of hardware is fault-free and that every piece of\nsoftware is bug-free\u2014then we must at least periodically check the integrity of our\ndata. If we don\u2019t check, we won\u2019t find out about corruption until it is too late and it\nhas caused some downstream damage, at which point it will be much harder and"}
{"554": "include in an integrity check, the fewer opportunities there are for corruption to go\nunnoticed at some stage of the process. If we can check that an entire derived data\npipeline is correct end to end, then any disks, networks, services, and algorithms\nalong the path are implicitly included in the check.\nHaving continuous end-to-end integrity checks gives you increased confidence about\nthe correctness of your systems, which in turn allows you to move faster [70]. Like\nautomated testing, auditing increases the chances that bugs will be found quickly,\nand thus reduces the risk that a change to the system or a new storage technology will\ncause damage. If you are not afraid of making changes, you can much better evolve\nan application to meet changing requirements.\nTools for auditable data systems\nAt present, not many data systems make auditability a top-level concern. Some appli\u2010\ncations implement their own audit mechanisms, for example by logging all changes\nto a separate audit table, but guaranteeing the integrity of the audit log and the data\u2010\nbase state is still difficult. A transaction log can be made tamper-proof by periodically\nsigning it with a hardware security module, but that does not guarantee that the right\ntransactions went into the log in the first place.\nIt would be interesting to use cryptographic tools to prove the integrity of a system in\na way that is robust to a wide range of hardware and software issues, and even poten\u2010\ntially malicious actions. Cryptocurrencies, blockchains, and distributed ledger tech\u2010\nnologies such as Bitcoin, Ethereum, Ripple, Stellar, and various others [71, 72, 73]\nhave sprung up to explore this area.\nI am not qualified to comment on the merits of these technologies as currencies or\nmechanisms for agreeing contracts. However, from a data systems point of view they\ncontain some interesting ideas. Essentially, they are distributed databases, with a data\nmodel and transaction mechanism, in which different replicas can be hosted by\nmutually untrusting organizations. The replicas continually check each other\u2019s integ\u2010\nrity and use a consensus protocol to agree on the transactions that should be exe\u2010\ncuted.\nI am somewhat skeptical about the Byzantine fault tolerance aspects of these technol\u2010\nogies (see \u201cByzantine Faults\u201d on page 304), and I find the technique of proof of work\n(e.g., Bitcoin mining) extraordinarily wasteful. The transaction throughput of Bitcoin\nis rather low, albeit for political and economic reasons more than for technical ones.\nHowever, the integrity checking aspects are interesting.\nCryptographic auditing and integrity checking often relies on Merkle trees [74],"}
{"555": "I could imagine integrity-checking and auditing algorithms, like those of certificate\ntransparency and distributed ledgers, becoming more widely used in data systems in\ngeneral. Some work will be needed to make them equally scalable as systems without\ncryptographic auditing, and to keep the performance penalty as low as possible. But I\nthink this is an interesting area to watch in the future.\nDoing the Right Thing\nIn the final section of this book, I would like to take a step back. Throughout this\nbook we have examined a wide range of different architectures for data systems, eval\u2010\nuated their pros and cons, and explored techniques for building reliable, scalable, and\nmaintainable applications. However, we have left out an important and fundamental\npart of the discussion, which I would now like to fill in.\nEvery system is built for a purpose; every action we take has both intended and unin\u2010\ntended consequences. The purpose may be as simple as making money, but the con\u2010\nsequences for the world may reach far beyond that original purpose. We, the\nengineers building these systems, have a responsibility to carefully consider those\nconsequences and to consciously decide what kind of world we want to live in.\nWe talk about data as an abstract thing, but remember that many datasets are about\npeople: their behavior, their interests, their identity. We must treat such data with\nhumanity and respect. Users are humans too, and human dignity is paramount.\nSoftware development increasingly involves making important ethical choices. There\nare guidelines to help software engineers navigate these issues, such as the ACM\u2019s\nSoftware Engineering Code of Ethics and Professional Practice [77], but they are\nrarely discussed, applied, and enforced in practice. As a result, engineers and product\nmanagers sometimes take a very cavalier attitude to privacy and potential negative\nconsequences of their products [78, 79, 80].\nA technology is not good or bad in itself\u2014what matters is how it is used and how it\naffects people. This is true for a software system like a search engine in much the\nsame way as it is for a weapon like a gun. I think it is not sufficient for software engi\u2010\nneers to focus exclusively on the technology and ignore its consequences: the ethical\nresponsibility is ours to bear also. Reasoning about ethics is difficult, but it is too\nimportant to ignore.\nPredictive Analytics\nFor example, predictive analytics is a major part of the \u201cBig Data\u201d hype. Using data"}
{"556": "Naturally, payment networks want to prevent fraudulent transactions, banks want to\navoid bad loans, airlines want to avoid hijackings, and companies want to avoid hir\u2010\ning ineffective or untrustworthy people. From their point of view, the cost of a missed\nbusiness opportunity is low, but the cost of a bad loan or a problematic employee is\nmuch higher, so it is natural for organizations to want to be cautious. If in doubt,\nthey are better off saying no.\nHowever, as algorithmic decision-making becomes more widespread, someone who\nhas (accurately or falsely) been labeled as risky by some algorithm may suffer a large\nnumber of those \u201cno\u201d decisions. Systematically being excluded from jobs, air travel,\ninsurance coverage, property rental, financial services, and other key aspects of soci\u2010\nety is such a large constraint of the individual\u2019s freedom that it has been called \u201calgo\u2010\nrithmic prison\u201d [82]. In countries that respect human rights, the criminal justice\nsystem presumes innocence until proven guilty; on the other hand, automated sys\u2010\ntems can systematically and arbitrarily exclude a person from participating in society\nwithout any proof of guilt, and with little chance of appeal.\nBias and discrimination\nDecisions made by an algorithm are not necessarily any better or any worse than\nthose made by a human. Every person is likely to have biases, even if they actively try\nto counteract them, and discriminatory practices can become culturally institutional\u2010\nized. There is hope that basing decisions on data, rather than subjective and instinc\u2010\ntive assessments by people, could be more fair and give a better chance to people who\nare often overlooked in the traditional system [83].\nWhen we develop predictive analytics systems, we are not merely automating a\nhuman\u2019s decision by using software to specify the rules for when to say yes or no; we\nare even leaving the rules themselves to be inferred from data. However, the patterns\nlearned by these systems are opaque: even if there is some correlation in the data, we\nmay not know why. If there is a systematic bias in the input to an algorithm, the sys\u2010\ntem will most likely learn and amplify that bias in its output [84].\nIn many countries, anti-discrimination laws prohibit treating people differently\ndepending on protected traits such as ethnicity, age, gender, sexuality, disability, or\nbeliefs. Other features of a person\u2019s data may be analyzed, but what happens if they\nare correlated with protected traits? For example, in racially segregated neighbor\u2010\nhoods, a person\u2019s postal code or even their IP address is a strong predictor of race.\nPut like this, it seems ridiculous to believe that an algorithm could somehow take\nbiased data as input and produce fair and impartial output from it [85]. Yet this belief\noften seems to be implied by proponents of data-driven decision making, an attitude"}
{"557": "past, moral imagination is required, and that\u2019s something only humans can provide\n[87]. Data and models should be our tools, not our masters.\nResponsibility and accountability\nAutomated decision making opens the question of responsibility and accountability\n[87]. If a human makes a mistake, they can be held accountable, and the person affec\u2010\nted by the decision can appeal. Algorithms make mistakes too, but who is accounta\u2010\nble if they go wrong [88]? When a self-driving car causes an accident, who is\nresponsible? If an automated credit scoring algorithm systematically discriminates\nagainst people of a particular race or religion, is there any recourse? If a decision by\nyour machine learning system comes under judicial review, can you explain to the\njudge how the algorithm made its decision?\nCredit rating agencies are an old example of collecting data to make decisions about\npeople. A bad credit score makes life difficult, but at least a credit score is normally\nbased on relevant facts about a person\u2019s actual borrowing history, and any errors in\nthe record can be corrected (although the agencies normally do not make this easy).\nHowever, scoring algorithms based on machine learning typically use a much wider\nrange of inputs and are much more opaque, making it harder to understand how a\nparticular decision has come about and whether someone is being treated in an\nunfair or discriminatory way [89].\nA credit score summarizes \u201cHow did you behave in the past?\u201d whereas predictive\nanalytics usually work on the basis of \u201cWho is similar to you, and how did people like\nyou behave in the past?\u201d Drawing parallels to others\u2019 behavior implies stereotyping\npeople, for example based on where they live (a close proxy for race and socioeco\u2010\nnomic class). What about people who get put in the wrong bucket? Furthermore, if a\ndecision is incorrect due to erroneous data, recourse is almost impossible [87].\nMuch data is statistical in nature, which means that even if the probability distribu\u2010\ntion on the whole is correct, individual cases may well be wrong. For example, if the\naverage life expectancy in your country is 80 years, that doesn\u2019t mean you\u2019re expected\nto drop dead on your 80th birthday. From the average and the probability distribu\u2010\ntion, you can\u2019t say much about the age to which one particular person will live. Simi\u2010\nlarly, the output of a prediction system is probabilistic and may well be wrong in\nindividual cases.\nA blind belief in the supremacy of data for making decisions is not only delusional, it\nis positively dangerous. As data-driven decision making becomes more widespread,\nwe will need to figure out how to make algorithms accountable and transparent, how\nto avoid reinforcing existing biases, and how to fix them when they inevitably make"}
{"558": "social characteristics of people\u2019s lives. On the one hand, this power could be used to\nfocus aid and support to help those people who most need it. On the other hand, it is\nsometimes used by predatory business seeking to identify vulnerable people and sell\nthem risky products such as high-cost loans and worthless college degrees [87, 90].\nFeedback loops\nEven with predictive applications that have less immediately far-reaching effects on\npeople, such as recommendation systems, there are difficult issues that we must con\u2010\nfront. When services become good at predicting what content users want to see, they\nmay end up showing people only opinions they already agree with, leading to echo\nchambers in which stereotypes, misinformation, and polarization can breed. We are\nalready seeing the impact of social media echo chambers on election campaigns [91].\nWhen predictive analytics affect people\u2019s lives, particularly pernicious problems arise\ndue to self-reinforcing feedback loops. For example, consider the case of employers\nusing credit scores to evaluate potential hires. You may be a good worker with a good\ncredit score, but suddenly find yourself in financial difficulties due to a misfortune\noutside of your control. As you miss payments on your bills, your credit score suffers,\nand you will be less likely to find work. Joblessness pushes you toward poverty, which\nfurther worsens your scores, making it even harder to find employment [87]. It\u2019s a\ndownward spiral due to poisonous assumptions, hidden behind a camouflage of\nmathematical rigor and data.\nWe can\u2019t always predict when such feedback loops happen. However, many conse\u2010\nquences can be predicted by thinking about the entire system (not just the computer\u2010\nized parts, but also the people interacting with it)\u2014an approach known as systems\nthinking [92]. We can try to understand how a data analysis system responds to dif\u2010\nferent behaviors, structures, or characteristics. Does the system reinforce and amplify\nexisting differences between people (e.g., making the rich richer or the poor poorer),\nor does it try to combat injustice? And even with the best intentions, we must beware\nof unintended consequences.\nPrivacy and Tracking\nBesides the problems of predictive analytics\u2014i.e., using data to make automated\ndecisions about people\u2014there are ethical problems with data collection itself. What is\nthe relationship between the organizations collecting data and the people whose data\nis being collected?\nWhen a system only stores data that a user has explicitly entered, because they want\nthe system to store and process it in a certain way, the system is performing a service"}
{"559": "no longer just does what the user tells it to do, but it takes on interests of its own,\nwhich may conflict with the user\u2019s interests.\nTracking behavioral data has become increasingly important for user-facing features\nof many online services: tracking which search results are clicked helps improve the\nranking of search results; recommending \u201cpeople who liked X also liked Y\u201d helps\nusers discover interesting and useful things; A/B tests and user flow analysis can help\nindicate how a user interface might be improved. Those features require some\namount of tracking of user behavior, and users benefit from them.\nHowever, depending on a company\u2019s business model, tracking often doesn\u2019t stop\nthere. If the service is funded through advertising, the advertisers are the actual cus\u2010\ntomers, and the users\u2019 interests take second place. Tracking data becomes more\ndetailed, analyses become further-reaching, and data is retained for a long time in\norder to build up detailed profiles of each person for marketing purposes.\nNow the relationship between the company and the user whose data is being collec\u2010\nted starts looking quite different. The user is given a free service and is coaxed into\nengaging with it as much as possible. The tracking of the user serves not primarily\nthat individual, but rather the needs of the advertisers who are funding the service. I\nthink this relationship can be appropriately described with a word that has more sin\u2010\nister connotations: surveillance.\nSurveillance\nAs a thought experiment, try replacing the word data with surveillance, and observe if\ncommon phrases still sound so good [93]. How about this: \u201cIn our surveillance-\ndriven organization we collect real-time surveillance streams and store them in our\nsurveillance warehouse. Our surveillance scientists use advanced analytics and sur\u2010\nveillance processing in order to derive new insights.\u201d\nThis thought experiment is unusually polemic for this book, Designing Surveillance-\nIntensive Applications, but I think that strong words are needed to emphasize this\npoint. In our attempts to make software \u201ceat the world\u201d [94], we have built the great\u2010\nest mass surveillance infrastructure the world has ever seen. Rushing toward an Inter\u2010\nnet of Things, we are rapidly approaching a world in which every inhabited space\ncontains at least one internet-connected microphone, in the form of smartphones,\nsmart TVs, voice-controlled assistant devices, baby monitors, and even children\u2019s\ntoys that use cloud-based speech recognition. Many of these devices have a terrible\nsecurity record [95].\nEven the most totalitarian and repressive regimes could only dream of putting a"}
{"560": "ence is just that the data is being collected by corporations rather than government\nagencies [96].\nNot all data collection necessarily qualifies as surveillance, but examining it as such\ncan help us understand our relationship with the data collector. Why are we seem\u2010\ningly happy to accept surveillance by corporations? Perhaps you feel you have noth\u2010\ning to hide\u2014in other words, you are totally in line with existing power structures,\nyou are not a marginalized minority, and you needn\u2019t fear persecution [97]. Not\neveryone is so fortunate. Or perhaps it\u2019s because the purpose seems benign\u2014it\u2019s not\novert coercion and conformance, but merely better recommendations and more per\u2010\nsonalized marketing. However, combined with the discussion of predictive analytics\nfrom the last section, that distinction seems less clear.\nWe are already seeing car insurance premiums linked to tracking devices in cars, and\nhealth insurance coverage that depends on people wearing a fitness tracking device.\nWhen surveillance is used to determine things that hold sway over important aspects\nof life, such as insurance coverage or employment, it starts to appear less benign.\nMoreover, data analysis can reveal surprisingly intrusive things: for example, the\nmovement sensor in a smartwatch or fitness tracker can be used to work out what\nyou are typing (for example, passwords) with fairly good accuracy [98]. And algo\u2010\nrithms for analysis are only going to get better.\nConsent and freedom of choice\nWe might assert that users voluntarily choose to use a service that tracks their activ\u2010\nity, and they have agreed to the terms of service and privacy policy, so they consent to\ndata collection. We might even claim that users are receiving a valuable service in\nreturn for the data they provide, and that the tracking is necessary in order to provide\nthe service. Undoubtedly, social networks, search engines, and various other free\nonline services are valuable to users\u2014but there are problems with this argument.\nUsers have little knowledge of what data they are feeding into our databases, or how\nit is retained and processed\u2014and most privacy policies do more to obscure than to\nilluminate. Without understanding what happens to their data, users cannot give any\nmeaningful consent. Often, data from one user also says things about other people\nwho are not users of the service and who have not agreed to any terms. The derived\ndatasets that we discussed in this part of the book\u2014in which data from the entire\nuser base may have been combined with behavioral tracking and external data sour\u2010\nces\u2014are precisely the kinds of data of which users cannot have any meaningful\nunderstanding.\nMoreover, data is extracted from users through a one-way process, not a relationship"}
{"561": "return: the relationship between the service and the user is very asymmetric and one-\nsided. The terms are set by the service, not by the user [99].\nFor a user who does not consent to surveillance, the only real alternative is simply not\nto use a service. But this choice is not free either: if a service is so popular that it is\n\u201cregarded by most people as essential for basic social participation\u201d [99], then it is not\nreasonable to expect people to opt out of this service\u2014using it is de facto mandatory.\nFor example, in most Western social communities, it has become the norm to carry a\nsmartphone, to use Facebook for socializing, and to use Google for finding informa\u2010\ntion. Especially when a service has network effects, there is a social cost to people\nchoosing not to use it.\nDeclining to use a service due to its tracking of users is only an option for the small\nnumber of people who are privileged enough to have the time and knowledge to\nunderstand its privacy policy, and who can afford to potentially miss out on social\nparticipation or professional opportunities that may have arisen if they had participa\u2010\nted in the service. For people in a less privileged position, there is no meaningful free\u2010\ndom of choice: surveillance becomes inescapable.\nPrivacy and use of data\nSometimes people claim that \u201cprivacy is dead\u201d on the grounds that some users are\nwilling to post all sorts of things about their lives to social media, sometimes mun\u2010\ndane and sometimes deeply personal. However, this claim is false and rests on a mis\u2010\nunderstanding of the word privacy.\nHaving privacy does not mean keeping everything secret; it means having the free\u2010\ndom to choose which things to reveal to whom, what to make public, and what to\nkeep secret. The right to privacy is a decision right: it enables each person to decide\nwhere they want to be on the spectrum between secrecy and transparency in each sit\u2010\nuation [99]. It is an important aspect of a person\u2019s freedom and autonomy.\nWhen data is extracted from people through surveillance infrastructure, privacy\nrights are not necessarily eroded, but rather transferred to the data collector. Compa\u2010\nnies that acquire data essentially say \u201ctrust us to do the right thing with your data,\u201d\nwhich means that the right to decide what to reveal and what to keep secret is trans\u2010\nferred from the individual to the company.\nThe companies in turn choose to keep much of the outcome of this surveillance\nsecret, because to reveal it would be perceived as creepy, and would harm their busi\u2010\nness model (which relies on knowing more about people than other companies do).\nIntimate information about users is only revealed indirectly, for example in the form"}
{"562": "Even if particular users cannot be personally reidentified from the bucket of people\ntargeted by a particular ad, they have lost their agency about the disclosure of some\nintimate information, such as whether they suffer from some illness. It is not the user\nwho decides what is revealed to whom on the basis of their personal preferences\u2014it\nis the company that exercises the privacy right with the goal of maximizing its profit.\nMany companies have a goal of not being perceived as creepy\u2014avoiding the question\nof how intrusive their data collection actually is, and instead focusing on managing\nuser perceptions. And even these perceptions are often managed poorly: for example,\nsomething may be factually correct, but if it triggers painful memories, the user may\nnot want to be reminded about it [100]. With any kind of data we should expect the\npossibility that it is wrong, undesirable, or inappropriate in some way, and we need to\nbuild mechanisms for handling those failures. Whether something is \u201cundesirable\u201d or\n\u201cinappropriate\u201d is of course down to human judgment; algorithms are oblivious to\nsuch notions unless we explicitly program them to respect human needs. As engi\u2010\nneers of these systems we must be humble, accepting and planning for such failings.\nPrivacy settings that allow a user of an online service to control which aspects of their\ndata other users can see are a starting point for handing back some control to users.\nHowever, regardless of the setting, the service itself still has unfettered access to the\ndata, and is free to use it in any way permitted by the privacy policy. Even if the ser\u2010\nvice promises not to sell the data to third parties, it usually grants itself unrestricted\nrights to process and analyze the data internally, often going much further than what\nis overtly visible to users.\nThis kind of large-scale transfer of privacy rights from individuals to corporations is\nhistorically unprecedented [99]. Surveillance has always existed, but it used to be\nexpensive and manual, not scalable and automated. Trust relationships have always\nexisted, for example between a patient and their doctor, or between a defendant and\ntheir attorney\u2014but in these cases the use of data has been strictly governed by ethical,\nlegal, and regulatory constraints. Internet services have made it much easier to amass\nhuge amounts of sensitive information without meaningful consent, and to use it at\nmassive scale without users understanding what is happening to their private data.\nData as assets and power\nSince behavioral data is a byproduct of users interacting with a service, it is some\u2010\ntimes called \u201cdata exhaust\u201d\u2014suggesting that the data is worthless waste material.\nViewed this way, behavioral and predictive analytics can be seen as a form of recy\u2010\ncling that extracts value from data that would have otherwise been thrown away.\nMore correct would be to view it the other way round: from an economic point of"}
{"563": "mation into the surveillance infrastructure [99]. The delightful human creativity and\nsocial relationships that often find expression in online services are cynically exploi\u2010\nted by the data extraction machine.\nThe assertion that personal data is a valuable asset is supported by the existence of\ndata brokers, a shady industry operating in secrecy, purchasing, aggregating, analyz\u2010\ning, inferring, and reselling intrusive personal data about people, mostly for market\u2010\ning purposes [90]. Startups are valued by their user numbers, by \u201ceyeballs\u201d\u2014i.e., by\ntheir surveillance capabilities.\nBecause the data is valuable, many people want it. Of course companies want it\u2014\nthat\u2019s why they collect it in the first place. But governments want to obtain it too: by\nmeans of secret deals, coercion, legal compulsion, or simply stealing it [101]. When a\ncompany goes bankrupt, the personal data it has collected is one of the assets that get\nsold. Moreover, the data is difficult to secure, so breaches happen disconcertingly\noften [102].\nThese observations have led critics to saying that data is not just an asset, but a \u201ctoxic\nasset\u201d [101], or at least \u201chazardous material\u201d [103]. Even if we think that we are capa\u2010\nble of preventing abuse of data, whenever we collect data, we need to balance the ben\u2010\nefits with the risk of it falling into the wrong hands: computer systems may be\ncompromised by criminals or hostile foreign intelligence services, data may be leaked\nby insiders, the company may fall into the hands of unscrupulous management that\ndoes not share our values, or the country may be taken over by a regime that has no\nqualms about compelling us to hand over the data.\nWhen collecting data, we need to consider not just today\u2019s political environment, but\nall possible future governments. There is no guarantee that every government elected\nin future will respect human rights and civil liberties, so \u201cit is poor civic hygiene to\ninstall technologies that could someday facilitate a police state\u201d [104].\n\u201cKnowledge is power,\u201d as the old adage goes. And furthermore, \u201cto scrutinize others\nwhile avoiding scrutiny oneself is one of the most important forms of power\u201d [105].\nThis is why totalitarian governments want surveillance: it gives them the power to\ncontrol the population. Although today\u2019s technology companies are not overtly seek\u2010\ning political power, the data and knowledge they have accumulated nevertheless gives\nthem a lot of power over our lives, much of which is surreptitious, outside of public\noversight [106].\nRemembering the Industrial Revolution\nData is the defining feature of the information age. The internet, data storage, pro\u2010"}
{"564": "The Industrial Revolution came about through major technological and agricultural\nadvances, and it brought sustained economic growth and significantly improved liv\u2010\ning standards in the long run. Yet it also came with major problems: pollution of the\nair (due to smoke and chemical processes) and the water (from industrial and human\nwaste) was dreadful. Factory owners lived in splendor, while urban workers often\nlived in very poor housing and worked long hours in harsh conditions. Child labor\nwas common, including dangerous and poorly paid work in mines.\nIt took a long time before safeguards were established, such as environmental protec\u2010\ntion regulations, safety protocols for workplaces, outlawing child labor, and health\ninspections for food. Undoubtedly the cost of doing business increased when facto\u2010\nries could no longer dump their waste into rivers, sell tainted foods, or exploit work\u2010\ners. But society as a whole benefited hugely, and few of us would want to return to a\ntime before those regulations [87].\nJust as the Industrial Revolution had a dark side that needed to be managed, our tran\u2010\nsition to the information age has major problems that we need to confront and solve.\nI believe that the collection and use of data is one of those problems. In the words of\nBruce Schneier [96]:\nData is the pollution problem of the information age, and protecting privacy is the\nenvironmental challenge. Almost all computers produce information. It stays around,\nfestering. How we deal with it\u2014how we contain it and how we dispose of it\u2014is central\nto the health of our information economy. Just as we look back today at the early deca\u2010\ndes of the industrial age and wonder how our ancestors could have ignored pollution\nin their rush to build an industrial world, our grandchildren will look back at us during\nthese early decades of the information age and judge us on how we addressed the chal\u2010\nlenge of data collection and misuse.\nWe should try to make them proud.\nLegislation and self-regulation\nData protection laws might be able to help preserve individuals\u2019 rights. For example,\nthe 1995 European Data Protection Directive states that personal data must be \u201ccol\u2010\nlected for specified, explicit and legitimate purposes and not further processed in a\nway incompatible with those purposes,\u201d and furthermore that data must be \u201cade\u2010\nquate, relevant and not excessive in relation to the purposes for which they are collec\u2010\nted\u201d [107].\nHowever, it is doubtful whether this legislation is effective in today\u2019s internet context\n[108]. These rules run directly counter to the philosophy of Big Data, which is to\nmaximize data collection, to combine it with other datasets, to experiment and to"}
{"565": "Companies that collect lots of data about people oppose regulation as being a burden\nand a hindrance to innovation. To some extent that opposition is justified. For exam\u2010\nple, when sharing medical data, there are clear risks to privacy, but there are also\npotential opportunities: how many deaths could be prevented if data analysis was\nable to help us achieve better diagnostics or find better treatments [110]? Over-\nregulation may prevent such breakthroughs. It is difficult to balance such potential\nopportunities with the risks [105].\nFundamentally, I think we need a culture shift in the tech industry with regard to\npersonal data. We should stop regarding users as metrics to be optimized, and\nremember that they are humans who deserve respect, dignity, and agency. We should\nself-regulate our data collection and processing practices in order to establish and\nmaintain the trust of the people who depend on our software [111]. And we should\ntake it upon ourselves to educate end users about how their data is used, rather than\nkeeping them in the dark.\nWe should allow each individual to maintain their privacy\u2014i.e., their control over\nown data\u2014and not steal that control from them through surveillance. Our individual\nright to control our data is like the natural environment of a national park: if we\ndon\u2019t explicitly protect and care for it, it will be destroyed. It will be the tragedy of the\ncommons, and we will all be worse off for it. Ubiquitous surveillance is not inevitable\n\u2014we are still able to stop it.\nHow exactly we might achieve this is an open question. To begin with, we should not\nretain data forever, but purge it as soon as it is no longer needed [111, 112]. Purging\ndata runs counter to the idea of immutability (see \u201cLimitations of immutability\u201d on\npage 463), but that issue can be solved. A promising approach I see is to enforce\naccess control through cryptographic protocols, rather than merely by policy [113,\n114]. Overall, culture and attitude changes will be necessary.\nSummary\nIn this chapter we discussed new approaches to designing data systems, and I\nincluded my personal opinions and speculations about the future. We started with\nthe observation that there is no one single tool that can efficiently serve all possible\nuse cases, and so applications necessarily need to compose several different pieces of\nsoftware to accomplish their goals. We discussed how to solve this data integration\nproblem by using batch processing and event streams to let data changes flow\nbetween different systems.\nIn this approach, certain systems are designated as systems of record, and other data"}
{"566": "problem in one area is prevented from spreading to unrelated parts of the system,\nincreasing the robustness and fault-tolerance of the system as a whole.\nExpressing dataflows as transformations from one dataset to another also helps\nevolve applications: if you want to change one of the processing steps, for example to\nchange the structure of an index or cache, you can just rerun the new transformation\ncode on the whole input dataset in order to rederive the output. Similarly, if some\u2010\nthing goes wrong, you can fix the code and reprocess the data in order to recover.\nThese processes are quite similar to what databases already do internally, so we recast\nthe idea of dataflow applications as unbundling the components of a database, and\nbuilding an application by composing these loosely coupled components.\nDerived state can be updated by observing changes in the underlying data. Moreover,\nthe derived state itself can further be observed by downstream consumers. We can\neven take this dataflow all the way through to the end-user device that is displaying\nthe data, and thus build user interfaces that dynamically update to reflect data\nchanges and continue to work offline.\nNext, we discussed how to ensure that all of this processing remains correct in the\npresence of faults. We saw that strong integrity guarantees can be implemented scala\u2010\nbly with asynchronous event processing, by using end-to-end operation identifiers to\nmake operations idempotent and by checking constraints asynchronously. Clients\ncan either wait until the check has passed, or go ahead without waiting but risk hav\u2010\ning to apologize about a constraint violation. This approach is much more scalable\nand robust than the traditional approach of using distributed transactions, and fits\nwith how many business processes work in practice.\nBy structuring applications around dataflow and checking constraints asynchro\u2010\nnously, we can avoid most coordination and create systems that maintain integrity\nbut still perform well, even in geographically distributed scenarios and in the pres\u2010\nence of faults. We then talked a little about using audits to verify the integrity of data\nand detect corruption.\nFinally, we took a step back and examined some ethical aspects of building data-\nintensive applications. We saw that although data can be used to do good, it can also\ndo significant harm: making justifying decisions that seriously affect people\u2019s lives\nand are difficult to appeal against, leading to discrimination and exploitation, nor\u2010\nmalizing surveillance, and exposing intimate information. We also run the risk of\ndata breaches, and we may find that a well-intentioned use of data has unintended\nconsequences.\nAs software and data are having such a large impact on the world, we engineers must"}
{"567": "References\n[1] Rachid Belaid: \u201cPostgres Full-Text Search is Good Enough!,\u201d rachbelaid.com, July\n13, 2015.\n[2] Philippe Ajoux, Nathan Bronson, Sanjeev Kumar, et al.: \u201cChallenges to Adopting\nStronger Consistency at Scale,\u201d at 15th USENIX Workshop on Hot Topics in Operat\u2010\ning Systems (HotOS), May 2015.\n[3] Pat Helland and Dave Campbell: \u201cBuilding on Quicksand,\u201d at 4th Biennial Con\u2010\nference on Innovative Data Systems Research (CIDR), January 2009.\n[4] Jessica Kerr: \u201cProvenance and Causality in Distributed Systems,\u201d blog.jessi\u2010\ntron.com, September 25, 2016.\n[5] Kostas Tzoumas: \u201cBatch Is a Special Case of Streaming,\u201d data-artisans.com, Sep\u2010\ntember 15, 2015.\n[6] Shinji Kim and Robert Blafford: \u201cStream Windowing Performance Analysis: Con\u2010\ncord and Spark Streaming,\u201d concord.io, July 6, 2016.\n[7] Jay Kreps: \u201cThe Log: What Every Software Engineer Should Know About Real-\nTime Data\u2019s Unifying Abstraction,\u201d engineering.linkedin.com, December 16, 2013.\n[8] Pat Helland: \u201cLife Beyond Distributed Transactions: An Apostate\u2019s Opinion,\u201d at\n3rd Biennial Conference on Innovative Data Systems Research (CIDR), January 2007.\n[9] \u201cGreat Western Railway (1835\u20131948),\u201d Network Rail Virtual Archive, network\u2010\nrail.co.uk.\n[10] Jacqueline Xu: \u201cOnline Migrations at Scale,\u201d stripe.com, February 2, 2017.\n[11] Molly Bartlett Dishman and Martin Fowler: \u201cAgile Architecture,\u201d at O\u2019Reilly\nSoftware Architecture Conference, March 2015.\n[12] Nathan Marz and James Warren: Big Data: Principles and Best Practices of Scala\u2010\nble Real-Time Data Systems. Manning, 2015. ISBN: 978-1-617-29034-3\n[13] Oscar Boykin, Sam Ritchie, Ian O\u2019Connell, and Jimmy Lin: \u201cSummingbird: A\nFramework for Integrating Batch and Online MapReduce Computations,\u201d at 40th\nInternational Conference on Very Large Data Bases (VLDB), September 2014.\n[14] Jay Kreps: \u201cQuestioning the Lambda Architecture,\u201d oreilly.com, July 2, 2014.\n[15] Raul Castro Fernandez, Peter Pietzuch, Jay Kreps, et al.: \u201cLiquid: Unifying Near\u2010\nline and Offline Big Data Integration,\u201d at 7th Biennial Conference on Innovative Data"}
{"568": "[16] Dennis M. Ritchie and Ken Thompson: \u201cThe UNIX Time-Sharing System,\u201d\nCommunications of the ACM, volume 17, number 7, pages 365\u2013375, July 1974. doi:\n10.1145/361011.361061\n[17] Eric A. Brewer and Joseph M. Hellerstein: \u201cCS262a: Advanced Topics in Com\u2010\nputer Systems,\u201d lecture notes, University of California, Berkeley, cs.berkeley.edu,\nAugust 2011.\n[18] Michael Stonebraker: \u201cThe Case for Polystores,\u201d wp.sigmod.org, July 13, 2015.\n[19] Jennie Duggan, Aaron J. Elmore, Michael Stonebraker, et al.: \u201cThe BigDAWG\nPolystore System,\u201d ACM SIGMOD Record, volume 44, number 2, pages 11\u201316, June\n2015. doi:10.1145/2814710.2814713\n[20] Patrycja Dybka: \u201cForeign Data Wrappers for PostgreSQL,\u201d vertabelo.com, March\n24, 2015.\n[21] David B. Lomet, Alan Fekete, Gerhard Weikum, and Mike Zwilling: \u201cUnbun\u2010\ndling Transaction Services in the Cloud,\u201d at 4th Biennial Conference on Innovative\nData Systems Research (CIDR), January 2009.\n[22] Martin Kleppmann and Jay Kreps: \u201cKafka, Samza and the Unix Philosophy of\nDistributed Data,\u201d IEEE Data Engineering Bulletin, volume 38, number 4, pages 4\u201314,\nDecember 2015.\n[23] John Hugg: \u201cWinning Now and in the Future: Where VoltDB Shines,\u201d\nvoltdb.com, March 23, 2016.\n[24] Frank McSherry, Derek G. Murray, Rebecca Isaacs, and Michael Isard: \u201cDiffer\u2010\nential Dataflow,\u201d at 6th Biennial Conference on Innovative Data Systems Research\n(CIDR), January 2013.\n[25] Derek G Murray, Frank McSherry, Rebecca Isaacs, et al.: \u201cNaiad: A Timely Data\u2010\nflow System,\u201d at 24th ACM Symposium on Operating Systems Principles (SOSP),\npages 439\u2013455, November 2013. doi:10.1145/2517349.2522738\n[26] Gwen Shapira: \u201cWe have a bunch of customers who are implementing \u2018database\ninside-out\u2019 concept and they all ask \u2018is anyone else doing it? are we crazy?\u2019\u201d twit\u2010\nter.com, July 28, 2016.\n[27] Martin Kleppmann: \u201cTurning the Database Inside-out with Apache Samza,\u201d at\nStrange Loop, September 2014.\n[28] Peter Van Roy and Seif Haridi: Concepts, Techniques, and Models of Computer\nProgramming. MIT Press, 2004. ISBN: 978-0-262-22069-9"}
{"569": "[30] Evan Czaplicki and Stephen Chong: \u201cAsynchronous Functional Reactive Pro\u2010\ngramming for GUIs,\u201d at 34th ACM SIGPLAN Conference on Programming Language\nDesign and Implementation (PLDI), June 2013. doi:10.1145/2491956.2462161\n[31] Engineer Bainomugisha, Andoni Lombide Carreton, Tom van Cutsem, Stijn\nMostinckx, and Wolfgang de Meuter: \u201cA Survey on Reactive Programming,\u201d ACM\nComputing Surveys, volume 45, number 4, pages 1\u201334, August 2013. doi:\n10.1145/2501654.2501666\n[32] Peter Alvaro, Neil Conway, Joseph M. Hellerstein, and William R. Marczak:\n\u201cConsistency Analysis in Bloom: A CALM and Collected Approach,\u201d at 5th Biennial\nConference on Innovative Data Systems Research (CIDR), January 2011.\n[33] Felienne Hermans: \u201cSpreadsheets Are Code,\u201d at Code Mesh, November 2015.\n[34] Dan Bricklin and Bob Frankston: \u201cVisiCalc: Information from Its Creators,\u201d\ndanbricklin.com.\n[35] D. Sculley, Gary Holt, Daniel Golovin, et al.: \u201cMachine Learning: The High-\nInterest Credit Card of Technical Debt,\u201d at NIPS Workshop on Software Engineering\nfor Machine Learning (SE4ML), December 2014.\n[36] Peter Bailis, Alan Fekete, Michael J Franklin, et al.: \u201cFeral Concurrency Control:\nAn Empirical Investigation of Modern Application Integrity,\u201d at ACM International\nConference on Management of Data (SIGMOD), June 2015. doi:\n10.1145/2723372.2737784\n[37] Guy Steele: \u201cRe: Need for Macros (Was Re: Icon),\u201d email to ll1-discuss mailing\nlist, people.csail.mit.edu, December 24, 2001.\n[38] David Gelernter: \u201cGenerative Communication in Linda,\u201d ACM Transactions on\nProgramming Languages and Systems (TOPLAS), volume 7, number 1, pages 80\u2013112,\nJanuary 1985. doi:10.1145/2363.2433\n[39] Patrick Th. Eugster, Pascal A. Felber, Rachid Guerraoui, and Anne-Marie Ker\u2010\nmarrec: \u201cThe Many Faces of Publish/Subscribe,\u201d ACM Computing Surveys, volume\n35, number 2, pages 114\u2013131, June 2003. doi:10.1145/857076.857078\n[40] Ben Stopford: \u201cMicroservices in a Streaming World,\u201d at QCon London, March\n2016.\n[41] Christian Posta: \u201cWhy Microservices Should Be Event Driven: Autonomy vs\nAuthority,\u201d blog.christianposta.com, May 27, 2016.\n[42] Alex Feyerke: \u201cSay Hello to Offline First,\u201d hood.ie, November 5, 2013."}
{"570": "29th European Conference on Object-Oriented Programming (ECOOP), July 2015.\ndoi:10.4230/LIPIcs.ECOOP.2015.568\n[44] Mark Soper: \u201cClearing Up React Data Management Confusion with Flux, Redux,\nand Relay,\u201d medium.com, December 3, 2015.\n[45] Eno Thereska, Damian Guy, Michael Noll, and Neha Narkhede: \u201cUnifying\nStream Processing and Interactive Queries in Apache Kafka,\u201d confluent.io, October\n26, 2016.\n[46] Frank McSherry: \u201cDataflow as Database,\u201d github.com, July 17, 2016.\n[47] Peter Alvaro: \u201cI See What You Mean,\u201d at Strange Loop, September 2015.\n[48] Nathan Marz: \u201cTrident: A High-Level Abstraction for Realtime Computation,\u201d\nblog.twitter.com, August 2, 2012.\n[49] Edi Bice: \u201cLow Latency Web Scale Fraud Prevention with Apache Samza, Kafka\nand Friends,\u201d at Merchant Risk Council MRC Vegas Conference, March 2016.\n[50] Charity Majors: \u201cThe Accidental DBA,\u201d charity.wtf, October 2, 2016.\n[51] Arthur J. Bernstein, Philip M. Lewis, and Shiyong Lu: \u201cSemantic Conditions for\nCorrectness at Different Isolation Levels,\u201d at 16th International Conference on Data\nEngineering (ICDE), February 2000. doi:10.1109/ICDE.2000.839387\n[52] Sudhir Jorwekar, Alan Fekete, Krithi Ramamritham, and S. Sudarshan: \u201cAuto\u2010\nmating the Detection of Snapshot Isolation Anomalies,\u201d at 33rd International Confer\u2010\nence on Very Large Data Bases (VLDB), September 2007.\n[53] Kyle Kingsbury: Jepsen blog post series, aphyr.com, 2013\u20132016.\n[54] Michael Jouravlev: \u201cRedirect After Post,\u201d theserverside.com, August 1, 2004.\n[55] Jerome H. Saltzer, David P. Reed, and David D. Clark: \u201cEnd-to-End Arguments\nin System Design,\u201d ACM Transactions on Computer Systems, volume 2, number 4,\npages 277\u2013288, November 1984. doi:10.1145/357401.357402\n[56] Peter Bailis, Alan Fekete, Michael J. Franklin, et al.: \u201cCoordination-Avoiding\nDatabase Systems,\u201d Proceedings of the VLDB Endowment, volume 8, number 3, pages\n185\u2013196, November 2014.\n[57] Alex Yarmula: \u201cStrong Consistency in Manhattan,\u201d blog.twitter.com, March 17,\n2016.\n[58] Douglas B Terry, Marvin M Theimer, Karin Petersen, et al.: \u201cManaging Update\nConflicts in Bayou, a Weakly Connected Replicated Storage System,\u201d at 15th ACM"}
{"571": "[59] Jim Gray: \u201cThe Transaction Concept: Virtues and Limitations,\u201d at 7th Interna\u2010\ntional Conference on Very Large Data Bases (VLDB), September 1981.\n[60] Hector Garcia-Molina and Kenneth Salem: \u201cSagas,\u201d at ACM International Con\u2010\nference on Management of Data (SIGMOD), May 1987. doi:10.1145/38713.38742\n[61] Pat Helland: \u201cMemories, Guesses, and Apologies,\u201d blogs.msdn.com, May 15,\n2007.\n[62] Yoongu Kim, Ross Daly, Jeremie Kim, et al.: \u201cFlipping Bits in Memory Without\nAccessing Them: An Experimental Study of DRAM Disturbance Errors,\u201d at 41st\nAnnual International Symposium on Computer Architecture (ISCA), June 2014. doi:\n10.1145/2678373.2665726\n[63] Mark Seaborn and Thomas Dullien: \u201cExploiting the DRAM Rowhammer Bug to\nGain Kernel Privileges,\u201d googleprojectzero.blogspot.co.uk, March 9, 2015.\n[64] Jim N. Gray and Catharine van Ingen: \u201cEmpirical Measurements of Disk Failure\nRates and Error Rates,\u201d Microsoft Research, MSR-TR-2005-166, December 2005.\n[65] Annamalai Gurusami and Daniel Price: \u201cBug #73170: Duplicates in Unique Sec\u2010\nondary Index Because of Fix of Bug#68021,\u201d bugs.mysql.com, July 2014.\n[66] Gary Fredericks: \u201cPostgres Serializability Bug,\u201d github.com, September 2015.\n[67] Xiao Chen: \u201cHDFS DataNode Scanners and Disk Checker Explained,\u201d blog.clou\u2010\ndera.com, December 20, 2016.\n[68] Jay Kreps: \u201cGetting Real About Distributed System Reliability,\u201d blog.empathy\u2010\nbox.com, March 19, 2012.\n[69] Martin Fowler: \u201cThe LMAX Architecture,\u201d martinfowler.com, July 12, 2011.\n[70] Sam Stokes: \u201cMove Fast with Confidence,\u201d blog.samstokes.co.uk, July 11, 2016.\n[71] \u201cSawtooth Lake Documentation,\u201d Intel Corporation, intelledger.github.io, 2016.\n[72] Richard Gendal Brown: \u201cIntroducing R3 Corda\u2122: A Distributed Ledger\nDesigned for Financial Services,\u201d gendal.me, April 5, 2016.\n[73] Trent McConaghy, Rodolphe Marques, Andreas M\u00fcller, et al.: \u201cBigchainDB: A\nScalable Blockchain Database,\u201d bigchaindb.com, June 8, 2016.\n[74] Ralph C. Merkle: \u201cA Digital Signature Based on a Conventional Encryption\nFunction,\u201d at CRYPTO \u201987, August 1987. doi:10.1007/3-540-48184-2_32\n[75] Ben Laurie: \u201cCertificate Transparency,\u201d ACM Queue, volume 12, number 8,"}
{"572": "[76] Mark D. Ryan: \u201cEnhanced Certificate Transparency and End-to-End Encrypted\nMail,\u201d at Network and Distributed System Security Symposium (NDSS), February\n2014. doi:10.14722/ndss.2014.23379\n[77] \u201cSoftware Engineering Code of Ethics and Professional Practice,\u201d Association for\nComputing Machinery, acm.org, 1999.\n[78] Fran\u00e7ois Chollet: \u201cSoftware development is starting to involve important ethical\nchoices,\u201d twitter.com, October 30, 2016.\n[79] Igor Perisic: \u201cMaking Hard Choices: The Quest for Ethics in Machine Learning,\u201d\nengineering.linkedin.com, November 2016.\n[80] John Naughton: \u201cAlgorithm Writers Need a Code of Conduct,\u201d theguar\u2010\ndian.com, December 6, 2015.\n[81] Logan Kugler: \u201cWhat Happens When Big Data Blunders?,\u201d Communications of\nthe ACM, volume 59, number 6, pages 15\u201316, June 2016. doi:10.1145/2911975\n[82] Bill Davidow: \u201cWelcome to Algorithmic Prison,\u201d theatlantic.com, February 20,\n2014.\n[83] Don Peck: \u201cThey\u2019re Watching You at Work,\u201d theatlantic.com, December 2013.\n[84] Leigh Alexander: \u201cIs an Algorithm Any Less Racist Than a Human?\u201d theguar\u2010\ndian.com, August 3, 2016.\n[85] Jesse Emspak: \u201cHow a Machine Learns Prejudice,\u201d scientificamerican.com,\nDecember 29, 2016.\n[86] Maciej Ceg\u0142owski: \u201cThe Moral Economy of Tech,\u201d idlewords.com, June 2016.\n[87] Cathy O\u2019Neil: Weapons of Math Destruction: How Big Data Increases Inequality\nand Threatens Democracy. Crown Publishing, 2016. ISBN: 978-0-553-41881-1\n[88] Julia Angwin: \u201cMake Algorithms Accountable,\u201d nytimes.com, August 1, 2016.\n[89] Bryce Goodman and Seth Flaxman: \u201cEuropean Union Regulations on Algorith\u2010\nmic Decision-Making and a \u2018Right to Explanation\u2019,\u201d arXiv:1606.08813, August 31,\n2016.\n[90] \u201cA Review of the Data Broker Industry: Collection, Use, and Sale of Consumer\nData for Marketing Purposes,\u201d Staff Report, United States Senate Committee on Com\u2010\nmerce, Science, and Transportation, commerce.senate.gov, December 2013.\n[91] Olivia Solon: \u201cFacebook\u2019s Failure: Did Fake News and Polarized Politics Get\nTrump Elected?\u201d theguardian.com, November 10, 2016."}
{"573": "[93] Daniel J. Bernstein: \u201cListening to a \u2018big data\u2019/\u2018data science\u2019 talk,\u201d twitter.com,\nMay 12, 2015.\n[94] Marc Andreessen: \u201cWhy Software Is Eating the World,\u201d The Wall Street Journal,\n20 August 2011.\n[95] J. M. Porup: \u201c\u2018Internet of Things\u2019 Security Is Hilariously Broken and Getting\nWorse,\u201d arstechnica.com, January 23, 2016.\n[96] Bruce Schneier: Data and Goliath: The Hidden Battles to Collect Your Data and\nControl Your World. W. W. Norton, 2015. ISBN: 978-0-393-35217-7\n[97] The Grugq: \u201cNothing to Hide,\u201d grugq.tumblr.com, April 15, 2016.\n[98] Tony Beltramelli: \u201cDeep-Spying: Spying Using Smartwatch and Deep Learning,\u201d\nMasters Thesis, IT University of Copenhagen, December 2015. Available at\narxiv.org/abs/1512.05616\n[99] Shoshana Zuboff: \u201cBig Other: Surveillance Capitalism and the Prospects of an\nInformation Civilization,\u201d Journal of Information Technology, volume 30, number 1,\npages 75\u201389, April 2015. doi:10.1057/jit.2015.5\n[100] Carina C. Zona: \u201cConsequences of an Insightful Algorithm,\u201d at GOTO Berlin,\nNovember 2016.\n[101] Bruce Schneier: \u201cData Is a Toxic Asset, So Why Not Throw It Out?,\u201d schne\u2010\nier.com, March 1, 2016.\n[102] John E. Dunn: \u201cThe UK\u2019s 15 Most Infamous Data Breaches,\u201d techworld.com,\nNovember 18, 2016.\n[103] Cory Scott: \u201cData is not toxic - which implies no benefit - but rather hazardous\nmaterial, where we must balance need vs. want,\u201d twitter.com, March 6, 2016.\n[104] Bruce Schneier: \u201cMission Creep: When Everything Is Terrorism,\u201d schneier.com,\nJuly 16, 2013.\n[105] Lena Ulbricht and Maximilian von Grafenstein: \u201cBig Data: Big Power Shifts?,\u201d\nInternet Policy Review, volume 5, number 1, March 2016. doi:10.14763/2016.1.406\n[106] Ellen P. Goodman and Julia Powles: \u201cFacebook and Google: Most Powerful and\nSecretive Empires We\u2019ve Ever Known,\u201d theguardian.com, September 28, 2016.\n[107] Directive 95/46/EC on the protection of individuals with regard to the process\u2010\ning of personal data and on the free movement of such data, Official Journal of the\nEuropean Communities No. L 281/31, eur-lex.europa.eu, November 1995."}
{"574": "[109] Michiel Rhoen: \u201cBeyond Consent: Improving Data Protection Through Con\u2010\nsumer Protection Law,\u201d Internet Policy Review, volume 5, number 1, March 2016. doi:\n10.14763/2016.1.404\n[110] Jessica Leber: \u201cYour Data Footprint Is Affecting Your Life in Ways You Can\u2019t\nEven Imagine,\u201d fastcoexist.com, March 15, 2016.\n[111] Maciej Ceg\u0142owski: \u201cHaunted by Data,\u201d idlewords.com, October 2015.\n[112] Sam Thielman: \u201cYou Are Not What You Read: Librarians Purge User Data to\nProtect Privacy,\u201d theguardian.com, January 13, 2016.\n[113] Conor Friedersdorf: \u201cEdward Snowden\u2019s Other Motive for Leaking,\u201d theatlan\u2010\ntic.com, May 13, 2014.\n[114] Phillip Rogaway: \u201cThe Moral Character of Cryptographic Work,\u201d Cryptology\nePrint 2015/1162, December 2015."}
{"575": "Glossary\nPlease note that the definitions in this glossary are short and sim\u2010\nple, intended to convey the core idea but not the full subtleties of a\nterm. For more detail, please follow the references into the main\ntext.\nasynchronous up with it. Also known as flow control. See\nNot waiting for something to complete \u201cMessaging Systems\u201d on page 441.\n(e.g., sending data over the network to\nbatch process\nanother node), and not making any\nA computation that takes some fixed (and\nassumptions about how long it is going to\nusually large) set of data as input and pro\u2010\ntake. See \u201cSynchronous Versus Asynchro\u2010\nduces some other data as output, without\nnous Replication\u201d on page 153, \u201cSynchro\u2010\nmodifying the input. See Chapter 10.\nnous Versus Asynchronous Networks\u201d on\npage 284, and \u201cSystem Model and Reality\u201d bounded\non page 306.\nHaving some known upper limit or size.\nUsed for example in the context of net\u2010\natomic\nwork delay (see \u201cTimeouts and Unboun\u2010\n1. In the context of concurrent operations:\nded Delays\u201d on page 281) and datasets\ndescribing an operation that appears to\n(see the introduction to Chapter 11).\ntake effect at a single point in time, so\nanother concurrent process can never Byzantine fault\nencounter the operation in a \u201chalf- A node that behaves incorrectly in some\nfinished\u201d state. See also isolation. arbitrary way, for example by sending\ncontradictory or malicious messages to\n2. In the context of transactions: grouping\nother nodes. See \u201cByzantine Faults\u201d on\ntogether a set of writes that must either all\npage 304.\nbe committed or all be rolled back, even if\nfaults occur. See \u201cAtomicity\u201d on page 223 cache\nand \u201cAtomic Commit and Two-Phase\nA component that remembers recently\nCommit (2PC)\u201d on page 354.\nused data in order to speed up future"}
{"576": "system that has a complete copy of the ized view. See \u201cSingle-Object and Multi-\ndata. Object Operations\u201d on page 228 and\n\u201cDeriving several views from the same\nCAP theorem\nevent log\u201d on page 461.\nA widely misunderstood theoretical result\nthat is not useful in practice. See \u201cThe derived data\nCAP theorem\u201d on page 336. A dataset that is created from some other\ndata through a repeatable process, which\ncausality\nyou could run again if necessary. Usually,\nThe dependency between events that ari\u2010\nderived data is needed to speed up a par\u2010\nses when one thing \u201chappens before\u201d\nticular kind of read access to the data.\nanother thing in a system. For example, a\nIndexes, caches, and materialized views\nlater event that is in response to an earlier\nare examples of derived data. See the\nevent, or builds upon an earlier event, or\nintroduction to Part III.\nshould be understood in the light of an\nearlier event. See \u201cThe \u201chappens-before\u201d deterministic\nrelationship and concurrency\u201d on page Describing a function that always pro\u2010\n186 and \u201cOrdering and Causality\u201d on page duces the same output if you give it the\n339. same input. This means it cannot depend\non random numbers, the time of day, net\u2010\nconsensus\nwork communication, or other unpredict\u2010\nA fundamental problem in distributed\nable things.\ncomputing, concerning getting several\nnodes to agree on something (for exam\u2010 distributed\nple, which node should be the leader for a Running on several nodes connected by a\ndatabase cluster). The problem is much network. Characterized by partial failures:\nharder than it seems at first glance. See some part of the system may be broken\n\u201cFault-Tolerant Consensus\u201d on page 364. while other parts are still working, and it\nis often impossible for the software to\ndata warehouse\nknow what exactly is broken. See \u201cFaults\nA database in which data from several dif\u2010\nand Partial Failures\u201d on page 274.\nferent OLTP systems has been combined\nand prepared to be used for analytics pur\u2010 durable\nposes. See \u201cData Warehousing\u201d on page Storing data in a way such that you\n91. believe it will not be lost, even if various\nfaults occur. See \u201cDurability\u201d on page 226.\ndeclarative\nDescribing the properties that something ETL\nshould have, but not the exact steps for Extract\u2013Transform\u2013Load. The process of\nhow to achieve it. In the context of quer\u2010 extracting data from a source database,\nies, a query optimizer takes a declarative transforming it into a form that is more\nquery and decides how it should best be suitable for analytic queries, and loading it\nexecuted. See \u201cQuery Languages for Data\u201d into a data warehouse or batch processing\non page 42. system. See \u201cData Warehousing\u201d on page\n91.\ndenormalize\nTo introduce some amount of redun\u2010 failover"}
{"577": "fault-tolerant index\nAble to recover automatically if some\u2010 A data structure that lets you efficiently\nthing goes wrong (e.g., if a machine search for all records that have a particu\u2010\ncrashes or a network link fails). See \u201cReli\u2010 lar value in a particular field. See \u201cData\nability\u201d on page 6. Structures That Power Your Database\u201d on\npage 70.\nflow control\nSee backpressure. isolation\nIn the context of transactions, describing\nfollower\nthe degree to which concurrently execut\u2010\nA replica that does not directly accept any\ning transactions can interfere with each\nwrites from clients, but only processes\nother. Serializable isolation provides the\ndata changes that it receives from a leader.\nstrongest guarantees, but weaker isolation\nAlso known as a secondary, slave, read\nlevels are also used. See \u201cIsolation\u201d on\nreplica, or hot standby. See \u201cLeaders and\npage 225.\nFollowers\u201d on page 152.\njoin\nfull-text search\nTo bring together records that have some\u2010\nSearching text by arbitrary keywords,\nthing in common. Most commonly used\noften with additional features such as\nin the case where one record has a refer\u2010\nmatching similarly spelled words or syno\u2010\nence to another (a foreign key, a docu\u2010\nnyms. A full-text index is a kind of secon\u2010\nment reference, an edge in a graph) and a\ndary index that supports such queries. See\nquery needs to get the record that the ref\u2010\n\u201cFull-text search and fuzzy indexes\u201d on\nerence points to. See \u201cMany-to-One and\npage 88.\nMany-to-Many Relationships\u201d on page 33\ngraph and \u201cReduce-Side Joins and Grouping\u201d on\npage 403.\nA data structure consisting of vertices\n(things that you can refer to, also known\nleader\nas nodes or entities) and edges (connec\u2010\nWhen data or a service is replicated across\ntions from one vertex to another, also\nseveral nodes, the leader is the designated\nknown as relationships or arcs). See\nreplica that is allowed to make changes. A\n\u201cGraph-Like Data Models\u201d on page 49.\nleader may be elected through some pro\u2010\nhash tocol, or manually chosen by an adminis\u2010\ntrator. Also known as the primary or\nA function that turns an input into a\nmaster. See \u201cLeaders and Followers\u201d on\nrandom-looking number. The same input\npage 152.\nalways returns the same number as out\u2010\nput. Two different inputs are very likely to\nlinearizable\nhave two different numbers as output,\nBehaving as if there was only a single copy\nalthough it is possible that two different\nof data in the system, which is updated by\ninputs produce the same output (this is\natomic operations. See \u201cLinearizability\u201d\ncalled a collision). See \u201cPartitioning by\non page 324.\nHash of Key\u201d on page 203.\nlocality\nidempotent\nA performance optimization: putting sev\u2010\nDescribing an operation that can be safely\neral pieces of data in the same place if they"}
{"578": "lock records. See \u201cTransaction Processing or\nA mechanism to ensure that only one Analytics?\u201d on page 90.\nthread, node, or transaction can access\nOLTP\nsomething, and anyone else who wants to\nOnline transaction processing. Access\naccess the same thing must wait until the\npattern characterized by fast queries that\nlock is released. See \u201cTwo-Phase Locking\nread or write a small number of records,\n(2PL)\u201d on page 257 and \u201cThe leader and\nusually indexed by key. See \u201cTransaction\nthe lock\u201d on page 301.\nProcessing or Analytics?\u201d on page 90.\nlog\npartitioning\nAn append-only file for storing data. A\nSplitting up a large dataset or computa\u2010\nwrite-ahead log is used to make a storage\ntion that is too big for a single machine\nengine resilient against crashes (see \u201cMak\u2010\ninto smaller parts and spreading them\ning B-trees reliable\u201d on page 82), a log-\nacross several machines. Also known as\nstructured storage engine uses logs as its\nsharding. See Chapter 6.\nprimary storage format (see \u201cSSTables\nand LSM-Trees\u201d on page 76), a replication percentile\nlog is used to copy writes from a leader to\nA way of measuring the distribution of\nfollowers (see \u201cLeaders and Followers\u201d on\nvalues by counting how many values are\npage 152), and an event log can represent\nabove or below some threshold. For\na data stream (see \u201cPartitioned Logs\u201d on\nexample, the 95th percentile response\npage 446).\ntime during some period is the time t such\nthat 95% of requests in that period com\u2010\nmaterialize\nplete in less than t, and 5% take longer\nTo perform a computation eagerly and\nthan t. See \u201cDescribing Performance\u201d on\nwrite out its result, as opposed to calculat\u2010\npage 13.\ning it on demand when requested. See\n\u201cAggregation: Data Cubes and Material\u2010 primary key\nized Views\u201d on page 101 and \u201cMaterializa\u2010\nA value (typically a number or a string)\ntion of Intermediate State\u201d on page 419.\nthat uniquely identifies a record. In many\napplications, primary keys are generated\nnode\nby the system when a record is created\nAn instance of some software running on\n(e.g., sequentially or randomly); they are\na computer, which communicates with\nnot usually set by users. See also secondary\nother nodes via a network in order to\nindex.\naccomplish some task.\nquorum\nnormalized\nThe minimum number of nodes that need\nStructured in such a way that there is no\nto vote on an operation before it can be\nredundancy or duplication. In a normal\u2010\nconsidered successful. See \u201cQuorums for\nized database, when some piece of data\nreading and writing\u201d on page 179.\nchanges, you only need to change it in one\nplace, not many copies in many different rebalance\nplaces. See \u201cMany-to-One and Many-to-\nTo move data or services from one node\nMany Relationships\u201d on page 33.\nto another in order to spread the load\nfairly. See \u201cRebalancing Partitions\u201d on\nOLAP"}
{"579": "accessible if a node becomes unreachable. skew in \u201cTimestamps for ordering events\u201d\nSee Chapter 5. on page 291.\nschema\nsplit brain\nA description of the structure of some\ndata, including its fields and datatypes. A scenario in which two nodes simultane\u2010\nWhether some data conforms to a schema ously believe themselves to be the leader,\ncan be checked at various points in the and which may cause system guarantees\ndata\u2019s lifetime (see \u201cSchema flexibility in to be violated. See \u201cHandling Node Out\u2010\nthe document model\u201d on page 39), and a ages\u201d on page 156 and \u201cThe Truth Is\nschema can change over time (see Chap\u2010 Defined by the Majority\u201d on page 300.\nter 4).\nstored procedure\nsecondary index A way of encoding the logic of a transac\u2010\nAn additional data structure that is main\u2010 tion such that it can be entirely executed\ntained alongside the primary data storage on a database server, without communi\u2010\nand which allows you to efficiently search cating back and forth with a client during\nfor records that match a certain kind of the transaction. See \u201cActual Serial Execu\u2010\ncondition. See \u201cOther Indexing Struc\u2010 tion\u201d on page 252.\ntures\u201d on page 85 and \u201cPartitioning and\nstream process\nSecondary Indexes\u201d on page 206.\nA continually running computation that\nserializable consumes a never-ending stream of events\nA guarantee that if several transactions as input, and derives some output from it.\nexecute concurrently, they behave the See Chapter 11.\nsame as if they had executed one at a time,\nsynchronous\nin some serial order. See \u201cSerializability\u201d\nThe opposite of asynchronous.\non page 251.\nsystem of record\nshared-nothing\nA system that holds the primary, authori\u2010\nAn architecture in which independent\ntative version of some data, also known as\nnodes\u2014each with their own CPUs, mem\u2010\nthe source of truth. Changes are first writ\u2010\nory, and disks\u2014are connected via a con\u2010\nten here, and other datasets may be\nventional network, in contrast to shared-\nderived from the system of record. See the\nmemory or shared-disk architectures. See\nintroduction to Part III.\nthe introduction to Part II.\ntimeout\nskew\nOne of the simplest ways of detecting a\n1. Imbalanced load across partitions, such\nfault, namely by observing the lack of a\nthat some partitions have lots of requests response within some amount of time.\nor data, and others have much less. Also However, it is impossible to know\nknown as hot spots. See \u201cSkewed Work\u2010 whether a timeout is due to a problem\nloads and Relieving Hot Spots\u201d on page with the remote node, or an issue in the\n205 and \u201cHandling skew\u201d on page 407. network. See \u201cTimeouts and Unbounded\n2. A timing anomaly that causes events to Delays\u201d on page 281.\nappear in an unexpected, nonsequential\ntotal order"}
{"580": "some things are incomparable (you can\u2010 transaction. See \u201cAtomic Commit and\nnot say which is greater or smaller) is Two-Phase Commit (2PC)\u201d on page 354.\ncalled a partial order. See \u201cThe causal\ntwo-phase locking (2PL)\norder is not a total order\u201d on page 341.\nAn algorithm for achieving serializable\ntransaction isolation that works by a transaction\nGrouping together several reads and acquiring a lock on all data it reads or\nwrites into a logical unit, in order to sim\u2010 writes, and holding the lock until the end\nplify error handling and concurrency of the transaction. See \u201cTwo-Phase Lock\u2010\nissues. See Chapter 7. ing (2PL)\u201d on page 257.\ntwo-phase commit (2PC) unbounded\nAn algorithm to ensure that several data\u2010 Not having any known upper limit or size.\nbase nodes either all commit or all abort a The opposite of bounded."}
{"581": "Index\nA in stream processes, 466\naborts (transactions), 222, 224 aggregation pipeline query language, 48\nin two-phase commit, 356 Agile, 22\nperformance of optimistic concurrency con\u2010 minimizing irreversibility, 414, 497\ntrol, 266 moving faster with confidence, 532\nretrying aborted transactions, 231 Unix philosophy, 394\nabstraction, 21, 27, 222, 266, 321 agreement, 365\naccess path (in network model), 37, 60 (see also consensus)\naccidental complexity, removing, 21 Airflow (workflow scheduler), 402\naccountability, 535 Ajax, 131\nACID properties (transactions), 90, 223 Akka (actor framework), 139\natomicity, 223, 228 algorithms\nconsistency, 224, 529 algorithm correctness, 308\ndurability, 226 B-trees, 79-83\nisolation, 225, 228 for distributed systems, 306\nacknowledgements (messaging), 445 hash indexes, 72-75\nactive/active replication (see multi-leader repli\u2010 mergesort, 76, 402, 405\ncation) red-black trees, 78\nactive/passive replication (see leader-based rep\u2010 SSTables and LSM-trees, 76-79\nlication) all-to-all replication topologies, 175\nActiveMQ (messaging), 137, 444 AllegroGraph (database), 50\ndistributed transaction support, 361 ALTER TABLE statement (SQL), 40, 111\nActiveRecord (object-relational mapper), 30, Amazon\n232 Dynamo (database), 177\nactor model, 138 Amazon Web Services (AWS), 8\n(see also message-passing) Kinesis Streams (messaging), 448\ncomparison to Pregel model, 425 network reliability, 279\ncomparison to stream processing, 468 postmortems, 9\nAdvanced Message Queuing Protocol (see RedShift (database), 93\nAMQP) S3 (object storage), 398"}
{"582": "of tail latency, 16, 207 Apache Storm (see Storm)\nwrite amplification, 84 Apache Tajo (see Tajo)\nAMQP (Advanced Message Queuing Protocol), Apache Tez (see Tez)\n444 Apache Thrift (see Thrift)\n(see also messaging systems) Apache ZooKeeper (see ZooKeeper)\ncomparison to log-based messaging, 448, Apama (stream analytics), 466\n451 append-only B-trees, 82, 242\nmessage ordering, 446 append-only files (see logs)\nanalytics, 90 Application Programming Interfaces (APIs), 5,\ncomparison to transaction processing, 91 27\ndata warehousing (see data warehousing) for batch processing, 403\nparallel query execution in MPP databases, for change streams, 456\n415 for distributed transactions, 361\npredictive (see predictive analytics) for graph processing, 425\nrelation to batch processing, 411 for services, 131-136\nschemas for, 93-95 (see also services)\nsnapshot isolation for queries, 238 evolvability, 136\nstream analytics, 466 RESTful, 133\nusing MapReduce, analysis of user activity SOAP, 133\nevents (example), 404 application state (see state)\nanti-caching (in-memory databases), 89 approximate search (see similarity search)\nanti-entropy, 178 archival storage, data from databases, 131\nApache ActiveMQ (see ActiveMQ) arcs (see edges)\nApache Avro (see Avro) arithmetic mean, 14\nApache Beam (see Beam) ASCII text, 119, 395\nApache BookKeeper (see BookKeeper) ASN.1 (schema language), 127\nApache Cassandra (see Cassandra) asynchronous networks, 278, 553\nApache CouchDB (see CouchDB) comparison to synchronous networks, 284\nApache Curator (see Curator) formal model, 307\nApache Drill (see Drill) asynchronous replication, 154, 553\nApache Flink (see Flink) conflict detection, 172\nApache Giraph (see Giraph) data loss on failover, 157\nApache Hadoop (see Hadoop) reads from asynchronous follower, 162\nApache HAWQ (see HAWQ) Asynchronous Transfer Mode (ATM), 285\nApache HBase (see HBase) atomic broadcast (see total order broadcast)\nApache Helix (see Helix) atomic clocks (caesium clocks), 294, 295\nApache Hive (see Hive) (see also clocks)\nApache Impala (see Impala) atomicity (concurrency), 553\nApache Jena (see Jena) atomic increment-and-get, 351\nApache Kafka (see Kafka) compare-and-set, 245, 327\nApache Lucene (see Lucene) (see also compare-and-set operations)\nApache MADlib (see MADlib) replicated operations, 246\nApache Mahout (see Mahout) write operations, 243\nApache Oozie (see Oozie) atomicity (transactions), 223, 228, 553\nApache Parquet (see Parquet) atomic commit, 353"}
{"583": "for multi-object transactions, 229 dataflow engines, 421-423\nfor single-object writes, 230 fault tolerance, 406, 414, 422, 442\nauditability, 528-533 for data integration, 494-498\ndesigning for, 531 graphs and iterative processing, 424-426\nself-auditing systems, 530 high-level APIs and languages, 403, 426-429\nthrough immutability, 460 log-based messaging and, 451\ntools for auditable data systems, 532 maintaining derived state, 495\navailability, 8 MapReduce and distributed filesystems,\n(see also fault tolerance) 397-413\nin CAP theorem, 337 (see also MapReduce)\nin service level agreements (SLAs), 15 measuring performance, 13, 390\nAvro (data format), 122-127 outputs, 411-413\ncode generation, 127 key-value stores, 412\ndynamically generated schemas, 126 search indexes, 411\nobject container files, 125, 131, 414 using Unix tools (example), 391-394\nreader determining writer\u2019s schema, 125 Bayou (database), 522\nschema evolution, 123 Beam (dataflow library), 498\nuse in Hadoop, 414 bias, 534\nawk (Unix tool), 391 big ball of mud, 20\nAWS (see Amazon Web Services) Bigtable data model, 41, 99\nAzure (see Microsoft) binary data encodings, 115-128\nAvro, 122-127\nB MessagePack, 116-117\nThrift and Protocol Buffers, 117-121\nB-trees (indexes), 79-83\nbinary encoding\nappend-only/copy-on-write variants, 82,\nbased on schemas, 127\n242\nby network drivers, 128\nbranching factor, 81\nbinary strings, lack of support in JSON and\ncomparison to LSM-trees, 83-85\nXML, 114\ncrash recovery, 82\nBinaryProtocol encoding (Thrift), 118\ngrowing by splitting a page, 81\nBitcask (storage engine), 72\noptimizations, 82\ncrash recovery, 74\nsimilarity to dynamic partitioning, 212\nBitcoin (cryptocurrency), 532\nbackpressure, 441, 553\nByzantine fault tolerance, 305\nin TCP, 282\nconcurrency bugs in exchanges, 233\nbackups\nbitmap indexes, 97\ndatabase snapshot for replication, 156\nblockchains, 532\nintegrity of, 530\nByzantine fault tolerance, 305\nsnapshot isolation for, 238\nblocking atomic commit, 359\nuse for ETL processes, 405\nBloom (programming language), 504\nbackward compatibility, 112\nBloom filter (algorithm), 79, 466\nBASE, contrast to ACID, 223\nBookKeeper (replicated log), 372\nbash shell (Unix), 70, 395, 503\nBottled Water (change data capture), 455\nbatch processing, 28, 389-431, 553\nbounded datasets, 430, 439, 553\ncombining with stream processing\n(see also batch processing)\nlambda architecture, 497"}
{"584": "brokerless messaging, 442 causality, 554\nBrubeck (metrics aggregator), 442 causal ordering, 339-343\nBTM (transaction coordinator), 356 linearizability and, 342\nbulk synchronous parallel (BSP) model, 425 total order consistent with, 344, 345\nbursty network traffic patterns, 285 consistency with, 344-347\nbusiness data processing, 28, 90, 390 consistent snapshots, 340\nbyte sequence, encoding data in, 112 happens-before relationship, 186\nByzantine faults, 304-306, 307, 553 in serializable transactions, 262-265\nByzantine fault-tolerant systems, 305, 532 mismatch with clocks, 292\nByzantine Generals Problem, 304 ordering events to capture, 493\nconsensus algorithms and, 366 violations of, 165, 176, 292, 340\nwith synchronized clocks, 294\nC CEP (see complex event processing)\ncertificate transparency, 532\ncaches, 89, 553\nchain replication, 155\nand materialized views, 101\nlinearizable reads, 351\nas derived data, 386, 499-504\nchange data capture, 160, 454\ndatabase as cache of transaction log, 460\nAPI support for change streams, 456\nin CPUs, 99, 338, 428\ncomparison to event sourcing, 457\ninvalidation and maintenance, 452, 467\nimplementing, 454\nlinearizability, 324\ninitial snapshot, 455\nCAP theorem, 336-338, 554\nlog compaction, 456\nCascading (batch processing), 419, 427\nchangelogs, 460\nhash joins, 409\nchange data capture, 454\nworkflows, 403\nfor operator state, 479\ncascading failures, 9, 214, 281\ngenerating with triggers, 455\nCascalog (batch processing), 60\nin stream joins, 474\nCassandra (database)\nlog compaction, 456\ncolumn-family data model, 41, 99\nmaintaining derived state, 452\ncompaction strategy, 79\nChaos Monkey, 7, 280\ncompound primary key, 204\ncheckpointing\ngossip protocol, 216\nin batch processors, 422, 426\nhash partitioning, 203-205\nin high-performance computing, 275\nlast-write-wins conflict resolution, 186, 292\nin stream processors, 477, 523\nleaderless replication, 177\nchronicle data model, 458\nlinearizability, lack of, 335\ncircuit-switched networks, 284\nlog-structured storage, 78\ncircular buffers, 450\nmulti-datacenter support, 184\ncircular replication topologies, 175\npartitioning scheme, 213\nclickstream data, analysis of, 404\nsecondary indexes, 207\nclients\nsloppy quorums, 184\ncalling services, 131\ncat (Unix tool), 391\npushing state changes to, 512\ncausal context, 191\nrequest routing, 214\n(see also causal dependencies)\nstateful and offline-capable, 170, 511\ncausal dependencies, 186-191\nclocks, 287-299\ncapturing, 191, 342, 494, 514"}
{"585": "skew, 291-294, 334 CompactProtocol encoding (Thrift), 119\nslewing, 289 compare-and-set operations, 245, 327\nsynchronization and accuracy, 289-291 implementing locks, 370\nsynchronization using GPS, 287, 290, 294, implementing uniqueness constraints, 331\n295 implementing with total order broadcast,\ntime-of-day versus monotonic clocks, 288 350\ntimestamping events, 471 relation to consensus, 335, 350, 352, 374\ncloud computing, 146, 275 relation to transactions, 230\nneed for service discovery, 372 compatibility, 112, 128\nnetwork glitches, 279 calling services, 136\nshared resources, 284 properties of encoding formats, 139\nsingle-machine reliability, 8 using databases, 129-131\nCloudera Impala (see Impala) using message-passing, 138\nclustered indexes, 86 compensating transactions, 355, 461, 526\nCODASYL model, 36 complex event processing (CEP), 465\n(see also network model) complexity\ncode generation distilling in theoretical models, 310\nwith Avro, 127 hiding using abstraction, 27\nwith Thrift and Protocol Buffers, 118 of software systems, managing, 20\nwith WSDL, 133 composing data systems (see unbundling data\u2010\ncollaborative editing bases)\nmulti-leader replication and, 170 compute-intensive applications, 3, 275\ncolumn families (Bigtable), 41, 99 concatenated indexes, 87\ncolumn-oriented storage, 95-101 in Cassandra, 204\ncolumn compression, 97 Concord (stream processor), 466\ndistinction between column families and, 99 concurrency\nin batch processors, 428 actor programming model, 138, 468\nParquet, 96, 131, 414 (see also message-passing)\nsort order in, 99-100 bugs from weak transaction isolation, 233\nvectorized processing, 99, 428 conflict resolution, 171, 174\nwriting to, 101 detecting concurrent writes, 184-191\ncomma-separated values (see CSV) dual writes, problems with, 453\ncommand query responsibility segregation happens-before relationship, 186\n(CQRS), 462 in replicated systems, 161-191, 324-338\ncommands (event sourcing), 459 lost updates, 243\ncommits (transactions), 222 multi-version concurrency control\natomic commit, 354-355 (MVCC), 239\n(see also atomicity; transactions) optimistic concurrency control, 261\nread committed isolation, 234 ordering of operations, 326, 341\nthree-phase commit (3PC), 359 reducing, through event logs, 351, 462, 507\ntwo-phase commit (2PC), 355-359 time and relativity, 187\ncommutative operations, 246 transaction isolation, 225\ncompaction write skew (transaction isolation), 246-251\nof changelogs, 456 conflict-free replicated datatypes (CRDTs), 174\n(see also log compaction) conflicts"}
{"586": "in log-based systems, 351, 521 crash recovery, 82\nin nonlinearizable systems, 343 enforcing constraints (see constraints)\nin serializable snapshot isolation (SSI), eventual, 162, 322\n264 (see also eventual consistency)\nin two-phase commit, 357, 364 in ACID transactions, 224, 529\nconflict resolution in CAP theorem, 337\nautomatic conflict resolution, 174 linearizability, 324-338\nby aborting transactions, 261 meanings of, 224\nby apologizing, 527 monotonic reads, 164-165\nconvergence, 172-174 of secondary indexes, 231, 241, 354, 491,\nin leaderless systems, 190 500\nlast write wins (LWW), 186, 292 ordering guarantees, 339-352\nusing atomic operations, 246 read-after-write, 162-164\nusing custom logic, 173 sequential, 351\ndetermining what is a conflict, 174, 522 strong (see linearizability)\nin multi-leader replication, 171-175 timeliness and integrity, 524\navoiding conflicts, 172 using quorums, 181, 334\nlost updates, 242-246 consistent hashing, 204\nmaterializing, 251 consistent prefix reads, 165\nrelation to operation ordering, 339 constraints (databases), 225, 248\nwrite skew (transaction isolation), 246-251 asynchronously checked, 526\ncongestion (networks) coordination avoidance, 527\navoidance, 282 ensuring idempotence, 519\nlimiting accuracy of clocks, 293 in log-based systems, 521-524\nqueueing delays, 282 across multiple partitions, 522\nconsensus, 321, 364-375, 554 in two-phase commit, 355, 357\nalgorithms, 366-368 relation to consensus, 374, 521\npreventing split brain, 367 relation to event ordering, 347\nsafety and liveness properties, 365 requiring linearizability, 330\nusing linearizable operations, 351 Consul (service discovery), 372\ncost of, 369 consumers (message streams), 137, 440\ndistributed transactions, 352-375 backpressure, 441\nin practice, 360-364 consumer offsets in logs, 449\ntwo-phase commit, 354-359 failures, 445, 449\nXA transactions, 361-364 fan-out, 11, 445, 448\nimpossibility of, 353 load balancing, 444, 448\nmembership and coordination services, not keeping up with producers, 441, 450,\n370-373 502\nrelation to compare-and-set, 335, 350, 352, context switches, 14, 297\n374 convergence (conflict resolution), 172-174, 322\nrelation to replication, 155, 349 coordination\nrelation to uniqueness constraints, 521 avoidance, 527\nconsistency, 224, 524 cross-datacenter, 168, 493\nacross different databases, 157, 452, 462, 492 cross-partition ordering, 256, 294, 348, 523\ncausal, 339-348, 493 services, 330, 370-373"}
{"587": "copy-on-write (B-trees), 82, 242 Crunch (batch processing), 419, 427\nCORBA (Common Object Request Broker hash joins, 409\nArchitecture), 134 sharded joins, 408\ncorrectness, 6 workflows, 403\nauditability, 528-533 cryptography\nByzantine fault tolerance, 305, 532 defense against attackers, 306\ndealing with partial failures, 274 end-to-end encryption and authentication,\nin log-based systems, 521-524 519, 543\nof algorithm within system model, 308 proving integrity of data, 532\nof compensating transactions, 355 CSS (Cascading Style Sheets), 44\nof consensus, 368 CSV (comma-separated values), 70, 114, 396\nof derived data, 497, 531 Curator (ZooKeeper recipes), 330, 371\nof immutable data, 461 curl (Unix tool), 135, 397\nof personal data, 535, 540 cursor stability, 243\nof time, 176, 289-295 Cypher (query language), 52\nof transactions, 225, 515, 529 comparison to SPARQL, 59\ntimeliness and integrity, 524-528\ncorruption of data D\ndetecting, 519, 530-533\ndata corruption (see corruption of data)\ndue to pathological memory access, 529\ndata cubes, 102\ndue to radiation, 305\ndata formats (see encoding)\ndue to split brain, 158, 302\ndata integration, 490-498, 543\ndue to weak transaction isolation, 233\nbatch and stream processing, 494-498\nformalization in consensus, 366\nlambda architecture, 497\nintegrity as absence of, 524\nmaintaining derived state, 495\nnetwork packets, 306\nreprocessing data, 496\non disks, 227\nunifying, 498\npreventing using write-ahead logs, 82\nby unbundling databases, 499-515\nrecovering from, 414, 460\ncomparison to federated databases, 501\nCouchbase (database)\ncombining tools by deriving data, 490-494\ndurability, 89\nderived data versus distributed transac\u2010\nhash partitioning, 203-204, 211\ntions, 492\nrebalancing, 213\nlimits of total ordering, 493\nrequest routing, 216\nordering events to capture causality, 493\nCouchDB (database)\nreasoning about dataflows, 491\nB-tree storage, 242\nneed for, 385\nchange feed, 456\ndata lakes, 415\ndocument data model, 31\ndata locality (see locality)\njoin support, 34\ndata models, 27-64\nMapReduce support, 46, 400\ngraph-like models, 49-63\nreplication, 170, 173\nDatalog language, 60-63\ncovering indexes, 86\nproperty graphs, 50\nCPUs\nRDF and triple-stores, 55-59\ncache coherence and memory barriers, 338\nquery languages, 42-48\ncaching and pipelining, 99, 428\nrelational model versus document model,"}
{"588": "concerns when designing, 5 observing derived state, 509-515\nfuture of, 489-544 datacenters\ncorrectness, constraints, and integrity, geographically distributed, 145, 164, 278,\n515-533 493\ndata integration, 490-498 multi-tenancy and shared resources, 284\nunbundling databases, 499-515 network architecture, 276\nheterogeneous, keeping in sync, 452 network faults, 279\nmaintainability, 18-22 replication across multiple, 169\npossible faults in, 221 leaderless replication, 184\nreliability, 6-10 multi-leader replication, 168, 335\nhardware faults, 7 dataflow, 128-139, 504-509\nhuman errors, 9 correctness of dataflow systems, 525\nimportance of, 10 differential, 504\nsoftware errors, 8 message-passing, 136-139\nscalability, 10-18 reasoning about, 491\nunreliable clocks, 287-299 through databases, 129\ndata warehousing, 91-95, 554 through services, 131-136\ncomparison to data lakes, 415 dataflow engines, 421-423\nETL (extract-transform-load), 92, 416, 452 comparison to stream processing, 464\nkeeping data systems in sync, 452 directed acyclic graphs (DAG), 424\nschema design, 93 partitioning, approach to, 429\nslowly changing dimension (SCD), 476 support for declarative queries, 427\ndata-intensive applications, 3 Datalog (query language), 60-63\ndatabase triggers (see triggers) datatypes\ndatabase-internal distributed transactions, 360, binary strings in XML and JSON, 114\n364, 477 conflict-free, 174\ndatabases in Avro encodings, 122\narchival storage, 131 in Thrift and Protocol Buffers, 121\ncomparison of message brokers to, 443 numbers in XML and JSON, 114\ndataflow through, 129 Datomic (database)\nend-to-end argument for, 519-520 B-tree storage, 242\nchecking integrity, 531 data model, 50, 57\ninside-out, 504 Datalog query language, 60\n(see also unbundling databases) excision (deleting data), 463\noutput from batch workflows, 412 languages for transactions, 255\nrelation to event streams, 451-464 serial execution of transactions, 253\n(see also changelogs) deadlocks\nAPI support for change streams, 456, detection, in two-phase commit (2PC), 364\n506 in two-phase locking (2PL), 258\nchange data capture, 454-457 Debezium (change data capture), 455\nevent sourcing, 457-459 declarative languages, 42, 554\nkeeping systems in sync, 452-453 Bloom, 504\nphilosophy of immutable events, CSS and XSL, 44\n459-464 Cypher, 52\nunbundling, 499-515 Datalog, 60"}
{"589": "delays issues with failover, 157\nbounded network delays, 285 limitations of distributed transactions, 363\nbounded process pauses, 298 multi-datacenter, 169, 335\nunbounded network delays, 282 network problems, 277-286\nunbounded process pauses, 296 quorums, relying on, 301\ndeleting data, 463 reasons for using, 145, 151\ndenormalization (data representation), 34, 554 synchronized clocks, relying on, 291-295\ncosts, 39 system models, 306-310\nin derived data systems, 386 use of clocks and time, 287\nmaterialized views, 101 distributed transactions (see transactions)\nupdating derived data, 228, 231, 490 Django (web framework), 232\nversus normalization, 462 DNS (Domain Name System), 216, 372\nderived data, 386, 439, 554 Docker (container manager), 506\nfrom change data capture, 454 document data model, 30-42\nin event sourcing, 458-458 comparison to relational model, 38-42\nmaintaining derived state through logs, document references, 38, 403\n452-457, 459-463 document-oriented databases, 31\nobserving, by subscribing to streams, 512 many-to-many relationships and joins, 36\noutputs of batch and stream processing, 495 multi-object transactions, need for, 231\nthrough application code, 505 versus relational model\nversus distributed transactions, 492 convergence of models, 41\ndeterministic operations, 255, 274, 554 data locality, 41\naccidental nondeterminism, 423 document-partitioned indexes, 206, 217, 411\nand fault tolerance, 423, 426 domain-driven design (DDD), 457\nand idempotence, 478, 492 DRBD (Distributed Replicated Block Device),\ncomputing derived data, 495, 526, 531 153\nin state machine replication, 349, 452, 458 drift (clocks), 289\njoins, 476 Drill (query engine), 93\nDevOps, 394 Druid (database), 461\ndifferential dataflow, 504 Dryad (dataflow engine), 421\ndimension tables, 94 dual writes, problems with, 452, 507\ndimensional modeling (see star schemas) duplicates, suppression of, 517\ndirected acyclic graphs (DAGs), 424 (see also idempotence)\ndirty reads (transaction isolation), 234 using a unique ID, 518, 522\ndirty writes (transaction isolation), 235 durability (transactions), 226, 554\ndiscrimination, 534 duration (time), 287\ndisks (see hard disks) measurement with monotonic clocks, 288\ndistributed actor frameworks, 138 dynamic partitioning, 212\ndistributed filesystems, 398-399 dynamically typed languages\ndecoupling from query engines, 417 analogy to schema-on-read, 40\nindiscriminately dumping data into, 415 code generation and, 127\nuse by MapReduce, 402 Dynamo-style databases (see leaderless replica\u2010\ndistributed systems, 273-312, 554 tion)\nByzantine faults, 304-306\ncloud versus supercomputing, 275 E"}
{"590": "(see also exactly-once semantics) Etherpad (collaborative editor), 170\npreservation of integrity, 525 ethics, 533-543\nelastic systems, 17 code of ethics and professional practice, 533\nElasticsearch (search server) legislation and self-regulation, 542\ndocument-partitioned indexes, 207 predictive analytics, 533-536\npartition rebalancing, 211 amplifying bias, 534\npercolator (stream search), 467 feedback loops, 536\nusage example, 4 privacy and tracking, 536-543\nuse of Lucene, 79 consent and freedom of choice, 538\nElephantDB (database), 413 data as assets and power, 540\nElm (programming language), 504, 512 meaning of privacy, 539\nencodings (data formats), 111-128 surveillance, 537\nAvro, 122-127 respect, dignity, and agency, 543, 544\nbinary variants of JSON and XML, 115 unintended consequences, 533, 536\ncompatibility, 112 ETL (extract-transform-load), 92, 405, 452, 554\ncalling services, 136 use of Hadoop for, 416\nusing databases, 129-131 event sourcing, 457-459\nusing message-passing, 138 commands and events, 459\ndefined, 113 comparison to change data capture, 457\nJSON, XML, and CSV, 114 comparison to lambda architecture, 497\nlanguage-specific formats, 113 deriving current state from event log, 458\nmerits of schemas, 127 immutability and auditability, 459, 531\nrepresentations of data, 112 large, reliable data systems, 519, 526\nThrift and Protocol Buffers, 117-121 Event Store (database), 458\nend-to-end argument, 277, 519-520 event streams (see streams)\nchecking integrity, 531 events, 440\npublish/subscribe streams, 512 deciding on total order of, 493\nenrichment (stream), 473 deriving views from event log, 461\nEnterprise JavaBeans (EJB), 134 difference to commands, 459\nentities (see vertices) event time versus processing time, 469, 477,\nepoch (consensus algorithms), 368 498\nepoch (Unix timestamps), 288 immutable, advantages of, 460, 531\nequi-joins, 403 ordering to capture causality, 493\nerasure coding (error correction), 398 reads as, 513\nErlang OTP (actor framework), 139 stragglers, 470, 498\nerror handling timestamp of, in stream processing, 471\nfor network faults, 280 EventSource (browser API), 512\nin transactions, 231 eventual consistency, 152, 162, 308, 322\nerror-correcting codes, 277, 398 (see also conflicts)\nEsper (CEP engine), 466 and perpetual inconsistency, 525\netcd (coordination service), 370-373 evolvability, 21, 111\nlinearizable operations, 333 calling services, 136\nlocks and leader election, 330 graph-structured data, 52\nquorum reads, 351 of databases, 40, 129-131, 461, 497\nservice discovery, 372 of message-passing, 138"}
{"591": "schema-on-read, 39, 111, 128 of distributed transactions, 362-364\nexactly-once semantics, 360, 476, 516 transaction atomicity, 223, 354-361\nparity with batch processors, 498 faults, 6\npreservation of integrity, 525 Byzantine faults, 304-306\nexclusive mode (locks), 258 failures versus, 7\neXtended Architecture transactions (see XA handled by transactions, 221\ntransactions) handling in supercomputers and cloud\nextract-transform-load (see ETL) computing, 275\nhardware, 7\nF in batch processing versus distributed data\u2010\nbases, 417\nFacebook\nin distributed systems, 274-277\nPresto (query engine), 93\nintroducing deliberately, 7, 280\nReact, Flux, and Redux (user interface libra\u2010\nnetwork faults, 279-281\nries), 512\nasymmetric faults, 300\nsocial graphs, 49\ndetecting, 280\nWormhole (change data capture), 455\ntolerance of, in multi-leader replication,\nfact tables, 93\n169\nfailover, 157, 554\nsoftware errors, 8\n(see also leader-based replication)\ntolerating (see fault tolerance)\nin leaderless replication, absence of, 178\nfederated databases, 501\nleader election, 301, 348, 352\nfence (CPU instruction), 338\npotential problems, 157\nfencing (preventing split brain), 158, 302-304\nfailures\ngenerating fencing tokens, 349, 370\namplification by distributed transactions,\nproperties of fencing tokens, 308\n364, 495\nstream processors writing to databases, 478,\nfailure detection, 280\n517\nautomatic rebalancing causing cascading\nFibre Channel (networks), 398\nfailures, 214\nfield tags (Thrift and Protocol Buffers), 119-121\nperfect failure detectors, 359\nfile descriptors (Unix), 395\ntimeouts and unbounded delays, 282,\nfinancial data, 460\n284\nFirebase (database), 456\nusing ZooKeeper, 371\nFlink (processing framework), 421-423\nfaults versus, 7\ndataflow APIs, 427\npartial failures in distributed systems,\nfault tolerance, 422, 477, 479\n275-277, 310\nGelly API (graph processing), 425\nfan-out (messaging systems), 11, 445\nintegration of batch and stream processing,\nfault tolerance, 6-10, 555\n495, 498\nabstractions for, 321\nmachine learning, 428\nformalization in consensus, 365-369\nquery optimizer, 427\nuse of replication, 367\nstream processing, 466\nhuman fault tolerance, 414\nflow control, 282, 441, 555\nin batch processing, 406, 414, 422, 425\nFLP result (on consensus), 353\nin log-based systems, 520, 524-526\nFlumeJava (dataflow library), 403, 427\nin stream processing, 476-479\nfollowers, 152, 555\natomic commit, 477"}
{"592": "Fossil (version control system), 463 (see also MapReduce)\nshunning (deleting data), 463 building search indexes, 411\nFoundationDB (database) task preemption, 418\nserializable transactions, 261, 265, 364 Pregel (graph processing), 425\nfractal trees, 83 Spanner (see Spanner)\nfull table scans, 403 TrueTime (clock API), 294\nfull-text search, 555 gossip protocol, 216\nand fuzzy indexes, 88 government use of data, 541\nbuilding search indexes, 411 GPS (Global Positioning System)\nLucene storage engine, 79 use for clock synchronization, 287, 290, 294,\nfunctional reactive programming (FRP), 504 295\nfunctional requirements, 22 GraphChi (graph processing), 426\nfutures (asynchronous operations), 135 graphs, 555\nfuzzy search (see similarity search) as data models, 49-63\nexample of graph-structured data, 49\nG property graphs, 50\nRDF and triple-stores, 55-59\ngarbage collection\nversus the network model, 60\nimmutability and, 463\nprocessing and analysis, 424-426\nprocess pauses for, 14, 296-299, 301\nfault tolerance, 425\n(see also process pauses)\nPregel processing model, 425\ngenome analysis, 63, 429\nquery languages\ngeographically distributed datacenters, 145,\nCypher, 52\n164, 278, 493\nDatalog, 60-63\ngeospatial indexes, 87\nrecursive SQL queries, 53\nGiraph (graph processing), 425\nSPARQL, 59-59\nGit (version control system), 174, 342, 463\nGremlin (graph query language), 50\nGitHub, postmortems, 157, 158, 309\ngrep (Unix tool), 392\nglobal indexes (see term-partitioned indexes)\nGROUP BY clause (SQL), 406\nGlusterFS (distributed filesystem), 398\ngrouping records in MapReduce, 406\nGNU Coreutils (Linux), 394\nhandling skew, 407\nGoldenGate (change data capture), 161, 170,\n455\nH\n(see also Oracle)\nGoogle Hadoop (data infrastructure)\nBigtable (database) comparison to distributed databases, 390\ndata model (see Bigtable data model) comparison to MPP databases, 414-418\npartitioning scheme, 199, 202 comparison to Unix, 413-414, 499\nstorage layout, 78 diverse processing models in ecosystem, 417\nChubby (lock service), 370 HDFS distributed filesystem (see HDFS)\nCloud Dataflow (stream processor), 466, higher-level tools, 403\n477, 498 join algorithms, 403-410\n(see also Beam) (see also MapReduce)\nCloud Pub/Sub (messaging), 444, 448 MapReduce (see MapReduce)\nDocs (collaborative editor), 170 YARN (see YARN)\nDremel (query engine), 93, 96 happens-before relationship, 340"}
{"593": "detecting corruption, 519, 530 high-performance computing (HPC), 275\nfaults in, 7, 227 hinted handoff, 183\nsequential write throughput, 75, 450 histograms, 16\nhardware faults, 7 Hive (query engine), 419, 427\nhash indexes, 72-75 for data warehouses, 93\nbroadcast hash joins, 409 HCatalog and metastore, 410\npartitioned hash joins, 409 map-side joins, 409\nhash partitioning, 203-205, 217 query optimizer, 427\nconsistent hashing, 204 skewed joins, 408\nproblems with hash mod N, 210 workflows, 403\nrange queries, 204 Hollerith machines, 390\nsuitable hash functions, 203 hopping windows (stream processing), 472\nwith fixed number of partitions, 210 (see also windows)\nHAWQ (database), 428 horizontal scaling (see scaling out)\nHBase (database) HornetQ (messaging), 137, 444\nbug due to lack of fencing, 302 distributed transaction support, 361\nbulk loading, 413 hot spots, 201\ncolumn-family data model, 41, 99 due to celebrities, 205\ndynamic partitioning, 212 for time-series data, 203\nkey-range partitioning, 202 in batch processing, 407\nlog-structured storage, 78 relieving, 205\nrequest routing, 216 hot standbys (see leader-based replication)\nsize-tiered compaction, 79 HTTP, use in APIs (see services)\nuse of HDFS, 417 human errors, 9, 279, 414\nuse of ZooKeeper, 370 HyperDex (database), 88\nHDFS (Hadoop Distributed File System), HyperLogLog (algorithm), 466\n398-399\n(see also distributed filesystems) I\nchecking data integrity, 530\nI/O operations, waiting for, 297\ndecoupling from query engines, 417\nIBM\nindiscriminately dumping data into, 415\nDB2 (database)\nmetadata about datasets, 410\ndistributed transaction support, 361\nNameNode, 398\nrecursive query support, 54\nuse by Flink, 479\nserializable isolation, 242, 257\nuse by HBase, 212\nXML and JSON support, 30, 42\nuse by MapReduce, 402\nelectromechanical card-sorting machines,\nHdrHistogram (numerical library), 16\n390\nhead (Unix tool), 392\nIMS (database), 36\nhead vertex (property graphs), 51\nimperative query APIs, 46\nhead-of-line blocking, 15\nInfoSphere Streams (CEP engine), 466\nheap files (databases), 86\nMQ (messaging), 444\nHelix (cluster manager), 216\ndistributed transaction support, 361\nheterogeneous distributed transactions, 360,\nSystem R (database), 222\n364\nWebSphere (messaging), 137\nheuristic decisions (in 2PC), 363\nidempotence, 134, 478, 555"}
{"594": "deriving state from event log, 459-464 B-trees, 79-83\nfor crash recovery, 75 building in batch processes, 411\nin B-trees, 82, 242 clustered, 86\nin event sourcing, 457 comparison of B-trees and LSM-trees, 83-85\ninputs to Unix commands, 397 concatenated, 87\nlimitations of, 463 covering (with included columns), 86\nImpala (query engine) creating, 500\nfor data warehouses, 93 full-text search, 88\nhash joins, 409 geospatial, 87\nnative code generation, 428 hash, 72-75\nuse of HDFS, 417 index-range locking, 260\nimpedance mismatch, 29 multi-column, 87\nimperative languages, 42 partitioning and secondary indexes,\nsetting element styles (example), 45 206-209, 217\nin doubt (transaction status), 358 secondary, 85\nholding locks, 362 (see also secondary indexes)\norphaned transactions, 363 problems with dual writes, 452, 491\nin-memory databases, 88 SSTables and LSM-trees, 76-79\ndurability, 227 updating when data changes, 452, 467\nserial transaction execution, 253 Industrial Revolution, 541\nincidents InfiniBand (networks), 285\ncascading failures, 9 InfiniteGraph (database), 50\ncrashes due to leap seconds, 290 InnoDB (storage engine)\ndata corruption and financial losses due to clustered index on primary key, 86\nconcurrency bugs, 233 not preventing lost updates, 245\ndata corruption on hard disks, 227 preventing write skew, 248, 257\ndata loss due to last-write-wins, 173, 292 serializable isolation, 257\ndata on disks unreadable, 309 snapshot isolation support, 239\ndeleted items reappearing, 174 inside-out databases, 504\ndisclosure of sensitive data due to primary (see also unbundling databases)\nkey reuse, 157 integrating different data systems (see data\nerrors in transaction serializability, 529 integration)\ngigabit network interface with 1 Kb/s integrity, 524\nthroughput, 311 coordination-avoiding data systems, 528\nnetwork faults, 279 correctness of dataflow systems, 525\nnetwork interface dropping only inbound in consensus formalization, 365\npackets, 279 integrity checks, 530\nnetwork partitions and whole-datacenter (see also auditing)\nfailures, 275 end-to-end, 519, 531\npoor handling of network faults, 280 use of snapshot isolation, 238\nsending message to ex-partner, 494 maintaining despite software bugs, 529\nsharks biting undersea cables, 279 Interface Definition Language (IDL), 117, 122\nsplit brain due to 1-minute packet delay, intermediate state, materialization of, 420-423\n158, 279 internet services, systems for implementing,\nvibrations in server rack, 14 275"}
{"595": "unreliability of, 277 MapReduce reduce-side joins, 403-408\nISDN (Integrated Services Digital Network), handling skew, 407\n284 sort-merge joins, 405\nisolation (in transactions), 225, 228, 555 parallel execution of, 415\ncorrectness and, 515 secondary indexes and, 85\nfor single-object writes, 230 stream joins, 472-476\nserializability, 251-266 stream-stream join, 473\nactual serial execution, 252-256 stream-table join, 473\nserializable snapshot isolation (SSI), table-table join, 474\n261-266 time-dependence of, 475\ntwo-phase locking (2PL), 257-261 support in document databases, 42\nviolating, 228 JOTM (transaction coordinator), 356\nweak isolation levels, 233-251 JSON\npreventing lost updates, 242-246 Avro schema representation, 122\nread committed, 234-237 binary variants, 115\nsnapshot isolation, 237-242 for application data, issues with, 114\niterative processing, 424-426 in relational databases, 30, 42\nrepresenting a r\u00e9sum\u00e9 (example), 31\nJ Juttle (query language), 504\nJava Database Connectivity (JDBC)\nK\ndistributed transaction support, 361\nnetwork drivers, 128 k-nearest neighbors, 429\nJava Enterprise Edition (EE), 134, 356, 361 Kafka (messaging), 137, 448\nJava Message Service (JMS), 444 Kafka Connect (database integration), 457,\n(see also messaging systems) 461\ncomparison to log-based messaging, 448, Kafka Streams (stream processor), 466, 467\n451 fault tolerance, 479\ndistributed transaction support, 361 leader-based replication, 153\nmessage ordering, 446 log compaction, 456, 467\nJava Transaction API (JTA), 355, 361 message offsets, 447, 478\nJava Virtual Machine (JVM) request routing, 216\nbytecode generation, 428 transaction support, 477\ngarbage collection pauses, 296 usage example, 4\nprocess reuse in batch processors, 422 Ketama (partitioning library), 213\nJavaScript key-value stores, 70\nin MapReduce querying, 46 as batch process output, 412\nsetting element styles (example), 45 hash indexes, 72-75\nuse in advanced queries, 48 in-memory, 89\nJena (RDF framework), 57 partitioning, 201-205\nJepsen (fault tolerance testing), 515 by hash of key, 203, 217\njitter (network delay), 284 by key range, 202, 217\njoins, 555 dynamic partitioning, 212\nby index lookup, 403 skew and hot spots, 205\nexpressing as relational operators, 427 Kryo (Java), 113\nin relational and document databases, 34 Kubernetes (cluster manager), 418, 506"}
{"596": "Large Hadron Collider (LHC), 64 need for fencing, 302\nlast write wins (LWW), 173, 334 ledgers, 460\ndiscarding concurrent writes, 186 distributed ledger technologies, 532\nproblems with, 292 legacy systems, maintenance of, 18\nprone to lost updates, 246 less (Unix tool), 397\nlate binding, 396 LevelDB (storage engine), 78\nlatency leveled compaction, 79\ninstability under two-phase locking, 259 Levenshtein automata, 88\nnetwork latency and resource utilization, limping (partial failure), 311\n286 linearizability, 324-338, 555\nresponse time versus, 14 cost of, 335-338\ntail latency, 15, 207 CAP theorem, 336\nleader-based replication, 152-161 memory on multi-core CPUs, 338\n(see also replication) definition, 325-329\nfailover, 157, 301 implementing with total order broadcast,\nhandling node outages, 156 350\nimplementation of replication logs in ZooKeeper, 370\nchange data capture, 454-457 of derived data systems, 492, 524\n(see also changelogs) avoiding coordination, 527\nstatement-based, 158 of different replication methods, 332-335\ntrigger-based replication, 161 using quorums, 334\nwrite-ahead log (WAL) shipping, 159 relying on, 330-332\nlinearizability of operations, 333 constraints and uniqueness, 330\nlocking and leader election, 330 cross-channel timing dependencies, 331\nlog sequence number, 156, 449 locking and leader election, 330\nread-scaling architecture, 161 stronger than causal consistency, 342\nrelation to consensus, 367 using to implement total order broadcast,\nsetting up new followers, 155 351\nsynchronous versus asynchronous, 153-155 versus serializability, 329\nleaderless replication, 177-191 LinkedIn\n(see also replication) Azkaban (workflow scheduler), 402\ndetecting concurrent writes, 184-191 Databus (change data capture), 161, 455\ncapturing happens-before relationship, Espresso (database), 31, 126, 130, 153, 216\n187 Helix (cluster manager) (see Helix)\nhappens-before relationship and concur\u2010 profile (example), 30\nrency, 186 reference to company entity (example), 34\nlast write wins, 186 Rest.li (RPC framework), 135\nmerging concurrently written values, Voldemort (database) (see Voldemort)\n190 Linux, leap second bug, 8, 290\nversion vectors, 191 liveness properties, 308\nmulti-datacenter, 184 LMDB (storage engine), 82, 242\nquorums, 179-182 load\nconsistency limitations, 181-183, 334 approaches to coping with, 17\nsloppy quorums and hinted handoff, 183 describing, 11\nread repair and anti-entropy, 178 load testing, 16"}
{"597": "in batch processing, 400, 405, 421 replication, 152, 158-161\nin stateful clients, 170, 511 change data capture, 454-457\nin stream processing, 474, 478, 508, 522 (see also changelogs)\nlocation transparency, 134 coordination with snapshot, 156\nin the actor model, 138 logical (row-based) replication, 160\nlocks, 556 statement-based replication, 158\ndeadlock, 258 trigger-based replication, 161\ndistributed locking, 301-304, 330 write-ahead log (WAL) shipping, 159\nfencing tokens, 303 scalability limits, 493\nimplementation with ZooKeeper, 370 loose coupling, 396, 419, 502\nrelation to consensus, 374 lost updates (see updates)\nfor transaction isolation LSM-trees (indexes), 78-79\nin snapshot isolation, 239 comparison to B-trees, 83-85\nin two-phase locking (2PL), 257-261 Lucene (storage engine), 79\nmaking operations atomic, 243 building indexes in batch processes, 411\nperformance, 258 similarity search, 88\npreventing dirty writes, 236 Luigi (workflow scheduler), 402\npreventing phantoms with index-range LWW (see last write wins)\nlocks, 260, 265\nread locks (shared mode), 236, 258 M\nshared mode and exclusive mode, 258\nmachine learning\nin two-phase commit (2PC)\nethical considerations, 534\ndeadlock detection, 364\n(see also ethics)\nin-doubt transactions holding locks, 362\niterative processing, 424\nmaterializing conflicts with, 251\nmodels derived from training data, 505\npreventing lost updates by explicit locking,\nstatistical and numerical algorithms, 428\n244\nMADlib (machine learning toolkit), 428\nlog sequence number, 156, 449\nmagic scaling sauce, 18\nlogic programming languages, 504\nMahout (machine learning toolkit), 428\nlogical clocks, 293, 343, 494\nmaintainability, 18-22, 489\nfor read-after-write consistency, 164\ndefined, 23\nlogical logs, 160\ndesign principles for software systems, 19\nlogs (data structure), 71, 556\nevolvability (see evolvability)\nadvantages of immutability, 460\noperability, 19\ncompaction, 73, 79, 456, 460\nsimplicity and managing complexity, 20\nfor stream operator state, 479\nmany-to-many relationships\ncreating using total order broadcast, 349\nin document model versus relational model,\nimplementing uniqueness constraints, 522\n39\nlog-based messaging, 446-451\nmodeling as graphs, 49\ncomparison to traditional messaging,\nmany-to-one and many-to-many relationships,\n448, 451\n33-36\nconsumer offsets, 449\nmany-to-one relationships, 34\ndisk space usage, 450\nMapReduce (batch processing), 390, 399-400\nreplaying old messages, 451, 496, 498\naccessing external services within job, 404,\nslow consumers, 450\n412"}
{"598": "comparison to stream processing, 464 meeting room booking (example), 249, 259,\ncomparison to Unix, 413-414 521\ndisadvantages and limitations of, 419 membership services, 372\nfault tolerance, 406, 414, 422 Memcached (caching server), 4, 89\nhigher-level tools, 403, 426 memory\nimplementation in Hadoop, 400-403 in-memory databases, 88\nthe shuffle, 402 durability, 227\nimplementation in MongoDB, 46-48 serial transaction execution, 253\nmachine learning, 428 in-memory representation of data, 112\nmap-side processing, 408-410 random bit-flips in, 529\nbroadcast hash joins, 409 use by indexes, 72, 77\nmerge joins, 410 memory barrier (CPU instruction), 338\npartitioned hash joins, 409 MemSQL (database)\nmapper and reducer functions, 399 in-memory storage, 89\nmaterialization of intermediate state, read committed isolation, 236\n419-423 memtable (in LSM-trees), 78\noutput of batch workflows, 411-413 Mercurial (version control system), 463\nbuilding search indexes, 411 merge joins, MapReduce map-side, 410\nkey-value stores, 412 mergeable persistent data structures, 174\nreduce-side processing, 403-408 merging sorted files, 76, 402, 405\nanalysis of user activity events (exam\u2010 Merkle trees, 532\nple), 404 Mesos (cluster manager), 418, 506\ngrouping records by same key, 406 message brokers (see messaging systems)\nhandling skew, 407 message-passing, 136-139\nsort-merge joins, 405 advantages over direct RPC, 137\nworkflows, 402 distributed actor frameworks, 138\nmarshalling (see encoding) evolvability, 138\nmassively parallel processing (MPP), 216 MessagePack (encoding format), 116\ncomparison to composing storage technolo\u2010 messages\ngies, 502 exactly-once semantics, 360, 476\ncomparison to Hadoop, 414-418, 428 loss of, 442\nmaster-master replication (see multi-leader using total order broadcast, 348\nreplication) messaging systems, 440-451\nmaster-slave replication (see leader-based repli\u2010 (see also streams)\ncation) backpressure, buffering, or dropping mes\u2010\nmaterialization, 556 sages, 441\naggregate values, 101 brokerless messaging, 442\nconflicts, 251 event logs, 446-451\nintermediate state (batch processing), comparison to traditional messaging,\n420-423 448, 451\nmaterialized views, 101 consumer offsets, 449\nas derived data, 386, 499-504 replaying old messages, 451, 496, 498\nmaintaining, using stream processing, slow consumers, 450\n467, 475 message brokers, 443-446\nMaven (Java build tool), 428 acknowledgements and redelivery, 445"}
{"599": "Meteor (web framework), 456 replication topologies, 175-177\nmicrobatching, 477, 495 use cases, 168\nmicroservices, 132 clients with offline operation, 170\n(see also services) collaborative editing, 170\ncausal dependencies across services, 493 multi-datacenter replication, 168, 335\nloose coupling, 502 multi-object transactions, 228\nrelation to batch/stream processors, 389, need for, 231\n508 Multi-Paxos (total order broadcast), 367\nMicrosoft multi-table index cluster tables (Oracle), 41\nAzure Service Bus (messaging), 444 multi-tenancy, 284\nAzure Storage, 155, 398 multi-version concurrency control (MVCC),\nAzure Stream Analytics, 466 239, 266\nDCOM (Distributed Component Object detecting stale MVCC reads, 263\nModel), 134 indexes and snapshot isolation, 241\nMSDTC (transaction coordinator), 356 mutual exclusion, 261\nOrleans (see Orleans) (see also locks)\nSQL Server (see SQL Server) MySQL (database)\nmigrating (rewriting) data, 40, 130, 461, 497 binlog coordinates, 156\nmodulus operator (%), 210 binlog parsing for change data capture, 455\nMongoDB (database) circular replication topology, 175\naggregation pipeline, 48 consistent snapshots, 156\natomic operations, 243 distributed transaction support, 361\nBSON, 41 InnoDB storage engine (see InnoDB)\ndocument data model, 31 JSON support, 30, 42\nhash partitioning (sharding), 203-204 leader-based replication, 153\nkey-range partitioning, 202 performance of XA transactions, 360\nlack of join support, 34, 42 row-based replication, 160\nleader-based replication, 153 schema changes in, 40\nMapReduce support, 46, 400 snapshot isolation support, 242\noplog parsing, 455, 456 (see also InnoDB)\npartition splitting, 212 statement-based replication, 159\nrequest routing, 216 Tungsten Replicator (multi-leader replica\u2010\nsecondary indexes, 207 tion), 170\nMongoriver (change data capture), 455 conflict detection, 177\nmonitoring, 10, 19\nmonotonic clocks, 288 N\nmonotonic reads, 164\nnanomsg (messaging library), 442\nMPP (see massively parallel processing)\nNarayana (transaction coordinator), 356\nMSMQ (messaging), 361\nNATS (messaging), 137\nmulti-column indexes, 87\nnear-real-time (nearline) processing, 390\nmulti-leader replication, 168-177\n(see also stream processing)\n(see also replication)\nNeo4j (database)\nhandling write conflicts, 171\nCypher query language, 52\nconflict avoidance, 172\ngraph data model, 50\nconverging toward a consistent state,\nNephele (dataflow engine), 421"}
{"600": "graph databases versus, 60 stateful, offline-capable clients, 170, 511\nimperative query APIs, 46 offline-first applications, 511\nNetwork Time Protocol (see NTP) offsets\nnetworks consumer offsets in partitioned logs, 449\ncongestion and queueing, 282 messages in partitioned logs, 447\ndatacenter network topologies, 276 OLAP (online analytic processing), 91, 556\nfaults (see faults) data cubes, 102\nlinearizability and network delays, 338 OLTP (online transaction processing), 90, 556\nnetwork partitions, 279, 337 analytics queries versus, 411\ntimeouts and unbounded delays, 281 workload characteristics, 253\nnext-key locking, 260 one-to-many relationships, 30\nnodes (in graphs) (see vertices) JSON representation, 32\nnodes (processes), 556 online systems, 389\nhandling outages in leader-based replica\u2010 (see also services)\ntion, 156 Oozie (workflow scheduler), 402\nsystem models for failure, 307 OpenAPI (service definition format), 133\nnoisy neighbors, 284 OpenStack\nnonblocking atomic commit, 359 Nova (cloud infrastructure)\nnondeterministic operations use of ZooKeeper, 370\naccidental nondeterminism, 423 Swift (object storage), 398\npartial failures in distributed systems, 275 operability, 19\nnonfunctional requirements, 22 operating systems versus databases, 499\nnonrepeatable reads, 238 operation identifiers, 518, 522\n(see also read skew) operational transformation, 174\nnormalization (data representation), 33, 556 operators, 421\nexecuting joins, 39, 42, 403 flow of data between, 424\nforeign key references, 231 in stream processing, 464\nin systems of record, 386 optimistic concurrency control, 261\nversus denormalization, 462 Oracle (database)\nNoSQL, 29, 499 distributed transaction support, 361\ntransactions and, 223 GoldenGate (change data capture), 161,\nNotation3 (N3), 56 170, 455\nnpm (package manager), 428 lack of serializability, 226\nNTP (Network Time Protocol), 287 leader-based replication, 153\naccuracy, 289, 293 multi-table index cluster tables, 41\nadjustments to monotonic clocks, 289 not preventing write skew, 248\nmultiple server addresses, 306 partitioned indexes, 209\nnumbers, in XML and JSON encodings, 114 PL/SQL language, 255\npreventing lost updates, 245\nO read committed isolation, 236\nReal Application Clusters (RAC), 330\nobject-relational mapping (ORM) frameworks,\nrecursive query support, 54\n30\nsnapshot isolation support, 239, 242\nerror handling and aborted transactions,\nTimesTen (in-memory database), 89\n232\nWAL-based replication, 160\nunsafe read-modify-write cycle code, 244"}
{"601": "partial order, 341 ballot number, 368\nlimits of total ordering, 493 Multi-Paxos (total order broadcast), 367\ntotal order broadcast, 348-352 percentiles, 14, 556\nOrleans (actor framework), 139 calculating efficiently, 16\noutliers (response time), 14 importance of high percentiles, 16\nOz (programming language), 504 use in service level agreements (SLAs), 15\nPercona XtraBackup (MySQL tool), 156\nP performance\ndescribing, 13\npackage managers, 428, 505\nof distributed transactions, 360\npacket switching, 285\nof in-memory databases, 89\npackets\nof linearizability, 338\ncorruption of, 306\nof multi-leader replication, 169\nsending via UDP, 442\nperpetual inconsistency, 525\nPageRank (algorithm), 49, 424\npessimistic concurrency control, 261\npaging (see virtual memory)\nphantoms (transaction isolation), 250\nParAccel (database), 93\nmaterializing conflicts, 251\nparallel databases (see massively parallel pro\u2010\npreventing, in serializability, 259\ncessing)\nphysical clocks (see clocks)\nparallel execution\npickle (Python), 113\nof graph analysis algorithms, 426\nPig (dataflow language), 419, 427\nqueries in MPP databases, 216\nreplicated joins, 409\nParquet (data format), 96, 131\nskewed joins, 407\n(see also column-oriented storage)\nworkflows, 403\nuse in Hadoop, 414\nPinball (workflow scheduler), 402\npartial failures, 275, 310\npipelined execution, 423\nlimping, 311\nin Unix, 394\npartial order, 341\npoint in time, 287\npartitioning, 199-218, 556\npolyglot persistence, 29\nand replication, 200\npolystores, 501\nin batch processing, 429\nPostgreSQL (database)\nmulti-partition operations, 514\nBDR (multi-leader replication), 170\nenforcing constraints, 522\ncausal ordering of writes, 177\nsecondary index maintenance, 495\nBottled Water (change data capture), 455\nof key-value data, 201-205\nBucardo (trigger-based replication), 161,\nby key range, 202\n173\nskew and hot spots, 205\ndistributed transaction support, 361\nrebalancing partitions, 209-214\nforeign data wrappers, 501\nautomatic or manual rebalancing, 213\nfull text search support, 490\nproblems with hash mod N, 210\nleader-based replication, 153\nusing dynamic partitioning, 212\nlog sequence number, 156\nusing fixed number of partitions, 210\nMVCC implementation, 239, 241\nusing N partitions per node, 212\nPL/pgSQL language, 255\nreplication and, 147\nPostGIS geospatial indexes, 87\nrequest routing, 214-216\npreventing lost updates, 245\nsecondary indexes, 206-209"}
{"602": "serializable snapshot isolation (SSI), 261 pure functions, 48\nsnapshot isolation support, 239, 242 putting computation near data, 400\nWAL-based replication, 160\nXML and JSON support, 30, 42 Q\npre-splitting, 212\nQpid (messaging), 444\nPrecision Time Protocol (PTP), 290\nquality of service (QoS), 285\npredicate locks, 259\nQuantcast File System (distributed filesystem),\npredictive analytics, 533-536\n398\namplifying bias, 534\nquery languages, 42-48\nethics of (see ethics)\naggregation pipeline, 48\nfeedback loops, 536\nCSS and XSL, 44\npreemption\nCypher, 52\nof datacenter resources, 418\nDatalog, 60\nof threads, 298\nJuttle, 504\nPregel processing model, 425\nMapReduce querying, 46-48\nprimary keys, 85, 556\nrecursive SQL queries, 53\ncompound primary key (Cassandra), 204\nrelational algebra and SQL, 42\nprimary-secondary replication (see leader-\nSPARQL, 59\nbased replication)\nquery optimizers, 37, 427\nprivacy, 536-543\nqueueing delays (networks), 282\nconsent and freedom of choice, 538\nhead-of-line blocking, 15\ndata as assets and power, 540\nlatency and response time, 14\ndeleting data, 463\nqueues (messaging), 137\nethical considerations (see ethics)\nquorums, 179-182, 556\nlegislation and self-regulation, 542\nfor leaderless replication, 179\nmeaning of, 539\nin consensus algorithms, 368\nsurveillance, 537\nlimitations of consistency, 181-183, 334\ntracking behavioral data, 536\nmaking decisions in distributed systems,\nprobabilistic algorithms, 16, 466\n301\nprocess pauses, 295-299\nmonitoring staleness, 182\nprocessing time (of events), 469\nmulti-datacenter replication, 184\nproducers (message streams), 440\nrelying on durability, 309\nprogramming languages\nsloppy quorums and hinted handoff, 183\ndataflow languages, 504\nfor stored procedures, 255 R\nfunctional reactive programming (FRP),\nR-trees (indexes), 87\n504\nRabbitMQ (messaging), 137, 444\nlogic programming, 504\nleader-based replication, 153\nProlog (language), 61\nrace conditions, 225\n(see also Datalog)\n(see also concurrency)\npromises (asynchronous operations), 135\navoiding with linearizability, 331\nproperty graphs, 50\ncaused by dual writes, 452\nCypher query language, 52\ndirty writes, 235\nProtocol Buffers (data format), 117-121\nin counter increments, 235\nfield tags and schema evolution, 120\nlost updates, 242-246"}
{"603": "sensitivity to network problems, 369 events in stream processing, 440\nterm number, 368 recursive common table expressions (SQL), 54\nuse in etcd, 353 redelivery (messaging), 445\nRAID (Redundant Array of Independent Redis (database)\nDisks), 7, 398 atomic operations, 243\nrailways, schema migration on, 496 durability, 89\nRAMCloud (in-memory storage), 89 Lua scripting, 255\nranking algorithms, 424 single-threaded execution, 253\nRDF (Resource Description Framework), 57 usage example, 4\nquerying with SPARQL, 59 redundancy\nRDMA (Remote Direct Memory Access), 276 hardware components, 7\nread committed isolation level, 234-237 of derived data, 386\nimplementing, 236 (see also derived data)\nmulti-version concurrency control Reed\u2013Solomon codes (error correction), 398\n(MVCC), 239 refactoring, 22\nno dirty reads, 234 (see also evolvability)\nno dirty writes, 235 regions (partitioning), 199\nread path (derived data), 509 register (data structure), 325\nread repair (leaderless replication), 178 relational data model, 28-42\nfor linearizability, 335 comparison to document model, 38-42\nread replicas (see leader-based replication) graph queries in SQL, 53\nread skew (transaction isolation), 238, 266 in-memory databases with, 89\nas violation of causality, 340 many-to-one and many-to-many relation\u2010\nread-after-write consistency, 163, 524 ships, 33\ncross-device, 164 multi-object transactions, need for, 231\nread-modify-write cycle, 243 NoSQL as alternative to, 29\nread-scaling architecture, 161 object-relational mismatch, 29\nreads as events, 513 relational algebra and SQL, 42\nreal-time versus document model\ncollaborative editing, 170 convergence of models, 41\nnear-real-time processing, 390 data locality, 41\n(see also stream processing) relational databases\npublish/subscribe dataflow, 513 eventual consistency, 162\nresponse time guarantees, 298 history, 28\ntime-of-day clocks, 288 leader-based replication, 153\nrebalancing partitions, 209-214, 556 logical logs, 160\n(see also partitioning) philosophy compared to Unix, 499, 501\nautomatic or manual rebalancing, 213 schema changes, 40, 111, 130\ndynamic partitioning, 212 statement-based replication, 158\nfixed number of partitions, 210 use of B-tree indexes, 80\nfixed number of partitions per node, 212 relationships (see edges)\nproblems with hash mod N, 210 reliability, 6-10, 489\nrecency guarantee, 324 building a reliable system from unreliable\nrecommendation engines components, 276\nbatch process outputs, 412 defined, 6, 22"}
{"604": "software errors, 8 approaches to, 214\nRemote Method Invocation (Java RMI), 134 parallel query execution, 216\nremote procedure calls (RPCs), 134-136 resilient systems, 6\n(see also services) (see also fault tolerance)\nbased on futures, 135 response time\ndata encoding and evolution, 136 as performance metric for services, 13, 389\nissues with, 134 guarantees on, 298\nusing Avro, 126, 135 latency versus, 14\nusing Thrift, 135 mean and percentiles, 14\nversus message brokers, 137 user experience, 15\nrepeatable reads (transaction isolation), 242 responsibility and accountability, 535\nreplicas, 152 REST (Representational State Transfer), 133\nreplication, 151-193, 556 (see also services)\nand durability, 227 RethinkDB (database)\nchain replication, 155 document data model, 31\nconflict resolution and, 246 dynamic partitioning, 212\nconsistency properties, 161-167 join support, 34, 42\nconsistent prefix reads, 165 key-range partitioning, 202\nmonotonic reads, 164 leader-based replication, 153\nreading your own writes, 162 subscribing to changes, 456\nin distributed filesystems, 398 Riak (database)\nleaderless, 177-191 Bitcask storage engine, 72\ndetecting concurrent writes, 184-191 CRDTs, 174, 191\nlimitations of quorum consistency, dotted version vectors, 191\n181-183, 334 gossip protocol, 216\nsloppy quorums and hinted handoff, 183 hash partitioning, 203-204, 211\nmonitoring staleness, 182 last-write-wins conflict resolution, 186\nmulti-leader, 168-177 leaderless replication, 177\nacross multiple datacenters, 168, 335 LevelDB storage engine, 78\nhandling write conflicts, 171-175 linearizability, lack of, 335\nreplication topologies, 175-177 multi-datacenter support, 184\npartitioning and, 147, 200 preventing lost updates across replicas, 246\nreasons for using, 145, 151 rebalancing, 213\nsingle-leader, 152-161 search feature, 209\nfailover, 157 secondary indexes, 207\nimplementation of replication logs, siblings (concurrently written values), 190\n158-161 sloppy quorums, 184\nrelation to consensus, 367 ring buffers, 450\nsetting up new followers, 155 Ripple (cryptocurrency), 532\nsynchronous versus asynchronous, rockets, 10, 36, 305\n153-155 RocksDB (storage engine), 78\nstate machine replication, 349, 452 leveled compaction, 79\nusing erasure coding, 398 rollbacks (transactions), 222\nwith heterogeneous data systems, 453 rolling upgrades, 8, 112\nreplication logs (see logs) routing (see request routing)"}
{"605": "Rubygems (package manager), 428 schema evolution, 120\nrules (Datalog), 61 traditional approach to design, fallacy in,\n462\nS searches\nbuilding search indexes in batch processes,\nsafety and liveness properties, 308\n411\nin consensus algorithms, 366\nk-nearest neighbors, 429\nin transactions, 222\non streams, 467\nsagas (see compensating transactions)\npartitioned secondary indexes, 206\nSamza (stream processor), 466, 467\nsecondaries (see leader-based replication)\nfault tolerance, 479\nsecondary indexes, 85, 557\nstreaming SQL support, 466\npartitioning, 206-209, 217\nsandboxes, 9\ndocument-partitioned, 206\nSAP HANA (database), 93\nindex maintenance, 495\nscalability, 10-18, 489\nterm-partitioned, 208\napproaches for coping with load, 17\nproblems with dual writes, 452, 491\ndefined, 22\nupdating, transaction isolation and, 231\ndescribing load, 11\nsecondary sorts, 405\ndescribing performance, 13\nsed (Unix tool), 392\npartitioning and, 199\nself-describing files, 127\nreplication and, 161\nself-joins, 480\nscaling up versus scaling out, 146\nself-validating systems, 530\nscaling out, 17, 146\nsemantic web, 57\n(see also shared-nothing architecture)\nsemi-synchronous replication, 154\nscaling up, 17, 146\nsequence number ordering, 343-348\nscatter/gather approach, querying partitioned\ngenerators, 294, 344\ndatabases, 207\ninsufficiency for enforcing constraints, 347\nSCD (slowly changing dimension), 476\nLamport timestamps, 345\nschema-on-read, 39\nuse of timestamps, 291, 295, 345\ncomparison to evolvable schema, 128\nsequential consistency, 351\nin distributed filesystems, 415\nserializability, 225, 233, 251-266, 557\nschema-on-write, 39\nlinearizability versus, 329\nschemaless databases (see schema-on-read)\npessimistic versus optimistic concurrency\nschemas, 557\ncontrol, 261\nAvro, 122-127\nserial execution, 252-256\nreader determining writer\u2019s schema, 125\npartitioning, 255\nschema evolution, 123\nusing stored procedures, 253, 349\ndynamically generated, 126\nserializable snapshot isolation (SSI),\nevolution of, 496\n261-266\naffecting application code, 111\ndetecting stale MVCC reads, 263\ncompatibility checking, 126\ndetecting writes that affect prior reads,\nin databases, 129-131\n264\nin message-passing, 138\ndistributed execution, 265, 364\nin service calls, 136\nperformance of SSI, 265\nflexibility in document model, 39\npreventing write skew, 262-265\nfor analytics, 93-95"}
{"606": "serialization, 113 clock skew, 291-294, 334\n(see also encoding) in transaction isolation\nservice discovery, 135, 214, 372 read skew, 238, 266\nusing DNS, 216, 372 write skew, 246-251, 262-265\nservice level agreements (SLAs), 15 (see also write skew)\nservice-oriented architecture (SOA), 132 meanings of, 238\n(see also services) unbalanced workload, 201\nservices, 131-136 compensating for, 205\nmicroservices, 132 due to celebrities, 205\ncausal dependencies across services, 493 for time-series data, 203\nloose coupling, 502 in batch processing, 407\nrelation to batch/stream processors, 389, slaves (see leader-based replication)\n508 sliding windows (stream processing), 472\nremote procedure calls (RPCs), 134-136 (see also windows)\nissues with, 134 sloppy quorums, 183\nsimilarity to databases, 132 (see also quorums)\nweb services, 132, 135 lack of linearizability, 334\nsession windows (stream processing), 472 slowly changing dimension (data warehouses),\n(see also windows) 476\nsessionization, 407 smearing (leap seconds adjustments), 290\nsharding (see partitioning) snapshots (databases)\nshared mode (locks), 258 causal consistency, 340\nshared-disk architecture, 146, 398 computing derived data, 500\nshared-memory architecture, 146 in change data capture, 455\nshared-nothing architecture, 17, 146-147, 557 serializable snapshot isolation (SSI),\n(see also replication) 261-266, 329\ndistributed filesystems, 398 setting up a new replica, 156\n(see also distributed filesystems) snapshot isolation and repeatable read,\npartitioning, 199 237-242\nuse of network, 277 implementing with MVCC, 239\nsharks indexes and MVCC, 241\nbiting undersea cables, 279 visibility rules, 240\ncounting (example), 46-48 synchronized clocks for global snapshots,\nfinding (example), 42 294\nwebsite about (example), 44 snowflake schemas, 95\nshredding (in relational model), 38 SOAP, 133\nsiblings (concurrent values), 190, 246 (see also services)\n(see also conflicts) evolvability, 136\nsimilarity search software bugs, 8\nedit distance, 88 maintaining integrity, 529\ngenome data, 63 solid state drives (SSDs)\nk-nearest neighbors, 429 access patterns, 84\nsingle-leader replication (see leader-based rep\u2010 detecting corruption, 519, 530\nlication) faults in, 227\nsingle-threaded execution, 243, 252 sequential write throughput, 75"}
{"607": "usage example, 4 read committed isolation, 236\nuse of Lucene, 79 recursive query support, 54\nsort (Unix tool), 392, 394, 395 serializable isolation, 257\nsort-merge joins (MapReduce), 405 snapshot isolation support, 239\nSorted String Tables (see SSTables) T-SQL language, 255\nsorting XML support, 30\nsort order in column storage, 99 SQLstream (stream analytics), 466\nsource of truth (see systems of record) SSDs (see solid state drives)\nSpanner (database) SSTables (storage format), 76-79\ndata locality, 41 advantages over hash indexes, 76\nsnapshot isolation using clocks, 295 concatenated index, 204\nTrueTime API, 294 constructing and maintaining, 78\nSpark (processing framework), 421-423 making LSM-Tree from, 78\nbytecode generation, 428 staleness (old data), 162\ndataflow APIs, 427 cross-channel timing dependencies, 331\nfault tolerance, 422 in leaderless databases, 178\nfor data warehouses, 93 in multi-version concurrency control, 263\nGraphX API (graph processing), 425 monitoring for, 182\nmachine learning, 428 of client state, 512\nquery optimizer, 427 versus linearizability, 324\nSpark Streaming, 466 versus timeliness, 524\nmicrobatching, 477 standbys (see leader-based replication)\nstream processing on top of batch process\u2010 star replication topologies, 175\ning, 495 star schemas, 93-95\nSPARQL (query language), 59 similarity to event sourcing, 458\nspatial algorithms, 429 Star Wars analogy (event time versus process\u2010\nsplit brain, 158, 557 ing time), 469\nin consensus algorithms, 352, 367 state\npreventing, 322, 333 derived from log of immutable events, 459\nusing fencing tokens to avoid, 302-304 deriving current state from the event log,\nspreadsheets, dataflow programming capabili\u2010 458\nties, 504 interplay between state changes and appli\u2010\nSQL (Structured Query Language), 21, 28, 43 cation code, 507\nadvantages and limitations of, 416 maintaining derived state, 495\ndistributed query execution, 48 maintenance by stream processor in stream-\ngraph queries in, 53 stream joins, 473\nisolation levels standard, issues with, 242 observing derived state, 509-515\nquery execution on Hadoop, 416 rebuilding after stream processor failure,\nr\u00e9sum\u00e9 (example), 30 478\nSQL injection vulnerability, 305 separation of application code and, 505\nSQL on Hadoop, 93 state machine replication, 349, 452\nstatement-based replication, 158 statement-based replication, 158\nstored procedures, 255 statically typed languages\nSQL Server (database) analogy to schema-on-write, 40\ndata warehousing support, 93 code generation and, 127"}
{"608": "stock market feeds, 442 maintaining derived state, 495\nSTONITH (Shoot The Other Node In The maintenance of materialized views, 467\nHead), 158 messaging systems (see messaging systems)\nstop-the-world (see garbage collection) reasoning about time, 468-472\nstorage event time versus processing time, 469,\ncomposing data storage technologies, 477, 498\n499-504 knowing when window is ready, 470\ndiversity of, in MapReduce, 415 types of windows, 472\nStorage Area Network (SAN), 146, 398 relation to databases (see streams)\nstorage engines, 69-104 relation to services, 508\ncolumn-oriented, 95-101 search on streams, 467\ncolumn compression, 97-99 single-threaded execution, 448, 463\ndefined, 96 stream analytics, 466\ndistinction between column families stream joins, 472-476\nand, 99 stream-stream join, 473\nParquet, 96, 131 stream-table join, 473\nsort order in, 99-100 table-table join, 474\nwriting to, 101 time-dependence of, 475\ncomparing requirements for transaction streams, 440-451\nprocessing and analytics, 90-96 end-to-end, pushing events to clients, 512\nin-memory storage, 88 messaging systems (see messaging systems)\ndurability, 227 processing (see stream processing)\nrow-oriented, 70-90 relation to databases, 451-464\nB-trees, 79-83 (see also changelogs)\ncomparing B-trees and LSM-trees, 83-85 API support for change streams, 456\ndefined, 96 change data capture, 454-457\nlog-structured, 72-79 derivative of state by time, 460\nstored procedures, 161, 253-255, 557 event sourcing, 457-459\nand total order broadcast, 349 keeping systems in sync, 452-453\npros and cons of, 255 philosophy of immutable events,\nsimilarity to stream processors, 505 459-464\nStorm (stream processor), 466 topics, 440\ndistributed RPC, 468, 514 strict serializability, 329\nTrident state handling, 478 strong consistency (see linearizability)\nstraggler events, 470, 498 strong one-copy serializability, 329\nstream processing, 464-481, 557 subjects, predicates, and objects (in triple-\naccessing external services within job, 474, stores), 55\n477, 478, 517 subscribers (message streams), 440\ncombining with batch processing (see also consumers)\nlambda architecture, 497 supercomputers, 275\nunifying technologies, 498 surveillance, 537\ncomparison to batch processing, 464 (see also privacy)\ncomplex event processing (CEP), 465 Swagger (service definition format), 133\nfault tolerance, 476-479 swapping to disk (see virtual memory)\natomic commit, 477 synchronous networks, 285, 557"}
{"609": "conflict detection, 172 CompactProtocol, 119\nsystem models, 300, 306-310 field tags and schema evolution, 120\nassumptions in, 528 throughput, 13, 390\ncorrectness of algorithms, 308 TIBCO, 137\nmapping to the real world, 309 Enterprise Message Service, 444\nsafety and liveness, 308 StreamBase (stream analytics), 466\nsystems of record, 386, 557 time\nchange data capture, 454, 491 concurrency and, 187\ntreating event log as, 460 cross-channel timing dependencies, 331\nsystems thinking, 536 in distributed systems, 287-299\n(see also clocks)\nT clock synchronization and accuracy, 289\nrelying on synchronized clocks, 291-295\nt-digest (algorithm), 16\nprocess pauses, 295-299\ntable-table joins, 474\nreasoning about, in stream processors,\nTableau (data visualization software), 416\n468-472\ntail (Unix tool), 447\nevent time versus processing time, 469,\ntail vertex (property graphs), 51\n477, 498\nTajo (query engine), 93\nknowing when window is ready, 470\nTandem NonStop SQL (database), 200\ntimestamp of events, 471\nTCP (Transmission Control Protocol), 277\ntypes of windows, 472\ncomparison to circuit switching, 285\nsystem models for distributed systems, 307\ncomparison to UDP, 283\ntime-dependence in stream joins, 475\nconnection failures, 280\ntime-of-day clocks, 288\nflow control, 282, 441\ntimeliness, 524\npacket checksums, 306, 519, 529\ncoordination-avoiding data systems, 528\nreliability and duplicate suppression, 517\ncorrectness of dataflow systems, 525\nretransmission timeouts, 284\ntimeouts, 279, 557\nuse for transaction sessions, 229\ndynamic configuration of, 284\ntelemetry (see monitoring)\nfor failover, 158\nTeradata (database), 93, 200\nlength of, 281\nterm-partitioned indexes, 208, 217\ntimestamps, 343\ntermination (consensus), 365\nassigning to events in stream processing,\nTerrapin (database), 413\n471\nTez (dataflow engine), 421-423\nfor read-after-write consistency, 163\nfault tolerance, 422\nfor transaction ordering, 295\nsupport by higher-level tools, 427\ninsufficiency for enforcing constraints, 347\nthrashing (out of memory), 297\nkey range partitioning by, 203\nthreads (concurrency)\nLamport, 345\nactor model, 138, 468\nlogical, 494\n(see also message-passing)\nordering events, 291, 345\natomic operations, 223\nTitan (database), 50\nbackground threads, 73, 85\ntombstones, 74, 191, 456\nexecution pauses, 286, 296-298\ntopics (messaging), 137, 440\nmemory barriers, 338\ntotal order, 341, 557\npreemption, 298"}
{"610": "implementation in ZooKeeper and etcd, 370 triple-stores, 55-59\nimplementing with linearizable storage, 351 SPARQL query language, 59\nusing, 349 tumbling windows (stream processing), 472\nusing to implement linearizable storage, 350 (see also windows)\ntracking behavioral data, 536 in microbatching, 477\n(see also privacy) tuple spaces (programming model), 507\ntransaction coordinator (see coordinator) Turtle (RDF data format), 56\ntransaction manager (see coordinator) Twitter\ntransaction processing, 28, 90-95 constructing home timelines (example), 11,\ncomparison to analytics, 91 462, 474, 511\ncomparison to data warehousing, 93 DistributedLog (event log), 448\ntransactions, 221-267, 558 Finagle (RPC framework), 135\nACID properties of, 223 Snowflake (sequence number generator),\natomicity, 223 294\nconsistency, 224 Summingbird (processing library), 497\ndurability, 226 two-phase commit (2PC), 353, 355-359, 558\nisolation, 225 confusion with two-phase locking, 356\ncompensating (see compensating transac\u2010 coordinator failure, 358\ntions) coordinator recovery, 363\nconcept of, 222 how it works, 357\ndistributed transactions, 352-364 issues in practice, 363\navoiding, 492, 502, 521-528 performance cost, 360\nfailure amplification, 364, 495 transactions holding locks, 362\nin doubt/uncertain status, 358, 362 two-phase locking (2PL), 257-261, 329, 558\ntwo-phase commit, 354-359 confusion with two-phase commit, 356\nuse of, 360-361 index-range locks, 260\nXA transactions, 361-364 performance of, 258\nOLTP versus analytics queries, 411 type checking, dynamic versus static, 40\npurpose of, 222\nserializability, 251-266 U\nactual serial execution, 252-256\nUDP (User Datagram Protocol)\npessimistic versus optimistic concur\u2010\ncomparison to TCP, 283\nrency control, 261\nmulticast, 442\nserializable snapshot isolation (SSI),\nunbounded datasets, 439, 558\n261-266\n(see also streams)\ntwo-phase locking (2PL), 257-261\nunbounded delays, 558\nsingle-object and multi-object, 228-232\nin networks, 282\nhandling errors and aborts, 231\nprocess pauses, 296\nneed for multi-object transactions, 231\nunbundling databases, 499-515\nsingle-object writes, 230\ncomposing data storage technologies,\nsnapshot isolation (see snapshots)\n499-504\nweak isolation levels, 233-251\nfederation versus unbundling, 501\npreventing lost updates, 242-246\nneed for high-level language, 503\nread committed, 234-238\ndesigning applications around dataflow,\ntransitive closure (graph algorithm), 424\n504-509"}
{"611": "uncertain (transaction status) (see in doubt) handling writes, 101\nuniform consensus, 365 replicas using different sort orders, 100\n(see also consensus) vertical scaling (see scaling up)\nuniform interfaces, 395 vertices (in graphs), 49\nunion type (in Avro), 125 property graph model, 50\nuniq (Unix tool), 392 Viewstamped Replication (consensus algo\u2010\nuniqueness constraints rithm), 366\nasynchronously checked, 526 view number, 368\nrequiring consensus, 521 virtual machines, 146\nrequiring linearizability, 330 (see also cloud computing)\nuniqueness in log-based messaging, 522 context switches, 297\nUnix philosophy, 394-397 network performance, 282\ncommand-line batch processing, 391-394 noisy neighbors, 284\nUnix pipes versus dataflow engines, 423 reliability in cloud services, 8\ncomparison to Hadoop, 413-414 virtualized clocks in, 290\ncomparison to relational databases, 499, 501 virtual memory\ncomparison to stream processing, 464 process pauses due to page faults, 14, 297\ncomposability and uniform interfaces, 395 versus memory management by databases,\nloose coupling, 396 89\npipes, 394 VisiCalc (spreadsheets), 504\nrelation to Hadoop, 499 vnodes (partitioning), 199\nUPDATE statement (SQL), 40 Voice over IP (VoIP), 283\nupdates Voldemort (database)\npreventing lost updates, 242-246 building read-only stores in batch processes,\natomic write operations, 243 413\nautomatically detecting lost updates, 245 hash partitioning, 203-204, 211\ncompare-and-set operations, 245 leaderless replication, 177\nconflict resolution and replication, 246 multi-datacenter support, 184\nusing explicit locking, 244 rebalancing, 213\npreventing write skew, 246-251 reliance on read repair, 179\nsloppy quorums, 184\nV VoltDB (database)\ncross-partition serializability, 256\nvalidity (consensus), 365\ndeterministic stored procedures, 255\nvBuckets (partitioning), 199\nin-memory storage, 89\nvector clocks, 191\noutput streams, 456\n(see also version vectors)\nsecondary indexes, 207\nvectorized processing, 99, 428\nserial execution of transactions, 253\nverification, 528-533\nstatement-based replication, 159, 479\navoiding blind trust, 530\ntransactions in stream processing, 477\nculture of, 530\ndesigning for auditability, 531\nW\nend-to-end integrity checks, 531\ntools for auditable data systems, 532 WAL (write-ahead log), 82\nversion control systems, reliance on immutable web services (see services)\ndata, 463 Web Services Description Language (WSDL),"}
{"612": "windows (stream processing), 466, 468-472 X\ninfinite windows for changelogs, 467, 474 XA transactions, 355, 361-364\nknowing when all events have arrived, 470 heuristic decisions, 363\nstream joins within a window, 473 limitations of, 363\ntypes of windows, 472 xargs (Unix tool), 392, 396\nwinners (conflict resolution), 173 XML\nWITH RECURSIVE syntax (SQL), 54 binary variants, 115\nworkflows (MapReduce), 402 encoding RDF data, 57\noutputs, 411-414 for application data, issues with, 114\nkey-value stores, 412 in relational databases, 30, 41\nsearch indexes, 411 XSL/XPath, 45\nwith map-side joins, 410\nworking set, 393 Y\nwrite amplification, 84\nYahoo!\nwrite path (derived data), 509\nPistachio (database), 461\nwrite skew (transaction isolation), 246-251\nSherpa (database), 455\ncharacterizing, 246-251, 262\nYARN (job scheduler), 416, 506\nexamples of, 247, 249\npreemption of jobs, 418\nmaterializing conflicts, 251\nuse of ZooKeeper, 370\noccurrence in practice, 529\nphantoms, 250\nZ\npreventing\nin snapshot isolation, 262-265 Zab (consensus algorithm), 366\nin two-phase locking, 259-261 use in ZooKeeper, 353\noptions for, 248 ZeroMQ (messaging library), 442\nwrite-ahead log (WAL), 82, 159 ZooKeeper (coordination service), 370-373\nwrites (database) generating fencing tokens, 303, 349, 370\natomic write operations, 243 linearizable operations, 333, 351\ndetecting writes affecting prior reads, 264 locks and leader election, 330\npreventing dirty writes with read commit\u2010 service discovery, 372\nted, 235 use for partition assignment, 215, 371\nWS-* framework, 133 use of Zab algorithm, 349, 353, 366\n(see also services)\nWS-AtomicTransaction (2PC), 355"}
{"613": "About the Author\nMartin Kleppmann is a researcher in distributed systems at the University of Cam\u2010\nbridge, UK. Previously he was a software engineer and entrepreneur at internet com\u2010\npanies including LinkedIn and Rapportive, where he worked on large-scale data\ninfrastructure. In the process he learned a few things the hard way, and he hopes this\nbook will save you from repeating the same mistakes.\nMartin is a regular conference speaker, blogger, and open source contributor. He\nbelieves that profound technical ideas should be accessible to everyone, and that\ndeeper understanding will help us develop better software.\nColophon\nThe animal on the cover of Designing Data-Intensive Applications is an Indian wild\nboar (Sus scrofa cristatus), a subspecies of wild boar found in India, Myanmar, Nepal,\nSri Lanka, and Thailand. They are distinctive from European boars in that they have\nhigher back bristles, no woolly undercoat, and a larger, straighter skull.\nThe Indian wild boar has a coat of gray or black hair, with stiff bristles running along\nthe spine. Males have protruding canine teeth (called tushes) that are used to fight\nwith rivals or fend off predators. Males are larger than females, but the species aver\u2010\nages 33\u201335 inches tall at the shoulder and 200\u2013300 pounds in weight. Their natural\npredators include bears, tigers, and various big cats.\nThese animals are nocturnal and omnivorous\u2014they eat a wide variety of things,\nincluding roots, insects, carrion, nuts, berries, and small animals. Wild boars are also\nknown to root through garbage and crop fields, causing a great deal of destruction\nand earning the enmity of farmers. They need to eat 4,000\u20134,500 calories a day. Boars\nhave a well-developed sense of smell, which helps them forage for underground plant\nmaterial and burrowing animals. However, their eyesight is poor.\nWild boars have long held significance in human culture. In Hindu lore, the boar is\nan avatar of the god Vishnu. In ancient Greek funerary monuments, it was a symbol\nof a gallant loser (in contrast to the victorious lion). Due to its aggression, it was\ndepicted on the armor and weapons of Scandinavian, Germanic, and Anglo-Saxon\nwarriors. In the Chinese zodiac, it symbolizes determination and impetuosity.\nMany of the animals on O\u2019Reilly covers are endangered; all of them are important to\nthe world. To learn more about how you can help, go to animals.oreilly.com.\nThe cover image is from Shaw\u2019s Zoology. The cover fonts are URW Typewriter and"}
{"45": "CHAPTER 4\nControl Code Versions and Development\nBranches\nI was working on the proof of one of my poems all the morning, and took out a\ncomma. In the afternoon I put it back again.\n\u2014Oscar Wilde\nBest Practice:\n\u2022 Use a standard version control system and keep track of\ndevelopment branches.\n\u2022 Integrate your code regularly and commit changes both reg\u2010\nularly and specifically.\n\u2022 This improves the development process because developers\ncan work in isolation without drifting apart.\nIn this chapter we apply the ideas of GQM to version control. Bugs and regressions\n(recurring defects) occur regularly during development and maintenance of any\ncodebase. To solve those you will need to reanalyze code, and you may need to revert\nsome code or configuration to earlier versions. Consider what happens if there is no\nversion control and all developers can only work on one version. It will be hard to see\nwho is changing what and hard to avoid that developers break each other\u2019s code.\nVersion control is a solution to this. It does what it says: controlling versions, which\nallows you to divide work among developers. In its most basic form, a version control"}
{"46": "To better understand what version control systems do, let us start at the beginning.\nThere is one main version of the code that is leading for what is being put into pro\u2010\nduction, referred as the trunk (main line). When developers make changes, they can\ncommit changes and additions to the trunk. When multiple developers work on the\ntrunk, they can get in each other\u2019s way. To avoid this, variations can be developed in\nbranches (meaning a new, parallel version of the software). Branches can later be\nmerged into the trunk.\nTherefore version control systems generally have integration functionality for merg\u2010\ning different versions. In that way, developers can work on isolated parts of the source\ncode. Usually, the merging of source code (which is text-based) proceeds automati\u2010\ncally, but when there is a conflict, it needs to be resolved manually.\nThere can be a central repository that holds the trunk (a centralized version control\nsystem). In that case, developers make a copy of that trunk to work on and commit to\nthe central version control system. With distributed version control, each developer\nhas a local repository, and changes can be shared among each other to create a ver\u2010\nsion (pushing their own changes and pulling changes of others).\nThere are several version control systems available, such as Git, Subversion, and Mer\u2010\ncurial, each with a slightly different vocabulary and different mechanisms. A notable\ndifference is that Git and Mercurial allow developers to commit changes into local\n(individual) branches on their own machines, which can later be merged with the\nmain repository. Subversion prefers to always commit changes directly to the central\nrepository.\nAlthough version control has the most visible benefits for source code, you should\nput all versionable parts of the software in version control, including test cases, data\u2010\nbase configuration, deployment scripts, and the like. That way, you can re-create the\ndifferent testing and production environments more easily. Documentation can be\nunder version control, and sometimes external libraries (when you cannot rely only\non the versioning of the library provider).\nDo not put generated code into version control. You do not need\nto, because the build system will generate that code for you. In case\nthat generated code is actually maintained, it should be versioned\nand be part of version control. But you should refrain from adjust\u2010\ning generated code as it will lead to problems when you need to re-\ngenerate it.\nThere are situations in which libraries need to be included in version control (e.g., for"}
{"47": "4.1 Motivation\nDifferent version control systems share common advantages: they allow the develop\u2010\nment team to track changes over time, they use branching to allow developers to\nwork independently on variations of the same codebase, and they merge files auto\u2010\nmatically when versions somehow conflict.\nTracking Changes\nTracking changes with a version control system has the advantage of going back in\ntime. When things go right, there may not be a reason to do so. However, when func\u2010\ntionality breaks, comparing old versions is a good tactic to find the cause of the issue\n(instead of reanalyzing the whole code). So the developer can revert the source code\nversion (in its working copy) to the moment when the bug was introduced. This\ncomparison helps with fixing the bug or simply replacing the new version with the\nold one. Then, a new (local) version can be merged back into the trunk again.\nVersion Control Allows Independent Modification\nIndependent modification in part of the source code avoids conflicts that happen\nwhen different pieces are adjusted at the same time. So when two developers need to\nimplement different functionalities but need the common codebase for testing, for\nexample, they can create two isolated branches of the same codebase that they can\nmodify independently. In that way, the code of the one developer will not interfere\nwith the work of the other. When one of the branches functions satisfactorily, it can\nbe pushed back and merged into the main development line. Then the changed func\u2010\ntionality becomes available to the whole team.\nVersion Control Allows Automatic Merging of Versions\nEvery time developers need to exchange their work, source files need to be merged.\nMerging source code versions manually is notoriously difficult if they are very differ\u2010\nent from each other. Luckily, version control can do most of the merging automati\u2010\ncally by analyzing the difference between a modified file and the original. It is only\nwhen there is no definite way to combine the two files that a problem (merge con\u2010\nflict) occurs. Merge conflicts need to be dealt with manually because they indicate\nthat two persons made different changes to the same line in a file, and you still need\nto agree on the modification that is most suitable."}
{"48": "4.2 How to Apply the Best Practice\nConsider that changes and additions are much easier to manage when done in small\niterations. Each iteration moves the software forward with a small step. Difficulties\narise when you want to make large and ambitious changes infrequently.\nTherefore, there are two main principles you should adhere to:\n\u2022 Commit specifically and regularly\n\u2022 Integrate your code regularly, by building the full product from source code and\ntesting it\nWe will show how you can use different metrics of the version control system to\nmeasure the quality of your development process.\nCommit Specifically and Regularly\nIn many cases, team members will not be aware of what other developers are working\non, so it is important to register this when you make changes in version control. So,\nevery commit should be specific to one piece of functionality or, when you have an\nissue tracking system in place, it should be linked to precisely one issue. Commits\nalso should be done on a regular basis: this helps to keep track of the flow of work\nand of the progress toward the team goal. Moreover, you will avoid merge conflicts by\ncommitting your changes to the version control server regularly.\nNote that \u201ckeeping changes small\u201d implies that you also divide\ndevelopment work into small parts.\nA typical indicator for this principle is the commit frequency of team members, and\nto link the commit messages with the issue tracker. You could further measure the\nbuild success rate to determine how quickly issues are solved, and in how many cases\nthe issues need extra work. This could serve as an alternative measure of team veloc\u2010\nity.\nIntegrate Your Code Regularly\nBesides committing new code on a regular basis, it is vital to integrate code between"}
{"49": "There should be a proper balance between the time a programmer operates inde\u2010\npendently on a branch and the number of merge conflicts. An indication of this is\n(average) branch lifespan or the relation between branch lifespan and the number of\nrelated merge conflicts.\nHaving long-lived branches (that exist for, say, more than two sprints) poses prob\u2010\nlems. In general, it makes truly isolated maintenance more difficult, and in some\ncases unlikely. Long-lived branches heighten the risk of merge conflicts, because the\nlonger they live, the more they tend to divert from the trunk. When branches start to\nevolve into notably different functionality, they may even become impossible to\nmerge and become forks (independent versions of the software). It is easy to imagine\nthe panic if one fast-paced developer that works independently tries to merge a\nmonth of work one day before the release deadline.\n4.3 Controlling Versions in Practice\nKeeping in mind that effectively managing code variations requires avoiding long-\nlived branches, you also wish to confirm whether the team works more productively\nwith the help of version control. Therefore, you would expect that issue resolution\ntime should not increase when applying version control. Assuming that the goals are\nformulated from the viewpoint of the team lead, you can come up with the following\nGQM model:\n\u2022 Goal A: To manage code variations by preventing long-lived development\nbranches.\n\u2014Question 1: How much time passes between commits?\n\u2014Metric 1a: Number of developers that did not commit the last 24 hours.\nThis is a simple metric that serves as an indicator of whether long-term\nbranches are developing. Of course, developers could be working on other\nthings, out of office, not committing code. Those explanations are easy.\nThe not-so-obvious cases are the interesting ones. Expect this metric to be\nfairly stable over time, yet it will almost never be zero.\n\u2014Metric 1b: Average branch lifespan. This metric attempts to measure that\nbranch lifespan is limited to avoid the risks of long-lived branches. Expect\na downward trend toward the period of time that it takes to implement\none work package (task, bug, user story, etc.). In most cases this means\nresolving the full work package and ideally that work package should be\nsized in a way that it can be fixed in a day. Therefore it is also dependent\non how well issues are registered."}
{"50": "downward trend toward daily commits. Note that in some situations, this\nmetric may be inaccurate due to, for example, bug fixes that require the\nuse of specialized frequent commits, such as in high-security and mission-\ncritical environments.\n\u2022 Goal B: To understand the causes influencing team productivity (by analyzing\nissue resolution time).\n\u2014Question 2: What is the typical issue resolution time for the system?\n\u2014Metric 2a: Average issue resolution time, calculated as the total issue reso\u2010\nlution time divided by number of issues. Expect a downward trend toward\na relatively stable minimum amount of effort where the team can resolve\nissues efficiently.\n\u2014Metric 2b: Percentage of issues that are both raised and solved within the\nsame sprint. A high percentage could signify that bugs are processed\nimmediately instead of postponed, which is desired behavior. Expect an\nupward trend.\n\u2014Metric 2c: Average issue resolution time between different levels of\nurgency. The distinction is relevant because different levels of urgency\nmay explain different resolution times. Expect a downward trend for each\ncategory.\n\u2014Question 3: Are enough issues resolved within a reasonable amount of time?\nIn this case, assume 24 hours is quick and 72 hours is acceptable.\n\u2014Metric 3a: Percentage of issues resolved within 24 hours. Expect an\nupward trend until a stable rate has been achieved.\n\u2014Metric 3b: Percentage of issues resolved within 72 hours. Expect an\nupward trend until a stable rate has been achieved.\nAlthough issue resolution time is a common metric, it is important to realize its limi\u2010\ntations. The metric could be distorted when issues are being defined in ever-smaller\nwork packages (which makes it seem that efficiency rises), or if team members are\nclosing unsolved issues instead of resolving them to clear the backlog. The latter may\nbe a case of treating the metric. This is particularly a problem if there is incentive to\nmake the metric more favorable. This would be especially at risk when team mem\u2010\nbers\u2019 performance is evaluated with (mainly) this metric. Therefore make sure that\nyou are not using only one metric (the pitfall one track metric), especially when that\nmetric is important in evaluating performance. That can lead to the pitfall treating the\nmetric, which causes unintended behavior."}
{"51": "\u2022 Goal B (continued)\n\u2014Question 4: How productive are we in terms of functionality implemented?\n\u2014Metric 4: Velocity/productivity in terms of story points. Compare the\naverage per sprint with the velocity measured over the system as a whole.\nExpect an upward trend initially as the team gets up to speed. Expect the\ntrend to move toward a stable rate of \u201cburning\u201d story points each sprint\n(assuming similar conditions such as team composition). If version con\u2010\ntrol is not present, we expect productivity to be lower, as it causes more\noverhead to check and merge code adjustments. The story point metric\nclearly assumes that story points are estimated consistently by the team. To\nuse this metric we need to assume that on average a story point reflects\napproximately the same amount of effort.\nNote that the initial measurements can serve as a baseline for later\ncomparison. Often, the trend is more meaningful than the (initial)\nvalues.\n4.4 Common Objections to Version Control Metrics\nWhen you have a version control system in place and you adhere to the two most\nimportant conventions of version control, you are in a better position to measure the\neffectiveness of your development process. Common objections to the best practice\nin this chapter is that using different version control systems inhibits analysis or that\nspecific measurements are unfeasible because commits are hard to trace to issues.\nObjection: We Use Different Version Control Systems\n\u201cWe cannot meaningfully measure branch lifespan because one part of the team likes Git\nwhile another prefers Subversion.\u201d\nThere seems to be a problem underlying this that is more important than the meas\u2010\nurement. Measuring the effectiveness of your version control is unfeasible if you use\nversion control in an inconsistent manner. Using different version control systems\nincreases complexity for merging and partially offsets its advantages. As a team you\nwill need to make a choice for one over the other version control system in order to\nachieve consistency."}
{"52": "Objection: Measuring the Recommendations Is Unfeasible (for\nExample, Whether Commits Are Specific)\n\u201cWe cannot determine in all cases where specifically commits refer to. We would need to\nread through all commit messages.\u201d\nConsider that it is a matter of discipline whether commit messages are specific. In\norder to find out whether commit messages are specific, you could sample commit\nmessages or ask developers directly. Many version control systems help you to be spe\u2010\ncific in pushing changes by requiring a commit message, but this can also be enforced\ntechnically. This could be done by adding a pre-commit check in the version control\nsystem that checks whether the commit message contains a valid ticket identifier.\n4.5 Metrics Overview\nAs a recap, Table 4-1 shows an overview of the metrics discussed in this chapter, with\ntheir corresponding goals.\nTable 4-1. Summary of metrics and goals in this chapter\nMetric # in text Metric description Corresponding goal\nVC 1a Number of developers that did not commit the last Preventing long-lived development branches\n24 hours\nVC 1b Average branch lifespan Preventing long-lived development branches\nVC 1c Commit frequency Preventing long-lived development branches\nVC 2a Average issue resolution time Team productivity\nVC 2b Percentage of issues raised and solved within sprint Team productivity\nVC 2c Average issue resolution time for each level of urgency Team productivity\nVC 3a Percentage of issues resolved within 1 day Team productivity\nVC 3b Percentage of issues resolved within 3 days Team productivity\nVC 4 Velocity in terms of story point burn rate Team productivity\nSee also Chapter 5 on controlling different environments: development, test, accept\u2010\nance, and production. This avoids surprises such as failed tests due to unequal testing\nand deployment environments."}
{"53": "Experience in the Field\nWe classify version control as a basic practice, as we believe it can be implemented\nfairly easily and is necessary for proper software development. In practice, proper\nversion control actually seems an intermediate practice: only 41% of development\nteams fully adhere to version control best practices, as can be seen in Figure 4-1.\nFigure 4-1. Benchmark results on proper usage of version control in development teams\nWe find that most teams initially apply version control properly, but find it difficult to\ncontrol the more advanced aspects of the best practice, especially keeping develop\u2010\nment branches in order.\nIn one particular situation, we saw that a development team had installed a version\ncontrol system, and that this initially helped them to keep track of the work and keep\nit all in sync. With business demands the team started growing until eventually, they\nneeded to use branching to be able to simultaneously work on different parts of the\ncodebase. With different styles of coding and long branch lifespans, half a year later\nthey were in trouble: the different branches diverged too much to merge them; merge\nconflicts were inexorable and backporting notoriously led to regression issues."}
{"54": ""}
{"55": "CHAPTER 5\nControl Development, Test, Acceptance,\nand Production Environments\nTesting shows the presence, not the absence of bugs.\n\u2014Edsger Dijkstra, Software Engineering Techniques\nBest Practice:\n\u2022 Create separate environments for various stages in the devel\u2010\nopment pipeline.\n\u2022 Keep these environments as similar as possible and strictly\ncontrol the progression of code from one environment to the\nnext.\n\u2022 This improves speed and predictability of development,\nbecause defects found in each environment can be diagnosed\nclearly and fixed without disturbing the development flow.\nImagine that in your development team, a sprint-specific Definition of Done is\ndefined and a version control system is in place. The developers are now able to write\ncode and quickly merge their versions. You notice that development of new features\nproceeds quickly, but the process of testing, acceptance testing, and production mon\u2010\nitoring takes a long time to perform and validate. There is still a fair amount of code\nthat was tested successfully in the test environment that fails during acceptance test\u2010\ning. The team encounters trouble when they need to fix bugs/defects for which they"}
{"56": "These are the kind of issues that may be caused by inconsistencies between different\nenvironments: Development, Test, Acceptance, and Production (DTAP in short). Let\nus briefly review these four environments:\n\u2022 Development, in which developers modify source code. This environment is\noptimized for developer productivity.\n\u2022 Test, in which the system is tested in various ways (e.g., validating whether tech\u2010\nnical requirements are being met). It is optimized for testing developers\u2019 products\nefficiently and in combination.\n\u2022 Acceptance, in which it is verified whether user needs are met. It is optimized for\nmimicking the production environment as realistically as possible.\n\u2022 Production, where the system is made available to its users. It therefore should be\noptimized for operation\u2014that is, it needs to be secure, reliable, and perform well.\nBy controlling this \u201cDTAP street\u201d (the pipeline from development through produc\u2010\ntion) you are in a better position to interpret problems. In particular, by ruling out\ninconsistent environments as a cause.\nControlling DTAP means defining, agreeing on, and standardizing three main char\u2010\nacteristics:\n\u2022 The configuration of different environments.\n\u2022 The transfer from one environment to another.\n\u2022 Responsibilities of each environment\u2014that is, which activities are performed\nwhere (notably, whether fixes for issues found in later stages always need to be\nretested in earlier stages).\nMaking assumptions about the behavior of different environments causes trouble.\nConsider an example in which bugs are found in production after acceptance testing.\nIf you can only guess that the acceptance and production environments are \u201cfairly\nsimilar\u201d in configurations, version numbers, etc., it will be a lot harder to find the\nunderlying causes of bugs.\nTake careful notice: for this best practice to work, the organization will need to be\ninvolved. Consider that an IT department could be reluctant to give up control over\nenvironments. But this separation of environments is a crucial step. Failure to have a\nproper separation of environments will jeopardize progress later!"}
{"57": "5.1 Motivation\nControlling your DTAP street is useful for at least the following reasons:\n\u2022 It clarifies responsibilities of the different development phases, avoiding undesir\u2010\nable behavior.\n\u2022 It allows predicting the required effort of each phase of development and thus\nplanning.\n\u2022 It allows identifying bottlenecks and problems in the development process early.\n\u2022 It reduces dependence on key personnel.\nConsider that without defining different environments, testing, acceptance, and\npushing to production \u201coccurs somewhere, sometime.\u201d The DTAP environments are\nundefined (Figure 5-1).\nFigure 5-1. Uncontrolled DTAP\nIn this situation it is rather likely that the environments are set up and configured dif\u2010\nferently. They might be using the same resources (or not), they might use similar test\ndata, they might be configured properly. Who knows? In fact, we see often that the\nanswer to this question is unclear.\nIn many organizations the availability of these environments is an issue, especially for\nacceptance environments. Oftentimes we see that acceptance tests have to be reserved\nand planned well in advance. Similarly, the test environment is often shared with\nother systems. Without clear separation of capacity it will then be hard to tell whether\nperformance test results are due to code changes or the environment itself.\nIn a highly controlled DTAP street (Figure 5-2), the environments\u2019 configuration and\nsetup are predictable and consistent with agreements. Notably test, acceptance, and\nproduction environments should be as similar as possible. Separation between may\nbe technical (e.g., transfer between different servers) or formal (handover), and it\nmay be physical (different servers) or virtualized (a physical server with different"}
{"58": "Figure 5-2. Controlled DTAP\nControlled DTAP Clarifies Responsibilities Between Development\nPhases\nDifferent development environments should separate concerns, just like good soft\u2010\nware should separate concerns in its implementation. Let us discuss the typical\nboundaries of different environments:\n\u2022 A separation between development and test environments distinguishes clearly\nwhich code is ready for testing and which code is under development. Unit tests\nare commonly performed locally by developers, but these ought to be repeated in\na test environment (typically managed by a Continuous Integration server).\n\u2022 The separation between test and acceptance environments is needed to avoid\ntime being wasted on verifying the system while the code is insufficiently tested\nand not ready. The test environment should be as similar to the production envi\u2010\nronment as possible in order to obtain realistic results.\n\u2022 The separation between acceptance and production is needed to prevent code\ngoing to production that is insufficiently verified; i.e., the system does not behave\nthe way it is supposed to. Typical examples of this are performance or reliability\nissues because of certain implementation/configuration flaws in the production\nenvironment.\nControlled DTAP Allows for Good Predictions\nWhen you have a clear separation between responsibilities in the DTAP street, you\ncan meaningfully measure the time spent in each phase. The more consistent the\nenvironments are among each other, the better you can compare those measure\u2010\nments. This is especially useful for making predictions and estimates for the time-to-\nmarket of new functionality. Clear separation of environment responsibilities\nfacilitates accurate estimation of the lead times required for each development phase\n(typically, a division into the four phases of DTAP, but that can be more specific)."}
{"59": "Controlled DTAP Reveals Development Bottlenecks and Explains\nProblems More Easily\nWhen you have an overview of the time it takes to develop new features, you can\ntrack the time between the discovery of a bug and its introduction and the time it\ntakes to resolve it. See Figure 5-3 for a simple visualization of such a feedback cycle.\nFigure 5-3. DTAP feedback cycles\nBy measuring you can verify the difference in effort when issues are identified early\nor late in the development process. Clearly, issues found in production take more\ntime to fix than the ones found in the test environment. Therefore, you want to keep\nthe loop as close as possible. And this is why effective automated testing will make a\nhuge difference: it will identify issues early and effortlessly.\nControlled DTAP Reduces Dependence on Key Personnel\nWith more consistency between environments, less specialist knowledge is required\nto fix problems or set up infrastructure. This is generally true for standardization:\nconsistency means less dependence on assumptions and specialist knowledge.\nIt is of particular interest because different specialties are involved: development, test\u2010\ning (including data, scenarios, scripting), and infrastructure. Consider how much\nexpertise is necessary when none of these is standardized. Not all of this expertise\nmight be part of the development team and therefore there may be dependencies on\nother teams, other departments, and even other companies (when infrastructure/\nhosting are outsourced).\n5.2 How to Apply the Best Practice\nUsually, the production environment is already clearly distinguished from the other\nenvironments in terms of organization. This is because often another department\n(operations) or another organization (hosting provider) is contracted to provide and\nmanage the system\u2019s operation. To get the most out of the distinction between envi\u2010"}
{"60": "For testing and acceptance environments, the distinction is less solid; e.g., when\nwork is performed with cross-functional teams (DevOps).\n\u2022 Only push code into the next environment when all tests have succeeded: when a\ntest fails, the code should quickly return to the development environment so that\nthe developers can fix the underlying cause. For consistency, bug fixes should be\ndone in the development environment only. This seems evident, but we often see\nbug fixes being made in the acceptance environment and then backported (merg\u2010\ning to an earlier version). See also Figure 5-3.\n\u2022 Have the test environments resemble production as much as possible. This\nincludes at least uniformity in versions, (virtualized) configuration of hardware,\nconfigured usage of frameworks, libraries, software, and having representative\ntest data and scenarios. A particular point of interest is the actual test data. The\nideal, most realistic dataset is an anonymized production copy, but this is often\nrestricted because of security requirements or may not be feasible because of the\nproduction database size or other technical issues. For systems in which data\nintegrity is especially important, one solution is to use an older backup that has\nlost its sensitivity.\n5.3 Measuring the DTAP Street in Practice\nSuppose that you have taken steps to make your DTAP environments clearly separa\u2010\nted. You now want to identify the effect on productivity: the effort that the team\nneeds to implement new features. Therefore, as team lead, you could come up with\nthe following GQM model:\n\u2022 Goal A: To understand productivity by gaining insight into effort spent within\nthe different development DTAP phases.\n\u2014Question 1: What is the average time spent in each development phase for a\nparticular feature?\n\u2014Metric 1: Time spent on realizing features/issues/user stories divided by\neach DTAP phase. This could be the average time spent from development\nto acceptance as measured by a Continuous Integration server (see Chap\u2010\nter 7). The time spent for the last push to production is typically manually\nregistered. At the start you may find much effort concentrated in the\nacceptance environment. Gradually you may expect a distribution where\nthe time spent in test and acceptance environments lowers, as tests\nimprove and processes run more smoothly. Ideally, the maximum amount"}
{"61": "\u2014Question 2: How many features pass through the testing environment but fail\nin the acceptance environment?\n\u2014Metric 2: Percentage of rejected features/issues/user stories during accept\u2010\nance testing. You could take the average for each sprint. Expect a down\u2010\nward trend. Investigate acceptance test rejections to see whether human\nerror played a role, or unclear expectations, configurations, or require\u2010\nments. This metric indicates how well code is developed and tested, but\nalso signals how well the test and acceptance environments are aligned.\n\u2014Question 3: How long are the feedback cycles on average?\n\u2014Metric 3: Time between checking in erroneous code into version control\nand discovery of issue. The timeline can be determined after analysis of\nthe bug, sifting through version control. Tracing the issue is clearly easier\nif you follow the version control guideline on doing specific commits with\nissue IDs (refer back to \u201cCommit Specifically and Regularly\u201d on page 30).\nExpect this metric to follow the feature phase time, and expect a declining\ntrend.\nThis model is initially designed to obtain a baseline: the initial value from which you\ncan measure improvement. Consider the first metric: the time spent in each phase\ncombined with the duration of feedback cycles gives you information on how well\ndevelopment moves through the DTAP street. In combination these metrics can help\nyou understand where improvements are possible. For example, when issues seem to\ntake a long time to resolve, an underlying problem could be that the team members\nare unevenly distributed over the different environments. It could be that there are\ntoo many developers compared to the number of testers, so that the testers cannot\nkeep up the work, resulting in a longer phase time in the testing environment.\nFor this, you can use the following GQM model:\n\u2022 Goal B: To understand whether the team is properly staffed/distributed over the\nDTAP environments.\n\u2014Question 4: Is the development capacity balanced with the testing capacity?\n\u2014Metric 4a: Number of work items for testers. Ideally, the amount of work\nshould be stable and according to the testing capacity; that is, the metric\nmoves toward a stable \u201cstandard backlog.\u201d If the backlog is rising it seems\nthat testers cannot keep up with the load. That may lead to different causes\n(e.g., a lack of test capacity, a lack of test automation, or unclarity of\nrequirements)."}
{"62": "actual working hours (as present in your time registration). Comparable\nto the number of work items for testers, a growing number of working\n(over)hours signals that a backlog is building up. You would have to\nassume that team members are writing hours consistently and accurately.\n\u2014Question 5: Are issues resolved at a faster pace than they are created?\n\u2014Metric 5: The sum of created issues versus resolved issues, averaged per\nweek. This would exclude closed issues that are unresolved. A negative\nsum signifies that the backlog is shrinking and that the team can manage\nthe issue load. Expect a downward trend that stabilizes at a point where\nthe number of created issues is on average about the same as the number\nof resolved issues. That signifies that the influx of issues can be managed,\neven when taking into consideration peaks and troughs. In Figure 5-4, the\nsurfaces signify whether more issues are resolved than created (green) or\nwhether the backlog is growing (red).\nFigure 5-4. Created versus resolved issues per day\nWith this GQM model you can determine whether a bottleneck exists in your staff\u2010"}
{"63": "sidered \u201cbusiness as usual;\u201d for example, a 1:1 relationship between development and\ntesting effort.\nConsider that it is advantageous to split functionality into small parts so that they\nreach the testing and acceptance phases more quickly. In terms of automation and\ntooling you can also improve the speed and predictability of code through the devel\u2010\nopment pipeline with the help of the following:\n\u2022 Automate your tests to speed up the two testing phases (see Chapter 6)\n\u2022 Implementing Continuous Integration to improve the speed and reliability of\nbuilding, integrating, and testing code modifications (see Chapter 7)\n\u2022 Automate deployment (see Chapter 8)\nThese practices are discussed in the following chapters. Notice that test automation is\nplaced before Continuous Integration because an important advantage of using Con\u2010\ntinuous Integration servers is that they kick-off tests automatically.\n5.4 Common Objections to DTAP Control Metrics\nObjections against controlling the DTAP street concern perceptions of slowing down\ndevelopment or the idea that it is unnecessarily complex to distinguish test and\nacceptance environments.\nObjection: A Controlled DTAP Street Is Slow\n\u201cControlling DTAP will actually slow down our development process because we need\nmore time to move our code between environments.\u201d\nThere may be some overhead time to move code from the development environment\nto the test environment and from the test environment to the acceptance environ\u2010\nment. But you \u201cwin back\u201d that time when bugs arise: analyzing bugs will be faster\nwhen you know in what phase they occur, as it gives you information about their\ncauses. Moreover, these transitions are suited for automation.\nThe pitfall of controlling different environments is to end up with a classical \u201cnothing\ngets in\u201d kind of pipeline, with formal tollgates or entry criteria (e.g., some sort of rigid\ninterpretation of service management approaches such as ITIL)."}
{"64": "Objection: There Is No Need to Distinguish Test and Acceptance\nEnvironments\n\u201cWe can do all the testing, including acceptance testing, in one environment, so it is\nunnecessarily complex to create a separate acceptance environment.\u201d\nAn acceptance environment requires an investment to have it resemble the produc\u2010\ntion environment as much as possible. But this has several advantages. First, you can\ndistinguish better between integration issues (essentially of technical nature) and\nacceptance issues (which typically have a deeper cause). Second, it distinguishes\nresponsibilities in testing. When acceptance testing starts, the technical tests should\nall have passed, which provides extra certainty that the system will behave well. Then,\nacceptance tests can make assumptions about the system\u2019s technical behavior. This\nnarrows down the scope somewhat for acceptance tests. Of course, to what extent\nthose assumptions hold depends on whether the configuration in the test environ\u2010\nment is sufficiently representative of production.\n5.5 Metrics Overview\nAs a recap, Table 5-1 shows an overview of the metrics discussed in this chapter, with\ntheir corresponding goals.\nTable 5-1. Summary of metrics and goals in this chapter\nMetric # in text Metric description Corresponding goal\nDTAP 1 Average feature phase time DTAP phase effort\nDTAP 2 Percentage of features that fail during acceptance testing DTAP phase effort\nDTAP 3 Feedback cycle time DTAP phase effort\nDTAP 4a Number of work items for testing Team distribution and workload\nDTAP 4b Workload of different roles Team distribution and workload\nDTAP 5 Sum created versus resolved issues Team distribution and workload\nWith a controlled DTAP street, you are in a good position to shorten feedback cycles\nand delivery times. The next three chapters are aimed at achieving those goals\nthrough automated testing (Chapter 6), Continuous Integration (Chapter 7), and\nautomated deployment (Chapter 8)."}
{"65": "Experience in the Field\nIn our development process assessment work, we check for the existence of separate\nenvironments, and we inspect how closely the production circumstances are simula\u2010\nted in the test environment (think data, infrastructure, configurations, etc.).\nHaving separate environments for development, testing, acceptance, and production\nis something we consider a basic practice: it is increasingly rare to see organizations\nin which that separation is not in order.\nThe difficult part is gaining control over the environments. There are many situations\nwe have seen where test data does not represent production data well. This is not nec\u2010\nessarily a problem, but can lead to unforeseen issues in production that you want to\ncatch beforehand. We regularly see situations where development teams are not\nallowed to replicate all production data, either for privacy or for cost reasons. Those\nreasons may be valid. We stress that business representatives should make an explicit\ntrade-off and determine whether the risks of nonrepresentative data (e.g., unpredicta\u2010\nble reliability or performance) are acceptable.\nThose risks vary with the system\u2019s usage intentions and requirements. A core banking\napplication certainly needs representative test data for testing reliability, whereas an\nHR system that is allowed to be offline for a day may not benefit a lot from investing\nin creating a (mock) dataset.\nOur benchmark shows that, even though we classify separation of DTAP as a basic\npractice, in reality it appears much harder. It appears as an intermediate practice with\nless than 40% not fully applying it (Figure 5-5).\nFigure 5-5. Benchmark results on proper control of DTAP in development teams"}
{"66": ""}
{"67": "CHAPTER 6\nAutomate Tests\nBeware of bugs in the above code; I have only proved it correct, not tried it.\n\u2014Donald Knuth\nBest Practice:\n\u2022 Write automated tests for anything that is worth testing.\n\u2022 Agree on guidelines and expectations for tests and keep\ntrack of test coverage.\n\u2022 Automated tests help to find and diagnose defects early and\nwith little effort.\nThe advantages of automation are easy to recognize. Automated tasks are effortless to\nrun. The more often an automated task is executed, the more you are saving. In addi\u2010\ntion to saving time and effort for repeating the same tasks, it also removes opportu\u2010\nnity for error. Therefore it adds reliability to your development process. This leads to\nless rework and thus faster development. Of course, to automate something, you need\nto invest in the automation itself and its maintenance (maintaining test code, script,\ntest data, etc.). After the investment, you save time at each repeat. You should aim to\nautomate as much as feasible in the development pipeline, as that gives developers\nmore time to spend on other, creative tasks. You should also make it easy for yourself\nby using testing frameworks and tooling as much as possible.\nTesting tasks are excellent candidates for automation. Table 6-1 provides a summary"}
{"68": "Table 6-1. Types of testing\nType What it tests Why Who\nUnit test Behavior of one unit in isolation Verify that units behave as Developer (preferably\nexpected of the unit)\nIntegration test Joint behavior of multiple parts (units, Verify that parts of the Developer\nclasses, components) of a system at once system work together\nEnd-to-end (or system) System interaction (with a user or Verify that system behaves Developer/tester\ntest another system) as expected\nRegression test (may be Previously erroneous behavior of a unit, Ensure that bugs do not Developer/tester\nunit/integration/end-to- class, or system interaction reappear\nend test)\nAcceptance test (may be System interaction (with a user or Confirm the system End-user\nend-to-end test if another system) behaves as required representative (never\nautomated) the developer)\nNote that in Table 6-1, only unit tests are white-box tests, in which the inner workings\nof the system are known to the tester. The other tests operate on a higher level of\naggregation. Thereby they are making assumptions about the system\u2019s internal logic\n(black-box tests). Different types of testing call for different specialized automation\nframeworks. Test frameworks should be used consistently and, therefore, the choice\nof test framework should be a team decision. The way in which (testing) responsibili\u2010\nties are divided may differ per team. For example, writing integration tests is a speci\u2010\nalized skill that may or may not reside within the development team, but unit testing\nis a skill that every developer should master.\n6.1 Motivation\nAutomated tests increase confidence in the reliability of your code. Automated tests\nfind causes of problems early and help to reduce bugs in your code.\nAutomated Testing Finds Root Causes of Bugs Earlier with Little Effort\nAutomated tests give more certainty on the root cause of problems because they are\nexecuted consistently. Therefore, if a test executes a piece of code at two different\npoints in time yet gives different results, you know that something has changed in the\nsystem to cause that outcome. With manual tests you do not have the same amount of\ncertainty.\nBecause automated tests generally run fast and their effort for execution is negligible,\nthey allow for early identification of problems. This early identification limits the"}
{"69": "This is why acceptance tests should ideally be automated as much as possible. Func\u2010\ntionality visible to users can be tested with frameworks that simulate user behavior\nand can \u201cwalk through\u201d the user interface. An example is scripted user interaction\nthrough the screen, for which frameworks may provide a specific scripting language.\nThis is especially useful for simulating browser interaction in web applications.\nAutomated Testing Reduces the Number of Bugs\nAutomated tests help your software become \u201cmore bug-free\u201d (there is no such thing\nas bug-free software). Take, for example, unit tests and integration tests: they test the\ntechnical inner logic of code and the cohesion/integration of that code. Certainty\nabout that inner working of your system avoids introduction of bugs (not all, of\ncourse). Therefore, unit tests allow for an effortless double-check of the entire code\u2010\nbase (isolated in units, of course), before turning to the next change.\nWriting tests also has two side effects that help reduce bugs:\n\u2022 By writing tests, developers tend to write code that is more easily testable. The act\nof designing the tests makes developers rethink the design of the code itself:\nclean, simple code is testable code. If a developer finds that units are hard to test,\nit provides a good reason and opportunity to simplify those units. This is typi\u2010\ncally done by refactoring\u2014for example, separating responsibilities into units, and\nsimplifying code structure. Writing tests thus results in code that is easier to\nunderstand and maintain: you easily consider that to be higher quality code.\nSome development approaches advocate writing a unit test before writing the\ncode that conforms to the test (this approach is popularized as Test-Driven Devel\u2010\nopment or TDD). TDD leads to writing methods that have at least one test case\nand therefore TDD tends to deliver code with fewer errors.\n\u2022 Tests document the code that is tested. Test code contains assertions about the\nexpected behavior of the system under test. This serves as documentation for the\nassumptions and expectations of the system: it defines what is correct and incor\u2010\nrect behavior.\nKeep in mind that tests only signal problems; they do not solve\ntheir root cause. Be aware that a team may acquire a false sense of\nsecurity when all tests pass. That should not release them from crit\u2010\nically reviewing code smells."}
{"70": "6.2 How to Apply the Best Practice\nBased on our experience, we discuss the most important principles for achieving a\ngreat level of test automation. Come to clear agreements on tests and implement the\nright range of them. Make sure that those tests limit themselves to the things you\nwant to test. Then plan and define responsibility for their maintenance:\nAgree on guidelines and expectations for tests\nIt is helpful to formalize test guidelines as part of the Definition of Done (see also\nChapter 3). The principles described in this section further help you define what\ncriteria should be adhered to for developing, configuring, and maintaining tests.\nAll relevant tests are in place\nThe development team must agree on which tests need to be in place and in what\norder. Generally the order moves from detailed, white-box testing to high-level,\nblack-box testing (see Table 6-1). Unit tests are most critical, as they provide cer\u2010\ntainty about the technical workings of the system on the lowest level. The behav\u2010\nior of code units is often the root cause of problems. You can therefore consider\nunit tests to be a primary safeguard against repeated defects (regression). But\nunit tests are no guarantee. This is because their isolated nature on the unit level\ndoes not tell you how the system performs as a whole. So higher-level tests always\nremain necessary.\nTherefore, the high-level tests such as integration tests and end-to-end tests give\nmore certainty about whether functionality is broken or incorrect. They are more\nsparse as they combine smaller functionalities into specific scenarios that the sys\u2010\ntem likely encounters. A specific and simple form of end-to-end testing is to con\u2010\nfirm that basic system operations are in good order. For example, testing whether\nthe system is responsive to basic interaction or testing whether system configura\u2010\ntion and deployment adhere to technical conventions. They are commonly\nreferred to as smoke tests or sanity tests. Typical examples of frameworks that\nallow for automated (functional) testing include SOAPUI, Selenium, and Fit\u2010\nNesse.\nThe occurrence of bugs (or unexpected system behavior) is an\nopportunity to write tests. These tests will verify that the bugs do\nnot reappear. Make sure to evaluate such situations, such that the\nteam learns in terms of, for example, sharper specifications or\nrequiring similar tests in the DoD."}
{"71": "unit tests. In practice, this roughly translates to a 1:1 relation between the volume\nof production code and test code. A 100% unit test coverage is unfeasible,\nbecause some fragments of code are too trivial to write tests for. For other types\nof tests, the most important test scenarios must be defined.\nGood tests are isolated\nThe outcomes of a test should only reflect behavior of the subject that is tested.\nFor example, for unit testing, this means that each test case should test only one\nfunctionality. If a test is isolated to a certain functionality, it is easier to conclude\nwhere deviations from that functionality are coming from: the cause is restricted\nto the functionality it is testing. This principle is rather straightforward, but also\nrequires that the code itself has a proper isolation and separation of concerns. If\nin a unit test the state or behavior of another unit is needed, those should be\nsimulated, not called directly. Otherwise, the test would not be isolated and\nwould test more than one unit. For simulation you may need techniques like\nstubbing (here: a fake object) and mocking (here: a fake object simulating behav\u2010\nior). Both are substitutes for actual code but differ in the level of logic they con\u2010\ntain. Typically a stub provides a standard answer while a mock tests behavior.\nTest maintenance is part of regular maintenance\nWritten tests should be as maintainable as the code that it is testing. When code\nis adjusted in the system, the changes should be reflected in tests, unit tests par\u2010\nticularly. Part of regular maintenance is therefore that developers check whether\ncode changes are reflected in modified and new test cases.\n6.3 Managing Test Automation in Practice\nImagine that your team has identified a backlog for developing tests. The team is\nstarting up, so to say. Unit test coverage is still low, there are a few integration tests,\nand no end-to-end tests yet. The team therefore depends largely on feedback from\nthe acceptance tests for identification of bugs. Sometimes those bugs are identified\nonly in production. When (regression) defects are found, they go back to develop\u2010\nment. The team wants to catch problems earlier. Let us consider the following GQM\nmodel formulated from the viewpoint of the team lead. Again, note that initial meas\u2010\nurements can serve as a baseline for later comparison. The ideal state would be\ndefined in the DoD and an idea of the future state should appear from a trend line.\nEven when the ideal level for a metric is unknown, you can tell whether you are\nimproving. Then, changes in the trend line ask for investigation. It is now important\nfor you to understand how you can efficiently and effectively focus efforts on writing\ntests:"}
{"72": "\u2014Question 1: What is the reach/coverage of our automated tests for functional\nand nonfunctional requirements?\n\u2014Metric 1a: Unit test coverage. Unit test coverage is a metric that is well\nsuited for a specific standard. The easiest way to measure this would be\nline coverage, with an industry standard objective of 80%. The coverage\nmay of course be higher if developers identify a need for that. A higher\ncoverage gives more certainty, but in practice 95% is about the maximum\ncoverage percentage that is still meaningful. That is typically because of\nboilerplate code in systems that is not sensible to test, or trivial code that\ndoes not need to be tested all the way.\n\u2014Metric 1b: Number of executed test cases grouped by test type. Expect an\nupward trend. Clearly this metric should move upward together with the\nnumber of tests developed. However, there may be reasons to exclude cer\u2010\ntain test cases. Exclusion needs some kind of documented justification\n(e.g., excluding when test cases are currently not relevant).\n\u2014Question 2: How much effort are we putting into developing automated tests?\n\u2014Metric 2a: Number of test cases developed per sprint, ordered by test type.\nExpect an upward trend until the team gets up to speed and gains experi\u2010\nence with test tooling. Because different test cases/scripts are typically pro\u2010\nduced with different frameworks (e.g., performance, security, GUI tests),\nthey have different productivity. Therefore they should be compared\namong their own type. After some fundamental tests are in place, expect\nthe number of tests developed to drop over time. However, they will still\nbe maintained. Source for this metric could be, for example, the Continu\u2010\nous Integration server or a manual count.\n\u2014Metric 2b: Average development + maintenance effort for tests ordered by\ntest type. Expect the effort for developing and maintaining tests to drop on\naverage as the team gains experience and as some fundamental tests are in\nplace. Maintenance effort could be a standard ratio of maintenance work\nor be separately administrated.\n\u2014Question 3: How much benefit do automated tests offer us?\n\u2014Metric 3a: Total development + maintenance effort for integration/regres\u2010\nsion/end-to-end tests divided by number of defects found. Initially, expect a\ncorrelation between a higher test development effort and fewer defects\nfound. This would signify that tests become better over time in preventing\ndefects. As the system grows naturally, more defects may occur because of\nmore complex interactions. Therefore, the effect between test develop\u2010"}
{"73": "fore you can assume that time for writing unit tests is not separately regis\u2010\ntered.\n\u2014Metric 3b: Manual test time in acceptance environment. With an increase\nin test automation, expect a downward trend in manual test time. It may\nstabilize at some \u201cminimum amount of manual acceptance testing.\u201d Some\ndevelopment teams do achieve a full automation of acceptance testing but\nyour organization might not have that ambition and constellation. In par\u2010\nticular, ambitions would need to translate to costs and efforts of automat\u2010\ning acceptance tests, and that is rather uncommon. There is an advantage\nof keeping part of acceptance tests manual: keeping in contact with users\nwith the help of demos after each sprint (sprint reviews) also provides\nunforeseen feedback on usability, for example.\n\u2014Metric 3c: Manual test time in acceptance divided by number of defects\nfound in acceptance. This metric can help you to determine whether full\nacceptance test automation is a realistic goal. In general, the correlation is\npositive: the less time you spend on manual acceptance testing, the fewer\ndefects you will find; the more time you spend, the more you will find. But\nwith a high level of test automation, outcomes of additional manual\nacceptance tests should not surprise you. At some point you should find\nthat extra effort spent in manual acceptance testing identifies no more\nbugs.\n\u2014Metric 3d: Velocity/productivity in terms of story points per sprint. You can\ncompare the velocity within a sprint with the average velocity over all\ndevelopment of the system. Initially you may expect an upward trend as\nthe team gets up to speed and moves toward a stable rate of \u201cburning\u201d\nstory points each sprint. We also expect productivity to gain with better\ntests. The velocity may be adversely affected by improperly implemented\nversion control (because of merging difficulties after adjustments). The\nstory point metric clearly assumes that story points are defined consis\u2010\ntently by the team, so that a story point reflects approximately the same\namount of effort over time.\nAchieving the goal of optimizing the number of automated tests (Goal A) requires\ncareful weighing. The question of whether you are doing enough testing should be\nanswered by trading off risks and effort. With a mission-critical system the tolerance\nfor bugs may be much lower than for an administrative system that does not cause\ntoo much trouble if it happens to be offline. A simple criterion to determine whether\nenough tests have been written is answering the question \u201cdoes writing a further test"}
{"74": "ing the datasets for performance tests when functionality has changed. So testing, just\nlike development, never really stops.\nOne way of visualizing test coverage is by using a treemap report: a chart that shows\nhow big parts of your codebase are and how much of it is covered by tests (they could\nbe unit tests, but also integration tests). Figure 6-1 shows a simplified visualization.\nFigure 6-1. An example treemap test coverage report\nAlthough such an image is not ideal for discovering a trend in your test coverage, it\ngives you insight into the overall test coverage of your system. In Figure 6-1, you can\nsee that the database layer is hardly covered by tests, as well as the user interface part\nof the system. This does not need to be a problem, but asks for some analysis.\nDatabase-related code should normally not include a lot of logic and you should\ntherefore expect tests in the database abstraction layer. Depending on your priorities,\nyou may want to increase these test coverages or you may want to focus on getting,\nfor example, the business logic layer fully covered.\nOnce you have more insight on the status of your test coverage, you can also better\nestimate the weak spots in your codebase:\n\u2022 Goal B: Knowing our weak spots in code that automated tests should catch.\n\u2014Question 4: Based on test results, how well is our code guarded against known\ndefects?\n\u2014Metric 4a: Total of passed and failed tests ordered by test category. Expect a\nconsistently high ratio of passed versus failed tests. Failed tests are not\nnecessarily a problem. In fact, if tests never fail, they may not be strict\nenough or otherwise they may not have proper coverage of functionality.\nA sudden increase of failing tests definitely demands investigation. It"}
{"75": "\u2014Metric 4b: Percentage of failed tests blocking for shipment (has highest\nurgency). Expect a consistent low toward zero. Deviations ask for investi\u2010\ngation.\n\u2014Metric 4c: Percentage of failed unit tests for each build. Expect a consistent\nlow percentage. Unit tests should fail sometimes, but they may signify that\ncertain pieces of code are particularly complex or entangled. That may be\na good starting point for refactoring efforts.\n\u2014Metric 4d: Number of defects found in Test, Acceptance, and Production.\nThis applies to all defects such as disappointing performance or found\nsecurity vulnerabilities, but notably regression bugs can be well identified\nand prevented by tests. Therefore, the ideal number is zero but expect that\nto never happen. Do expect a downward trend and decreasing percentages\nbetween Test, Acceptance, and Production. The later the bugs are found,\nthe more effort they require to solve. They may also signify the complexity\nof the bugs and give you new information about how tests can be\nimproved.\n\u2014Question 5: How well do we solve defects when they are identified?\n\u2014Metric 5a: Average defect resolution time after identification. Consider the\naverage resolution time for the current sprint and compare with the aver\u2010\nages of all sprints. Expect a downward trend over time toward a stable\namount of time needed to resolve defects, as tests are becoming more\nadvanced. When comparing trends on system or sprint levels, you can tell\nwhether in the current sprint, solving defects was easier or tougher than\nnormal.\n\u2014Metric 5b: Number of reoccurrences of the same or similar bug. Expect a\ndownward slope with an ideal count of zero. Good tests particularly\nshould avoid regression bugs from reappearing. This assumes mainly that\nreoccurrences are traceable. See also the following assumptions.\nAssumptions Regarding These Metrics\nAgain we will make some assumptions about the nature of the metrics. These\nassumptions should allow us to keep the metrics fairly simple and help understand\npossible explanations of unexpected trend behavior.\nFor the metrics discussed here, consider the following assumptions:\n\u2022 Defects cannot be avoided completely but good tests identify failures/regression"}
{"76": "\u2022 On average, the odds of bugs and defects occurring within a system are roughly\nthe same. We know this is not true in practice, because as a system grows in size\nand complexity, code tends to become more entangled and therefore there are\nmore possibilities of defects occurring. However, we want to keep this assump\u2010\ntion in mind because good tests should still ensure that these bugs are caught\nearly.\n\u2022 We ignore weekends/holidays for the defect resolution metrics. Refer back to the\ndiscussion in \u201cMake Assumptions about Your Metrics Explicit\u201d on page 15.\n\u2022 Defects (in production) will be reported consistently when they occur. This\nassumption keeps us from concluding too quickly that a change in the number of\ndefects is caused by a change in user behavior or defect administration.\n\u2022 Defects are registered in a way that reoccurrences refer to an earlier defect with,\nfor example, a ticket identifier.\n\u2022 Defects are preferably registered with an estimate of resolution effort. However, it\nis generally beneficial to leave the defect effort estimates out of consideration, as\notherwise we are essentially measuring how good the team is at effort estimation.\nWhat we really want to know is how good the team is at resolving defects. If\nthose defects always happen to be tough ones, it is fair to investigate whether\nthose large defects could have been avoided with code improvements and more,\nbetter tests.\n\u2022 Hours of effort are registered in a way specific enough to distinguish different\ntypes of development and test activities, for example, as development time for\ntests, and time for manual acceptance tests.\n\u2022 Writing unit tests is considered as an integral part of development and therefore\nnot separately registered.\n6.4 Common Objections to Test Automation Metrics\nIn this section, we discuss some common objections with respect to automating tests\nand measuring their implementation. The objections deal with the visibility of failing\ntests in production and the trade-off for writing unit tests.\nObjection: Failing Tests Have No Noticeable Effects\n\u201cIn our measurements we see that tests keep failing but they do not have noticeable\neffects in production.\u201d"}
{"77": "Suddenly failing regression tests\nTests may fail because of changes in test data, test scripts, external systems\u2019\nbehavior or data, or the test environment itself (tooling, setup), etc. Changes in\nany of the aforementioned may have been intentional. However, they still ask for\nanalysis of the causes and revision of the tests.\nConsistently failing unit tests without noticeable effects\nFailing unit tests are a showstopper for going to production! However, we know\nfrom experience that this occurs. An explanation can be that the failing unit tests\nconcern code that does not run in production (and thus the test should be adjus\u2010\nted). It is more likely that effects are not noticeable because functionality is rarely\nused, or because the test is dependent on rare and specific system behavior. In\nfact, such rare and specific circumstances are typically the cases for which unit\ntests are insufficiently complete: tests should fail but they do not, because they\nhave not been taken into account during development.\nThus, tests that fail but do not (immediately) have noticeable effects in produc\u2010\ntion can still be highly useful tests. Remember that functionality that is rarely\nused in production may still be used at any moment and could even become fre\u2010\nquently used in the future. Your automated tests should give you confidence for\nthose situations as well.\nObjection: Why Invest Effort in Writing Unit Tests for Code That Is\nAlready Working?\n\u201cWhy and to what extent should we invest in writing extra unit tests for code that\nalready works?\u201d\nThe best time to write a unit test is when the unit itself is being written, because the\nreasoning behind the unit is still fresh in the developer\u2019s mind. When a very large sys\u2010\ntem has little to no unit test code, it would be a significant investment to start writing\nunit tests from scratch. This should only be done if its effort is worth the added cer\u2010\ntainty. The effort is especially worth it for critical, central functionality and when\nthere is reason to believe that units are behaving unpredictably.\nA common variation of this objection is that there is no time to write (unit) tests\nbecause developers are pushed to deliver functionality before a deadline. This is\nunfortunately common but very risky. We can be curt about this: no unit tests means\nno quality. If all else fails, the team may choose to write higher-level tests that add\nconfidence on the functionality such as interfaces and end-to-end tests. However,\nwithout proper unit testing you will lack confidence knowing whether or not the sys\u2010"}
{"78": "Make it a habit for all developers to review (or add) unit tests each\ntime units are changed or added. This practice of leaving code bet\u2010\nter than you found it is known as the \u201cBoy Scout Rule.\u201d Consider\nthat it is especially worth refactoring code during maintenance\nwhen unit test coverage is low because the code is hard to test.\n6.5 Metrics Overview\nAs a recap, Table 6-2 shows an overview of the metrics discussed in this chapter, with\ntheir corresponding goals.\nTable 6-2. Summary of metrics and goals in this chapter\nMetric # in text Metric description Corresponding goal\nAT 1a Unit test coverage Optimal amount of testing (coverage)\nAT 1b Executed test cases Optimal amount of testing (coverage)\nAT 2a Developed test cases Optimal amount of testing (investment)\nAT 2b Test case development/maintenance effort Optimal amount of testing (investment)\nAT 3a Development effort versus defects Optimal amount of testing (effectiveness)\nAT 3b Manual acceptance test time Optimal amount of testing (effectiveness)\nAT 3c Manual acceptance test time versus defects Optimal amount of testing (effectiveness)\nAT 3d Velocity Optimal amount of testing (effectiveness)\nAT 4a Passed versus failed tests Robustness against bugs\nAT 4b Percentage failed blocking tests Robustness against bugs\nAT 4c Percentage failed unit tests for each build Robustness against bugs\nAT 4d Defects found in Test, Acceptance, Production Robustness against bugs\nAT 5a Average defect resolution time Defect resolution effectiveness\nAT 5b Reoccurrences of same bug Defect resolution effectiveness\nTo fully benefit from automated tests you must integrate test runs in the development\npipeline so that they are also kicked off automatically. Notably this concerns unit tests\nas part of the build cycle. Chapter 7 elaborates on this."}
{"79": "Experience in the Field\nBecause we believe that testing is paramount to high-quality software, we assess it\nextensively in our daily work. Notably we benchmark the maturity of an organiza\u2010\ntion\u2019s testing strategy and the fitness of tests given the requirements.\nWe see that having a clear test strategy is already quite difficult: you need to think\nabout the types of tests you perform, how much coverage you require, and then\nensure that this strategy is known to everyone involved.\nRequirements testing turns out to be an advanced practice, only fully applied by 13%\nof the teams. Often testing criteria are not defined at all, or they are not SMART. We\nalso see cases where SMART test criteria are defined, test results are available, but\nthey are not looked into until problems occur in production.\nThis leads to the benchmark result shown in Figure 6-2.\nFigure 6-2. Benchmark results on proper (automated) testing in development teams"}
{"80": ""}
{"81": "CHAPTER 7\nUse Continuous Integration\nFall seven times, stand up eight times\n\u2014Japanese proverb\nBest Practice:\n\u2022 Achieve Continuous Integration by setting up a CI server\nafter you have version control, build automation, and automa\u2010\nted tests in place.\n\u2022 Keep track of the build time and test time.\n\u2022 This improves the development process because it relieves\ndevelopers of building the system and it shortens the feed\u2010\nback loop of issues.\nThe term continuous in Continuous Integration (CI) says it all: continuously integrate\n(merge) code. CI puts into practice the idea that code integration and building should\nbe done as often as possible. The integration happens at each commit to a CI server\nthat merges the commit into a central repository. Fortunately, most CI servers do\nmore: after each commit, they perform a clean checkout of the branch that was\nchanged, build it, and perform the supplied tests (unit, integration, and regression\ntests). Running automated tests is not \u201crequired\u201d in order to be called CI, but why\nwould you not? Using a CI server is a great opportunity to have your tests run auto\u2010\nmatically. You can even automate further and run additional scripts after successful\nbuilds, to achieve automated deployment. The latter is a topic for Chapter 8."}
{"82": "7.1 Motivation\nContinuous Integration speeds up development because it is fast, has short feedback\ncycles, and is more reliable than manual integration. Thereby it allows for further\nautomation steps such as \u201ccontinuous delivery.\u201d\nCI Is Efficient\nWith test automation as part of the CI server, developers are relieved from tedious\nmanual integration efforts and integration testing. Because automated tests are fast\nand effortless, you can frequently test your system. This controls the consequences of\nhuman error: instead of fearing that integration goes wrong, you merge and test code\nas often as possible.\nOf course, when merge conflicts happen during a commit, the version control system\nasks the developer which change should take precedence over the other.\nCI Gives Feedback Quickly and Thereby Speeds Up Bug Fixing\nCI gives developers quick feedback on whether integration of code succeeds. Quick\nfeedback means identifying problems early, when they are relatively easy to fix. Imag\u2010\nine that your team integrates all code at the end of the week, and it turns out that\napplication builds fail because of integration errors. This can mean a lot of work and\ncode reversions in order to get things to work again.\nCI Is More Reliable Than Manual Merging\nWith CI, the building, merging, and testing process is consistent. As with all automa\u2010\nted tasks, this consistency avoids a fair level of human error. A CI server should start\nprocessing a commit with setting up a clean checkout. That controls the risk that\ncached files or a corrupted database will influence the build process.\nCI Facilitates Further Automation\nUsing a CI server paves the way for other kinds of \u201ccontinuous\u201d development such as\n\u201ccontinuous delivery.\u201d Continuous delivery shares the goals of automating the devel\u2010\nopment pipeline as much as possible and achieving short iterations of deploying new\ncode. Continuous delivery implies that you can deliver each code modification into\nproduction. This demands a high level of certainty on whether the system is working\nas intended. Therefore, it is a horizon for developers as to what can be achieved by\nautomating tests and deployment."}
{"83": "7.2 How to Apply the Best Practice\nBefore you can achieve CI, you need three things: version control, automated builds,\nand (obviously) a CI server. In particular, to get the most out of CI you need automa\u2010\nted tests: CI without testing will do nothing more than tell you whether the code can\nbe merged and compiled.\nRequirement: Version Control\nTo integrate code automatically after each commit, you need a version control system.\nThis is also necessary for the CI server to initiate a build on the central code reposi\u2010\ntory. See Chapter 4 for elaboration on the use of version control.\nRequirement: Automated Builds\nThe CI server will need build scripts and configurations (i.e., configuration of code\nstructure and conventions) in order to execute builds. It is important that you are able\nto synchronize configuration files from developers, not just for Integrated Develop\u2010\nment Environments (IDEs) but also for databases and any other elements that may be\ncustomized for their development environment. In this way, newly introduced depen\u2010\ndencies are taken into account when the CI server executes new builds.\nRequirement: CI Server\nA CI server is a dedicated server that checks out code from version control, uses the\nbuild scripts and configurations to execute builds, and performs tests after building.\nFor basic setups and mainstream technologies, most CI servers already have default\nbuild tools, so this can save you time. For less common technologies, you may need to\nadapt your CI server configuration to accommodate the desired build tools.\nRequirement: Automated Tests\nStrictly speaking, when you have set up the previous parts, you already have Continu\u2010\nous Integration in place. But really you should add automated testing to the mix. A CI\nserver is the ideal tool to perform all necessary unit, integration, and regression tests\nimmediately and report the results to developers.\nAdditions to the CI Server\nMost CI servers allow the execution of additional scripts, depending on the outcome\nof a build. So you can add deployment scripts on successful builds when you have\nautomated your deployment process (this is the subject of Chapter 8). When the"}
{"84": "Important Best Practices for CI\nContinuous Integration works by virtue of regular commits and quick builds.\nIf commits are not done on a regular basis\u2014say, at least daily\u2014you can still end up\nwith integration errors that are hard to fix. After all, after a day a lot of time has been\nspent writing code that may break the build. The person fixing it (not necessarily the\none who \u201cbroke it\u201d) has to go through a lot of code. So commits should be done regu\u2010\nlarly, as often as possible. This assumes that tasks can be broken down into pieces of\nwork that take less than a day, but that is normally not a problem.\nWith branching, separate builds and tests become a greater concern. If developers use\na new branch for every feature they implement, they should be built and tested\nthrough the CI server before they are merged into the master branch. This require\u2010\nment can be configured in the CI server. In this way, you ensure that developers must\nhave a successful build before they merge their new feature into the master branch.\nThis is notably helpful when developers are making feature branches that need exten\u2010\nsive testing (which may be the reason to use feature branches in the first place). This\nsounds trivial, but may be of particular concern when development teams are scat\u2010\ntered geographically and/or have different release cycles.\nWhen an application grows really large, it could be that build and test times increase\nto a point where it becomes impractical to always build after every commit. This\nshould not happen too easily and it is a cue to revisit the design of the system to be\nable to build smaller parts. However, most CI servers can also cope with this techni\u2010\ncally. They allow parallel build jobs and dependency caching to speed up your builds.\nOf course, when changes are layered upon each other and build/test jobs take a long\ntime, there is still a risk that after a certain change, all other changes need revisiting as\nwell.\n7.3 Controlling Continuous Integration\nWith the advantages of CI in speed, reliability, and flexibility, you would expect that\nyour development process is speeding up with fewer issues. Fundamentally you\nwould like to know whether and how productivity is improving. To have more confi\u2010\ndence in a successful implementation of CI for your team, consider the following\nGQM model and metrics."}
{"85": "For the following metrics, the value is again mostly in large deviations, since these\nsignal a problem. In agreement with the team you should determine a norm for\nwhich test and build durations are \u201cshort\u201d enough.\n\u2022 Goal A: Understanding how CI influences development productivity.\n\u2014Question 1: Do our developers receive timely feedback on integration errors?\n\u2014Metric 1: Average feedback time after a commit. Compared to a situation\nwithout CI, the feedback time will be lower, as CI deals with building and\ntesting automatically. Over time, expect the trend line to go up: as the\ncodebase grows, more tests are written and builds take more time. Because\ncodebase volume of systems in maintenance is typically a few percent per\nyear, those times should not raise alarms. The interest of this metric is\nmostly in large deviations, which warrant an investigation: is new develop\u2010\nment somehow too complex? Are tests being skipped?\n\u2014Question 2: Can we integrate/merge fast enough?\n\u2014Metric 2a: Average build time (comparing builds of the master branch).\nExpect a downward trend of this metric if you are comparing with a situa\u2010\ntion without CI. Expect a slight upward trend line over time as the size of\nthe codebase grows.\n\u2014Metric 2b: Average test run time per type of test. Expect this metric to be\nfairly stable, assuming that the different test types (e.g., unit tests, sanity\nchecks, end-to-end tests) are internally consistent (i.e., for each test type,\ntheir tests are similar in size, complexity, etc.).\nIn Question 2, what is \u201cfast\u201d is typically a comparison with the baseline. Primarily, the\nfeedback time depends on server processing time. So if the processing times are out\nof control, it will influence all of the above metrics. You will then need to speed up the\nCI process first before gaining benefits of quick feedback. Beware that speeding up\nbuild time at the expense of creating a clean environment may lower reliability of test\nresults.\nAn easy way to see the progress of your CI server is to show the average build and test\ntimes per week. We show a simplified example in the following figure, where results\nare aggregated per week. Figure 7-1 shows an example chart that tracks the build and\ntest time on a weekly basis."}
{"86": "Figure 7-1. Trend line of build and test times\n7.4 Common Objections to Continuous Integration Metrics\nTypical objections to using CI metrics concern whether its processing time can be\ncontrolled and how one should deal with expectations of who will fix the build. They\nare discussed in this section.\nObjection: We Cannot Get Control Over Our Build Time\n\u201cMy CI server reinstalls all dependencies when a new build is available, so we cannot\nkeep our build time low enough.\u201d\nWe do not wish to be puritans: if reinstalls keep you from doing regular builds, do\nnot reinstall everything. For instance, a database does not have to be reinstalled every\ntime you do a build as long as you clean and refill the database with data that is con\u2010\nsistent with other environments. Of course, the ideal is that the test data realistically\nrepresents data in the production environment. Or you could opt for a special nightly\nbuild that carries out reinstallation of your dependencies. For added certainty it is\nmore important to be sure that builds in the acceptance environment reinstall every\u2010\nthing."}
{"87": "Objection: My Colleague Broke the Build, Not Me\n\u201cAfter my commit, the new build was broken, but I am sure my code is perfect. It is only\nbecause the code of my colleague in the other room does not align with mine that the\nbuild failed.\u201d\nA broken build is a broken system, so this is an urgent matter. These are the situa\u2010\ntions that can be solved by having good agreements about what the leading principles\nof build success are. If you have agreed as a team that the tests determine build suc\u2010\ncess, then your colleague committed code that has passed the test, so your code\nshould be revised. If for some reason the (unit) tests are no longer adequate, then\nthey should have been revised in the latest commit. Who will do the fix is a matter of\nagreement. Effective teams share responsibilities: if the task seems small, any devel\u2010\noper with available time should feel responsible to take it up and fix it.\n7.5 Metrics Overview\nAs a recap, Table 7-1 shows an overview of the metrics discussed in this chapter, with\ntheir corresponding goals.\nTable 7-1. Summary of metrics and goals in this chapter\nMetric # in text Metric description Corresponding goal\nCI 1 Commit feedback time Team productivity\nCI 2a Build time Team productivity\nCI 2b Test run time Team productivity\nWhen you have CI in place in a stable manner, you have already made great progress\ntoward continuous delivery and automation. In the following chapter, we will look at\nautomated deployment: automating manual configuration of environments and their\ndeployment."}
{"88": "Experience in the Field\nContinuous Integration is one of the first steps toward an automated development\npipeline. We check for the proper usage of CI by observing whether there is a CI\nserver used for builds, and whether the server automatically starts a build after each\ncommit. Additionally we check for automated test runs: when there are automated\ntests, does the CI server automatically perform those when a build has passed?\nThe results are shown in Figure 7-2. It turns out that having Continuous Integration\nis an advanced practice when we look at our collected data, although we would clas\u2010\nsify it as intermediate in difficulty. Combining CI with automated test runs seems to\nbe slightly more difficult, but they correlate strongly: teams that do apply CI typically\ninclude automated tests as well.\nFigure 7-2. Benchmark results on Continuous Integration in development teams"}
{"89": "CHAPTER 8\nAutomate Deployment\nThere should be two tasks for a human being to perform to deploy software into a\ndevelopment, test, or production environment: to pick the version and environment\nand to press the \u201cdeploy\u201d button.\n\u2014Jez Humble and David Farley in Continuous Delivery\nBest Practice:\n\u2022 Use appropriate tools to automate deployment to various\nenvironments.\n\u2022 Keep track of deployment times.\n\u2022 This improves the development process because no time is\nwasted on manual deployment tasks or errors caused by\nmanual work.\nOnce your DTAP street is under control, including automated tests that are kicked off\nin the development pipeline by a CI server, a further step for automation is automated\ndeployment. In its general sense, here we mean automatically transferring code from\none environment to the next one. Because this saves time for manual configuration\nsteps, deployment times to new environments may drop from hours to minutes.\nConsider the difference with Continuous Integration: CI is about automating builds\nand the tests that go with it, within the environment. Automated deployment is about\nautomation of the deployment to each environment, typically when integration\n(including tests) has succeeded on the previous environment."}
{"90": "8.1 Motivation\nWhen comparing automated with manual deployment, automated deployment is\nmore reliable, faster, more flexible, and simplifies root cause analysis.\nAutomated Deployment Is Reliable\nIdeally, deployment automation requires no manual intervention at all. In that case,\nonce a build is finished and fully tested, the code is automatically pushed to the next\nenvironment. Such a process is typically controlled by a CI server. Because this is\nautomated and repeatable, it minimizes the amount of (manual) mistakes and errors.\nAutomated Deployment Is Fast and Efficient\nAutomated processes run faster than manual ones, and when they need to be exe\u2010\ncuted repeatedly, the initial investment to automate them is quickly earned back. Typ\u2010\nically, deployment is a specialization of deployment/infrastructure experts who know\ntheir way around different environments. There is always \u201cthat one person\u201d who\nknows how to do it. When this process is automated, such expertise is only needed\nfor significant changes (in the deployment scripts) and root cause analysis of prob\u2010\nlems.\nAutomated Deployment Is Flexible\nBy extension of its reliability and efficiency, automated deployment makes a system\nmore portable (the ability to move a system to another environment). This is relevant\nbecause systems may change infrastructure multiple times in their lifecycle (mainly\nthe production environment, that is). Causes are various but often they are based on\nconsiderations of scalability (dealing with temporary high loads), transfer of a hosting\ncontract to a new party, cost reductions, and strategic decisions (moving infrastruc\u2010\nture to the cloud or outsourcing it altogether).\nAutomated Deployment Simplifies Root Cause Analysis\nGiven that automation makes for a reliable (or at least consistent) process, finding the\ncause of problems is easier. This is because it excludes human error. Causes are then\nmore likely to be found in changed circumstances (e.g., different frameworks, data,\nconfigurations) that somehow have rendered the automation process outdated."}
{"91": "8.2 How to Apply the Best Practice\nIn this section, we discuss specifics on how to achieve automated deployment. The\nmain principle behind this is that you need to think ahead about what steps need to\nbe performed in the pipeline, and what that requires (e.g., in terms of configuration).\nYou will need at least the following controls:\nDefine your environments clearly\nDefine for each environment its goals and intended usage. An inconsistent or\nunstable environment may disrupt your deployment process and makes root\ncause analysis difficult. It is preferable to use virtual environments that are state\u2010\nless in the sense that they clean themselves up after use. By \u201creinstalling\u201d them\nafter use, you do not need to make assumptions about their configuration. See\nChapter 5 for the separation of different environments.\nDefine all necessary steps\nWhat special needs does the system have for deployment? Are different versions\nrun in production? If so, how do they vary? Use such assumptions and invariants\nfor tests that check whether configuration and deployment are set up as intended.\nGet your configuration right\nThe more uniform development environments are, the better. Script configura\u2010\ntion defaults in a uniform manner, so that an environment can be built/rebuilt\nquickly without human intervention. Your tooling should include provisioning\nfunctionality that checks (third-party code) dependencies and installs their\nappropriate versions.\nMake sure you can do rollbacks early\nIn case of unexpected issues in production, such as performance issues or secu\u2010\nrity bugs, you need to be able to quickly revert to an earlier version of your sys\u2010\ntem. Also allow for a rollback during the process\u2014for example, when the\ndeployment process freezes and you want to return to the begin state.\nUse deployment-specialized tooling\nTooling will help you supervise and configure deployment in a consistent man\u2010\nner (e.g., Chef). When portability to other environments has special interest,\ntooling that \u201ccontainerizes\u201d your application is especially useful (e.g., Docker).\nSuch tooling can package deployment configurations, required software compo\u2010\nnents, and other dependencies in a way that testing in different environments can\nbe done consistently. This would benefit you if you intend to use, for example,"}
{"92": "8.3 Measuring the Deployment Process\nWith deployment automation, you would expect that deployment times between\nenvironments will decrease. By extension you can also expect overall delivery time to\ndecrease (from development to production). With added reliability you would also\nexpect a decrease in the frequency and time to analyze deployment errors. Of main\ninterest is then knowing whether deployment automation is helping you to achieve\nimproved productivity and reliability. Then consider the following GQM model:\n\u2022 Goal A: Understand the impact of automated deployment on development effort\nand reliability of the deployment process.\n\u2014Question 1: How much time are we gaining by automating deployment?\n\u2014Metric 1a: Difference between (former) average time spent and current\ntime spent on deployment from acceptance (after a manual or automatic\n\u201cgo\u201d) to production. Expect an upward trend in the beginning because of\nthe investment in writing deployment scripts, testing them, and setting up\ntooling. Then you should expect a downward trend as the scripts and pro\u2010\ncess become more solid and dependable. If the savings are marginal, con\u2010\nsider where most manual work is still required (e.g., troubleshooting, fine-\ntuning, error analysis) and focus on their automation.\n\u2014Metric 1b: Amount of time it takes for code to move from development to\nproduction (as measured by the CI server/deployment tooling). When all\ntests (including acceptance tests) are automated, the gains in time will be\napparent when you compare this metric with a process in which each step\nneeds manual intervention. Expect a downward trend over time as the\nteam gains experience with automating and less manual work is needed.\n\u2014Metric 1c: Percentage of overall development time spent on deployment.\nBy automating, expect the percentage to decrease over time. If absolute\ndelivery time decreases together with this percentage (thus, the relative\neffort for deployment for development as a whole), you can assume you\nhave implemented automated deployment well.\nConsider a situation in which the average deployment time (the baseline) has been\nidentified at 8 hours of effort and you want deployment time on average to stay below\nthe baseline. A trend report may then look like Figure 8-1."}
{"93": "Figure 8-1. Deployment time after automation (in week 10, deployment scripts are ready\nand only minor fixes need to be performed)\n\u2022 Goal A (continued):\n\u2014Question 2: Is our deployment process more reliable after automating it?\n\u2014Metric 2a: Number of deployment issues. Expect this number to drop. If\nyou went from manual to automated deployment, typically you may\nexpect a significant decrease in the number of bugs arising during deploy\u2010\nment, simply because human error is much less of an issue. If this change\nis not visible, your deployment process may not be stable yet. That asks for\nissue analysis. It is most probable that the deployment scripts are yet\nunstable (faulty) or insufficiently tested. Deeper issues could be that the\nproduction environment is inconsistent over time or is inconsistent com\u2010\npared to test environments. You would probably have identified that\nbefore because it will probably distort other measurements as well.\n\u2014Metric 2b: Amount of time spent on solving deployment bugs. Expect a\ndownward trend line. This metric should follow the trend of the previous\nmetric: having fewer issues means less time fixing them. If this is not the\ncase, then bugs apparently are harder to fix when they appear. Then you\nshould investigate why that is the case."}
{"94": "8.4 Common Objections to Deployment Automation\nMetrics\nCommon objections to deployment automation (and its measurement) are that it cre\u2010\nates unnecessary overhead, that automated deployment to production is prohibited,\nor that the developer\u2019s technology of choice cannot be automatically deployed.\nObjection: Single Platform Deployment Does Not Need Automation\n\u201cWe only deploy on a single platform so there is no gain in automating our deployment\nprocess.\u201d\nWhile generally it is easier to deploy on a single platform than on several platforms,\nthe benefits of deployment automation still hold. Automated deployment will still be\nfaster than manual deployment, it makes issue analysis easier (more structured), and\nallows for flexibility such as switching deployment platforms. Just like code itself,\nplatforms requirements will change over time, though not that often. Consider\ndeployment platform updates, new operating systems, switches to mobile applica\u2010\ntions, cloud hosting, 32-bit versus 64-bit systems, and so on.\nObjection: Time Spent on Fixing Deployment Issues Is Increasing\n\u201cAfter automating deployment, we now spend more time on fixing deployment bugs!\u201d\nAutomating deployment requires an elaborate investment. Time spent on fixing\ndeployment issues is initially part of the deal. Gain experience by doing it more often.\nThat experience will ease the process and solidify the scripts. This is one of the rea\u2010\nsons why releases should be done regularly and frequently, given that a sufficient\nwidth and depth of tests add certainty on the system\u2019s behavior.\nObjection: We Are Not Allowed to Deploy in Production By Ourselves\n\u201cFor security reasons, we cannot deploy new releases of our software without permission\nof our user representatives.\u201d\nThis objection concerns the last step in the development pipeline in which code is\npushed to production. However, the advantages of automating the pushes between\ndevelopment, test, and acceptance environments should still hold. When your team\nand the user representatives gain positive experience with the effects of deployment\nautomation, eventually this last push may also be (allowed to be) more automated.\nIn practice we see that some large organizations use acceptance and production envi\u2010"}
{"95": "test automation should help manage those feared consequences, facing such a situa\u2010\ntion may be out of the span of control of the development team.\nObjection: No Need to Automate Because of Infrequent Releases\n\u201cWe only release a few times a year so there is little to gain by automating deployment.\u201d\nClearly, this should not be the case when you are practicing Agile development. We\noften hear this argument with legacy systems, though. Those are the archetypes of\n\u201canti-continuous-deployment.\u201d This situation is likely due to a combination of tech\u2010\nnology/system properties, available expertise, and constrained resources. The more\ninfrequent your deployments, the more likely that they are difficult, requiring exper\u2010\ntise, and thus expensive. Due to these costs, the business may not be willing to pay\n(\u201creleases are really expensive, which is why we only have three of them\u201d), which rein\u2010\nforces the cycle.\nThe \u201cright solution\u201d is to do it more often, to \u201cdiminish the pain of the experience.\u201d\nBut when technical or organizational constraints force you to release only a few times\na year, you may have bigger problems to solve in your development process.\nObjection: Automating Deployment Is Too Costly\n\u201cIt is very hard to get all the people and resources together to automate the deployment.\u201d\nIt could be the case that in your development you are dealing with multiple disparate\nsystems that require manual steps for deployment. This is a particular type of com\u2010\nplexity in which each system may require specific knowledge or skills. Automation of\nthose steps and therefore of the process can seem costly in terms of resources. But it\nshould also be evident that this complexity can benefit greatly from automation. Also,\nmost systems can be automated at least partially. Those parts should be your initial\nfocus to reap the immediate benefits.\n8.5 Metrics Overview\nAs a recap, Table 8-1 shows an overview of the metrics discussed in this chapter, with\ntheir corresponding goals.\nTable 8-1. Summary of metrics and goals in this chapter\nMetric # in text Metric description Corresponding goal\nAD 1a Deployment time acceptance to production compared with baseline Deployment effort and reliability\nAD 1b Deployment time to production Deployment effort and reliability"}
{"96": "This is the last chapter on automation of the development process. After you have\nautomated testing, integration, and deployment, you are right on track to deliver soft\u2010\nware (almost) continuously. In the subsequent chapters, we will look at best practices\nin software development from an organizational perspective. This includes practices\nfor standardization (Chapter 9), managing usage of third-party code (Chapter 10),\nand proper yet minimal documentation (Chapter 11).\nExperience in the Field\nWe consider deployment automation as a two-step process in practice: first we assess\nthe level of automation for moving between environments, especially production.\nThis can range from completely manually copying files from a local machine into a\nproduction environment (basic practice), semi-automatic deployment using a few\ndeployment scripts (intermediate practice), up to complete automation where any\ncommit triggers a build, a full test run, and a deployment given that all tests have\npassed. The latter is a very advanced and rare practice, though.\nThen we also consider existence of patching and/or updating policies. This serves as a\nrisk control of deployment reliability.\nIn the resulting graphs shown in Figure 8-2, we see that these two practices both get\nranked as intermediate.\nFigure 8-2. Benchmark results on deployment automation in development teams\nCompared with deployment automation, a policy is relatively easy to implement but"}
{"97": "CHAPTER 9\nStandardize the Development Environment\nTo accomplish anything whatsoever one must have standards. None have yet accom\u2010\nplished anything without them.\n\u2014Mozi\nBest Practice:\n\u2022 Define and agree on standards in the development process.\n\u2022 Ensure that developers adhere to standards and that stand\u2010\nards reflect the goals of the team.\n\u2022 This improves the development process because standards\nhelp enforce best practices and standards simplify both the\ndevelopment process and code.\nThis chapter deals with standardization in the development process. Standards make\nthe results of development work more predictable. This predictability leads to a\n\u201chygienic development environment,\u201d in which developers work in a consistent man\u2010\nner. By this standardization, we mean four things:\nStandardized tooling and technologies\nAgreeing on default tools and technologies, and not using a complex technology\nstack for solving a simple problem.\nProcess standards\nAgreeing on steps that developers need to perform during development."}
{"98": "Code quality control\nDefining and continuously measuring code quality in terms of complexity, read\u2010\nability, and testability of code.\nArriving at standards that are the best fit for your development team or organization\nis not trivial: if you put three developers in a room, they will usually offer three differ\u2010\nent ways to solve a certain problem, all three according to best practice. The team\nshould ultimately agree on what is \u201cbest.\u201d Settling the issue is up to a quality cham\u2010\npion, which typically is a team lead or (software) architect.\nThe following section explains the benefits of having these standards defined.\n9.1 Motivation\nDevelopment standards make software more predictable, which eases maintenance.\nStandards also help in enforcing best practices, simplifying development and avoid\u2010\ning discussion overhead.\nAs a result, standards also ease comparison of metrics. Comparison allows for trend\nanalysis and may be about comparing developers, teams, systems, or other factors.\nWhen the development process is consistent, there is less \u201cnoise\u201d in observations that\nwould otherwise make metrics hard to compare.\nDevelopment Standards Lead to Predictable Software Development\nStandards lead to consistency and consistency is good for predictability. Therefore,\nagreeing on standards is a useful way to achieve predictable behavior and products\n(the software). The most important advantages for maintenance are:\n\u2022 It lowers the effort to understand how other developers have made decisions\nabout design and implementation.\n\u2022 It makes it more likely that code and tests can be reused by others. Not by copy\u2010\ning and pasting, of course!\nDevelopment Standards Help Enforce Best Practices\nStandards should reflect best practices. With the exclusion of externally set standards\n(e.g., legal requirements), standards appear when a user community apparently pre\u2010\nfers to do things a certain way. Therefore, standards reflect what commonly works\nwell in practice."}
{"99": "Thus indirectly, standards help to enforce development best practices. For example,\nwith consistent naming conventions, the relationships between different pieces of\ncode (be it methods, classes, components) are clear. This helps dividing code into the\nright abstractions and allows, for example, for proper code encapsulation.1\nA naming convention is a common type of standard. Identifiers that are very long or\nhard to understand are confusing for other developers, such as (a real example) Add\nPaymentOrderToPayOrderPaymentGroupTaskHelper. If those identifiers are named in\na consistent manner, it serves as a system\u2019s documentation of itself, because they show\nhow functionality is organized technically.\nThis consistency is important. Standards are only effective when they are applied\nconsistently. And tooling can help you with that (by facilitating or enforcing). A com\u2010\nmon example is to have a CI server automatically check code commits for certain\ncharacteristics (such as an issue identifier) and based on the result, allow or deny the\ncommit.\nDevelopment Standards Simplify Both the Development Process and\nCode\nStandardization decreases the number of specific decisions that developers need to\nmake. Variations in implementations complicate maintenance, such as using different\ntechnologies to solve a similar problem. Using a single technology per domain\nensures that developers can understand each other\u2019s code, and learn from each other.\nThis improves code reusability.\nFor large software development efforts it is common to have separate development\nteams working rather isolated from each other. Without standards, different teams\nmay use a different development process and implement \u201ccustom solutions\u201d that can\u2010\nnot be reused or they may duplicate functionality already present elsewhere.\nDevelopment Standards Decrease Discussion Overhead\nUsing standards limits discussion in development teams to the exceptions outside the\nstandard. This is because with good standards, their context and application are\nunderstood. This eases understanding and communicating what is going on in source\ncode. This also means that code is easier to transfer to new developers or other devel\u2010\nopment teams."}
{"100": "9.2 How to Apply the Best Practice\nWithout standards, questions arise such as \u201cWhat technology should I use?\u201d or \u201cHow\nshould I deal with version control?\u201d With development process standards, these are\ndefined and agreed upon. The following best practices are in place to obtain clear\nstandards.\nStandardize Tooling and Technologies\u2014and Document It\nStandardization of tooling and technologies should include a summary description of\ntheir context. Context information should list a few simple situations with their solu\u2010\ntion. That leaves less room for developers to misunderstand in which situation (and\nhow) they should use which technology.\nAgreeing on standards verbally may not be enough to have a lasting effect. Therefore,\ncentralize documentation of these standards (e.g., in a wiki or document). This allows\nfor access from different teams that may be physically separated.\nA clear standard for tooling and technologies includes the following:\n\u2022 Prescribing a simple technology stack\u2014for example, agreeing to use only one\nback-end programming language for implementing business logic.\n\u2022 Scoping of technologies and frameworks\u2014for example, using multiple special\u2010\nized tools instead of using all features of one framework whose features might\nnot be mature yet. Typically, areas such as testing, security, and performance war\u2010\nrant specialized tooling. This tends to require some deliberation, especially given\ncost trade-offs (licensing/open source). See Chapter 10 for elaboration.\n\u2022 Default configurations of technologies or tooling (e.g., IDE, test suite, plug-ins).\nTo achieve consistency, it is beneficial if default technologies and configurations\nare pushed over the internal network.\nChapter 11 elaborates on documentation efforts.\nConsiderations for Combinations of Technologies\nStandards should be applicable and suitable to your organization and development\nteam. The following are typical considerations for choosing combinations of pro\u2010\ngramming languages and frameworks:\nAssimilation\nIs it likely that one of the technologies will evolve quickly with advanced capabili\u2010"}
{"101": "Compatibility\nHow compatible are the different technologies? Do they require custom code to\nwork together?\nTime frame\nAre the technology\u2019s benefits really needed right now, or are you anticipating\nfuture demands?\nMaturity\nHow long has the technology been around? How viable is usage of the technol\u2010\nogy?\nIndependence\nCan different technologies be maintained independently or are they dependent\non each other?\nSpecialized skills\nDo the different technologies require essentially different technical skills? Is it\nlikely that one developer can work with the complete stack or is specific training\nneeded (or even different staff)?\nTestability\nWill the usage of different technologies significantly complicate testing?\nNonfunctionals\nAre there foreseeable effects on non-functionals, such as performance or security,\nwhen using multiple technologies? This is likely to be the case when you foresee\nthat testability is negatively influenced when using certain combinations.\nDefining Process Standards\nProcess standards describe the steps that should be followed during the development\nprocess. Think of standards such as:\n\u2022 \u201cEvery commit should include an issue identifier.\u201d\n\u2022 \u201cCode should have a sufficient unit test coverage before it is committed.\u201d\n\u2022 \u201cEvery implemented feature should be peer reviewed on code quality by a senior\ndeveloper.\u201d\nThese examples could be enforced with pre-commit hooks that perform a check on\nthe code commit before pushing it to the CI server. Peer review could be enforced by\nrequiring authorization from a developer other than the one doing the commit. The"}
{"102": "Though these are only a few examples, any of the best practices in this book can be\nstandardized. When they are actions that developers must perform, you could also\nput them in your Definition of Done (see Chapter 3).\nCoding Style\nMost IDEs already have default style checking features that give warnings or even\nerrors when your code formatting is not compliant with the predefined style configu\u2010\nration in the IDE. It is a good idea to use such a style checker, if only to be consistent\nin your code formatting. Do make sure that your style guidelines are not being sup\u2010\npressed. You would rather have an overview of violations than assuming that all code\nis according to standards. Commonly we see CHECKSTYLE tags such as the following:\n// CHECKSTYLE:OFF\npiece of code that violates coding style standards\n// CHECKSTYLE:ON\nHere, the style checker is bypassed by enclosing the violating code in between sup\u2010\npressing tags. This is something you should definitely avoid! There are several ways\nto facilitate this technically. For instance, pre-commit hooks can be configured to\ncheck for these tags or to ignore suppress messages.\nAnother good way to standardize coding style is to adhere to conventions and\ndefaults rather than specific configurations. So, instead of writing lengthy mapping\ntables to relate the objects in your data access layer to the corresponding database\ntable, you assume the convention that an object like \u201cBook\u201d will be mapped to a\n\u201cbook\u201d table by default. This principle is known as convention over configuration,\nbecause you typically adhere to the convention and only violate it when exceptions\noccur. In general that leads to less configuration and less code. Using the conventions\nof the technology you use also improves transferability of the software, for example\nwhen hiring new developers. Conventions can be understood by newcomers to the\nteam even when they originate from a completely different environment. Tooling and\ntechnologies help you in adhering to conventions with default presets.\nCode Quality Control\nWhat makes high-quality code? With maintainability as a quality aspect, some basic\nmeasurements for quality are system size in lines of code, lines of code per unit, the\namount of duplication, and complexity of decision paths within units.\nThere are numerous static code analysis tools available for determining and enforcing\ncode quality. Most of them can measure a few simple metrics such as size, duplica\u2010"}
{"103": "the CI server. Quality control checks can also be checked before a release, but by\nadding preconditions on a build in the CI server, you can do quality checks automati\u2010\ncally. You can even ensure that all code has passed quality control before it is built for\nintegration testing. This is rather strict, but practice shows that this is the best way to\nobtain high-quality code because it cannot be ignored.\nPerform code quality checks before a build is performed. In the\nmost strict form of control you can choose not to build when the\ncode is not up to code quality standards.\n9.3 Controlling Standards Using GQM\nSuppose that you have agreed upon standards for tooling and technologies and that\nnow you want to put those into practice. The IDEs that developers use support code\nstyle checks such as duplication, unit test coverage, and complexity of units/methods.\nYou know why it is useful to measure those software characteristics. But if standards\nfor complexity and test coverage are enforced (e.g., checked by the CI server before a\ncommit triggers a new build), you may want to know whether the standards are in\nfact appropriate. Therefore, consider the following GQM model:\n\u2022 Goal A: To understand the effect of our development standards by monitoring\nadherence to those standards.\n\u2014Question 1: Do developers adhere to process and coding standards?\n\u2014Metric 1a: Number of process standard violations as a percentage of the\nnumber of commits. A violation here could be the omission of a peer\nreview before committing code. You would expect this percentage to be\nlow, such as 5%, but not 0%. If there are never any violations, then the\nstandard may be too conservative. When there are many violations you\nwill hear complaints soon enough, about the standards breaking the flow\nof work or being unreasonably strict. Both cases are worth investigating.\nThe standard itself may be unfeasible, or the developers may lack certain\nskills necessary to comply with the standard.\n\u2014Metric 1b: Number of coding style violations as a percentage of the num\u2010\nber of commits. Expect a downward trend as developers learn the stan\u2010\ndard. If the standard is enforced by tooling, this can be near zero, because\ncoding style can be corrected in real time.\n\u2014Metric 1c: Number of code quality violations as a percentage of the num\u2010"}
{"104": "The reason to normalize the number of code quality violations is that the number of\nviolations alone may not be meaningful enough to the team: imagine a productivity\npeak in which the team produces much more code than usual. In that case the num\u2010\nber of code quality violations will surely rise, even though the relative number of vio\u2010\nlations might decrease. We capture this in Figures 9-1 and 9-2. Although the absolute\nnumber of violations did not decrease dramatically, compared to the increasing num\u2010\nber of commits we see that the relative number of violations decreases more quickly.\nFigure 9-1. Absolute number of violations versus the number of commits\nFigure 9-2. Normalized number of violations (as percentage of the number of commits)"}
{"105": "works mainly when the team understands and agrees with the standards. Clearly,\nstandards should not be too lenient, otherwise they do not lead to improvement.\nThere should be a middle way to achieve this. You can measure perceptions manually\nwith the following GQM model:\n\u2022 Goal B: To understand the quality of our development standards by taking\ninventory of developers\u2019 opinions of them.\n\u2014Question 2: Do developers think that the standards are fair?\n\u2014Metric 2: Opinion per developer about the fairness of the standards, on a\nscale (say, from 1 to 10). The outcome of this measurement should be dis\u2010\ncussed with the team. When developers have widely different opinions,\nthis may be caused by difference in experience. When the measurement\nshows that everyone agrees that the standards are unreasonable, you\nshould distinguish between the parts that are unreasonable and the parts\nthat you can keep. Then you can focus on tackling the standards that are\nunfair.\n\u2014Question 3: Do developers find it easy or hard to adhere to the standards?\n\u2014Metric 3: Opinion per developer about the effort to adhere to standards,\non a scale (say, from 1 to 10). Again, different opinions may reflect differ\u2010\nence in experience. The important observations come from the trend line.\nMaybe some developers do not find it easier to adhere to standards over\ntime. Then consider pairing up more experienced developers with less\nexperienced ones, which is a good practice anyway. For instance, you\ncould define in your process standards that the commits of junior devel\u2010\nopers with less than 2 years of experience should be peer reviewed by a\ndeveloper with at least 5 years of development experience. Or you could\norganize regular plenary sessions in which the top violations are discussed\nin terms of their causes and solutions. Then, everyone can learn from each\nother and code quality will increase.\nIt is important to monitor your standards periodically (say, quarterly), because stand\u2010\nards should change (however slightly) over time to reflect new gains in experience\nand technology.\n9.4 Common Objections to Standardization\nTypical objections to standardization are that certain standards do not apply. You may\nchoose to accept certain violations, as long as they are not being covered up. Keep in"}
{"106": "Objection: We Cannot Work Like This!\n\u201cOur code keeps violating our standards. We cannot work like this!\u201d\nWe do not expect that standards are a perfect fit right away, or that they should for\u2010\never be held on to. Developers should understand the reasoning behind and the need\nfor standards and what they intend to achieve. Resistance to standards may invite\ndevelopers to circumvent them in clever ways. Hiding or not knowing violations is a\nbigger problem than knowing where violations occur. This may mean going back to\nthe goals of the standards and possibly loosening them. Be careful when doing so,\nbecause having certain standards may serve a particular purpose. For instance, the\nstandards you introduce may be high, but they reflect a quality level you want to have\nin the future. Then you should be explicit about this fact and have the team agree that\nthey do not have to do it right first, as long as they understand this is something the\nteam should work toward.\nObjection: Can You Standardize on Multiple Technologies?\n\u201cCan we standardize on multiple technologies, such as using two programming lan\u2010\nguages for frontend code?\u201d\nTry to stick with one technology stack that is consistent, ideally with a single pro\u2010\ngramming language. Simplicity is an important determinant notably for maintaina\u2010\nbility and security. Complex relations between technologies require a lot of testing\nand are at risk of breaking. There may be good reasons to use multiple technologies,\nto benefit from the specialties and strengths of each technology. For common consid\u2010\nerations, see \u201cConsiderations for Combinations of Technologies\u201d on page 82.\nObjection: Organization Unfit for Standardization\n\u201cOur organization/project is not fit for standardization. We cannot get it through.\u201d\nThis is essentially a management issue. If formal standardization is not possible, an\nintermediate solution is to verbally agree on standards; for example, agreeing within\nyour team to apply a standard within (part of) a system. If a \u201cbad\u201d standard applies\nconsistently to a whole organization, it is generally better to adhere to it consistently\nthan to ignore it with custom solutions. Deviating from an organization-wide stan\u2010\ndard tends to lead to problems in maintenance and failing interfaces, in addition to a\nlot of miscommunication and trouble with other departments.\n9.5 Metrics Overview"}
{"107": "Table 9-1. Summary of metrics and goals in this chapter\nMetric # in text Metric description Corresponding goal\nST 1a Number of process standard violations Adherence to standards\nST 1b Number of coding style violations Adherence to standards\nST 1c Number of code quality violations Adherence to standards\nST 2 Developer opinion about fairness of standards Quality of standards\nST 3 Developer opinion about effort to adhere to standards Quality of standards\nIn the next two chapters, we will further refine the hygiene of the development pro\u2010\ncess by looking at two other kinds of standards: the usage of third-party code (Chap\u2010\nter 10) and documentation (Chapter 11).\nExperience in the Field\nThe topic of standardization is very broad, and we could not claim that we assess all\nof its aspects of all the development teams we encounter. In process assessments, the\nquestion of standardization mostly relates to coding and building aspects about the\nuse of a fit IDE, the use of issue tracking, and the use of coding standards and/or\nquality control. For all these practices, we observe whether they are used, but also\nwhether they are used by the whole team. For example, issue tracking is often not\nused consistently. In many situations, issues are not linked to actual development\nwork, making it harder to see the benefits of administering them.\nThe most difficult practice in standardization is consistent code quality control. Even\nif teams use standard coding style checks (say, for automatically formatting or com\u2010\npleting their code), code metrics may be hard to implement. This makes the code\nquality practice an advanced one, whereas choosing a fit IDE and applying issue\ntracking are considered basic.\nThe results from our benchmark are shown in Figure 9-3."}
{"108": ""}
{"109": "CHAPTER 10\nManage Usage of Third-Party Code\nOscar Wilde: \u201cI wish I had said that.\u201d Whistler: \u201cYou will, Oscar; you will.\u201d\n\u2014James McNeill Whistler\nBest Practice:\n\u2022 Manage the usage of third-party code well.\n\u2022 Determine the specific advantages of using an external code\u2010\nbase, and keep third-party code up-to-date.\n\u2022 This improves the development process because using third-\nparty code saves times and effort, and proper third-party\ncode management makes the system\u2019s own behavior more\npredictable.\nThird-party code is code that you have not written yourself. It comes in several var\u2010\niants: sometimes in the form of a single library, sometimes in the form of a complete\nframework. The code can be open source and maintained by a (volunteer) commu\u2010\nnity, it can be a paid product that derives from open source code, or it can be a paid\nproduct using only proprietary code.\nThere is a lot of third-party code and there are good reasons to use it. However, man\u2010\naging the use of third-party code requires a policy based on the right requirements\nfor your team and/or organization."}
{"110": "10.1 Motivation\nWhile using third-party code may feel like being out of control (you are not the\nwriter or maintainer of the code), it comes with a lot of benefits. For one, it saves you\nfrom having to \u201creinvent the wheel,\u201d and so saves you development effort. Usually,\nthird-party code has also passed some form of quality control.\nUsing third-party code is, however, not trivial: external code can become outdated,\ncause security concerns, and thereby it may require additional maintenance. In com\u2010\nplex cases the number of dependencies of your own codebase can explode. The deci\u2010\nsion to use third-party code is essentially a risk versus reward decision: the risk of\nsecurity concerns or added maintenance versus the reward of reduced development\ntime. So the usage of third-party code should be managed to ensure predictable\nbehavior of that code.\nUsing Third-Party Code Saves Time and Effort\nUsing third-party code saves the development team time and effort in reinventing the\nwheel. It relieves them from writing custom code for which standard solutions are\nalready present.\nThis is relevant as many applications share the same type of behavior. Consider basic\nneeds for almost all systems: UI interaction, database access, manipulation of com\u2010\nmon data structures, administrating settings, or security measures. Using third-party\nlibraries is especially helpful for such generic functionality, as it avoids unnecessary\nover-engineering. Widely known examples include web application frameworks such\nas Spring for Java, Django for Python, and AngularJS for JavaScript. For testing, there\nare the JUnit framework and its corresponding ports for other languages. For data\u2010\nbase communication there are also numerous frameworks available, such as NHiber\u2010\nnate and the Entity Framework. So there is plenty of third-party code to choose from,\nsaving you time and effort in the end.\nThird-Party Code Has at Least Base-Level Quality\nThird-party code is widely available as open source code next to commercial alterna\u2010\ntives. All variations provide some form of quality control. Free open source solutions\nare maintained by the community: when that community is sufficiently large and\nactive, they maintain and control the source code of the product. Although this is not\na quality guarantee, the popularity of open source products suggests that many more\nbenefit from its usage. When many users disagree with the quality or content of that"}
{"111": "Typically, within paid subscriptions of open source derivatives, you have the possibil\u2010\nity to request support in installing/using the products. In some cases you can com\u2010\nmunicate with the developers to report bugs or request improvements.\nThe commercial variations are hopefully quality controlled by the developers/organi\u2010\nzation itself. They clearly have the incentive to provide proper quality because they\ndepend on satisfied clients and clients may expect proper quality. What is more, they\nconstitute a professional organization that has some level of maturity, and so applies\ncertain standards before a new version is released.\nManaging Usage of Third-Party Code Makes a System\u2019s Behavior More\nPredictable\nHaving control over external dependencies makes the behavior of those dependencies\nmore predictable. By extension, this also applies to your system as a whole: to know\nthe expected behavior of third-party code helps to predict the behavior of your own\nsystem. With a clear overview of external dependencies, developers do not need to\nmake assumptions about which library versions are used where. This gives a fair indi\u2010\ncation of how third-party libraries and frameworks affect the behavior of your own\nsystem.\nIt is rather common that different versions of a library are used within a system. This\nintroduces unnecessary maintenance complexity. There may be, for example, con\u2010\nflicting behavior of libraries that perform the same functions. Therefore, the starting\npoint for managing third-party code is to have an overview of its usage. That is, gain\u2010\ning insight on the specific frameworks and libraries and the versions that are used.\n10.2 How to Apply the Best Practice\nMaking decisions on how to use third-party code is essentially a matter of standardi\u2010\nzation. It includes decisions on a general approach (policy, for which you should list\ngeneral requirements) and specific choices (e.g., listing advantages and disadvantages\nfor a particular library). Requirements include an update policy and maintenance\nrequirements. We elaborate on these points in this section.\nDetermine the Specific Maintainability Advantages of Using an\nExternal Codebase\nUsing third-party code for general functionality is a best practice. That is, insofar as it\ndecreases the actual maintenance burden on your own source code. Using third-party"}
{"112": "a library. This is especially true for general security functionality (such as the algo\u2010\nrithms for setting up secure connections between systems). They should never be\nwritten yourself, as it is hard to guarantee the security of that code.\nIn order to determine the quality of a library or framework, answer or give estimates\nfor the following questions:\nReplacing functionality\nIs the functionality that you are trying to implement utility functionality that is\nwidely used by other systems? Is that functionality complex and specialized, i.e.,\nis coding it yourself error-sensitive?\n\u2022 Expectations: Is it likely that functionality in the library will be expanded\nsoon such that you can use it also to replace other functionality that you are\ncoding yourself now?\nMaintenance\nDoes the specific third-party code have a reasonably active community with fre\u2010\nquent updates? How widely is it used (e.g., number of downloads, number of\nforum topics, or mentions on popular developer forums)?\n\u2022 Experience/knowledge: Does the development team already have experience\nwith the third-party code? Is knowledge readily available\u2014is the code well-\ndocumented, either in the form of a book or online tutorials/forums?\nCompatibility/reliability\nIs the third-party code compatible with other technologies used in the system\nand the deployment environment?\n\u2022 Trustworthiness: Has the source code been audited with good results? This is\nespecially relevant for security functionality.\n\u2022 Licensing: Is the source code licensed in a way that it is compatible with your\nform of licensing?\n\u2022 Intrusiveness: Can the third-party code be used in such a way that your own\ncode is relatively loosely coupled to the framework you use? Will upgrading\nnot break your own code because of coupling? Every \u201cyes\u201d to these questions\nimplies an advantage and argument for using that particular library/frame\u2010\nwork."}
{"113": "Keep Third-Party Code Up-to-Date\nUpdates are needed to stay up to speed with bug fixes and improved functionality.\nHowever, without a policy this tends to lag behind. Checking and updating libraries\ncosts time and the advantages may not seem evident. Therefore, before creating a pol\u2010\nicy the following must be known:\nEffort\nThe amount of work required to perform updates each check/update cycle. By all\nmeans, automate as much as possible with tooling.\nPriority\nWhat has highest priority in the system\u2019s development when it comes to updates?\nTypically, security has high priority and thereby policies tend to prescribe to\nalways update libraries immediately when security flaws are patched. Consider\nthat such a security emergency is especially hard to solve when libraries have\nbeen lagging behind major versions.\nThen the policy should define how/when to check, and how/when to update. Note\nthat the policy execution can be managed and automated with CI/dependency man\u2010\nagement tooling. (Examples of such tooling are Nexus, JFrog, Maven, and Gradle.)\nHow to check\nAutomatically scan recency of updates daily or manually check it (e.g., at the start\nof a release cycle).\nWhen to update\nUpdating immediately when updates are available, or bundling all update work in\na certain release cycle.\nUpdate to what exactly\nUpdating to the newest versions, even if that is a beta version, or to the latest sta\u2010\nble version.\n\u2022 Staying behind: You may choose to consistently wait a certain amount of time\nbefore updating, for example, to see whether the community experiences\nupdating problems. You might therefore choose to always stay one version\nbehind the latest stable release.\n\u2022 Not updating at all: You may choose to remain at the current version and not\nupdate, for example, when updates introduce a notable instability concern\nwith respect to your own codebase."}
{"114": "nologies that are updated in the meantime) and security (because new flaws are not\nbeing fixed).\nEnsure Quick Response to Dependency Updates\nRegardless of your update strategy, you should be able to detect and perform library\nupdates quickly. To this end, the intrusiveness of a library or framework is very rele\u2010\nvant. If your codebase is tightly coupled with library code, it becomes harder to per\u2010\nform updates because you have to fix a lot of your own code to make sure it works as\nit did before. This is yet another reason to take unit tests seriously: when you update a\nlibrary and several unit tests fail, there is a good chance that the update caused it.\nIf you decide that you want to keep up with certain versions of\nlibraries, do the work as soon as you can. Postponing this increases\nthe effort required to adjust your own code when the next update\narrives.\nDo Not Let Developers Change Library Source Code\nDevelopers should be able to change and update libraries with minimal effort. There\u2010\nfore, agree with developers that they do not make changes to the source code of a\nthird-party library. If developers do that, the library code has become part of your\nown codebase and that defeats the purpose of third-party code. Updates of changed\nlibraries are especially cumbersome and can easily lead to bugs. It requires developers\nto analyze exactly what has been changed in the library code and how that impacts\nthe adjusted code. If a library does not perfectly fit the functionality you need (but it\nsolves part of a difficult problem), it is easier to use it anyway and write custom code\naround it.\nFor large or complex functionality, it is well worth it to consider\nadjusting functionality to fit it to third-party code, instead of build\u2010\ning a custom solution (or adjusting third-party libraries).\nManage the Usage and Versions of Libraries and Frameworks\nCentrally\nTo keep libraries up-to-date, you need an overview of what versions are used.\nDependency management tooling can help with this. To facilitate library updates, a"}
{"115": "You can also manually document the usage of types/versions of libraries and frame\u2010\nworks centrally, but in practice this is a huge maintenance issue. If the list cannot be\nfully relied upon, it loses its effectiveness completely.\n10.3 Measuring Your Dependency Management\nSuppose that you have a dependency management tool in place that automatically\nchecks for and notifies you about new versions of libraries that the development team\nuses. You want to make sure that the team keeps updating their dependencies, but\nyou also want to make sure that updates do not cost too much effort. Consider the\nfollowing GQM model, to give you insight into the problems of the team and for\nchecking whether your dependency management is done right:\n\u2022 Goal A: To understand how the team manages the use of third-party code by\nassessing the effort required to keep the system up-to-date with external depen\u2010\ndencies.\n\u2014Question 1: How much time does the team spend on updating external\ndependencies and fixing potential bugs introduced to the system?\n\u2014Metric 1a: Number of bugs found after updating a library. This metric will\nnot be useful directly, but will provide useful insight over time. This is\nbecause it tells you something about how the team uses specific libraries:\nsome will introduce more bugs than others. Expect the trend line to stabi\u2010\nlize per library. If some library updates require a lot of work, you can\ninvestigate whether the team is, for example, behind with updating. A\ncause could be that issues are accumulating because developers do not\nhave the time for updates or because the library is altered very often. Or\npossibly, the library is not very suitable for the system, in which case you\nshould consider switching to another library.\n\u2014Metric 1b: Number of versions the team is behind per library. This num\u2010\nber may indicate trouble with updating when the team is working with\nlibraries that have already had two major updates, for example. It could be\nthat the team postpones it because they think it is too much work or that\nthey cannot allocate time. Or they may have run into technical issues try\u2010\ning to update. In any case, when the metric signals trouble, you should\nfind out what the problem is.\nIf you use this model to assess your library usage, you may discover that you need to\nupdate your standards for using third-party code, or that you are better off by switch\u2010"}
{"116": "chart in Figure 10-1 indicates the status of support for a list of libraries. For each\nlibrary or framework the latest version is shown, together with the version that is cur\u2010\nrently in use. The colors show how far behind you are: blue means the framework is\nstable, while gray indicates that you should update to a newer version. Burgundy\nindicates an unsupported framework, meaning that you either should update as soon\nas possible, or consider switching to another framework.\nFigure 10-1. An example of library freshness\nSuppose now that you need to select a new library for encryption functionality. This\nis a definite example of functionality you should never try to write yourself because of\nits complexity and impact of dysfunction. You may have found a few open source\nlibraries, but want to select the right one. These considerations can be answered in a\nmeasurable manner:\n\u2022 Goal B: To select the most suitable library by measuring its community activity\nand trustworthiness.\n\u2014Question 2: For which of these libraries are issues fixed the fastest?\n\u2014Metric 2: Per library, the issue resolution time. Consider this metric as an\nindicator for the level of support you can expect from the maintainers.\nThat is, when you report a bug or an issue, how fast the maintainers will\nrespond and solve the bug or issue. Of course, a lower issue resolution\ntime is usually better. A codebase that releases very often can probably\nalso fix bugs very fast.\n\u2014Question 3: Which of the libraries is most actively maintained?"}
{"117": "can see the activity. Here, the number of contributions is a signal of main\u2010\ntenance activity).\n\u2014Metric 3b: Number of contributors per library. A higher number of con\u2010\ntributors indicates an active community. Having several contributors work\non one library also makes it more likely that some form of quality control\nis used. Be cautious, however, of too many contributors as this could also\nsignify a distorted library. Too many contributors may also lead to forking\nwhen the contributors disagree about the contents.\nThis GQM model can help you in deciding which third-party codebase is most suit\u2010\nable for you. Remember that finding the best library is a trade-off: you should neither\nchoose a library when you do not have the capacity to keep up with its pace, nor a\nlibrary that is stagnant, as this may leave bugs unfixed for a long time.\nWhen you have decided to use an external component, you may want to track the\ntime you save on custom implementation and the time you spend on adapting to a\nframework or library. In this way, you gain insight into the benefits of specific third-\nparty code. So you could use the following GQM model for this:\n\u2022 Goal C: To determine the time gain of a specific framework by measuring the\ntime spent on adapting to the framework versus the time saved on implementing\ncustom functionality.\n\u2014Question 4: How much time did we save by implementing functionality using\nthe framework?\n\u2014Metric 4: For each functionality that is using the framework, the estimated\ntime it would take to implement it from scratch minus the time it took to\nimplement using the framework. This metric gives a raw estimate of the\ntime gain in using the framework. Therefore you should expect this value\nto at least be positive, otherwise it would mean that building it from\nscratch is faster!\n\u2014Question 5: How much time is spent on learning about and staying up-to-\ndate with the framework?\n\u2014Metric 5: Time spent specifically on updates, or studying the framework.\nThis time should be relatively high when you start using the framework\n(unless developers already know it, of course), and should gradually drop.\nThe point is that it contributes negatively to the time you save by using the\nframework."}
{"118": "there are no convincing reasons, then decide whether to keep using it or to switch to\nanother framework.\n10.4 Common Objections to Third-Party Code Metrics\nControlling the usage of third-party code is important from a maintainability per\u2010\nspective. However, common objections are concerns of their trustworthiness, mainte\u2010\nnance benefits, and inability to update.\nObjection: Safety and Dependability of Third-Party Libraries\n\u201cWe cannot know whether third-party libraries are dependable and safe. Should we test\nthem?\u201d\nAs libraries are imported pieces of code, you cannot unit test them directly the way\nyou do with normal production code. Also, once you have determined that the\nlibrary is dependable and adequate, you should avoid efforts to test it yourself and\ntrust it is working. However, testing is possible in the following ways:\n\u2022 You could use the unit test code of the library itself to find bugs that may arise\nbecause your system has uncommon requirements or input. However, this\nmainly enables you to find bugs that the community should have found as well.\nProper documentation of the library should clarify what it can and cannot do.\n\u2022 If you have further concerns on a library, you could test it while abstracting the\nlibrary behind an interface. This does cost you extra work and may lead to\nunnecessary code, so you should only do this when you have particular concerns\nabout the library. It does, however, give you the possibility of easily switching to\nanother library.\nObjection: We Cannot Update a Particular Library\n\u201cWe cannot update a certain library\u2014doing so leads to trouble/regression in another\nsystem.\u201d\nIf the usage of a certain library leads to problems in another system, there is a deeper\nproblem at work. The library may have changed significantly in a way that it is no\nlonger compatible. In most cases, failing unit tests should signal what kind of func\u2010\ntionality is in trouble. There could also be other reasons: young frameworks usually\nintroduce a lot of breaking changes. If you do not have the capacity to keep up with\nsuch changes, this is all the more reason to switch libraries."}
{"119": "Objection: Does Third-Party Code Actually Lead to Maintenance\nBenefits?\n\u201cHow can we determine whether using third-party code leads to benefits in mainte\u2010\nnance?\nAs third-party code is not built by yourself, you do not know exactly how much effort\nwas spent building it. But it is clear that for highly complex functionality it is much\neasier to import it than it is to code, test, and maintain it yourself. You do need to\nmake sure that your system remains compatible with that library (which requires\nsome testing and maintenance) and that effects of library changes are isolated in your\ncode.\nThe important consideration is whether you are using your time to write code that\nmakes your system unique and useful to your goals, or you are using that time to\nimplement functionality that is already available in the form of a framework or\nlibrary. It turns out in practice that a lot of functionality is very common among sys\u2010\ntems, so that most of the time there are already solutions readily available.\nThere are lots of ways in which you determine maintenance benefits: you can use sto\u2010\nries or function points to determine how much work you save on implementing those\nwhile at the same time considering the investment you make to understand and use a\nthird-party component. You can use the metrics provided in the previous section to\ngain insight into these times.\n10.5 Metrics Overview\nAs a recap, Table 10-1 shows an overview of the metrics discussed in this chapter,\nwith their corresponding goals.\nTable 10-1. Summary of metrics and goals in this chapter\nMetric # in text Metric description Corresponding goal\nTPC 1a Number of bugs found after updating a library Third-party code effort\nTPC 1b Number of versions behind on a library Third-party code effort\nTPC 2 Issue resolution time per library-related issue Library selection\nTPC 3a Number of contributions per week per library Library selection\nTPC 3b Number of contributors per library Library selection\nTPC 4 Difference between time effort for implementing functionality Framework effectiveness\nfrom scratch and using a framework"}
{"120": "This is the penultimate chapter that has dealt with standardization issues. The follow\u2010\ning and last best practice chapter covers (code) documentation.\nExperience in the Field\nManaging third-party code is hard in practice. In process assessments, we check for\nthe existence of a library management strategy. Then, we assess whether the strategy\nis actually adhered to: are updates executed regularly and in line with the strategy?\nHow many unsupported libraries are used? Is tooling supporting the library manage\u2010\nment strategy?\nIt turns out that this is hard for many development teams we encounter. Usually there\nis some kind of consensus about how and when to update libraries or frameworks,\nbut this is not set in stone. We also see that developers have difficulty keeping third-\nparty libraries up-to-date. Not only because they do not have the time to perform an\nupdate, but because they may need to change their own code as well and that time is\nnot accounted for. The result is often that the used versions increasingly fall behind,\nuntil they become unsupported. Typically, the more versions you lag behind, the\nharder it is to get back on track. This requires discipline but teams would also be\nhugely supported by tooling telling them of version updates (or updating them cen\u2010\ntrally and automatically).\nSo it is unsurprising that this practice is an advanced one. So why this particular\nspread in the data? The answer is that completely doing it right is extremely difficult,\nbut adhering to at least some basic principles of library management is easy. The dis\u2010\ntribution can be seen in Figure 10-2."}
{"121": "CHAPTER 11\nDocument Just Enough\nJe n\u2019ai fait celle-ci plus longue que parce que je n\u2019ai pas eu le loisir de la faire plus courte.\n(The only reason why I have written this long is because I did not have the time to\nshorten it.)\n\u2014Blaise Pascal\nBest Practice:\n\u2022 Document just enough for the development team to have a\ncommon understanding of design decisions and nonfunc\u2010\ntional requirements.\n\u2022 Keep documentation current, concise, and accessible.\n\u2022 This improves the development process because documenta\u2010\ntion retains knowledge and helps to avoid discussion.\nSome of the previous chapters have already dealt with documenting in some way,\nsuch as the Definition of Done, coding style standards, and third-party code policies.\nBecause documentation is a form of knowledge transfer, consider that for all develop\u2010\ners the following should also be clear:\nDesign decisions\nBy this we mean the fundamental choices that are made about the subdivision of\nthe system into components, but also how the system is implemented, how input\nis handled, and how data is processed. In this sense, design decisions cover high-\nlevel architectural choices as well as low-level design patterns. They are based on"}
{"122": "Requirements and criteria\nFunctional requirements and nonfunctional criteria describe what the system\nshould deliver when, and how.\nTesting approach\nDecisions about what, when, and how to test.\nCode documentation\nCode is ideally self-documenting, but for complex pieces of code you can supply\nreference implementations, and when supplying an API, you can use automati\u2010\ncally generated API specifications.\nWe explain why good documentation is important, and how you can arrive at good\ndocumentation.\n11.1 Motivation\nDocumentation is important for your development team, in order to have a common\nunderstanding of how the system is structured and how it should behave. Technical\ndocumentation is also important to retain knowledge of the system, whether it is for\nnew developers or for the external world (in case others are to maintain the system as\nwell). Documentation also helps you to see whether you are reaching your goals.\nDocumentation Retains Knowledge and Helps to Avoid Discussion\nDocumentation of your design, assumptions, and requirements is a means of knowl\u2010\nedge transfer, in case new developers join the team or when your codebase becomes\navailable to external developers. When introducing new developers, they should be\nable to get acquainted quickly with the system and its design.\nWhen you have documented why certain design decisions were made, and explain\nthe rationale behind these decisions, this helps avoid discussions later. That way\ndevelopers can consider, for example, whether assumptions made in the past are still\nvalid. When the code that is written is aimed to be exposed to the outside world with\nAPIs, you require an API specification and greatly help others with a reference imple\u2010\nmentation.\nDocumentation Helps You to Know Whether You Are Reaching Your\nGoals\nWith an overview of (non)functional requirements you can assess whether the system"}
{"123": "11.2 How to Apply the Best Practice\nGood documentation is concise, current, and complete, and these properties should\nbe enforced by quality control.\nKeep Documentation Current, Concise, and Accessible\nDocumentation is only useful if you can depend on a reasonable amount of currency.\nTherefore, good documentation is concise and is actively kept up-to-date (thus\nrequires maintenance effort during development).\nDocumentation tends to be complete only up to a certain level of detail and therefore\ncan remain concise. Indeed, it is not necessary to document all implementation\ndetails, as source code (structure) and tests should be self-explanatory. High-level\ndesign decisions such as technology choices should be documented in order to be\nreviewable when circumstances change. Such documents should in general not con\u2010\nsist of more than 10 pages for any subject. If it does, it usually contains a high level of\ndetail. This can be useful in some cases, but make sure these are documents to refer to\nfrom other, shorter documents. The general rule of thumb is that a document that is\nnot a reference manual can be read and understood within an hour.\nIf documentation is unretrievable or scattered, it cannot be used properly. Therefore\nit is a prerequisite that documentation is stored and maintained in some central loca\u2010\ntion that allows for access to those who need it (for proper security that should be on\na need-to-know basis). This can be part of a central version control system, a wiki\n(that contains version control management), or similar other repositories.\nDo not think of documentation as a \u201cWrite Once, Read Never\u201d\ndevice. It should be actively maintained simultaneously with the\nsystem to be valuable.\nRequired Elements of System Documentation\nAll design choices should include basic assumptions underlying those choices. Those\nassumptions are based on the circumstances in which the system is available. For\nexample, when defining a performance requirement one may assume a certain num\u2010\nber of concurrent users and intensity of usage. Once those assumptions are docu\u2010\nmented, they can be reviewed, challenged, and improved when the circumstances\nchange."}
{"124": "oper if you are building on it. Therefore, in whatever form, system documentation\nshould include the following:\nGoals and functionality\nThe goals of the system in terms of functionality and value for users and the\norganization as well as what functionality and behavior the system should pro\u2010\nvide to achieve those goals.\nInternal architecture\nDivision of the system in functional components. A well-designed source code\norganization should be able to show this division.\nDeployment\nThe way a system behaves can largely be influenced by the environment in which\nit runs. Therefore its deployment architecture and its assumptions should some\u2010\nhow be documented; e.g., visualized and described.\nStandardization\nAgreements on decision choices for the technology stack, technology-specific\nusage, and the internal (code) architecture. For more on standardization, see\nChapter 9.\nOwner and process\nTo ensure that the documentation is and stays current, each type of documenta\u2010\ntion needs an owner and a process understood by the maintenance team that\nthey adjust documentation once assumptions, design choices and /or circum\u2010\nstances change significantly. For code, tests, and scripts, the one that is adjusting\nit tends to take responsibility naturally to update it (be it a document, comments,\ntest cases, etc.).\nQuality control is enforced to keep the nonfunctional requirements in check\nAfter defining nonfunctional requirements, development should be actively con\u2010\ntrolled for adherence to those requirements. Preferably, this is an automated process\n(part of the development/test pipeline), but it should at least appear as part of the\nDefinition of Done. In case of an automated process of quality control, the nonfunc\u2010\ntional requirements also require a testing strategy. For more on quality control, refer\nback to Chapter 9."}
{"125": "11.3 Managing Your Documentation\nAssessing your documentation is a delicate issue. With too much documentation, the\nmaintenance burden becomes high, knowledge is harder to find, and often large\ndocuments are neglected. With too little, you are at risk of not knowing enough about\nthe system, which may lead to making wrong decisions and assumptions. However,\nthe code should be clear enough to document itself to a large extent. Perceptions of\nwhat is good documentation may also depend on experience. More experienced\ndevelopers generally need less documentation to understand the technical intricacies\nof a system.\nWith the following GQM model we focus on the maintenance burden and the quality\nof documentation.\n\u2022 Goal A: To understand the maintenance burden of documentation.\n\u2014Question 1: How much time is being spent on maintaining documentation?\n\u2014Metric 1: Number of hours spent on documenting. This assumes that\ndevelopers can and do write hours specific for administration. This should\nneither be zero, nor a significant investment. Expect documentation\ninvestment to be high in beginning stages of system development, reduc\u2010\ning to a stable figure. The risk notably is in arriving at \u201ctoo much docu\u2010\nmentation to maintain\u201d after which maintenance becomes neglected.\nWhat is \u201chigh\u201d should therefore be discussed with the team based on this\nmetric.\n\u2022 Goal B: To understand the quality of our documentation (completeness, concise\u2010\nness, and currency) by taking inventory of developers\u2019 opinions of them (after all,\nthey are the ones using it).\n\u2014Question 2: What do developers think about the documentation\u2019s quality?\n\u2014Metric 2a: Opinion per developer about the quality of documentation\noverall.\n\u2014Metric 2b: Opinion per developer about the documentation\u2019s complete\u2010\nness for their purpose.\n\u2014Metric 2c: Opinion per developer about the documentation\u2019s conciseness\nfor their purpose.\n\u2014Metric 2d: Opinion per developer about the documentation\u2019s currency for\ntheir purpose."}
{"126": "These opinions can be rated on a scale (e.g., from 1 to 10). The outcome of these\nmeasurements should be discussed with the team. Just as with standards, when devel\u2010\nopers have widely different opinions, this may be caused by difference in experience.\nWhen the measurement shows that everyone agrees that the documentation is not of\nhigh quality, you know that improvements are necessary. This assessment could be\nrepeated every six months, for example.\nThere is a clear trade-off between completeness and conciseness. There is also a rela\u2010\ntionship between the maintenance burden and currency. Finding the right balance is\ndependent on team preference and development experience.\n11.4 Common Objections to Documentation\nCommon objections to documentation are that the team does not get the time to\nwrite documentation, that there is disagreement in the team, or that knowledge about\nthe system is too scattered to write documentation.\nObjection: No Time to Write Documentation\n\u201cWe do not get the time to write documentation from the business!\u201d\nConsider that maintaining documentation is not a task in itself. It is part of regular\ndevelopment, when software changes thoroughly or when the software design has\nintricacies that are hard to understand quickly (for new developers). Documentation\nis not necessary for each modification, but lack of it may especially hurt future main\u2010\ntenance when modification concerns high-level design decisions. Consider also that\nthe requirements or changes as defined by the business may themselves serve as doc\u2010\numentation.\nObjection: Disagreement in the Team\n\u201cMy development team does not agree on the internal architecture of the system.\u201d\nDocumentation is something that the whole team should agree on. When there is\nfriction between team members, you should appoint a quality champion within the\nteam with the task of pulling the rest of the team along. Typically a lead/senior devel\u2010\noper with a software architect\u2019s role can make the final decisions on architectural\nissues, in consultation with the team. If this is still difficult, consider having a work\u2010\nshop to evaluate the possibilities of revising the architecture or to explore the current\nissues."}
{"127": "Objection: Knowledge about System Is Scattered\n\u201cKnowledge about the system is too scattered to achieve consistent documentation.\u201d\nConsider that you cannot document all knowledge. Start by documenting what you\nknow now and write down the assumptions that you make under the current circum\u2010\nstances. This does require active maintenance when the situation changes or new\nknowledge is gained. This maintenance should be part of a standard (e.g., part of the\nDoD). That way, it becomes a shared responsibility for the team. In practice it works\nwell if one person is assigned an authority responsibility to remind others to retain\ndiscipline to maintain documentation.\n11.5 Metrics Overview\nAs a recap, Table 11-1 shows an overview of the metrics discussed in this chapter,\nwith their corresponding goals.\nTable 11-1. Summary of metrics and goals in this chapter\nMetric # in text Metric description Corresponding goal\nDOC 1 Number of hours spent on documentation Documentation maintenance burden\nDOC 2a Developers\u2019 opinions on documentation quality overall Documentation quality\nDOC 2b Developers\u2019 opinions on documentation completeness Documentation quality\nDOC 2c Developers\u2019 opinions on documentation conciseness Documentation quality\nDOC 2d Developers\u2019 opinions on documentation currency Documentation quality\nBecause documentation should describe common understanding, this chapter is\nclosely related to the topics of standardization (Chapter 9) and the usage of third-\nparty code (Chapter 10)."}
{"128": "Experience in the Field\nWhen we assess the quality and extent of documentation of a system, we assess how\nbalanced that documentation is: not too much detail, not too little. Typically we\nexpect to see a general, high-level design document explaining the functional and\ntechnical architecture of a system as well as some details on its implementation. This\ncommonly is in the form of a wiki. We take the assumption of a new developer that\nwill be maintaining a system with little former experience with it. How well would\nthat developer be helped with the available documentation?\nThe results show a fairly proportional distribution: about one third in our benchmark\nhas been evaluated between having proper documentation (i.e., most areas are cov\u2010\nered for maintenance), improper documentation (\u201cnot applied\u201d), and a group in\nbetween, in which some key areas about the software system are missing or too com\u2010\nprehensive to be useful. See the details of this distribution in Figure 11-1.\nFigure 11-1. Benchmark results on documentation in development teams"}
{"129": "CHAPTER 12\nNext Steps\nLe secret d\u2019ennuyer est celui de tout dire.\n(The secret of being boring is to tell everything.)\n\u2014Voltaire\nOf course, there are more best practices for software development than those dis\u2010\ncussed in this book. At this point you will know the essential ingredients of a mature\ndevelopment process. The power of these practices and their metrics is that you can\nmake improvement visible and manageable.\n12.1 Applying the Best Practices Requires Persistence\nWe know, it is a bit of a downer. The best practices in this book will only work for\ndisciplined and persistent teams. Persistence requires discipline and a belief that what\nyou are doing is right. Discipline is not about being in the office at the same time\nevery day or having a clean desk. It is about working consistently, standing your\nground, and being able to say \u201cfor this issue we will not make an exception.\u201d\nPersistence also implies patience. It takes time to master best practices and that will\nrequire an investment in time.\n12.2 One Practice at a Time\nWe have presented our ten best practices in an order that reflects the typical order in\nwhich you want to accomplish them. For instance, Continuous Integration (Chap\u2010\nter 7) cannot possibly be effective without version control (Chapter 4) and to benefit"}
{"130": "tion (Chapter 11). In fact, all best practices in this book are standards themselves\n(Chapter 9).\nIt is more effective to implement one best practice in a complete and dependable way,\ninstead of doing multiple ones partially. Partial implementation of best practices does\nnot demonstrate their full benefits and that may be discouraging: it seems like you are\ninvesting all the time, and not reaping the advertised benefits.\n12.3 Avoid the Metric Pitfalls\nWe know, the metric pitfalls (\u201cMotivation\u201d on page 10) are hard to avoid and their\neffects may not be visible immediately. Using the Goal-Question-Metric approach in\na rigorous manner (Chapter 2) helps to define the right metrics for your purposes. Of\nparticular interest are metrics that provide \u201cperverse\u201d incentives (that is, the opposite\nof what you are trying to achieve), leading your team to treat or misinterpret metrics\nin a manner that is not helping achieve the team goals. To find out whether this is the\ncase, you will need to able to discuss this openly with the team. This may lead you to\nchange or remove metrics. After all, if the metrics do not help you, change them!\n12.4 What Is Next?\nWe hope that this book is helping you to improve the quality of your software, by\napplying the mentioned best practices. And that you have found or will find proper\nmetrics to help you to confidently follow progress.\nIf you want to hone your individual coding skills, we refer you to our companion\nbook Building Maintainable Software: Ten Guidelines for Future-Proof Code. For other\nprocess-related issues, consult the books listed in \u201cRelated Books\u201d on page x."}
{"131": "Index\nA measuring development effort for, 54\nacceptance environment metrics overview, 60\ncharacteristics of, 38 motivation for, 50\nfeature failures that passed in test environ\u2010 optimal amount of, 53\nment, 43 unit tests for code that already works, 59\nproduction environment boundary and, 40 when failed tests have no noticeable effects\ntest environment boundary and, 40 in production, 58\ntest environment vs., 46 automation, CI as facilitator of, 64\nAgile/Scrum methodology, 11 automobile industry, 1\n(see also Scrum) averages, outliers vs., 16\nAgile planning, 20\nB\nDoD's origins in, 20\nshort iterations, 20 baseline, 43\nassumptions, about metrics, 15-17 best practices\nautomated builds, 65 automated deployment, 73\nautomated deployment, 71-78 automated testing, 52-53\nbest practice application, 73 CI, 65\nCI vs., 71 controlled DTAP, 41\ncommon objections to metrics, 76 development standards, 80, 82-85\nfield experience, 78 DoD, 21-23\nmeasurement of process, 74-75 for documentation, 105\nmeasuring reliability gained by, 75 GQM approach, 11-14\nmeasuring time gained by, 74 implementing one practice at a time, 111\nmetrics overview, 77 metric pitfalls and, 112\nmotivation for, 72 overview of, 6\nautomated testing, 49-61 persistence in application of, 111\nassumptions about metrics, 57 third-party code, 93-97\nbest practice application, 52-53 version control systems, 30-31\nCI and, 63, 65 black-box tests, 50\ncommon objections to metrics, 58 branches, long-lived, 31"}
{"132": "automated, 65 separation of concerns between develop\u2010\naverage time for, 68 ment phases, 40\nCI and, 66 speed of DTAP street, 45\nburndown chart, 20 test/acceptance environment separation, 46\nconvention over configuration principle, 84\nC cost, automated deployment and, 77\ncoverage, by good tests, 52\ncar industry, 1\ncraft, 1\nchanges, tracking, 29\nCI (see Continuous Integration)\nD\nclassification, scaling of, 15\ncode integration, regularity of, 30 decision-making, metrics and, 17\ncode quality control, 84, 89 Definition of Done (DoD), 24\ncode, testable, 51 Agile/Scrum background for, 20\ncodebase, estimating weak spots in, 56 best practice application, 21-23\ncoding style, 84 common objections to using, 23-25\ncommit messages, version control systems and, elements of, 22\n34 extra work with, 25\ncommits field experience with, 26\nCI and, 66 making explicit, 19-26\nprocess standards and, 83 motivation for using, 21\nspecificity/regularity of, 30 overhead and, 23\ntime between, 31 proving success with, 21\nconsistency, development standards and, 80 software quality assurance, 21\ncontinuous delivery, 64 team responsibility and, 24\nContinuous Integration (CI), 63-70 technical maintenance and, 24\nautomated deployment vs., 71 tracking progress with, 21\nbest practice application, 65 DeMarco, Tom, vii\ncommon objections to metrics, 68 dependency management\ncontrolling, 66 reinstalls, 68\ndedicated server for, 65 third-party code and, 97-100\nfield experience, 70 deployment issues, 76\nmetrics overview, 69 deployment, automated (see automated deploy\u2010\nmotivation for, 64 ment)\ncontrol derived metrics, 13\ndefined, 2 design decisions\nmeasurement and, vii, 2 documentation and, 105\ncontrolled DTAP, 37-47 documentation of, 103\nbest practice application, 41 developers, importance of contributions, 4\ncommon objections to metrics, 45 development best practices (see best practices)\ndevelopment bottleneck discovery with, 41 development bottlenecks, 41\nfield experience, 47 development capacity, test capacity balance\nGQM model for measuring in practice, with, 43\n42-45 development environment\nlead time predictions and, 40 characteristics of, 38\nmetrics overview, 46 test environment boundaries and, 40"}
{"133": "choosing combinations of technologies, 82 E\ncode quality control, 84 end-to-end tests, 52\ncoding style, 84 environments (see also development environ\u2010\ncommon objections to, 87 ment; production environment; test envi\u2010\ncontrolling using GQM, 85-87 ronment; acceptance environment)\ndefining process standards, 83 inconsistencies between, 38\ndevelopers' opinion inventory, 87 separation of concerns between develop\u2010\ndiscussion overhead and, 81 ment phases, 40\ndocumentation and, 82 expectations, deviations of metrics from, 16\nfield experience, 89 experienced (subjective) quality, 4\nmetrics overview, 88 explanations, judgments vs., 16\nmotivation for, 80 external dependencies, third-party code and, 97\non multiple technologies, 88\norganizational fit, 88 F\npredictability and, 80\nFarley, David, 71\nsimplification of process/code, 81\nfeedback\nstandardization of tooling/technologies, 82\naverage cycle length, 43\nviolations and, 88\nCI and, 64\nDevelopment, Test, Acceptance, and Produc\u2010\nintegration errors and, 67\ntion (DTAP)\ncontrolled (see controlled DTAP)\nG\ncontrolled vs. uncontrolled, 39\nDijkstra, Edsger, 37 generated code, version control and, 28\ndirect metrics, 13 Git, 28\ndistributed version control, 28 goal assessment, documentation as means of,\ndocumentation, 103-110 104\nbest practice application, 105 Goal-Question-Metric (GQM) approach, 5\ncommon objections to, 108 as means of choosing metrics, 3\ndiscussion minimization, 104 automated deployment, 74-75\nfield experience, 110 automated testing, 53-58\ngoal assessment and, 104 best practice application with, 11-14\nkeeping current, concise, accessible, 105 common objections to, 17\nknowledge retention, 104 controlling development standards with,\nlevel of detail for, 105, 110 85-87\nmaintenance burden of, 107 deriving metrics from measurement goals,\nmanaging, 107-108 9-18\nmetrics overview, 109 documentation management, 107\nmotivation for, 104 for measuring DTAP street in practice,\nrequired elements, 105 42-45\nscattered knowledge about system and, 109 for third-party code dependency manage\u2010\nteam disagreements on, 108 ment, 97-100\ntesting and, 51 goal definition with, 11\ntime limitations, 108 making assumptions about metrics explicit,\nDoD (see Definition of Done) 15-17\nDPA (Development Process Assessment), 5 metrics in, 13"}
{"134": "version control systems, 31-33 third-party code and, 93, 93, 101\ngoals manual merging, CI vs., 64\ndefinition for GQM, 11 maturity of team, 26\nderiving metrics from, 9-18 McCabe complexity, 13\n(see also Goal-Question-Metric measured observations, 3\napproach) measurement, 3\nfive characteristics of, 12 (see also metrics)\nGQM approach (see Goal-Question-Metric control and, vii, 3\napproach) scaling of classification, 15\nMercurial, 28\nH merging\nCI vs. manual, 64\nhigh-level tests, 52\nspeed of, 67\nHumble, Jez, 71\nversion control systems and, 29\nI metric in a bubble, 10\nmetric pitfalls, 10, 112\ninconsistencies\nmetrics\nbetween environments, 38\nassumptions about, 15-17\nin metrics, 18\naverages vs. outliers, 16\nindustrial process, 1\ncommon objections to/solutions for, 18\ninherent (product) quality, 4\ncommon pitfalls in selecting/applying, 10\nintegration\ncomparable meanings of, 15\nspeed of, 67\ndecision-making and, 17\ntests, 52\nderiving from measurement goals, 9-18\nISO 25010, x\n(see also Goal-Question-Metric\nquality in use model, 4\napproach)\nsoftware product quality model, 4\nfinding explanations for deviations from\nsoftware quality according to, 4\nexpectations, 16\nisolation of tests, 53\ntrends vs. facts, 15\nissue resolution time, 32, 44\nmetrics galore, 10\niterations, short, 20\nmocking, 53\nmodification, independent, 29\nJ\nMozi, 79\njudgments, explanations vs., 16\nN\nK\nnorms, GQM and, 16\nkanban boards, 20\nKnuth, Donald, 49 O\nobjective metrics, 13\nL\none-track metric, 10, 32\nlead time predictions, 40 open-source third-party code, 92, 98\nlegacy systems, 77 outliers, averages vs., 16\noverhead\nM discussion, 81\nDoD and, 23"}
{"135": "patching policies, 78 rollbacks, automated deployment and, 73\npermission, for deployment, 76 root cause analysis\npersonnel, key automated deployment and, 72\nautomated deployment and, 72 of bugs, 50\ncontrolled DTAP and, 41\nperverse incentives, 112 S\nplanning, Agile, 20\nsanity tests, 52\nportability, automated deployment and, 72\n(see also end-to-end tests)\nprecision, trends vs., 15\nscaling of classification, 15\npredictability\nscope, DoD and, 22\ndevelopment standards and, 80\nScrum, 19\nprocess and, 1\n(see also Agile/Scrum methodology)\npredictions, lead time, 40\nshort iterations, 20\nprocess control\nSIG (Software Improvement Group), x\nas continuous activity, 2\nsingle platform deployment, 76\ndefined, 3\nSMART nonfunctional requirements, 26\nprocess frameworks, ix\nSMART test criteria, 61\nprocess standards, 83\nsmoke tests, 52\nproduct (inherent) quality, 4\n(see also end-to-end tests)\nProduct owner, 20\nsoftware development\nproduction environment\nas observable process, 2\nacceptance environment boundary and, 40\nbest practices overview, 6\ncharacteristics of, 38\nimportance of each developer's contribu\u2010\nimportance of resembling test environment,\ntion, 4\n42\nmeasuring/benchmarking development\nproductivity, functionality implemented as\nprocess maturity, 5\nmeasure of, 33\nseparation of concerns between develop\u2010\nprogress, tracking with DoD, 21\nment phases, 40\nSoftware Improvement Group (SIG), x\nQ\nsoftware quality, 21\nquality (see also quality)\nDoD and, 21 according to ISO 25010 standard, 4\nexperienced vs. inherent, 4 DoD and, 21\nimportance of defining, 23 source code, independent modification of, 29\nISO 25010 standard, 4 source files, merging of, 29\nunit tests and, 59 specialist knowledge\nquality champion, 108 automated deployment and, 72\nquality control controlled DTAP and, 41\ncode standards and, 84, 89 sprint planning, 20\ndocumentation and, 106 stakeholder, 20\nthird-party code, 92 standardization\nquestions, in GQM approach, 13 of development (see development stand\u2010\nards)\nR third-party code and, 93\nstatic code analysis, 84\nregressions, 27"}
{"136": "subjective (experienced) quality, 4 safety/dependability, 100\nsubjective metrics, 13 time/effort savings with, 92\nSubversion, 28 update management, 95\nsuccess, proving with DoD, 21 update problems, 100\ntooling\nT automated deployment and, 73\nstandardization of, 82\nTDD (Test Driven Development), 51\ntreating the metric, 10, 32\nteam maturity, 26\ntreemap report, 56\ntechnical maintenance, 24\ntrends, precise numbers vs., 15\ntechnology choices, documentation of, 105\ntest capacity, development capacity and, 43\nU\nTest Driven Development (TDD), 51\ntest environment unit tests, 50, 52\nacceptance environment boundary, 40 for code that is already working, 59\nacceptance environment failures, 43 quality and, 59\nacceptance environment vs., 46 third-party code and, 96\ncharacteristics of, 38 unsupported libraries, 95\ndevelopment environment boundaries, 40 updates\nimportance of resembling production envi\u2010 deployment automation and, 78\nronment, 42 third-party code, 95\ntest failures, 42 user acceptance, DoD and, 19\ntest maintenance, 53 user stories, 20\ntesting approach\nautomated (see automated testing) V\nblack-box tests, 50\nvelocity, of a development team, 20\nend-to-end tests, 52\nversion control systems\nhigh-level tests, 52\nautomatic merging, 29\nsanity tests, 52\nbasics, 28\nsmoke tests, 52\nbest practice application, 30-31\nthird-party code, 100\nCI and, 65\nunit tests (see unit tests)\ncode integration and, 30\nthird-party code\ncommit messages and, 34\nbase-level quality of, 92\ncommits and, 30\nbest practice application, 93-97\ncommon objects to metrics for, 33\ncentral repository for, 96\nfield experience, 35\nchange of library source code by developers,\nGQM and, 27-35\n96\nin practice, 31-33\ncommon objections to metrics, 100\ninconsistent use of, 33\ndependency management measurement,\nmetrics overview, 34\n97-100\nmotivation for using, 29\ndependency updates, 96\nrecommendation measurement, 34\ndetermining maintainability advantages of,\nversions, automatic merging of, 29\n93\nviolations, development standards and, 85-87,\nfield experience, 102\n88\nmaintenance benefit determination, 101\nVoltaire, 111"}
{"137": "white-box tests, 50 Wilde, Oscar, 1, 27, 91"}
{"138": "Colophon\nThe animals on the cover of Building Software Teams are three species of pipit: water,\nRichard\u2019s, and tawny.\nPipits are slender songbirds who prefer open country. The genus is widespread,\noccurring across most of the world with the exception of the driest deserts, rainfor\u2010\nests, and mainland Antarctica. Molecular studies of the pipits suggest that the genus\narose in East Asia around seven million years ago and that the genus had spread to\nthe Americas, Africa, and Europe between five and six million years ago.\nThe plumage of the pipit is generally drab and brown, buff, or faded white. The\nundersides are usually darker than the top, and there is a variable amount of barring\nand streaking on the back, wings, and breast. The mottled brown colors provide cam\u2010\nouflage against the soil and stones of their environment. It feeds primarily on insects,\nlarvae, and plant matter (especially berries), by picking items from the ground or\nfrom low-lying vegetation as it walks. Pipits are monogamous and territorial. The\nnest tends to be situated on the side of a steep bank or in a hollow and is made from\nsurrounding vegetation by the female. Females lay two clutches a year, consisting of\nfour to five eggs, which are incubated for 15 to 16 days. The male and female both\nforage for their young and tend to feed them larger and slower arthropods that are\neasy to catch, in order to obtain the most food they can in the shortest amount of\ntime.\nMany of the animals on O\u2019Reilly covers are endangered; all of them are important to\nthe world. To learn more about how you can help, go to animals.oreilly.com.\nThe cover image is from Lydekker\u2019s Royal Natural History. The cover fonts are URW\nTypewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\nis Adobe Myriad Condensed; and the code font is Dalton Maag\u2019s Ubuntu Mono."}
{"45": "Out of the 3,220 units in this system, 3,071 (95.4%) are at most 15 lines of code, while\n149 units (4.6% of all units) are longer. This shows that it is very possible in practice\nto write short units\u2014at least for a vast majority of units.\nAgree on formatting conventions in your team. Keep units short\nand comply with these conventions.\nThis Unit Is Impossible to Split Up\n\u201cMy unit really cannot be split up.\u201d\nSometimes, splitting a method is indeed difficult. Take, for instance, a properly for\u2010\nmatted switch statement in Java. For each case of the switch statement, there is a line\nfor the case itself, at least one line to do anything useful, and a line for the break\nstatement. So, anything beyond four cases becomes very hard to fit into 15 lines of\ncode, and a case statement cannot be split. In Chapter 3, we present some guidelines\non how to deal specifically with switch statements.\nHowever, it is true that sometimes a source code statement simply cannot be split. A\ntypical example in enterprise software is SQL query construction. Consider the fol\u2010\nlowing example (adapted from a real-world system analyzed by the authors of this\nbook):\npublic static void printDepartmentEmployees(String department) {\nQuery q = new Query();\nfor (Employee e : q.addColumn(\"FamilyName\")\n.addColumn(\"Initials\")\n.addColumn(\"GivenName\")\n.addColumn(\"AddressLine1\")\n.addColumn(\"ZIPcode\")\n.addColumn(\"City\")\n.addTable(\"EMPLOYEES\")\n.addWhere(\"EmployeeDep='\" + department + \"'\")\n.execute()) {\nSystem.out.println(\"<div name='addressDiv'\" + e.getFamilyName()\n+ \", \" + e.getInitials() + \"<br />\" + e.getAddressLine1()\n+ \"<br />\" + e.getZipCode() + e.getCity() + \"</div>\");\n}\n}\nThis example has 16 lines of code. However, there are just three statements. The sec\u2010\nond statement contains an expression that spans nine lines. Indeed, you cannot"}
{"46": "method. But before doing that (and seeing the newly created method grow to over 15\nlines when the query gets more complex in the future), rethink the architecture. Is it\nwise to create a SQL query piece by piece as in this snippet? Should the HTML\nmarkup really appear here? A templating solution such as JSP or JSF may be more\nsuitable for the job at hand.\nSo, if you are faced with a unit that seems impossible to refactor, do not ignore it and\nmove on to another programming task, but indeed raise the issue with your team\nmembers and team lead.\nWhen a refactoring seems possible but doesn\u2019t make sense, rethink\nthe architecture of your system.\nThere Is No Visible Advantage in Splitting Units\n\u201cPutting code in doSomethingOne, doSomethingTwo, doSomethingThree has no benefit\nover putting the same code all together in one long doSomething.\u201d\nActually, it does, provided you choose better names than doSomethingOne, doSome\nthingTwo, and so on. Each of the shorter units is, on its own, easier to understand\nthan the long doSomething. More importantly, you may not even need to consider all\nthe parts, especially since each of the method names, when chosen carefully, serves as\ndocumentation indicating what the unit of code is supposed to do. Moreover, the long\ndoSomething typically will combine multiple tasks. That means that you can only\nreuse doSomething if you need the exact same combination. Most likely, you can\nreuse each of doSomethingOne, doSomethingTwo, and so on much more easily.\nPut code in short units (at most 15 lines of code) that have carefully\nchosen names that describe their function.\n2.4 See Also\nSee Chapters 3, 4, and 5 for additional refactoring techniques. For a discussion on\nhow to test methods, see Chapter 10."}
{"47": "How SIG Rates Unit Size\nThe size (length) of units (methods and constructors in Java) is one of the eight sys\u2010\ntem properties of the SIG/T\u00dcViT Evaluation Criteria for Trusted Product Maintaina\u2010\nbility. To rate unit size, every unit of the system is categorized in one of four risk\ncategories depending on the number of lines of code it contains. Table 2-2 lists the\nfour risk categories used in the 2015 version of the SIG/T\u00dcViT Evaluation Criteria.\nThe criteria (rows) in Table 2-2 are conjunctive: a codebase needs to comply with all\nfour of them. For example, if 6.9% of all lines of code are in methods longer than\n60 lines, the codebase can still be rated at 4 stars. However, in that case, at most\n22.3% \u2013 6.9% = 15.4% of all lines of code can be in methods that are longer than 30\nlines but not longer than 60 lines. To the contrary, if a codebase does not have any\nmethods of more than 60 lines of code, at most 22.3% of all lines of code can be in\nmethods that are longer than 30 lines but not longer than 60 lines.\nTable 2-2. Minimum thresholds for a 4-star unit size rating (2015 version of the SIG/\nT\u00dcViT Evaluation Criteria)\nLines of code in methods with \u2026 Percentage allowed for 4 stars for unit size\n\u2026 more than 60 lines of code At most 6.9%\n\u2026 more than 30 lines of code At most 22.3%\n\u2026 more than 15 lines of code At most 43.7%\n\u2026 at most 15 lines of code At least 56.3%\nSee the three quality profiles shown in Figure 2-2 as an example:\n\u2022 Left: an open source system, in this case Jenkins\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for unit size\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic"}
{"48": "Figure 2-2. Three quality profiles for unit size"}
{"49": "CHAPTER 3\nWrite Simple Units of Code\nEach problem has smaller problems inside.\n\u2014Martin Fowler\nGuideline:\n\u2022 Limit the number of branch points per unit to 4.\n\u2022 Do this by splitting complex units into simpler ones and\navoiding complex units altogether.\n\u2022 This improves maintainability because keeping the number of\nbranch points low makes units easier to modify and test.\nComplexity is an often disputed quality characteristic. Code that appears complex to\nan outsider or novice developer can appear straightforward to a developer that is inti\u2010\nmately familiar with it. To a certain extent, what is \u201ccomplex\u201d is in the eye of the\nbeholder. There is, however, a point where code becomes so complex that modifying\nit becomes extremely risky and very time-consuming task, let alone testing the modi\u2010\nfications afterward. To keep code maintainable, we must put a limit on complexity.\nAnother reason to measure complexity is knowing the minimum number of tests we\nneed to be sufficiently certain that the system acts predictably. Before we can define\nsuch a code complexity limit, we must be able to measure complexity.\nA common way to objectively assess complexity is to count the number of possible\npaths through a piece of code. The idea is that the more paths can be distinguished,"}
{"50": "follows later). Branch points can be counted for a complete codebase, a class, a pack\u2010\nage, or a unit. The number of branch points of a unit is equal to the minimum num\u2010\nber of paths needed to cover all branches created by all branch points of that unit.\nThis is called branch coverage. However, when you consider all paths through a unit\nfrom the first line of the unit to a final statement, combinatory effects are possible.\nThe reason is that it may matter whether a branch follows another in a particular\norder. All possible combinations of branches are the execution paths of the unit\u2014that\nis, the maximum number of paths through the unit.\nConsider a unit containing two consecutive if statements. Figure 3-1 depicts the con\u2010\ntrol flow of the unit and shows the difference between branch coverage and execution\npath coverage.\nFigure 3-1. Branch coverage and execution path coverage\nSuppose the point to the left of the first if statement modifies a database, and the\nif"}
{"51": "In summary, the number of branch points is the number of paths that cover all\nbranches created by branch points. It is the minimum number of paths and can be\nzero (for a unit that has no branch points). The number of execution paths is a maxi\u2010\nmum, and can be very large due to combinatorial explosion. Which one to choose?\nThe answer is to take the number of branch points plus one. This is called cyclomatic\ncomplexity or McCabe complexity. Consequently, the guideline \u201climit the number of\nbranch points per unit to 4\u201d is equal to \u201climit code McCabe complexity to 5.\u201d This is\nthe minimum number of test cases that you need to cover a unit such that every path\nhas a part not covered by the other paths. The cyclomatic (McCabe) complexity of a\nunit is at least one, which is easy to understand as follows. Consider a unit with no\nbranch points. According to the definition, its cyclomatic complexity is one (number\nof branch points plus one). It also fits intuitively: a unit with no branch points has\none execution path, and needs at least one test.\nFor the sake of completeness: only for units with one exit point, the cyclomatic or\nMcCabe complexity is equal to the number of branch points plus one. It becomes\nmore complex for units with more than one exit point. Do not worry about that:\nfocus on limiting the number of branch points to four.\nThe minimum number of tests needed to cover all independent\nexecution paths of a unit is equal to the number of branch points\nplus one.\nNow consider the following example. Given a nationality, the getFlagColors method\nreturns the correct flag colors:\npublic List<Color> getFlagColors(Nationality nationality) {\nList<Color> result;\nswitch (nationality) {\ncase DUTCH:\nresult = Arrays.asList(Color.RED, Color.WHITE, Color.BLUE);\nbreak;\ncase GERMAN:\nresult = Arrays.asList(Color.BLACK, Color.RED, Color.YELLOW);\nbreak;\ncase BELGIAN:\nresult = Arrays.asList(Color.BLACK, Color.YELLOW, Color.RED);\nbreak;\ncase FRENCH:\nresult = Arrays.asList(Color.BLUE, Color.WHITE, Color.RED);\nbreak;"}
{"52": "default:\nresult = Arrays.asList(Color.GRAY);\nbreak;\n}\nreturn result;\n}\nThe switch statement in the method body needs to handle all cases of the nationality\nenumeration type and return the correct flag colors. As there are five possible nation\u2010\nalities and the unclassified/default case, the number of isolated paths to be tested\n(control flow branches) is six.\nOn first sight, the getFlagColors method might seem harmless. Indeed, the method\nis quite readable, and its behavior is as expected. Still, if we want to test the behavior\nof this method, we would need six unique test cases (one for each nationality plus one\nfor the default/unclassified case). Writing automated tests might seem excessive for\nthe getFlagColors method, but suppose a developer adds the flag of Luxembourg\n(which is very similar to the Dutch flag) as a quick fix:\n...\ncase DUTCH:\nresult = Arrays.asList(Color.RED, Color.WHITE, Color.BLUE);\ncase LUXEMBOURGER:\nresult = Arrays.asList(Color.RED, Color.WHITE, Color.LIGHT_BLUE);\nbreak;\ncase GERMAN:\n....\nBeing in a hurry, the developer copied the constructor call for the Dutch flag\nand updated the last argument to the right color. Unfortunately, the break statement\nescaped the developer\u2019s attention, and now all Dutch nationalities will see the flag\nfrom Luxembourg on their profile page!\nThis example looks like a forged scenario, but we know from our consultancy prac\u2010\ntice that this is what happens to complex code in reality. These types of simple mis\u2010\ntakes are also responsible for many \u201ctrivial\u201d bugs that could be easily prevented.\nTo understand why complex code is such a problem for maintenance, it is important\nto realize that code that starts out quite straightforward tends to grow much more\ncomplex over time. Consider the following snippet taken from the codebase of the\nopen source build server Jenkins. There are 20 control flow branches in this code\nsnippet. Imagine having to modify or test this method:\n/**\n* Retrieve a user by its ID, and create a new one if requested.\n* @return"}
{"53": "boolean create) {\nString idkey = idStrategy().keyFor(id);\nbyNameLock.readLock().lock();\nUser u;\ntry {\nu = byName.get(idkey);\n} finally {\nbyNameLock.readLock().unlock();\n}\nfinal File configFile = getConfigFileFor(id);\nif (!configFile.isFile() && !configFile.getParentFile().isDirectory()) {\n// check for legacy users and migrate if safe to do so.\nFile[] legacy = getLegacyConfigFilesFor(id);\nif (legacy != null && legacy.length > 0) {\nfor (File legacyUserDir : legacy) {\nfinal XmlFile legacyXml = new XmlFile(XSTREAM,\nnew File(legacyUserDir, \"config.xml\"));\ntry {\nObject o = legacyXml.read();\nif (o instanceof User) {\nif (idStrategy().equals(id, legacyUserDir.getName())\n&& !idStrategy()\n.filenameOf(legacyUserDir.getName())\n.equals(legacyUserDir.getName())) {\nif (!legacyUserDir\n.renameTo(configFile.getParentFile())) {\nLOGGER.log(Level.WARNING,\n\"Failed to migrate user record \" +\n\"from {0} to {1}\",\nnew Object[] {legacyUserDir,\nconfigFile.getParentFile()});\n}\nbreak;\n}\n} else {\nLOGGER.log(FINE,\n\"Unexpected object loaded from {0}: {1}\",\nnew Object[] {legacyUserDir, o});\n}\n} catch (IOException e) {\nLOGGER.log(Level.FINE,\nString.format(\n\"Exception trying to load user from {0}: {1}\",\nnew Object[] {legacyUserDir, e.getMessage()}),\ne);\n}\n}"}
{"54": "User prev;\nbyNameLock.readLock().lock();\ntry {\nprev = byName.putIfAbsent(idkey, u = tmp);\n} finally {\nbyNameLock.readLock().unlock();\n}\nif (prev != null) {\nu = prev; // if someone has already put a value in the map, use it\nif (LOGGER.isLoggable(Level.FINE)\n&& !fullName.equals(prev.getFullName())) {\nLOGGER.log(Level.FINE,\n\"mismatch on fullName (\u2018\" + fullName + \"\u2019 vs. \u2018\"\n+ prev.getFullName() + \"\u2019) for \u2018\" + id + \"\u2019\",\nnew Throwable());\n}\n} else\nif (!id.equals(fullName) && !configFile.exists()) {\n// JENKINS-16332: since the fullName may not be recoverable\n// from the id, and various code may store the id only, we\n// must save the fullName\ntry {\nu.save();\n} catch (IOException x) {\nLOGGER.log(Level.WARNING, null, x);\n}\n}\n}\nreturn u;\n}\n3.1 Motivation\nBased on the code examples in the previous section, keeping your units simple is\nimportant for two main reasons:\n\u2022 A simple unit is easier to understand, and thus modify, than a complex one.\n\u2022 Simple units ease testing.\nSimple Units Are Easier to Modify\nUnits with high complexity are generally hard to understand, which makes them hard\nto modify. The first code example of the first section was not overly complicated, but\nit would be when it checks for, say, 15 or more nationalities. The second code exam\u2010"}
{"55": "Simple Units Are Easier to Test\nThere is a good reason you should keep your units simple: to make the process of\ntesting easier. If there are six control flow branches in a unit, you will need at least six\ntest cases to cover all of them. Consider the getFlagColors method: six tests to cover\nfive nationalities plus the default case would prevent trivial bugs from being intro\u2010\nduced by maintenance work.\n3.2 How to Apply the Guideline\nAs explained at the beginning of this chapter, we need to limit the number of branch\npoints to four. In Java the following statements and operators count as branch points:\n\u2022 if\n\u2022 case\n\u2022 ?\n\u2022 &&, ||\n\u2022 while\n\u2022 for\n\u2022 catch\nSo how can we limit the number of branch points? Well, this is mainly a matter of\nidentifying the proper causes of high complexity. In a lot of cases, a complex unit\nconsists of several code blocks glued together, where the complexity of the unit is the\nsum of its parts. In other cases, the complexity arises as the result of nested if-then-\nelse statements, making the code increasingly harder to understand with each level\nof nesting. Another possibility is the presence of a long chain of if-then-else state\u2010\nments or a long switch statement, of which the getFlagColors method in the intro\u2010\nduction is an example.\nEach of these cases has its own problem, and thus, its own solution. The first case,\nwhere a unit consists of several code blocks that execute almost independently, is a\ngood candidate for refactoring using the Extract Method pattern. This way of reduc\u2010\ning complexity is similar to Chapter 2. But what to do when faced with the other\ncases of complexity?"}
{"56": "Dealing with Conditional Chains\nA chain of if-then-else statements has to make a decision every time a conditional\nif is encountered. An easy-to-handle situation is the one in which the conditionals\nare mutually exclusive; that is, they each apply to a different situation. This is also the\ntypical use case for a switch statement, like the switch from the getFlagColors\nmethod.\nThere are many ways to simplify this type of complexity, and selecting the best solu\u2010\ntion is a trade-off that depends on the specific situation. For the getFlagColors\nmethod we present two alternatives to reduce complexity. The first is the introduction\nof a Map data structure that maps nationalities to specific Flag objects. This refactor\u2010\ning reduces the complexity of the getFlagColors method from McCabe 7 to\nMcCabe 2.\nprivate static Map<Nationality, List<Color>> FLAGS =\nnew HashMap<Nationality, List<Color>>();\nstatic {\nFLAGS.put(DUTCH, Arrays.asList(Color.RED, Color.WHITE, Color.BLUE));\nFLAGS.put(GERMAN, Arrays.asList(Color.BLACK, Color.RED, Color.YELLOW));\nFLAGS.put(BELGIAN, Arrays.asList(Color.BLACK, Color.YELLOW, Color.RED));\nFLAGS.put(FRENCH, Arrays.asList(Color.BLUE, Color.WHITE, Color.RED));\nFLAGS.put(ITALIAN, Arrays.asList(Color.GREEN, Color.WHITE, Color.RED));\n}\npublic List<Color> getFlagColors(Nationality nationality) {\nList<Color> colors = FLAGS.get(nationality);\nreturn colors != null ? colors : Arrays.asList(Color.GRAY);\n}\nA second, more advanced way to reduce the complexity of the getFlagColors\nmethod is to apply a refactoring pattern that separates functionality for different flags\nin different flag types. You can do this by applying the Replace Conditional with Poly\u2010\nmorphism pattern: each flag will get its own type that implements a general interface.\nThe polymorphic behavior of the Java language will ensure that the right functional\u2010\nity is called during runtime."}
{"57": "For this refactoring, we start with a general Flag interface:\npublic interface Flag {\nList<Color> getColors();\n}\nand specific flag types for different nationalities, such as for the Dutch:\npublic class DutchFlag implements Flag {\npublic List<Color> getColors() {\nreturn Arrays.asList(Color.RED, Color.WHITE, Color.BLUE);\n}\n}\nand the Italian:\npublic class ItalianFlag implements Flag {\npublic List<Color> getColors() {\nreturn Arrays.asList(Color.GREEN, Color.WHITE, Color.RED);\n}\n}\nThe getFlagColors method now becomes even more concise and less error-prone:\nprivate static final Map<Nationality, Flag> FLAGS =\nnew HashMap<Nationality, Flag>();\nstatic {\nFLAGS.put(DUTCH, new DutchFlag());\nFLAGS.put(GERMAN, new GermanFlag());\nFLAGS.put(BELGIAN, new BelgianFlag());\nFLAGS.put(FRENCH, new FrenchFlag());\nFLAGS.put(ITALIAN, new ItalianFlag());\n}\npublic List<Color> getFlagColors(Nationality nationality) {\nFlag flag = FLAGS.get(nationality);\nflag = flag != null ? flag : new DefaultFlag();\nreturn flag.getColors();\n}\nThis refactoring offers the most flexible implementation. For example, it allows the\nflag type hierarchy to grow over time by implementing new flag types and testing\nthese types in isolation. A drawback of this refactoring is that it introduces more code\nspread out over more classes. The developer much choose between extensibility and\nconciseness."}
{"58": "Dealing with Nesting\nSuppose a unit has a deeply nested conditional, as in the following example. Given a\nbinary search tree root node and an integer, the calculateDepth method determines\nwhether the integer occurs in the tree. If so, the method returns the depth of the inte\u2010\nger in the tree; otherwise, it throws a TreeException:\npublic static int calculateDepth(BinaryTreeNode<Integer> t, int n) {\nint depth = 0;\nif (t.getValue() == n) {\nreturn depth;\n} else {\nif (n < t.getValue()) {\nBinaryTreeNode<Integer> left = t.getLeft();\nif (left == null) {\nthrow new TreeException(\"Value not found in tree!\");\n} else {\nreturn 1 + calculateDepth(left, n);\n}\n} else {\nBinaryTreeNode<Integer> right = t.getRight();\nif (right == null) {\nthrow new TreeException(\"Value not found in tree!\");\n} else {\nreturn 1 + calculateDepth(right, n);\n}\n}\n}\n}\nTo improve readability, we can get rid of the nested conditional by identifying the dis\u2010\ntinct cases and insert return statements for these. In terms of refactoring, this is\ncalled the Replace Nested Conditional with Guard Clauses pattern. The result will be\nthe following method:\npublic static int calculateDepth(BinaryTreeNode<Integer> t, int n) {\nint depth = 0;\nif (t.getValue() == n)\nreturn depth;\nif (n < t.getValue() && t.getLeft() != null)\nreturn 1 + calculateDepth(t.getLeft(), n);\nif (n > t.getValue() && t.getRight() != null)\nreturn 1 + calculateDepth(t.getRight(), n);\nthrow new TreeException(\"Value not found in tree!\");\n}"}
{"59": "public static int calculateDepth(BinaryTreeNode<Integer> t, int n) {\nint depth = 0;\nif (t.getValue() == n)\nreturn depth;\nelse\nreturn traverseByValue(t, n);\n}\nprivate static int traverseByValue(BinaryTreeNode<Integer> t, int n) {\nBinaryTreeNode<Integer> childNode = getChildNode(t, n);\nif (childNode == null) {\nthrow new TreeException(\"Value not found in tree!\");\n} else {\nreturn 1 + calculateDepth(childNode, n);\n}\n}\nprivate static BinaryTreeNode<Integer> getChildNode(\nBinaryTreeNode<Integer> t, int n) {\nif (n < t.getValue()) {\nreturn t.getLeft();\n} else {\nreturn t.getRight();\n}\n}\nThis actually does decrease the complexity of the unit. Now we have achieved two\nthings: the methods are easier to understand, and they are easier to test in isolation\nsince we can now write unit tests for the distinct functionalities.\n3.3 Common Objections to Writing Simple Units of Code\nOf course, when you are writing code, units can easily become complex. You may\nargue that high complexity is bound to arise or that reducing unit complexity in your\ncodebase will not help to increase the maintainability of your system. Such objections\nare discussed next.\nObjection: High Complexity Cannot Be Avoided\n\u201cOur domain is very complex, and therefore high code complexity is unavoidable.\u201d\nWhen you are working in a complex domain\u2014such as optimizing logistical prob\u2010\nlems, real-time visualizations, or anything that demands advanced application logic\u2014\nit is natural to think that the domain\u2019s complexity carries over to the implementation,\nand that this is an unavoidable fact of life."}
{"60": "We argue against this common interpretation. Complexity in the domain does not\nrequire the technical implementation to be complex as well. In fact, it is your respon\u2010\nsibility as a developer to simplify problems such that they lead to simple code. Even if\nthe system as a whole performs complex functionality, it does not mean that units on\nthe lowest level should be complex as well. In cases where a system needs to process\nmany conditions and exceptions (such as certain legislative requirements), one solu\u2010\ntion may be to implement a default, simple process and model the exceptions\nexplicitly.\nIt is true that the more demanding a domain is, the more effort the developer must\nexpend to build technically simple solutions. But it can be done! We have seen many\nhighly maintainable systems solving complex business problems. In fact, we believe\nthat the only way to solve complex business problems and keep them under control is\nthrough simple code.\nObjection: Splitting Up Methods Does Not Reduce Complexity\n\u201cReplacing one method with McCabe 15 by three methods with McCabe 5 each means\nthat overall McCabe is still 15 (and therefore, there are 15 control flow branches overall).\nSo nothing is gained.\u201d\nOf course, you will not decrease the overall McCabe complexity of a system by refac\u2010\ntoring a method into several new methods. But from a maintainability perspective,\nthere is an advantage to doing so: it will become easier to test and understand the\ncode that was written. So, as we already mentioned, newly written unit tests allow you\nto more easily identify the root cause of your failing tests.\nPut your code in simple units (at most four branch points) that\nhave carefully chosen names describing their function and cases.\n3.4 See Also\nSee also Chapter 2 on refactoring patterns for splitting units up in smaller units."}
{"61": "How SIG Rates Unit Complexity\nThe complexity (McCabe) of units (methods and constructors in Java) is one of the\neight system properties of the SIG/T\u00dcViT Evaluation Criteria for Trusted Product\nMaintainability. To rate unit complexity, every unit of the system is categorized in one\nof four risk categories depending on its McCabe measurement. Table 3-1 lists the four\nrisk categories used in the 2015 version of the SIG/T\u00dcViT Evaluation Criteria.\nThe criteria (rows) in Table 3-1 are conjunctive: a codebase needs to comply with\nall four of them. For example, if 1.5% of all lines of code are in methods with a\nMcCabe over 25, it can still be rated at 4 stars. However, in that case, at most\n10.0% - 1.5% = 8.5% of all lines of code can be in methods that have a McCabe over\n10 but not over 25.\nTable 3-1. Minimum thresholds for a 4-star unit complexity rating (2015 version of the\nSIG/T\u00dcViT Evaluation Criteria)\nLines of code in methods with \u2026 Percentage allowed for 4 stars for unit complexity\n\u2026 a McCabe above 25 At most 1.5%\n\u2026 a McCabe above 10 At most 10.0%\n\u2026 a McCabe above 5 At most 25.2%\n\u2026 a McCabe of at most 5 At least 74.8%\nSee the three quality profiles in Figure 3-2 as an example:\n\u2022 Left: an open source system, in this case Jenkins\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for unit complexity\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic"}
{"62": "Figure 3-2. Three quality profiles for unit complexity"}
{"63": "CHAPTER 4\nWrite Code Once\nNumber one in the stink parade is duplicated code.\n\u2014Kent Beck and Martin Fowler,\nBad Smells in Code\nGuideline:\n\u2022 Do not copy code.\n\u2022 Do this by writing reusable, generic code and/or calling\nexisting methods instead.\n\u2022 This improves maintainability because when code is copied,\nbugs need to be fixed at multiple places, which is inefficient\nand error-prone.\nCopying existing code looks like a quick win\u2014why write something anew when it\nalready exists? The point is: copied code leads to duplicates, and duplicates are a\nproblem. As the quote above indicates, some even say that duplicates are the biggest\nsoftware quality problem of all.\nConsider a system that manages bank accounts. In this system, money transfers\nbetween accounts are represented by objects of the Transfer class (not shown here).\nThe bank offers checking accounts represented by class CheckingAccount:\npublic class CheckingAccount {\nprivate int transferLimit = 100;"}
{"64": "throw new BusinessException(\"Limit exceeded!\");\n}\n// 2. Assuming result is 9-digit bank account number, validate 11-test:\nint sum = 0;\nfor (int i = 0; i < counterAccount.length(); i++) {\nsum = sum + (9-i) * Character.getNumericValue(\ncounterAccount.charAt(i));\n}\nif (sum % 11 == 0) {\n// 3. Look up counter account and make transfer object:\nCheckingAccount acct = Accounts.findAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\nreturn result;\n} else {\nthrow new BusinessException(\"Invalid account number!\");\n}\n}\n}\nGiven the account number of the account to transfer money to (as a string), the make\nTransfer method creates a Transfer object. makeTransfer first checks whether the\namount to be transferred does not exceed a certain limit. In this example, the limit is\nsimply hardcoded. makeTransfer then checks whether the number of the account to\ntransfer the money to complies with a checksum (see the sidebar \u201cThe 11-Check for\nBank Account Numbers\u201d on page 12 for an explanation of the checksum used). If that\nis the case, the object that represents this account is retrieved, and a Transfer object\nis created and returned.\nNow assume the bank introduces a new account type, called a savings account. A sav\u2010\nings account does not have a transfer limit, but it does have a restriction: money can\nonly be transferred to one particular (fixed) checking account. The idea is that the\naccount owner chooses once to couple a particular checking account with a savings\naccount.\nA class is needed to represent this new account type. Suppose the existing class is sim\u2010\nply copied, renamed, and adapted. This would be the result:\npublic class SavingsAccount {\nCheckingAccount registeredCounterAccount;\npublic Transfer makeTransfer(String counterAccount, Money amount)\nthrows BusinessException {\n// 1. Assuming result is 9-digit bank account number, validate 11-test:\nint sum = 0;\nfor (int i = 0; i < counterAccount.length(); i++) {\nsum = sum + (9 - i) * Character.getNumericValue("}
{"65": "CheckingAccount acct = Accounts.findAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\n// 3. Check whether withdrawal is to registered counter account:\nif (result.getCounterAccount().equals(this.registeredCounterAccount))\n{\nreturn result;\n} else {\nthrow new BusinessException(\"Counter-account not registered!\");\n}\n} else {\nthrow new BusinessException(\"Invalid account number!!\");\n}\n}\n}\nStart of code clone.\nEnd of code clone.\nBoth classes exist in the same codebase. By copying and pasting an existing class, we\nhave introduced some duplicated code in the codebase. There are now two fragments\n(eight lines of code each) of consecutive lines of code that are exactly the same. These\nfragments are called code clones or duplicates.\nNow suppose a bug is discovered in the implementation of the 11-test (the for loop\nthat iterates over the characters in counterAccount). This bug now needs to be fixed\nin both duplicates. This is additional work, making maintenance less efficient. More\u2010\nover, if the fix is only made in one duplicate but the other is overlooked, the bug is\nonly half fixed.\nResist the temptation of gaining a short-term advantage by copying\nand pasting code. For every future adjustment to either duplicate,\nyou will need to revisit all duplicates.\nCoding is about finding generic solutions for specific problems. Either reuse (by call\u2010\ning) an existing, generic method in your codebase, or make an existing method more\ngeneric.\nTypes of Duplication\nWe define a duplicate or code clone as an identical piece of code at least six lines long.\nThe line count excludes whitespace and comments, just like in the regular definition"}
{"66": "different methods in the same class, or in different methods in different classes in the\nsame codebase. Code clones can cross method boundaries. For instance, if the follow\u2010\ning fragment appears twice in the codebase, it is considered one clone of six lines of\ncode, not two clones of three lines each:\npublic void setGivenName(String givenName) {\nthis.givenName = givenName;\n}\npublic void setFamilyName(String familyName) {\nthis.familyName = familyName;\n}\nThe following two methods are not considered duplicates of each other even though\nthey differ only in literals and the names of identifiers:\npublic void setPageWidthInInches(float newWidth) {\nfloat cmPerInch = 2.54;\nthis.pageWidthInCm = newWidth * cmPerInch;\n// A few more lines.\n}\npublic void setPageWidthInPoints(float newWidth) {\nfloat cmPerPoint = 0.0352777;\nthis.pageWidthInCm = newWidth * cmPerPica;\n// A few more lines (same as in setPageWidthInInches).\n}\nTwo fragments of code that are syntactically the same (as opposed to textually) are\ncalled Type 2 clones. Type 2 clones differ only in whitespace, comments, names of\nidentifiers, and literals. Every Type 1 clone is always also a Type 2 clone, but some\nType 2 clones are not Type 1 clones. The methods setPageWidthInInches and setPa\ngeWidthInPoints are Type 2 clones but not Type 1 clones.\nThe guideline presented in this chapter is about Type 1 clones, for two reasons:\n\u2022 Source code maintenance benefits most from the removal of Type 1 clones.\n\u2022 Type 1 clones are easier to detect and recognize (both by humans and computers,\nas detecting Type 2 clones requires full parsing).\nThe limit of six lines of code may appear somewhat arbitrary, since other books and\ntools use a different limit. In our experience, the limit of six lines is the right balance\nbetween identifying too many and too few clones. As an example, a toString method\ncould be three or four lines, and those lines may occur in many domain objects.\nThose clones can be ignored, as they are not what we are looking for\u2014namely, delib\u2010"}
{"67": "4.1 Motivation\nTo understand the advantages of a codebase with little duplication, in this section we\ndiscuss the effects that duplication has on system maintainability.\nDuplicated Code Is Harder to Analyze\nIf you have a problem, you want to know how to fix it. And part of that \u201chow\u201d is\nwhere to locate the problem. When you are calling an existing method, you can easily\nfind the source. When you are copying code, the source of the problem may exist\nelsewhere as well. However, the only way to find out is by using a clone detection\ntool. A well-known tool for clone detection is CPD, which is included in a source\ncode analysis tool called PMD. CPD can be run from inside Eclipse as well as from\nMaven.\nThe fundamental problem of duplication is not knowing whether\nthere is another copy of the code that you are analyzing, how many\ncopies exist, and where they are located.\nDuplicated Code Is Harder to Modify\nAll code may contain bugs. But if duplicated code contains a bug, the same bug\nappears multiple times. Therefore, duplicated code is harder to modify; you may need\nto repeat bug fixes multiple times. This, in turn, requires knowing that a fix has to be\nmade in a duplicate in the first place! This is why duplication is a typical source of so-\ncalled regression bugs: functionality that has worked normally before suddenly stops\nworking (because a duplicate was overlooked).\nThe same problem holds for regular changes. When code is duplicated, changes may\nneed to be made in multiple places, and having many duplicates makes changing a\ncodebase unpredictable.\n4.2 How to Apply the Guideline\nTo avoid the problem of duplicated bugs, never reuse code by copying and pasting\nexisting code fragments. Instead, put it in a method if it is not already in one, so that\nyou can call it the second time that you need it. That is why, as we have covered in the\nprevious chapters, the Extract Method refactoring technique is the workhorse that\nsolves many duplication problems."}
{"68": "extracted into a new method which is then called multiple times, once from each\nduplicate.\nIn Chapter 2, the new extracted method became a private method of the class in\nwhich the long method occurs. That does not work if duplication occurs across\nclasses, as in CheckingAccount and SavingsAccount. One option in that case is to\nmake the extracted method a method of a utility class. In the example, we already\nhave an appropriate class for that (Accounts). So the new static method, isValid, is\nsimply a method of that class:\npublic static boolean isValid(String number) {\nint sum = 0;\nfor (int i = 0; i < number.length(); i++) {\nsum = sum + (9 - i) * Character.getNumericValue(number.charAt(i));\n}\nreturn sum % 11 == 0;\n}\nThis method is called in CheckingAccount:\npublic class CheckingAccount {\nprivate int transferLimit = 100;\npublic Transfer makeTransfer(String counterAccount, Money amount)\nthrows BusinessException {\n// 1. Check withdrawal limit:\nif (amount.greaterThan(this.transferLimit)) {\nthrow new BusinessException(\"Limit exceeded!\");\n}\nif (Accounts.isValid(counterAccount)) {\n// 2. Look up counter account and make transfer object:\nCheckingAccount acct = Accounts.findAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\nreturn result;\n} else {\nthrow new BusinessException(\"Invalid account number!\");\n}\n}\n}\nStart of short clone (three lines of code).\nEnd of short clone (three lines of code).\nAnd also in SavingsAccount:"}
{"69": "// 1. Assuming result is 9-digit bank account number,\n// validate with 11-test:\nif (Accounts.isValid(counterAccount)) {\n// 2. Look up counter account and make transfer object:\nCheckingAccount acct = Accounts.findAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\nif (result.getCounterAccount().equals(this.registeredCounterAccount))\n{\nreturn result;\n} else {\nthrow new BusinessException(\"Counter-account not registered!\");\n}\n} else {\nthrow new BusinessException(\"Invalid account number!!\");\n}\n}\n}\nStart of short clone (three lines of code).\nEnd of short clone (three lines of code).\nMission accomplished: according to the definition of duplication presented at the\nbeginning of this chapter, the clone has disappeared (because the repeated fragment is\nfewer than six lines of code). But the following issues remain:\n\u2022 Even though according to the definition, the clone has disappeared, there is still\nlogic repeated in the two classes.\n\u2022 The extracted fragment had to be put in a third class, just because in Java every\nmethod needs to be in a class. The class to which the extracted method was\nadded runs the risk of becoming a hodgepodge of unrelated methods. This leads\nto a large class smell and tight coupling. Having a large class is a smell because it\nsignals that there are multiple unrelated functionalities within the class. This\ntends to lead to tight coupling when methods need to know implementation\ndetails in order to interact with such a large class. (For elaboration, see Chap\u2010\nter 6.)\nThe refactoring technique presented in the next section solves these problems.\nThe Extract Superclass Refactoring Technique\nIn the preceding code snippets, there are separate classes for a checking account and\na savings account. They are functionally related. However, they are not related in"}
{"70": "CheckingAccount. One could say that a checking account is a special type of a (gen\u2010\neral) bank account, and that a savings account is also a special type of a (general)\nbank account. Java (and other object-oriented languages) has a feature to represent\nthe relationship between something general and something specific: inheritance from\na superclass to a subclass.\nThe Extract Superclass refactoring technique uses this feature by extracting a frag\u2010\nment of code lines not just to a method, but to a new class that is the superclass of the\noriginal class. So, to apply this technique, you create a new Account class like so:\npublic class Account {\npublic Transfer makeTransfer(String counterAccount, Money amount)\nthrows BusinessException {\n// 1. Assuming result is 9-digit bank account number, validate 11-test:\nint sum = 0;\nfor (int i = 0; i < counterAccount.length(); i++) {\nsum = sum + (9 - i) * Character.\ngetNumericValue(counterAccount.charAt(i));\n}\nif (sum % 11 == 0) {\n// 2. Look up counter account and make transfer object:\nCheckingAccount acct = Accounts.findAcctByNumber(counterAccount);\nTransfer result = new Transfer(this, acct, amount);\nreturn result;\n} else {\nthrow new BusinessException(\"Invalid account number!\");\n}\n}\n}\nStart of extracted clone.\nEnd of extracted clone.\nThe new superclass, Account, contains logic shared by the two types of special\naccounts. You can now turn both the CheckingAccount and SavingsAccount classes\ninto subclasses of this new superclass. For CheckingAccount, the result looks like this:\npublic class CheckingAccount extends Account {\nprivate int transferLimit = 100;\n@Override\npublic Transfer makeTransfer(String counterAccount, Money amount)\nthrows BusinessException {\nif (amount.greaterThan(this.transferLimit)) {\nthrow new BusinessException(\"Limit exceeded!\");"}
{"71": "The CheckingAccount class declares its own member, transferLimit, and overrides\nmakeTransfer. The makeTransfer method first checks to be sure the amount to be\ntransferred does not exceed the limit for checking accounts. If that is the case, it calls\nmakeTransfer in the superclass to create the actual transfer.\nThe new version of SavingsAccount works likewise:\npublic class SavingsAccount extends Account {\nCheckingAccount registeredCounterAccount;\n@Override\npublic Transfer makeTransfer(String counterAccount, Money amount)\nthrows BusinessException {\nTransfer result = super.makeTransfer(counterAccount, amount);\nif (result.getCounterAccount().equals(this.registeredCounterAccount)) {\nreturn result;\n} else {\nthrow new BusinessException(\"Counter-account not registered!\");\n}\n}\n}\nThe SavingsAccount class declares registeredCounterAccount and, just like Check\ningAccount, overrides makeTransfer. The makeTransfer method does not need to\ncheck a limit (because savings accounts do not have a limit). Instead, it calls make\nTransfer directly in the superclass to create a transfer. It then checks whether the\ntransfer is actually with the registered counter account.\nAll functionality is now exactly where it belongs. The part of making a transfer that is\nthe same for all accounts is in the Account class, while the parts that are specific to\ncertain types of accounts are in their respective classes. All duplication has been\nremoved.\nAs the comments indicate, the makeTransfer method in the Account superclass has\ntwo responsibilities. Although the duplication introduced by copying and pasting\nCheckingAccount has already been resolved, one more refactoring\u2014extracting the\n11-test to its own method\u2014makes the new Account class even more maintainable:\npublic class Account {\npublic Transfer makeTransfer(String counterAccount, Money amount)\nthrows BusinessException {\nif (isValid(counterAccount)) {\nCheckingAccount acct = Accounts.findAcctByNumber(counterAccount);\nreturn new Transfer(this, acct, amount);\n} else {\nthrow new BusinessException(\"Invalid account number!\");"}
{"72": "int sum = 0;\nfor (int i = 0; i < number.length(); i++) {\nsum = sum + (9 - i) * Character.getNumericValue(number.charAt(i));\n}\nreturn sum % 11 == 0;\n}\n}\nThe Account class is now a natural place for isValid, the extracted method.\n4.3 Common Objections to Avoiding Code Duplication\nThis section discusses common objections regarding code duplication. From our\nexperience, these are developers\u2019 arguments for allowing duplication, such as copying\nfrom other codebases, claiming there are \u201cunavoidable\u201d cases, and insisting that some\ncode will \u201cnever change.\u201d\nCopying from Another Codebase Should Be Allowed\n\u201cCopying and pasting code from another codebase is not a problem because it will not\ncreate a duplicate in the codebase of the current system.\u201d\nTechnically, that is correct: it does not create a duplicate in the codebase of the cur\u2010\nrent system. Copying code from another system may seem beneficial if the code\nsolves the exact same problem in the exact same context. However, in any of the fol\u2010\nlowing situations you will run into problems:\nThe other (original) codebase is still maintained\nYour copy will not benefit from the improvements made in the original codebase.\nTherefore, do not copy, but rather import the functionality needed (that is, add\nthe other codebase to your classpath).\nThe other codebase is no longer maintained and you are working on rebuilding this\ncodebase\nIn this case, you definitely should not copy the code. Often, rebuilds are caused\nby maintainability problems or technology renewals. In the case of maintainabil\u2010\nity issues, you would be defeating the purpose by copying code. You are introduc\u2010\ning code that is determined to be (on average) hard to maintain. In the case of\ntechnology renewals, you would be introducing limitations of the old technology\ninto the new codebase, such as an inability to use abstractions that are needed for\nreusing functionality efficiently."}
{"73": "Slight Variations, and Hence Duplication, Are Unavoidable\n\u201cDuplication is unavoidable in our case because we need slight variations of common\nfunctionality.\u201d\nIndeed, systems often contain slight variations of common functionality. For\ninstance, some functionality is slightly different for different operating systems, for\nother versions (for reasons of backward compatibility), or for different customer\ngroups. However, this does not imply that duplication is unavoidable. You need to\nfind those parts of the code that are shared by all variants and move them to a com\u2010\nmon superclass, as in the examples presented in this chapter. You should strive to\nmodel variations in the code in such a way that they are explicit, isolated, and\ntestable.\nThis Code Will Never Change\n\u201cThis code will never, ever change, so there is no harm in duplicating it.\u201d\nIf it is absolutely, completely certain that code will never, ever change, duplication\n(and every other aspect of maintainability) is not an issue. For a start, you have to be\nabsolutely, completely certain that the code in question also does not contain any\nbugs that need fixing. Apart from that, the reality is that systems change for many\nreasons, each of which may eventually lead to changes in parts deemed to never, ever\nchange:\n\u2022 The functional requirements of the system may change because of changing users,\nchanging behavior, or a change in the way the organization does business.\n\u2022 The organization may change in terms of ownership, responsibilities, develop\u2010\nment approach, development process, or legislative requirements.\n\u2022 Technology may change, typically in the system\u2019s environment, such as the operat\u2010\ning system, libraries, frameworks, or interfaces to other applications.\n\u2022 Code itself may change, because of bugs, refactoring efforts, or even cosmetic\nimprovements.\nThat is why we argue that most of the time the expectation that code never changes is\nunfounded. So accepting duplication is really nothing more than accepting the risk\nthat someone else will have to deal with it later if it happens.\nYour code will change. Really."}
{"74": "Duplicates of Entire Files Should Be Allowed as Backups\n\u201cWe are keeping copies of entire files in our codebase as backups. Every backup is an\nunavoidable duplicate of all other versions.\u201d\nWe recommend keeping backups, but not in the way implied by this objection (inside\nthe codebase). Version control systems such as SVN and Git provide a much better\nbackup mechanism. If those are not available, move backup files to a directory next\nto the root of the codebase, not inside it. Why? Because sooner or later you will lose\ntrack of which variant of a file is the right one.\nUnit Tests Are Covering Me\n\u201cUnit tests will sort out whether something goes wrong with a duplicate.\u201d\nThis is true only if the duplicates are in the same method, and the unit test of the\nmethod covers both. If the duplicates are in other methods, it can be true only if a\ncode analyzer alerts you if duplicates are changing. Otherwise, unit tests would not\nnecessarily signal that something is wrong if only one duplicate has changed. Hence,\nyou cannot rely only on the tests (identifying symptoms) instead of addressing the\nroot cause of the problem (using duplicate code). You should not assume that even\u2010\ntual problems will be fixed later in the development process, when you could avoid\nthem altogether right now.\nDuplication in String Literals Is Unavoidable and Harmless\n\u201cI need long string literals with a lot of duplication in them. Duplication is unavoidable\nand does not hurt because it is just in literals.\u201d\nThis is a variant of one of the objections discussed in Chapter 2 (\u201cThis unit is impos\u2010\nsible to split\u201d). We often see code that contains long SQL queries or XML or HTML\ndocuments appearing as string literals in Java code. Sometimes such literals are com\u2010\nplete clones, but more often parts of them are repeated. For instance, we have seen\nSQL queries of more than a hundred lines of code that differed only in the sorting\norder (order by asc versus order by desc). This type of duplication is not harmless\neven though technically they are not in the Java logic itself. It is also not unavoidable;\nin fact this type of duplication can be avoided in a straightforward fashion:\n\u2022 Extract to a method that uses string concatenation and parameters to deal with\nvariants.\n\u2022 Use a templating engine to generate HTML output from smaller, nonduplicated"}
{"75": "4.4 See Also\nLess duplication leads to a smaller codebase; for elaboration, see Chapter 9. See the\nExtract Method refactoring technique in Chapter 2 for splitting units to make them\neasier to reuse.\nHow SIG Rates Duplication\nThe amount of duplication is one of the eight system properties of the SIG/T\u00dcViT\nEvaluation Criteria for Trusted Product Maintainability. To rate duplication, all Type\n1 (i.e., textually equal) code clones of at least six lines of code are considered, except\nclones consisting entirely of import statements. Code clones are then categorized in\ntwo risk categories: redundant clones and nonredundant clones, as follows. Take a\nfragment of 10 lines of code that appears three times in the codebase. In other words,\nthere is a group of three code clones, each 10 lines of code. Theoretically, two of these\ncan be removed: they are considered technically redundant. Consequently, 10 + 10 =\n20 lines of code are categorized as redundant. One clone is categorized as nonredun\u2010\ndant, and hence, 10 lines of code are categorized as nonredundant. To be rated at 4\nstars, at most 4.6% of the total number of lines of code in the codebase can be catego\u2010\nrized as redundant. See Table 4-1\nTable 4-1. Minimum thresholds for a 4-star duplication rating (2015 version of the SIG/\nT\u00dcViT Evaluation Criteria)\nLines of code categorized as \u2026 Percentage allowed for 4 stars\n\u2026 nonredundant At least 95.4%\n\u2026 redundant At most 4.6%\nSee the three quality profiles in Figure 4-1 as an example:\n\u2022 Left: an open source system, in this case Jenkins\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for duplication\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic"}
{"76": "Figure 4-1. Three code duplication quality profiles"}
{"77": "CHAPTER 5\nKeep Unit Interfaces Small\nBunches of data that hang around together really ought to be made into their own\nobject.\n\u2014Martin Fowler\nGuideline:\n\u2022 Limit the number of parameters per unit to at most 4.\n\u2022 Do this by extracting parameters into objects.\n\u2022 This improves maintainability because keeping the number of\nparameters low makes units easier to understand and reuse.\nThere are many situations in the daily life of a programmer where long parameter\nlists seem unavoidable. In the rush of getting things done, you might add a few\nparameters more to that one method in order to make it work for exceptional cases.\nIn the long term, however, such a way of working will lead to methods that are hard\nto maintain and hard to reuse. To keep your code maintainable it is essential to avoid\nlong parameter lists, or unit interfaces, by limiting the number of parameters they\nhave.\nA typical example of a unit with many parameters is the render method in the Board\nPanel class of JPacman. This method renders a square and its occupants (e.g., a ghost,\na pellet) in a rectangle given by the x,y,w,h parameters.\n/**"}
{"78": "* The square to render.\n* @param g\n* The graphics context to draw on.\n* @param x\n* The x position to start drawing.\n* @param y\n* The y position to start drawing.\n* @param w\n* The width of this square (in pixels).\n* @param h\n* The height of this square (in pixels).\n*/\nprivate void render(Square square, Graphics g, int x, int y, int w, int h) {\nsquare.getSprite().draw(g, x, y, w, h);\nfor (Unit unit : square.getOccupants()) {\nunit.getSprite().draw(g, x, y, w, h);\n}\n}\nThis method exceeds the parameter limit of 4. Especially the last four arguments, all\nof type int, make the method harder to understand and its usage more error-prone\nthan necessary. It is not unthinkable that after a long day of writing code, even an\nexperienced developer could mix up the x,y,w and h parameters\u2014a mistake that the\ncompiler and possibly even the unit tests will not catch.\nBecause the x,y,w, and h variables are related (they define a rectangle with a 2D\nanchor point, a width and a height), and the render method does not manipulate\nthese variables independently, it makes sense to group them into an object of type\nRectangle. The next code snippets show the Rectangle class and the refactored\nrender method:\npublic class Rectangle {\nprivate final Point position;\nprivate final int width;\nprivate final int height;\npublic Rectangle(Point position, int width, int height) {\nthis.position = position;\nthis.width = width;\nthis.height = height;\n}\npublic Point getPosition() {\nreturn position;\n}\npublic int getWidth() {"}
{"79": "return height;\n}\n}\n/**\n* Renders a single square on the given graphics context on the specified\n* rectangle.\n*\n* @param square\n* The square to render.\n* @param g\n* The graphics context to draw on.\n* @param r\n* The position and dimension for rendering the square.\n*/\nprivate void render(Square square, Graphics g, Rectangle r) {\nPoint position = r.getPosition();\nsquare.getSprite().draw(g, position.x, position.y, r.getWidth(),\nr.getHeight());\nfor (Unit unit : square.getOccupants()) {\nunit.getSprite().draw(g, position.x, position.y, r.getWidth(),\nr.getHeight());\n}\n}\nNow the render method has only three parameters instead of six. Next to that, in the\nwhole system we now have the Rectangle class available to work with. This allows us\nto also create a smaller interface for the draw method:\nprivate void render(Square square, Graphics g, Rectangle r) {\nPoint position = r.getPosition();\nsquare.getSprite().draw(g, r);\nfor (Unit unit : square.getOccupants()) {\nunit.getSprite().draw(g, r);\n}\n}\nThe preceding refactorings are an example of the Introduce Parameter Object refac\u2010\ntoring pattern. Avoiding long parameter lists, as shown in the previous example,\nimproves the readability of your code. In the next section, we explain why small inter\u2010\nfaces contribute to the overall maintainability of a system.\n5.1 Motivation\nAs we already discussed in the introduction, there are good reasons to keep interfaces\nsmall and to introduce suitable objects for the parameters you keep passing around in\nconjunction. Methods with small interfaces keep their context simple and thus are"}
{"80": "Small Interfaces Are Easier to Understand and Reuse\nAs the codebase grows, the core classes become the API upon which a lot of other\ncode in the system builds. In order to keep the volume of the total codebase low (see\nalso Chapter 9) and the speed of development high, it is important that the methods\nin the core classes have a clear and small interface. Suppose you want to store a Pro\nductOrder object in the database: would you prefer a ProductOrderDao.store(Pro\nductOrder order) method or a ProductOrderDao.store(ProductOrder order,\nString databaseUser, String databaseName, boolean validateBeforeStore,\nboolean closeDbConnection) method?\nMethods with Small Interfaces Are Easier to Modify\nLarge interfaces do not only make your methods obscure, but in many cases also\nindicate multiple responsibilities (especially when you feel that you really cannot\ngroup your objects together anymore). In this sense, interface size correlates with unit\nsize and unit complexity. So it is pretty obvious that methods with large interfaces are\nhard to modify. If you have, say, a method with eight parameters and a lot is going on\nin the method body, it can be difficult to see where you can split your method into\ndistinct parts. However, once you have done so, you will have several methods with\ntheir own responsibility, and moreover, each method will have a small number of\nparameters! Now it will be much easier to modify each of these methods, because you\ncan more easily locate exactly where your modification needs to be done.\n5.2 How to Apply the Guideline\nBy the time you have read this, you should be convinced that having small interfaces\nis a good idea. How small should an interface be? In practice, an upper bound of four\nseems reasonable: a method with four parameters is still reasonably clear, but a\nmethod with five parameters is already getting difficult to read and has too many\nresponsibilities.\nSo how can you ensure small interfaces? Before we show you how you can fix meth\u2010\nods with large interfaces, keep in mind that large interfaces are not the problem, but\nrather are indicators of the actual problem\u2014a poor data model or ad hoc code modi\u2010\nfication. So, you can view interface size as a code smell, to see whether your data\nmodel needs improvement.\nLarge interfaces are usually not the main problem; rather, they are a\ncode smell that indicates a deeper maintainability problem."}
{"81": "Let us say you have a buildAndSendMail method that takes a list of nine parameters\nin order to construct and send an email message. However, if you looked at just the\nparameter list, it would not be very clear what would happen in the method body:\npublic void buildAndSendMail(MailMan m, String firstName, String lastName,\nString division, String subject, MailFont font, String message1,\nString message2, String message3) {\n// Format the email address\nString mId = firstName.charAt(0) + \".\" + lastName.substring(0, 7) + \"@\"\n+ division.substring(0, 5) + \".compa.ny\";\n// Format the message given the content type and raw message\nMailMessage mMessage = formatMessage(font,\nmessage1 + message2 + message3);\n// Send message\nm.send(mId, subject, mMessage);\n}\nThe buildAndSendMail method clearly has too many responsibilities; the construc\u2010\ntion of the email address does not have much to do with sending the actual email.\nFurthermore, you would not want to confuse your fellow programmer with five\nparameters that together will make up a message body! We propose the following\nrevision of the method:\npublic void buildAndSendMail(MailMan m, MailAddress mAddress,\nMailBody mBody) {\n// Build the mail\nMail mail = new Mail(mAddress, mBody);\n// Send the mail\nm.sendMail(mail);\n}\nprivate class Mail {\nprivate MailAddress address;\nprivate MailBody body;\nprivate Mail(MailAddress mAddress, MailBody mBody) {\nthis.address = mAddress;\nthis.body = mBody;\n}\n}\nprivate class MailBody {\nString subject;\nMailMessage message;\npublic MailBody(String subject, MailMessage message) {\nthis.subject = subject;"}
{"82": "private String mId;\nprivate MailAddress(String firstName, String lastName,\nString division) {\nthis.mId = firstName.charAt(0) + \".\" + lastName.substring(0, 7)\n+ \"@\"\n+ division.substring(0, 5) + \".compa.ny\";\n}\n}\nThe buildAndSendMail method is now considerably less complex. Of course, you\nnow have to construct the email address and message body before you invoke the\nmethod. But if you want to send the same message to several addresses, you only have\nto build the message once, and similarly for the case where you want to send a bunch\nof messages to one email address. In conclusion, we have now separated concerns,\nand while we did so we introduced some nice, structured classes.\nThe examples presented in this chapter all group parameters into objects. Such\nobjects are often called data transfer objects or parameter objects. In the examples,\nthese new objects actually represent meaningful concepts from the domain. A point, a\nwidth, and a height represent a rectangle, so grouping these in a class called Rectan\ngle makes sense. Likewise, a first name, a last name, and a division make an address,\nso grouping these in a class called MailAddress makes sense, too. It is not unlikely\nthat these classes will see a lot of use in the codebase because they are useful generali\u2010\nzations, not just because they may decrease the number of parameters of a method.\nWhat if we have a number of parameters that do not fit well together? We can always\nmake a parameter object out of them, but probably, it will be used only once. In such\ncases, another approach is often possible, as illustrated by the following example.\nSuppose we are creating a library that can draw charts, such as bar charts and pie\ncharts, on a java.awt.Graphics canvas. To draw a nice-looking chart, you usually\nneed quite a bit of information, such as the size of the area to draw on, configuration\nof the category axis and value axis, the actual dataset to chart, and so forth. One way\nto supply this information to the charting library is like this:\npublic static void drawBarChart(Graphics g,\nCategoryItemRendererState state,\nRectangle graphArea,\nCategoryPlot plot,\nCategoryAxis domainAxis,\nValueAxis rangeAxis,\nCategoryDataset dataset) {\n// ..\n}"}
{"83": "possible? One way to implement this is to use method overloading and define, for\ninstance, a two-parameter version of drawBarChart:\npublic static void drawBarChart(Graphics g, CategoryDataset dataset) {\nCharts.drawBarChart(g,\nCategoryItemRendererState.DEFAULT,\nnew Rectangle(new Point(0, 0), 100, 100),\nCategoryPlot.DEFAULT,\nCategoryAxis.DEFAULT,\nValueAxis.DEFAULT,\ndataset);\n}\nThis covers the case where we want to use defaults for all parameters whose data\ntypes have a default value defined. However, that is just one case. Before you know it,\nyou are defining more than a handful of alternatives like these. And the version with\nseven parameters is still there.\nAnother way to solve this is to use the Replace Method with Method Object refactoring\ntechnique presented in Chapter 2. This refactoring technique is primarily used to\nmake methods shorter, but it can also be used to reduce the number of method\nparameters.\nTo apply the Replace Method with Method Object technique to this example, we\ndefine a BarChart class like this:\npublic class BarChart {\nprivate CategoryItemRendererState state = CategoryItemRendererState.DEFAULT;\nprivate Rectangle graphArea = new Rectangle(new Point(0, 0), 100, 100);\nprivate CategoryPlot plot = CategoryPlot.DEFAULT;\nprivate CategoryAxis domainAxis = CategoryAxis.DEFAULT;\nprivate ValueAxis rangeAxis = ValueAxis.DEFAULT;\nprivate CategoryDataset dataset = CategoryDataset.DEFAULT;\npublic BarChart draw(Graphics g) {\n// ..\nreturn this;\n}\npublic ValueAxis getRangeAxis() {\nreturn rangeAxis;\n}\npublic BarChart setRangeAxis(ValueAxis rangeAxis) {\nthis.rangeAxis = rangeAxis;\nreturn this;\n}"}
{"84": "The static method drawBarChart from the original version is replaced by the (non\u2010\nstatic) method draw in this class. Six of the seven parameters of drawBarChart have\nbeen turned into private members of BarChart class, and have public getters and set\u2010\nters. All of these have default values. We have chosen to keep parameter g (of type\njava.awt.Graphics) as a parameter of draw. This is a sensible choice: draw always\nneeds a Graphics object, and there is no sensible default value. But it is not necessary:\nwe could also have made g into the seventh private member and supplied a getter and\nsetter for it.\nWe made another choice: all setters return this to create what is called a fluent inter\u2010\nface. The setters can then be called in a cascading style, like so:\nprivate void showMyBarChart() {\nGraphics g = this.getGraphics();\nBarChart b = new BarChart()\n.setRangeAxis(myValueAxis)\n.setDataset(myDataset)\n.draw(g);\n}\nIn this particular call of draw, we provide values for the range axis, dataset, and g, and\nuse default values for the other members of BarChart. We could have used more\ndefault values or fewer, without having to define additional overloaded draw methods.\n5.3 Common Objections to Keeping Unit Interfaces Small\nIt may take some time to get rid of all large interfaces. Typical objections to this effort\nare discussed next.\nObjection: Parameter Objects with Large Interfaces\n\u201cThe parameter object I introduced now has a constructor with too many parameters.\u201d\nIf all went well, you have grouped a number of parameters into an object during the\nrefactoring of a method with a large interface. It may be the case that your object now\nhas a lot of parameters because they apparently fit together. This usually means that\nthere is a finer distinction possible inside the object. Remember the first example,\nwhere we refactored the render method? Well, the defining parameters of the rectan\u2010\ngle were grouped together, but instead of having a constructor with four arguments\nwe actually put the x and y parameters together in the Point object. So, in general,\nyou should not refuse to introduce a parameter object, but rather think about the\nstructure of the object you are introducing and how it relates to the rest of your code."}
{"85": "Refactoring Large Interfaces Does Not Improve My Situation\n\u201cWhen I refactor my method, I am still passing a lot of parameters to another method.\u201d\nGetting rid of large interfaces is not always easy. It usually takes more than refactor\u2010\ning one method. Normally, you should continue splitting responsibilities in your\nmethods, so that you access the most primitive parameters only when you need to\nmanipulate them separately. For instance, the refactored version of the render\nmethod needs to access all parameters in the Rectangle object because they are input\nto the draw method. But it would be better, of course, to also refactor the draw\nmethod to access the x,y,w, and h parameters inside the method body. In this way,\nyou have just passed a Rectangle in the render method, because you do not actually\nmanipulate its class variables before you begin drawing!\nFrameworks or Libraries Prescribe Interfaces with Long Parameter\nLists\n\u201cThe interface of a framework we\u2019re using has nine parameters. How can I implement\nthis interface without creating a unit interface violation?\u201d\nSometimes frameworks/libraries define interfaces or classes with methods that have\nlong parameter lists. Implementing or overriding these methods will inevitably lead\nto long parameter lists in your own code. These types of violations are impossible to\nprevent, but their impact can be limited. To limit the impact of violations caused by\nthird-party frameworks or libraries, it is best to isolate these violations\u2014for instance,\nby using wrappers or adapters. Selecting a different framework/library is also a viable\nalternative, although this can have a large impact on other parts of the codebase.\n5.4 See Also\nMethods with multiple responsibilities are more likely when the methods are large\nand complex. Therefore, make sure that you understand the guidelines for achieving\nshort and simple units. See Chapters 2 and 3."}
{"86": "How SIG Rates Unit Interfacing\nUnit interfacing is one of the eight system properties of the SIG/T\u00dcViT Evaluation\nCriteria for Trusted Product Maintainability. To rate unit interfacing, every unit of the\nsystem is categorized in one of four risk categories depending on its number of\nparameters. Table 5-1 lists the four risk categories used in the 2015 version of the SIG/\nT\u00dcViT Evaluation Criteria.\nTable 5-1. Minimum thresholds for a 4-star unit size rating (2015 version of the SIG/\nT\u00dcViT Evaluation Criteria)\nLines of code in methods with \u2026 Percentage allowed for 4 stars for unit interfacing\n\u2026 more than seven parameters At most 0.7%\n\u2026 five or more parameters At most 2.7%\n\u2026 three or more parameters At most 13.8%\n\u2026 at most two parameters At least 86.2%\nSee the three quality profiles shown in Figure 5-1 as an example:\n\u2022 Left: an open source system, in this case Jenkins\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for unit interfacing\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic\nFigure 5-1. Three quality profiles for unit interfacing"}
{"87": "CHAPTER 6\nSeparate Concerns in Modules\nIn a system that is both complex and tightly coupled, accidents are inevitable.\n\u2014Charles Perrow\u2019s Normal Accidents\ntheory in one sentence\nGuideline:\n\u2022 Avoid large modules in order to achieve loose coupling\nbetween them.\n\u2022 Do this by assigning responsibilities to separate modules\nand hiding implementation details behind interfaces.\n\u2022 This improves maintainability because changes in a loosely\ncoupled codebase are much easier to oversee and execute than\nchanges in a tightly coupled codebase.\nThe guidelines presented in the previous chapters are all what we call unit guidelines:\nthey address improving maintainability of individual units (methods/constructors) in\na system. In this chapter, we move up from the unit level to the module level.\nRemember that the concept of a module translates to a class in\nobject-oriented languages such as Java."}
{"88": "We will use a true story to illustrate what tight coupling between classes is and why it\nleads to maintenance problems. This story is about how a class called UserService in\nthe service layer of a web application started growing while under development and\nkept on growing until it violated the guideline of this chapter.\nIn the first development iteration, the UserService class started out as a class with\nonly three methods, the names and responsibilities of which are shown in this code\nsnippet:\npublic class UserService {\npublic User loadUser(String userId) { ... }\npublic boolean doesUserExist(String userId) { ... }\npublic User changeUserInfo(UserInfo userInfo) { ... }\n}\nIn this case, the backend of the web application provides a REST interface to the\nfrontend code and other systems.\nA REST interface is an approach for providing web services in a simplified manner.\nREST is a common way to expose functionality outside of the system. The class in\nthe REST layer that implements user operations uses the UserService class like this:\nimport company.system.services.UserService;\n// The @Path and @GET attributes are defined by the the Java REST Service API\n@Path(\"/user\")\npublic class UserRestAPI {\nprivate final UserService userService = new UserService();\n...\n@GET\n@Path(\"/{userId}\")\npublic Response getUser(@PathParam(value = \"userId\") String userId) {\nUser user = userService.loadUser(userId);\nreturn toJson(user);\n}\n}\nDuring the second development iteration, the UserService class is not modified at\nall. In the third development iteration, new requirements were implemented that\nallowed a user to register to receive certain notifications. Three new methods were\nadded to the UserService class for this requirement:\npublic class UserService {\npublic User loadUser(String userId) { ... }"}
{"89": "public List<NotificationType> getNotificationTypes(User user) { ... }\npublic void registerForNotifications(User user, NotificationType type) {\n...\n}\npublic void unregisterForNotifications(User user, NotificationType type) {\n...\n}\n}\nThese new functionalities were also exposed via a separate REST API class:\nimport company.system.services.UserService;\n@Path(\"/notification\")\npublic class NotificationRestAPI {\nprivate final UserService userService = new UserService();\n...\n@POST\n@Path(\"/register/{userId}/{type}\")\npublic Response register(@PathParam(value = \"userId\") String userId,\n@PathParam(value = \"type\" String notificationType)) {\nUser user = userService.loadUser(userId);\nuserService.registerForNotifications(user, notificationType);\nreturn toJson(HttpStatus.SC_OK);\n}\n@POST\n@Path(\"/unregister/{userId}/{type}\")\npublic Response unregister(@PathParam(value = \"userId\") String\nuserId, @PathParam(value = \"type\" String notificationType)) {\nUser user = userService.loadUser(userId);\nuserService.unregisterForNotifications(user, notificationType);\nreturn toJson(HttpStatus.SC_OK);\n}\n}\nIn the fourth development iteration, new requirements for searching users, blocking\nusers, and listing all blocked users were implemented (management requested that\nlast requirement for reporting purposes). All of these requirements caused new meth\u2010\nods to be added to the UserService class.\npublic class UserService {\npublic User loadUser(String userId) { ... }\npublic boolean doesUserExist(String userId) { ... }"}
{"90": "...\n}\npublic void unregisterForNotifications(User user, NotificationType type) {\n...\n}\npublic List<User> searchUsers(UserInfo userInfo) { ... }\npublic void blockUser(User user) { ... }\npublic List<User> getAllBlockedUsers() { ... }\n}\nAt the end of this development iteration, the class had grown to an impressive size. At\nthis point the UserService class had become the most used service in the service\nlayer of the system. Three frontend views (pages for Profile, Notifications, and\nSearch), connected through three REST API services, used the UserService class.\nThe number of incoming calls from other classes (the fan-in) has increased to over\n50. The size of class has increased to more than 300 lines of code.\nThese kind of classes have what is called the large class smell, briefly discussed in\nChapter 4. The code contains too much functionality and also knows implementation\ndetails about the code that surrounds it. The consequence is that the class is now\ntightly coupled. It is called from a large number of places in the code, and the class\nitself knows details on other parts of the codebase. For example, it uses different data\nlayer classes for user profile management, the notification system, and searching/\nblocking other users.\nCoupling means that two parts of a system are somehow connected\nwhen changes are needed. That may be direct calls, but classes\ncould also be connected via a configuration file, database structure,\nor even assumptions they make (in terms of business logic).\nThe problem with these classes is that they become a maintenance hotspot. All func\u2010\ntionalities related (even remotely) to users are likely to end up in the UserService\nclass. This is an example of an improper separation of concerns. Developers will also\nfind the UserService class increasingly more difficult to understand as it becomes\nlarge and unmanageable. Less experienced developers on the team will find the class\nintimidating and will hesitate to make changes to it.\nTwo principles are necessary to understand the significance of coupling between"}
{"91": "However, it is the combination of methods in the UserService class that makes\nUserService tightly coupled with the classes that use it.\n\u2022 Tight and loose coupling are a matter of degree. The actual maintenance conse\u2010\nquence of tight coupling is determined by the number of calls to that class and the\nsize of that class. Therefore, the more calls to a particular class that is tightly cou\u2010\npled, the smaller its size should be. Consider that even when classes are split up,\nthe number of calls may not necessarily be lower. However, the coupling is then\nlower, because less code is coupled.\n6.1 Motivation\nThe biggest advantage of keeping classes small is that it provides a direct path toward\nloose coupling between classes. Loose coupling means that your class-level design will\nbe much more flexible to facilitate future changes. By \u201cflexibility\u201d we mean that you\ncan make changes while limiting unexpected effects of those changes. Thus, loose\ncoupling allows developers to work on isolated parts of the codebase without creating\nchange ripples that affect the rest of the codebase. A third advantage, which cannot be\nunderestimated, is that the codebase as a whole will be much more open to less expe\u2010\nrienced developers.\nThe following sections discuss the advantages of having small, loosely coupled classes\nin your system.\nSmall, Loosely Coupled Modules Allow Developers to Work on Isolated\nParts of the Codebase\nWhen a class is tightly coupled with other classes, changes to the implementation of\nthe class tend to create ripple effects through the codebase. For example, changing the\ninterface of a public method leads to code changes everywhere the method is called.\nBesides the increased development effort, this also increases the risk that class modi\u2010\nfications lead to bugs or inconsistencies in remote parts of the codebase.\nSmall, Loosely Coupled Modules Ease Navigation Through the\nCodebase\nNot only does a good separation of concerns keep the codebase flexible to facilitate\nfuture changes, it also improves the analyzability of the codebase since classes encap\u2010\nsulate data and implement logic to perform a single task. Just as it is easier to name\nmethods that only do one thing, classes also become easier to name and understand"}
{"92": "Small, Loosely Coupled Modules Prevent No-Go Areas for New\nDevelopers\nClasses that violate the single responsibility principle become tightly coupled and accu\u2010\nmulate a lot of code over time. As with the UserService example in the introduction\nof this chapter, these classes become intimidating to less experienced developers, and\neven experienced developers are hesitant to make changes to their implementation. A\ncodebase that has a large number of classes that lack a good separation of concerns is\nvery difficult to adapt to new requirements.\n6.2 How to Apply the Guideline\nIn general, this guideline prescribes keeping your classes small (by addressing only\none concern) and limiting the number of places where a class is called by code out\u2010\nside the class itself. Following are three development best practices that help to pre\u2010\nvent tight coupling between classes in a codebase.\nSplit Classes to Separate Concerns\nDesigning classes that collectively implement functionality of a software system is the\nmost essential step in modeling and designing object-oriented systems. In typical\nsoftware projects we see that classes start out as logical entities that implement a sin\u2010\ngle functionality but over time gain more responsibilities. To prevent classes from\ngetting a large class smell, it is crucial that developers take action if a class has more\nthan one responsibility by splitting up the class.\nTo demonstrate how this works with the UserService class from the introduction, we\nsplit the class into three separate classes. Here are the two newly created classes and\nthe modified UserService class:\npublic class UserNotificationService {\npublic List<NotificationType> getNotificationTypes(User user) { ... }\npublic void register(User user, NotificationType type) { ... }\npublic void unregister(User user, NotificationType type) { ... }\n}\npublic class UserBlockService {\npublic void blockUser(User user) { ... }\npublic List<User> getAllBlockedUsers() { ... }\n}"}
{"93": "public User changeUserInfo(UserInfo userInfo) { ... }\npublic List<User> searchUsers(UserInfo userInfo) { ... }\n}\nAfter we rewired the calls from the REST API classes, the system now has a more\nloosely coupled implementation. For example, the UserService class has no knowl\u2010\nedge about the notification system or the logic for blocking users. Developers are also\nmore likely to put new functionalities in separate classes instead of defaulting to the\nUserService class.\nHide Specialized Implementations Behind Interfaces\nWe can also achieve loose coupling by hiding specific and detailed implementations\nbehind a high-level interface. Consider the following class, which implements the\nfunctionality of a digital camera that can take snapshots with the flash on or off:\npublic class DigitalCamera {\npublic Image takeSnapshot() { ... }\npublic void flashLightOn() { ... }\npublic void flaslLightOff() { ... }\n}\nAnd suppose this code runs inside an app on a smartphone device, like this:\n// File SmartphoneApp.java:\npublic class SmartphoneApp {\nprivate DigitalCamera camera = new DigitalCamera();\npublic static void main(String[] args) {\n...\nImage image = camera.takeSnapshot();\n...\n}\n}\nA more advanced digital camera becomes available. Apart from taking snapshots, it\ncan also record video, has a timer feature, and can zoom in and out. The DigitalCa\nmera class is extended to support the new features:\npublic class DigitalCamera {\npublic Image takeSnapshot() { ... }\npublic void flashLightOn() { ... }"}
{"94": "public Video record() { ... }\npublic void setTimer(int seconds) { ... }\npublic void zoomIn() { ... }\npublic void zoomOut() { ... }\n}\nFrom this example implementation, it is not difficult to imagine that the extended\nversion of the DigitalCamera class will be much larger than the initial version, which\nhas fewer features.\nThe codebase of the smartphone app still uses only the original three methods. How\u2010\never, because there is still just one DigitalCamera class, the app is forced to use this\nlarger class. This introduces more coupling in the codebase than necessary. If one (or\nmore) of the additional methods of DigitalCamera changes, we have to review the\ncodebase of the smartphone app, only to find that it is not affected. While the smart\u2010\nphone app does not use any of the new methods, they are available to it.\nTo lower coupling, we use an interface that defines a limited list of camera features\nimplemented by both basic and advanced cameras:\n// File SimpleDigitalCamera.java:\npublic interface SimpleDigitalCamera {\npublic Image takeSnapshot();\npublic void flashLightOn();\npublic void flashLightOff();\n}\n// File DigitalCamera.java:\npublic class DigitalCamera implements SimpleDigitalCamera {\n...\n}\n// File SmartphoneApp.java:\npublic class SmartphoneApp {\nprivate SimpleDigitalCamera camera = SDK.getCamera();\npublic static void main(String[] args) {\n...\nImage image = camera.takeSnapshot();\n...\n}\n}"}
{"95": "accesses only the SimpleDigitalCamera interface. This guarantees that Smart\nphoneApp does not use any of the methods of the more advanced camera.\nAlso, this way your system becomes more modular: it is composed such that a change\nto one class has minimal impact on other classes. This, in turn, increases modifiabil\u2010\nity: it is easier and less work to modify the system, and there is less risk that modifica\u2010\ntions introduce defects.\nReplace Custom Code with Third-Party Libraries/Frameworks\nA third situation that typically leads to tight module coupling are classes that provide\ngeneric or utility functionality. Classic examples are classes called StringUtils and\nFileUtils. Since these classes provide generic functionality, they are obviously called\nfrom many places in the codebase. In many cases this is an occurrence of tight cou\u2010\npling that is hard to avoid. A best practice, though, is to keep the class sizes limited\nand to periodically review (open source) libraries and frameworks to check if they\ncan replace the custom implementation. Apache Commons and Google Guava are\nwidespread libraries with frequently used utility functionality. In some cases, utility\ncode can be replaced with new Java language features or a company-wide shared\nlibrary.\n6.3 Common Objections to Separating Concerns\nThe following are typical objections to the principle explained in this chapter.\nObjection: Loose Coupling Conflicts With Reuse\n\u201cTight coupling is a side effect of code reuse, so this guideline conflicts with that best\npractice.\u201d\nOf course, code reuse can increase the number of calls to a method. However, there\nare two reasons why this should not lead to tight coupling:\n\u2022 Reuse does not necessarily lead to methods that are called from as many places as\npossible. Good software design\u2014for example, using inheritance and hiding\nimplementation behind interfaces\u2014will stimulate code reuse while keeping the\nimplementation loosely coupled, since interfaces hide implementation details.\n\u2022 Making your code more generic, to solve more problems with less code, does not\nmean it should become a tightly coupled codebase. Clearly, utility functionality is\nexpected to be called from more places than specific functionality. Utility func\u2010"}
{"96": "Objection: Java Interfaces Are Not Just for Loose Coupling\n\u201cIt doesn\u2019t make sense to use Java interfaces to prevent tight coupling.\u201d\nIndeed, using interfaces is a great way to improve encapsulation by hiding implemen\u2010\ntations, but it does not make sense to provide an interface for every class. As a rule of\nthumb, an interface should be implemented by at least two classes in your codebase.\nConsider splitting your class if the only reason to put an interface in front of your\nclass is to limit the amount of code that other classes see.\nObjection: High Fan-in of Utility Classes Is Unavoidable\n\u201cUtility code will always be called from many locations in the codebase.\u201d\nThat is true. In practice, even highly maintainable codebases contain a small amount\nof code that is so generic that it is used by many places in the codebase (for example,\nlogging functionality or I/O code). Highly generic, reusable code should be small, and\nsome of it may be unavoidable. However, if the functionality is indeed that common,\nthere may be a framework or library available that already implements it and can be\nused as is.\nObjection: Not All Loose Coupling Solutions Increase Maintainability\n\u201cFrameworks that implement inversion of control (IoC) achieve loose coupling but make\nit harder to maintain the codebase.\u201d\nInversion of control is a design principle to achieve loose coupling. There are frame\u2010\nworks available that implement this for you. IoC makes a system more flexible for\nextension and decreases the amount of knowledge that pieces of code have of each\nother.\nThis objection holds when such frameworks add complexity for which the maintain\u2010\ning developers are not experienced enough. Therefore, in cases where this objection\nis true, it is not IoC that is the problem, but the framework that implements it.\nThus, the design decision to use a framework for implementing IoC should be con\u2010\nsidered with care. As with all engineering decisions, this is a trade-off that does not\npay off in all cases. Using these types of frameworks just to achieve loose coupling is a\nchoice that can almost never be justified."}
{"97": "How SIG Rates Module Coupling\nModule coupling is one of the eight system properties of the SIG/T\u00dcViT Evaluation\nCriteria for Trusted Product Maintainability. To rate module coupling, the fan-in of\nevery method is calculated. Each module (class in Java) is then categorized in one of\nfour risk categories depending on the total fan-in of all methods in the class. Table 6-1\nlists the four risk categories used in the 2015 version of the SIG/T\u00dcViT Evaluation\nCriteria. The table shows the maximum amount of code that may fall in the risk cate\u2010\ngories in order to achieve a 4-star rating. For example, a maximum of 21.8% of code\nvolume may be in classes with a fan-in in the moderate risk category, and likewise for\nthe other risk categories.\nTable 6-1. Module coupling risk categories (2015 version of the SIG/T\u00dcViT Evaluation\nCriteria)\nFan-in of modules in the category Percentage allowed for 4 stars\n51+ At most 6.6%\n21\u201350 At most 13.8%\n11\u201320 At most 21.6%\n1\u201310 No constraint\nSee the three quality profiles in Figure 6-1 as an example:\n\u2022 Left: an open source system, in this case Jenkins. Note that Jenkins does not fulfill\nthe 4-star requirement here for the highest risk category (in red).\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for module coupling.\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic."}
{"98": "Figure 6-1. Three quality profiles for module coupling"}
{"99": "CHAPTER 7\nCouple Architecture Components Loosely\nThere are two ways of constructing a software design: one way is to make it so simple\nthat there are obviously no deficiencies, and the other way is to make it so complicated\nthat there are no obvious deficiencies.\n\u2014C.A.R. Hoare\nGuideline:\n\u2022 Achieve loose coupling between top-level components.\n\u2022 Do this by minimizing the relative amount of code within\nmodules that is exposed to (i.e., can receive calls from) mod\u2010\nules in other components.\n\u2022 This improves maintainability because independent compo\u2010\nnents ease isolated maintenance.\nHaving a clear view on software architecture is essential when you are building and\nmaintaining software. A good software architecture gives you insight into what the\nsystem does, how the system does it, and how functionality is organized (in compo\u2010\nnent groupings, that is). It shows you the high-level structure, the \u201cskeleton\u201d so to\nspeak, of the system. Having a good architecture makes it easier to find the source\ncode that you are looking for and to understand how (high-level) components inter\u2010\nact with other components.\nThis chapter deals with dependencies on the component level. A component is part of\nthe top-level division of a system. It is defined by a system\u2019s software architecture, so"}
{"100": "However, the implementation of software architecture always remains the responsi\u2010\nbility of you as a developer.\nComponents should be loosely coupled; that is, they should be clearly separated by\nhaving few entry points for other components and a limited amount of information\nshared among components. In that case, implementation details of methods are hid\u2010\nden (or encapsulated) which makes the system more modular.\nSounds familiar? Yes, both as a general design principle and on a module level, loose\ncoupling has been discussed in Chapter 6. Component coupling applies the same rea\u2010\nsoning but at the higher level of components rather than modules. Module coupling\nfocuses on the exposure of individual modules (classes) to the rest of the codebase.\nComponent coupling focuses specifically on the exposure of modules in one compo\u2010\nnent (group of modules) to the modules in another component.\nSo a module being called from a module in the same component is\nconsidered to be an internal call if we assess at the component level,\nbut when we assess it at the module level, there is module coupling\nindeed.\nIn this chapter, we refer to the characteristic of being loosely coupled on a component\nlevel as component independence. The opposite of component independence is com\u2010\nponent dependence. In that case, the inner workings of components are exposed too\nmuch to other components that rely on them. That kind of entanglement makes it\nharder to oversee effects that code changes in one component may have on others,\nbecause it does not behave in an isolated manner. This complicates testing, when we\nmust make assumptions or simulations of what happens within another component.\n7.1 Motivation\nSystem maintenance is easier when changes within a component have effects that are\nisolated within that component. To clarify the advantages of having loosely coupled\ncomponents, let us elaborate on the consequences of different types of dependencies\nwith Figure 7-1."}
{"101": "Figure 7-1. Low component dependence (left) and high component dependence (right)\nThe left side of the figure shows a low level of component dependence. Most calls\nbetween modules are internal (within the component). Let us elaborate on internal\nand noninternal dependencies.\nCalls that improve maintainability:\n\u2022 Internal calls are healthy. Since the modules calling each other are part of the\nsame component, they should implement closely related functionality. Their\ninner logic is hidden from the outside.\n\u2022 Outgoing calls are also healthy. As they delegate tasks to other components, they\ncreate a dependency outward. In general, delegation of distinct concerns to other\ncomponents is a good thing. Delegation can be done from anywhere within a\ncomponent and does not need to be restricted to a limited set of modules within\nthe component.\nNote that outgoing calls from one component are incoming calls\nfor another component.\nCalls that have a negative impact on maintainability:\n\u2022 Incoming calls provide functionality for other components by offering an inter\u2010"}
{"102": "improves information hiding. Also, modifying code involved in incoming depen\u2010\ndencies potentially has a large impact on other components. By having a small\npercentage of code involved in incoming dependencies, you may dampen the\nnegative ripple effects of modifications to other components.\n\u2022 Throughput code is risky and must be avoided. Throughput code both receives\nincoming calls and delegates to other components. Throughput code accom\u2010\nplishes the opposite of information hiding: it exposes its delegates (implementa\u2010\ntion) to its clients. It is like asking a question to a help desk that does not\nformulate its own answer but instead forwards your question to another com\u2010\npany. Now you are dependent on two parties for the answer. In the case of code,\nthis indicates that responsibilities are not well divided over components. As it is\nhard to trace back the path that the request follows, it is also hard to test and\nmodify: tight coupling may cause effects to spill over to other components.\nThe right side of the figure shows a component with a high level of component\ndependence. The component has many dependencies with modules outside the com\u2010\nponent and is thus tightly coupled. It will be hard to make isolated changes, since the\neffects of changes cannot be easily overseen.\nNote that the effects of component independence are enhanced by\ncomponent balance. Component balance is achieved when the num\u2010\nber of components and their relative size are balanced. For elabora\u2010\ntion on this topic, see chapter Chapter 8.\nTo give you an idea of how coupling between components evolves over time, consider\nhow entanglements seem to appear naturally in systems. Entanglements evolve over\ntime because of hasty hacks in code, declining development discipline, or other rea\u2010\nsons why the intended architecture cannot be applied consistently. Figure 7-2 illus\u2010\ntrates a situation that we encounter often in our practice. A system has a clear\narchitecture with one-way dependencies, but over time they become blurred and\nentangled. In this case, the entanglement is between layers, but similar situations\noccur between components."}
{"103": "Figure 7-2. Designed versus implemented architecture\nLow Component Dependence Allows for Isolated Maintenance\nA low level of dependence means that changes can be made in an isolated manner.\nThis applies when most of a component\u2019s code volume is either internal or outgoing.\nIsolated maintenance means less work, as coding changes do not have effects outside\nthe functionality that you are modifying.\nNote that this reasoning about isolation applies to code on a\nsmaller level. For example, a system consisting of small, simple\nclasses signals a proper separation of concerns, but does not guar\u2010\nantee it. For that, you will need to investigate the actual dependen\u2010\ncies (see, for example, Chapter 6).\nLow Component Dependence Separates Maintenance Responsibilities\nIf all components are independent from each other, it is easier to distribute responsi\u2010\nbilities for maintenance among separate teams. This follows from the advantage of\nisolated modification. Isolation is in fact a prerequisite for efficient division of devel\u2010\nopment work among team members or among different teams.\nBy contrast, if components are tightly intertwined with each other, one cannot isolate\nand separate maintenance responsibilities among teams, since the effects of modifica\u2010\ntions will spill over to other teams. Aside from that code being hard to test, the effects\nof modifications may also be unpredictable. So, dependencies may lead to inconsis\u2010\ntencies, more time spent on communication between developers, and time wasted\nwaiting for others to complete their modifications."}
{"104": "Low Component Dependence Eases Testing\nCode that has a low dependence on other components (modules with mainly internal\nand outgoing code) is easier to test. For internal calls, functionality can be traced and\ntested within the component. For outgoing calls, you do not need to mock or stub\nfunctionality that is provided by other components (given that functionality in that\nother component is finished).\nFor elaboration on (unit) testing, see also Chapter 10.\n7.2 How to Apply the Guideline\nThe goal for this chapter\u2019s guideline is to achieve loose coupling between compo\u2010\nnents. In practice, we find that you can help yourself by adhering to the following\nprinciples for implementing interfaces and requests between components.\nThe following principles help you apply the guideline of this chapter:\n\u2022 Limit the size of modules that are the component\u2019s interface.\n\u2022 Define component interfaces on a high level of abstraction. This limits the types\nof requests that cross component borders. That avoids requests that \u201cknow too\nmuch\u201d about the implementation details.\n\u2022 Avoid throughput code, because it has the most serious effect on testing func\u2010\ntionality. In other words, avoid interface modules that put through calls to other\ncomponents. If throughput code exists, analyze the concerned modules in order\nto solve calls that are put through to other components.\nAbstract Factory Design Pattern\nComponent independence reflects the high-level architecture of a software system.\nHowever, this is not a book on software architecture. In this section, we discuss only\none design pattern that we frequently see applied in practice to successfully limit the\namount of interface code exposed by a component: the Abstract Factory design pat\u2010\ntern. A system that is loosely coupled is characterized by relying more on contracts\nand less on implementation details.\nMany more design patterns and software architecture styles can help in keeping your\narchitecture components loosely coupled. An example is using a framework for\ndependency injection (which allows Inversion of Control). For elaboration on other"}
{"105": "The Abstract Factory design pattern hides (or encapsulates) the creation of specific\n\u201cproducts\u201d behind a generic \u201cproduct factory\u201d interface. In this context, products are\ntypically entities for which more than one variant exists. Examples are audio format\ndecoder/encoder algorithms or user interface widgets that have different themes\nfor \u201clook and feel.\u201d In the following example, we use the Abstract Factory design pat\u2010\nten to encapsulate the specifics of cloud hosting platforms behind a small factory\ninterface.\nSuppose our codebase contains a component, called PlatformServices, that imple\u2010\nments the management of services from a cloud hosting platform. Two specific cloud\nhosting providers are supported by the PlatformServices component: Amazon AWS\nand Microsoft Azure (more could be added in the future).\nTo start/stop servers and reserve storage space, we have to implement the following\ninterface for a cloud hosting platform:\npublic interface CloudServerFactory {\nCloudServer launchComputeServer();\nCloudServer launchDatabaseServer();\nCloudStorage createCloudStorage(long sizeGb);\n}\nBased on this interface, we create two specific factory classes for AWS and Azure:\npublic class AWSCloudServerFactory implements CloudServerFactory {\npublic CloudServer launchComputeServer() {\nreturn new AWSComputeServer();\n}\npublic CloudServer launchDatabaseServer() {\nreturn new AWSDatabaseServer();\n}\npublic CloudStorage createCloudStorage(long sizeGb) {\nreturn new AWSCloudStorage(sizeGb);\n}\n}\npublic class AzureCloudServerFactory implements CloudServerFactory {\npublic CloudServer launchComputeServer() {\nreturn new AzureComputeServer();\n}\npublic CloudServer launchDatabaseServer() {\nreturn new AzureDatabaseServer();"}
{"106": "}\n}\nNote that these factories make calls to specific AWS and Azure implementation\nclasses (which in turn do specific AWS and Azure API calls), but return generic inter\u2010\nface types for servers and storage.\nCode outside the PlatformServices component can now use the concise interface\nmodule CloudServerFactory\u2014for example, like this:\npublic class ApplicationLauncher {\npublic static void main(String[] args) {\nCloudServerFactory factory;\nif (args[1].equals(\"-azure\")) {\nfactory = new AzureCloudServerFactory();\n} else {\nfactory = new AWSCloudServerFactory();\n}\nCloudServer computeServer = factory.launchComputeServer();\nCloudServer databaseServer = factory.launchDatabaseServer();\nThe CloudServerFactory interface of the PlatformServices provides a small inter\u2010\nface for other components in the codebase. This way, these other components can be\nloosely coupled to it.\n7.3 Common Objections to Loose Component Coupling\nThis section discusses objections regarding component dependence, whether they\nconcern the difficulty of fixing the component dependence itself, or dependency\nbeing a requirement within the system.\nObjection: Component Dependence Cannot Be Fixed Because the\nComponents Are Entangled\n\u201cWe cannot get component dependence right because of mutual dependencies between\ncomponents.\u201d\nEntangled components are a problem that you experience most clearly during main\u2010\ntenance. You should start by analyzing the modules in the throughput category, as it\nhas the most serious effect on the ease of testing and on predicting what exactly the\nfunctionality does.\nWhen you achieve clearer boundaries for component responsibilities, it improves the\nanalyzability and testability of the modules within. For example, modules with an"}
{"107": "Objection: No Time to Fix\n\u201cIn the maintenance team, we understand the importance of achieving low component\ndependence, but we are not granted time to fix it.\u201d\nWe understand how this is an issue. Development deadlines are real, and there may\nnot be time for refactoring, or what a manager may see as \u201ctechnical aesthetics.\u201d What\nis important is the trade-off. One should resolve issues that pose a real problem for\nmaintainability. So dependencies should be resolved if the team finds that they inhibit\ntesting, analysis, or stability. You can solidify your case by measuring what percentage\nof issues arises/maintenance effort is needed in components that are tightly coupled\nwith each other.\nFor example, throughput code follows complex paths that are hard to test for devel\u2010\nopers. There may be more elegant solutions that require less time and effort.\nObjection: Throughput Is a Requirement\n\u201cWe have a requirement for a software architecture for a layer that puts through calls.\u201d\nIt is true that some architectures are designed to include an intermediate layer. Typi\u2010\ncally, this is a service layer that collects requests from one side (e.g., the user interface)\nand bundles them for passing on to another layer in the system. The existence of such\na layer is not necessarily a problem\u2014given that this layer implements loose coupling.\nIt should have a clear separation of incoming and outgoing requests. So the module\nthat receives requests in this layer:\n\u2022 Should not process the request itself.\n\u2022 Should not know where and how to process that request (its implementation\ndetails).\nIf both are true, the receiving module in the service layer has an incoming request\nand an outgoing request, instead of putting requests through to a specific module in\nthe receiving component.\nA large-volume service layer containing much logic is a typical code smell. In that\ncase, the layer does not merely abstract and pass on requests, but also transforms\nthem. Hence, for transformation, the layer knows about the implementation details.\nThat means that the layer does not properly encapsulate both request and implemen\u2010\ntation. If throughput code follows from software architecture requirements, you may\nraise the issue to the software or enterprise architect."}
{"108": "7.4 See Also\nA related concept to component independence is that of component balance, dis\u2010\ncussed in Chapter 8. That chapter deals with achieving an overseeable number of\ncomponents that are balanced in size.\nHow SIG Rates Component Independence\nSIG defines and measures loose coupling between components as \u201ccomponent inde\u2010\npendence.\u201d The independence is measured on the module level, as each module in a\nsystem should be contained in a component. \u201cModule\u201d here is the smallest grouping\nof code units, typically a file.\nYou can assess dependence between modules by measuring calls between them (in\nstatic source code analysis). For classifying dependence between components, we\nmake a distinction between hidden code and interface code.\n\u2022 Hidden code is composed of modules that have no incoming dependencies from\nmodules in other components: they call only within their own component (inter\u2010\nnal) and may have calls outside their own component (outgoing calls).\n\u2022 Interface code is composed of modules that have incoming dependencies from\nmodules in other components. They consist of code in modules with incoming\nand throughput code.\nFollowing the principle of loose coupling, a low level of dependence between modules\nis better than a high level of dependence. That signals the risk that changes within one\ncomponent propagate to other components.\nSIG measures component independence as the percentage of code that is classified as\nhidden code. To achieve a SIG/T\u00dcViT rating of 4 stars for highly-maintainable soft\u2010\nware, the percentage of code residing in modules with incoming dependencies from\nother components (incoming or throughput) should not exceed 14.2%.\nSee the three quality profiles in Figure 7-3 as an example:\n\u2022 Left: an open source system, in this case Jenkins\n\u2022 Center: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for component independence\n\u2022 Right: the cutoff points for achieving 4-star quality for this quality characteristic"}
{"109": "Figure 7-3. Three quality profiles for component independence"}
{"110": ""}
{"111": "CHAPTER 8\nKeep Architecture Components Balanced\nBuilding encapsulation boundaries is a crucial skill in software architecture.\n\u2014George H. Fairbanks in\nJust Enough Architecture\nGuideline:\n\u2022 Balance the number and relative size of top-level compo\u2010\nnents in your code.\n\u2022 Do this by organizing source code in a way that the number\nof components is close to 9 (i.e., between 6 and 12) and that\nthe components are of approximately equal size.\n\u2022 This improves maintainability because balanced components\nease locating code and allow for isolated maintenance.\nA well-balanced software architecture is one with not too many and not too few com\u2010\nponents, with sizes that are approximately equal. The architecture then has a good\ncomponent balance.\nAn example of component imbalance would be having a few very large components\nthat contain a disproportionate amount of system logic and many small ones that\ndwindle in comparison.\nFigure 8-1 gives an impression of component balance and what the ideal situation\nwould be. The least desirable situation is on the top left because changes cannot be"}
{"112": "skewed distribution of volume over components. When one component is extraordi\u2010\nnarily large, the architecture becomes monolithic; it becomes hard to navigate the\ncodebase and do isolated maintenance. In the third situation (bottom left), where the\narchitecture is scattered among many components, it becomes hard to keep a mental\nmap of the codebase and to grasp how components interact."}
{"113": "8.1 Motivation\nNow we know what component balance is, but not why it is important. The reason is\nsimple: software maintenance is easier when the software architecture is balanced.\nThis section discusses in what ways you can benefit from a good system component\nbalance: it makes it easier to find and analyze code, it better isolates effects of mainte\u2010\nnance, and it separates maintenance responsibilities.\nA Good Component Balance Eases Finding and Analyzing Code\nA clear code organization in components makes it easier to find the piece of code that\nyou want to change. Of course, proper code hygiene helps in this process as well, such\nas using a consistent naming convention (see Chapter 11). When the number of com\u2010\nponents is manageable (around nine) and their volume is consistent, they allow for a\ndrill-down each time that you need to analyze code to modify it.\nIn contrast, an unbalanced organization of components is more likely to have unclear\nfunctional boundaries. For example, a component that is very large compared to oth\u2010\ners is more likely to contain functionalities that are unrelated, and therefore that\ncomponent is harder to analyze.\nA Good Component Balance Better Isolates Maintenance Effects\nWhen a system\u2019s component balance clearly describes functional boundaries, it has a\nproper separation of concerns, which makes for isolated behavior in the system. Iso\u2010\nlated behavior within system components is relevant because it guards against unex\u2010\npected effects, such as regression.\nMore broadly, isolation of code within components has the general advantage of\nmodularity: components with clear functional and technical boundaries are easier to\nsubstitute, remove, and test than components with mixed functionalities and techni\u2010\ncal intertwinement.\nNote that a good component balance in itself clearly does not guarantee isolation of\nchanges. After all, grouping code in different components does not necessarily make\nthose components independent from each other. So, the degree of dependence\nbetween components is relevant as well, as we will discuss in Chapter 7."}
{"114": "This reasoning about isolation applies to code on a smaller level as\nwell. For example, a system that consists of small, simple classes\nsignals a proper separation of concerns, but does not guarantee it.\nFor that you will need to investigate the actual dependencies (see,\ne.g., Chapter 6).\nA Good Component Balance Separates Maintenance Responsibilities\nHaving clear functional boundaries between components makes it easier to distribute\nresponsibilities for maintenance among separate teams. The number of components\nof a system and their relative size should indicate the system\u2019s decomposition into\nfunctional groups.\nWhen a system has too many or too few components, it is considered more difficult\nto understand and harder to maintain. If the number of components is too low, it\ndoes not help you much to navigate through the functionalities of the system. On the\nother hand, too many components make it hard to get a clear overview of the entire\nsystem.\n8.2 How to Apply the Guideline\nThe two principles of component balance are:\n\u2022 The number of top-level system components should ideally be 9, and generally\nbetween 6 and 12.\n\u2022 The components\u2019 volume in terms of source code should be roughly equal.\nNote that component balance is an indicator for a clear component\nseparation, not a goal in itself. It should follow from the system\ndesign and development process. The division of the system into\ncomponents should be natural, not forced to nine components for\nthe sake of having nine components."}
{"115": "Decide on the Right Conceptual Level for Grouping Functionality into\nComponents\nTo achieve a good system division that is easy to navigate for developers, you need to\nchoose the right conceptual level for grouping functionality. Usually, software systems\nare organized along high-level functional domains that describe what kind of func\u2010\ntions the system performs for the user. Alternatively, a division is made along the sep\u2010\narations of technical specialities.\nFor example, a system that bases component division on function domains might\nhave components like Data Retrieval, Invoice Administration, Reporting, Adminis\u2010\ntrator, and so on. Each component contains source code that offers end-to-end func\u2010\ntionality, ranging from the database to the frontend. A functional division has\nthe advantage of being available during design, before development starts. For devel\u2010\nopers, it has the advantage that they can analyze source code while thinking in\nhigh-level functionalities. A disadvantage can be that developers may need to be pro\u2010\nficient and comfortable in multiple technical domains to make changes to a single\ncomponent.\nAn example of a system that uses technical division might have components like\nFrontend, Backend, Interfaces, Logging, and so on. This approach has advantages for\nteams that have a division of responsibilities based on technology specialization. The\ncomponent division then reflects the division of labor among various specialists.\nChoosing the right concepts for grouping functionality within a system is part of the\nsoftware architect role. This role may be assigned to a single person, or it may be dis\u2010\ntributed over various people within the development team. When changes are needed\nto the component division, those in the architect role must be consulted.\nClarify the System\u2019s Domains and Apply Those Consistently\nOnce a choice for the type of system division into components has been made, you\nneed to apply it consistently. An inconsistent architecture is a bad architecture.\nTherefore, the division into components should be formalized and controlled. While\nmaking the design choices may be an architect\u2019s role, the discipline to create and\nrespect the component boundaries in the code organization applies to all developers.\nA way to achieve this is to agree as a team in which components certain changes need\nto be implemented. It is a collective responsibility to ensure that this is done in a con\u2010\nsistent manner."}
{"116": "8.3 Common Objections to Balancing Components\nThis section discusses objections regarding component balance. Common objections\nare that component imbalance is not really a problem, or that it is a problem that can\u2010\nnot be fixed.\nObjection: Component Imbalance Works Just Fine\n\u201cOur system may seem to have bad component balance, but we\u2019re not having any prob\u2010\nlems with it.\u201d\nComponent balance, as we define it, is not binary. There are different degrees of bal\u2010\nance, and its definition allows for some deviation from the \u201cideal\u201d of nine compo\u2010\nnents of equal size. Whether a component imbalance is an actual maintenance\nburden depends on the degree of deviation, the experience of the maintenance team,\nand the cause of the imbalance.\nThe most important maintenance burden occurs when the imbalance is caused by\nlack of discipline during maintenance\u2014when developers do not put code in the com\u2010\nponent where it belongs. Since inconsistency is the enemy of predictability, that may\nlead to unexpected effects. Code that is placed in the wrong components may lead to\nunintended dependencies between components, which hurts testability and flexibility.\nObjection: Entanglement Is Impairing Component Balance\n\u201cWe cannot get component balance right because of entanglement among components.\u201d\nThis situation points to another problem: technical dependence between compo\u2010\nnents. Entanglement between components signals an improper separation of con\u2010\ncerns. This issue and guideline are further described in Chapter 7. In this case, it is\nmore important and urgent to fix component dependencies\u2014for example, by hiding\nimplementation details behind interfaces and fixing circular dependencies. After that,\nyou can revise your component structure to improve its balance.\n8.4 See Also\nComponent coupling is closely related to the idea of component balance discussed in\nthis chapter. Component coupling is discussed in Chapter 7."}
{"117": "How SIG Rates Component Balance\nSIG defines and measures component balance as a combined calculation (i.e., multi\u2010\nplication) of the following:\n\u2022 The number of top-level system components\n\u2022 The uniformity of component size\nThe ideal number of top-level system components is nine, as SIG has identified that as\nthe median in its benchmark. The closer the actual number of components is to nine,\nthe better.\nThe score for the number of top-level system components ranges from 0 to 1. A sys\u2010\ntem with nine components gives a score of 1, linearly decreasing to 0 for a system\nwith one component. A correction is applied upward, to allow for a more lenient\ncount when the number of components is higher than 17, which would otherwise\nlead to a score of 0 with a linear model. The correction is based on the 95th percentile\nscores within the benchmark.\nUniformity of component size means the distribution of source code volume between\ncomponents. An equally sized distribution of top-level components is better than an\nunequal distribution.\nSIG uses the adjusted Gini coefficient as a measure of component size uniformity.\nThe Gini coefficient measures the inequality of distribution between things and\nranges from 0 (perfect equality) to 1 (perfect inequality).\nTo achieve a SIG/T\u00dcViT rating of 4 stars for highly-maintainable software, the num\u2010\nber of components should be close to the ideal of nine, and the adjusted Gini coeffi\u2010\ncient of the component sizes should be 0.71 maximum.\nSee the volume charts in Figure 8-2 as an example:\n\u2022 Left: an anonymous system in the SIG benchmark that scores 2 stars on compo\u2010\nnent balance. Note that both the number of components (six) and their code vol\u2010\nume are off.\n\u2022 Right: an anonymous system in the SIG benchmark that complies with a 4-star\nrating for component balance. Note that even though the volume of the compo\u2010\nnents is not equal, the number of components is exactly nine, which levels out\nthe influence of the component size inequality."}
{"118": ""}
{"119": "CHAPTER 9\nKeep Your Codebase Small\nProgram complexity grows until it exceeds the capability of the programmer who must\nmaintain it.\n\u20147th Law of Computer Programming\nGuideline:\n\u2022 Keep your codebase as small as feasible.\n\u2022 Do this by avoiding codebase growth and actively reducing\nsystem size.\n\u2022 This improves maintainability because having a small prod\u2010\nuct, project, and team is a success factor.\nA codebase is a collection of source code that is stored in one repository, can be com\u2010\npiled and deployed independently, and is maintained by one team. A system has at\nleast one codebase. Larger systems sometimes have more than one codebase. A typi\u2010\ncal example is packaged software. There may be a codebase for the standard function\u2010\nality, and there are different, independently maintained codebases for customer- or\nmarket-specific plugins."}
{"120": "Given two systems with the same functionality, in which one has a small codebase\nand the other has a large codebase, you surely would prefer the small system. In a\nsmall system it is easier to search through, analyze, and understand code. If you mod\u2010\nify something, it is easier to tell whether the change has effects elsewhere in the sys\u2010\ntem. This ease of maintenance leads to fewer mistakes and lower costs. That much is\nobvious.\n9.1 Motivation\nSoftware development and maintenance become increasingly hard with growing sys\u2010\ntem size. Building larger systems requires larger teams and longer-lasting projects,\nwhich bring additional overhead and risks of (project) failure. The rest of this section\ndiscusses the adverse effects of large software systems.\nA Project That Sets Out to Build a Large Codebase Is More Likely to Fail\nThere is a strong correlation between project size and project risks. A large project\nleads to a larger team, complex design, and longer project duration. As a result, there\nis more complex communication and coordination among stakeholders and team\nmembers, less overview over the software design, and a larger number of require\u2010\nments that change during the project. This all increases the chance of reduced quality,\nproject delays, and project failure. The probabilities in the graph in Figure 9-1 are\ncumulative: for example, for all projects over 500 man-years of development effort,\nmore than 90% are indentified as \u201cpoor project quality.\u201d A subset of this is projects\nwith delays (80\u201390% of the total) and failed projects (50% of the total).\nFigure 9-1 illustrates the relationship between project size and project failure: it\nshows that as the size of a project increases, the chances of project failure (i.e., project\nis terminated or does not deliver results), of project delay, and of a project delivered\nwith poor quality are increasingly high."}
{"121": "Figure 9-1. Probability of project failures by project size1"}
{"122": "Large Codebases Are Harder to Maintain\nFigure 9-2 illustrates how codebase size affects maintainability.\nFigure 9-2. Distribution of system maintainability in SIG benchmark among different\nvolume groups\nThe graph is based on a set of codebases of over 1,500 systems in the SIG Software\nAnalysis Warehouse. Volume is measured as the amount of development effort in\nman-years to reproduce the system (see also \u201cHow SIG Rates Codebase Volume\u201d on\npage 106). Each bar shows the distribution of systems in different levels of maintaina\u2010\nbility (benchmarked in stars). As the graph shows, over 30% of systems in the small\u2010\nest volume category manage to reach 4- or 5-star maintainability, while in the largest\nvolume category only a tiny percentage reaches this level.\nLarge Systems Have Higher Defect Density\nYou may expect that a larger system has more defects in absolute numbers. But the\ndefect density (defined as the number of defects per 1,000 lines of code) also increases\nsubstantially as systems grow larger. Figure 9-3 shows the relationship between code\nvolume and the number of defects per 1,000 lines of code. Since the number of\ndefects rises when code volume grows, the graph shows that larger systems have"}
{"123": "Figure 9-3. Impact of code volume on the number of defects2\n9.2 How to Apply the Guideline\nAll other things being equal, a system that has less functionality will be smaller than a\nsystem that has more functionality. Then, the implementation of that functionality\nmay be either concise or verbose. Therefore, achieving a small codebase first requires\nkeeping the functionality of a system limited, and then requires attention to keep the\namount of code limited.\nFunctional Measures\nFunctionality-related measures are not always within your span of control, but when\u2010\never new or adapted functionality is being discussed with developers, the following\nshould be considered:\nFight scope creep:\nIn projects, scope creep is a common phenomenon in which requirements\nextend during development. This may lead to \u201cnice-to-have functionality\u201d that\nadds growth to the system without adding much value to the business or the user.\nFight scope creep by confronting the business with the price of additional func\u2010\ntionality, in terms of project delays or higher future maintenance costs."}
{"124": "Standardize functionality:\nBy standardization of functionality we mean consistency in the behavior and\ninteractions of the program. First of all, this is intended to avoid the implementa\u2010\ntion of the same core functionality in multiple, slightly different ways. Secondly,\nstandardization of functionality offers possibilities for reuse of code\u2014assuming\nthe code itself is written in a reusable way.\nTechnical Measures\nFor the technical implementation, the goal is to use less code to implement the same\nfunctionality. You can achieve this mainly through reusing code by referral (instead\nof writing or copying and pasting code again) or by avoiding coding altogether, but\nusing existing libraries or frameworks.\nDo not copy and paste code:\nReferring to existing code is always preferable to copying and pasting code in\npieces that will need to be maintained individually. If there are multiple copies of\na piece of code, maintenance needs to occur in multiple places, too. Mistakes\neasily crop up if an update in one piece of logic requires individual adjustment\n(or not) and testing of multiple, scattered copies. Note that the intention of the\nguideline presented in Chapter 4 is precisely to avoid copying and pasting.\nRefactor existing code:\nWhile refactoring has many merits for code maintainability, it can have an imme\u2010\ndiate and visible effect in reducing the codebase. Typically, refactoring involves\nrevisiting code, simplifying its structure, removing code redundancies, and\nimproving the amount of reuse. This may be as simple as removing unused/obso\u2010\nlete functionality. See, for example, the refactoring patterns in Chapter 4.\nUse third-party libraries and frameworks:\nMany applications share the same type of behavior for which a vast number of\nframeworks and libraries exist\u2014for example, UI behavior (e.g., jQuery), database\naccess (e.g., Hibernate), security measurements (e.g., Spring Security), logging\n(e.g., SLF4J), or utilities (e.g., Google Guava). Using third-party libraries is espe\u2010\ncially helpful for such generic functionality. If functionality is used and main\u2010\ntained by other parties, why invent your own? Using third-party code is\nespecially helpful because it avoids unnecessary over-engineering. It is well worth\nconsidering adjusting functionality to fit it to third-party code instead of building\na custom solution."}
{"125": "Do not make changes to the source code of a third-party library. If\nyou do, essentially you have made the library code part of your\nown codebase. In particular, updates of changed libraries are cum\u2010\nbersome and can easily lead to bugs. Typically, difficulties arise\nwhen developers try to update the library to a newer version, since\nthey need to analyze what has been changed in the library code and\nhow that impacts the locally changed code.\nSplit up a large system:\nSplitting up a large system into multiple smaller systems is a way to minimize the\nissues that come with larger systems. A prerequisite is that the system can be\ndivided into parts that are independent, from a functional, technical, and lifecycle\nperspective. To the users, the systems (or plugins) must be clearly separated.\nTechnically, the code in the different systems must be loosely coupled; that is,\ntheir code is related via interfaces instead of direct dependencies. Systems are\nonly really independent if their lifecycles are decoupled (i.e., they are developed\nand released independently). Note that the split systems may well have some\nmutual or shared dependencies. There is an additional advantage. It might turn\nout that some of the new subsystems can be replaced by a third-party package,\ncompletely removing the need to have any codebase for this subsystem. An\nexample is a Linux distribution such as Ubuntu. The Linux kernel is a codebase\nthat lives at kernel.org and is maintained by a large team of volunteers headed by\nLinus Torvalds. Next to the actual Linux kernel, a distribution contains thou\u2010\nsands of other software applications, each of which has its own codebase. These\nare the types of plugins that we mean here.\nDecoupling (on a code level) is discussed in more detail in the chapters that deal with\nloose coupling, particularly Chapter 7.\n9.3 Common Objections to Keeping the Codebase Small\nThe measures described in this chapter are applicable to all phases of software devel\u2010\nopment. They support the primary maintainability goal of achieving a small\ncodebase.\nThere are generally two familiar strategies with which you can actively pursue the\ngoal of a small codebase: avoiding the problem (avoiding further codebase growth) or\nfixing the problem (reducing the size of the codebase).\nThe biggest long-term gains are achieved when you are working on a system that is\nalready quite small or in an early stage of development. Technical adjustments such as"}
{"126": "The most visible improvements will appear once a system is big and parts of it can be\nremoved\u2014for example, when functionality is being replaced by third-party code or\nafter a system has been split into multiple parts.\nObjection: Reducing the Codebase Size Is Impeded by Productivity\nMeasures\n\u201cI cannot possibly reduce the size of my system, since my programming productivity is\nbeing measured in terms of added code volume.\u201d\nIf this is the case, we suggest escalating this issue. Measuring development productiv\u2010\nity in terms of added code volume is a bad practice. It provides a negative incentive,\nas it encourages the bad habit of copying and pasting code. Code reference is better\nbecause it improves analyzing, testing, and changing code.\nWe understand that the number of code additions can help managers monitor pro\u2010\ngress and predict timelines. However, productivity should be measured in terms of\nvalue added, not lines of code added. Experienced developers can often add function\u2010\nality with a minimum number of additional lines of code, and they will refactor the\ncode whenever they see an opportunity, often resulting in reduction of the code size.\nObjection: Reducing the Codebase Size is Impeded by the\nProgramming Language\n\u201cI work with a language that is more verbose than others, so I cannot achieve a small\ncodebase.\u201d\nIn most projects, the programming language is a given. It may very well be true that\nin some programming languages, it is impossible to get a small codebase (SQL-based\nlanguages come to mind). However, you can always strive to get a smaller codebase\nthan you currently have, in the same programming language. Every codebase benefits\nfrom decreasing its size, even those in low-level languages with little possibility for\nabstraction.\nObjection: System Complexity Forces Code Copying\n\u201cMy system is so complicated that we can only add functionality by copying large pieces\nof existing code. Hence, it is impossible to keep the codebase small.\u201d\nDifficulty in understanding existing code, and hence the fear of touching that code, is\na common reason that programmers resort to copying and pasting. This is particu\u2010\nlarly the case if the code has an insufficient number of automated tests."}
{"127": "original functionality can be split up into multiple parts, then ideally you end up with\na piece of code that can be referred to independently by the new functionality, avoid\u2010\ning duplication and taming codebase growth. Write unit tests for the new units to\nverify that you understand the inner workings of the unit. Besides, it is recommended\npractice; see Chapter 10.\nObjection: Splitting the Codebase Is Impossible Because of Platform\nArchitecture\n\u201cWe cannot split the system into smaller parts because we are building for a platform\nwhere all functionality is tied to a common codebase.\u201d\nYes, platform-based software tends to grow large over time because it assimilates new\nfunctionality and rarely reduces functionality. One way to dramatically decrease the\nsize of the codebase is to decouple the system into a plug-in architecture. This leads\nto multiple codebases that are each smaller than the original one. There is a codebase\nfor the common core, and one or more codebases for the plugins. If those plugins are\ntechnically decoupled, they allow for separate release cycles. That means that small\nchanges in functionality do not need an update of the whole system. Keep in mind\nthat those small updates still need full integration/regression tests to ensure that the\nsystem as a whole still functions as expected.\nObjection: Splitting the Codebase Leads to Duplication\n\u201cSplitting the codebase forces me to duplicate code.\u201d\nThere may be cases in which decoupling a system into separate parts (such as plu\u2010\ngins/extensions) requires (interfaces to) common functionality or data structures to\nbe duplicated in those extensions.\nIn such a case, duplication is a bigger problem than having a large codebase, and the\nguideline of Chapter 4 prevails over this guideline of achieving a small codebase. It is\nthen preferable to code common functionality either as a separate extension or as\npart of a common codebase.\nObjection: Splitting the Codebase Is Impossible Because of Tight\nCoupling\n\u201cI cannot split up my system since the system parts are tightly coupled.\u201d\nThen decouple the system first. To achieve this, you can write specific interfaces that\nact as a uniform entry point to functionality. This can be achieved with WebServices,"}
{"128": "Keep in mind that the goal is to have subsystems that can be\nmaintained independently, not necessarily systems that operate\nindependently.\nHow SIG Rates Codebase Volume\nThe metric for codebase volume does not have different risk categories, since it con\u2010\nsists of only one metric. To be rated at 4 stars, the codebase should be at most equiva\u2010\nlent to 20 man-years of rebuild value. If Java is the only technology in a system, this\ntranslates to at most 175,000 lines of code.\nMan-months and man-years\nThe total volume in a codebase is the volume in lines of code converted to man-\nmonths. A man-month is a standard measure of source code volume. It is the amount\nof source code that one developer with average productivity could write in one\nmonth. The advantage of \u201cman-month\u201d is that it allows for comparisons of source\ncode volume between technologies. This is relevant because programming languages\nhave different productivity measures, or \u201clevels of verbosity.\u201d Therefore, a system with\nmultiple programming languages can be converted to an aggregate measure that tells\nyou the approximate effort it would take to rebuild it: the \u201crebuild value.\u201d\nSIG\u2019s experience has shown that the man-month is an effective metric to assess the\nsize of a system and to compare systems with each other. A man-year is simply 12\nman-months. Of course, actual productivity is also dependent on skill and program\u2010\nming style. The volume metric does not tell you how many months or years of effort\nactually went into building the system."}
{"129": "CHAPTER 10\nAutomate Tests\nKeep the bar green to keep the code clean.\n\u2014The jUnit motto\nGuideline:\n\u2022 Automate tests for your codebase.\n\u2022 Do this by writing automated tests using a test framework.\n\u2022 This improves maintainability because automated testing\nmakes development predictable and less risky.\nIn Chapter 4, we have presented isValid, a method to check whether bank account\nnumbers comply with a checksum. That method contains a small algorithm that\nimplements the checksum. It is easy to make mistakes in a method like this. That is\nwhy probably every programmer in the world at some point has written a little, one-\noff program to test the behavior of such a method, like so:\npackage eu.sig.training.ch10;\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport eu.sig.training.ch04.v1.Accounts;\npublic class Program {"}
{"130": "do {\nSystem.out.println(\"Type a bank account number on the next line.\");\nacct = isr.readLine();\nSystem.out.println(\"Bank account number '\" + acct + \"' is\" +\n(Accounts.isValid(acct) ? \"\" : \" not\") + \" valid.\");\n} while (acct != null && acct.length() != 0);\n}\n}\nThis is a Java class with a main method, so it can be run from the command line:\n$ java Program\nType a bank account number on the next line.\n123456789\nBank account number '123456789' is valid.\nType a bank account number on the next line.\n123456788\nBank account number '123456788' is not valid.\n$\nA program like this can be called a manual unit test. It is a unit test because it is used\nto test just one unit, isValid. It is manual because the user of this program has to\ntype in test cases manually, and manually assess whether the output of the program is\ncorrect.\nWhile better than having no unit testing at all, this approach has several problems:\n\u2022 Test cases have to be provided by hand, so the test cannot be executed automati\u2010\ncally in an easy way.\n\u2022 The developer who has written this test is focusing on logic to execute the test\n(the do \u2026 while loop, all input/output handling), not on the test itself.\n\u2022 The program does not show how isValid is expected to behave.\n\u2022 The program is not recognizable as a test (although the rather generic name\nProgram is an indication it is meant as a one-off experiment).\nThat is why you should write automated unit tests instead of manual unit tests. These\nare tests of code units themselves described in code that runs autonomously. The\nsame holds for other types of testing, such as regression tests and user acceptance\ntests: automate as much as possible, using a standard test framework. For unit tests, a\ncommon framework is jUnit."}
{"131": "10.1 Motivation\nThis section describes the advantages of automating your tests as much as possible.\nAutomated Testing Makes Testing Repeatable\nJust like other programs and scripts, automated tests are executed in exactly the same\nway every time they are run. This makes testing repeatable: if a certain test executes at\ntwo different points in time yet gives different answers, it cannot be that the test exe\u2010\ncution itself was faulty. One can conclude that something has changed in the system\nthat has caused the different outcome. With manual tests, there is always the possibil\u2010\nity that tests are not performed consistently or that human errors are made.\nAutomated Testing Makes Development Efficient\nAutomated tests can be executed with much less effort than manual tests. The effort\nthey require is negligible and can be repeated as often as you see fit. They are also\nfaster than manual code review. You should also test as early in the development pro\u2010\ncess as possible, to limit the effort it takes to fix problems.\nPostponing testing to a late stage in the development pipeline risks\nlate identification of problems. That costs more effort to fix,\nbecause code needs to go back through the development pipeline\nand be merged, and tests must be rerun.\nAutomated Testing Makes Code Predictable\nTechnical tests can be automated to a high degree. Take unit tests and integration\ntests: they test the technical inner workings of code and the cohesion/integration of\nthat code. Without being sure of the inner workings of your system, you might get\nthe right results by accident. It is a bit like driving a car: you might arrive at an\nintended destination by following the wrong directions, but when you want to go to\nanother destination, you are uncertain whether the new directions are reliable and\nwill actually take you there.\nA common advantage of automated testing is identifying when regression is occur\u2010\nring. Without a batch of automated unit tests, development quickly turns into a game\nof whack-a-mole: you implement a change in one piece of code, and while you are\nworking on the next change in another piece of code, you realize you have introduced"}
{"132": "Thus, running automated tests provides certainty about how the code works. There\u2010\nfore, the predictability of automated tests also makes the quality of developed code\nmore predictable.\nTests Document the Code That Is Tested\nThe script or program code of a test contains assertions about the expected\nbehavior of the system under test. For example, as will be illustrated later in this\nchapter, an appropriate test of isValid contains the following line of code:\nassertFalse(isValid(\"\")). This documents, in Java code, that we expect isValid\nto return false when checking the empty string. In this way, the assertFalse state\u2010\nment plays a double role: as the actual test, and as documentation of the expected\nbehavior. In other words, tests are examples of what the system does.\nWriting Tests Make You Write Better Code\nWriting tests helps you to write testable code. As a side effect, this leads to code con\u2010\nsisting of units that are shorter, are simpler, have fewer parameters, and are more\nloosely coupled (as the guidelines in the previous chapters advise). For example, a\nmethod is more difficult to test when it performs multiple functions instead of only\none. To make it easier to test, you move responsibilities to different methods, improv\u2010\ning the maintainability of the whole. That is why some development approaches\nadvocate writing a unit test before writing the code that conforms to the test. Such\napproaches are called test-driven development (TDD) approaches. You will see that\ndesigning a method becomes easier when you think about how you are going to test\nit: what are the valid arguments of the method, and what should the method return as\na result?\n10.2 How to Apply the Guideline\nHow you automate tests differs by the types of tests you want to automate. Test types\ndiffer in what is tested, by whom, and why, as detailed in Table 10-1. They are ordered\nfrom top to bottom based on the scope of the tests. For example, a unit test has the\nunit as scope, while an end-to-end test, a regression test, and an acceptance test are\non the system level."}
{"133": "Table 10-1. Types of testing\nType What it tests Why Who\nUnit test Functionality of one unit in isolation Verify that unit behaves as Developer (preferably\nexpected of the unit)\nIntegration test Functionality, performance, or other quality Verify that parts of the system Developer\ncharacteristic of at least two classes work together\nEnd-to-end test System interaction (with a user or another Verify that system behaves as Developer\nsystem) expected\nRegression test Previously erroneous behavior of a unit, class, Ensure that bugs do not re- Developer\nor system interaction appear\nAcceptance test System interaction (with a user or another Confirm the system behaves as End-user\nsystem) required representative (never\nthe developer)\nTable 10-1 shows that a regression test is a unit test, an integration test, or an end-to-\nend test that has been created when a bug was fixed. Acceptance tests are end-to-end\ntests executed by end user representatives.\nDifferent types of testing call for different automation frameworks. For unit testing,\nseveral well-known Java frameworks are available, such as jUnit. For automated\nend-to-end testing, you need a framework that can mimic user input and capture\noutput. A well-known framework that does just that for web development is Sele\u2010\nnium. For integration testing, it all depends on the environment in which you are\nworking and the quality characteristics you are testing. SoapUI is a framework for\nintegration tests that focuses on web services and messaging middleware. Apache\njMeter is a framework for testing the performance of Java applications under heavy\nworkloads.\nChoosing a test framework needs to be done at the team level. Writing integration\ntests is a specialized skill\u2014but unit testing is for each and every individual developer.\nThat is why the rest of this chapter focuses on writing unit tests using the most well-\nknown framework for Java: jUnit.\nContrary to specialized integration and end-to-end tests, writing\nunit tests is a skill that every developer needs to master."}
{"134": "Writing unit tests also requires the smallest upfront investment: just download jUnit\nfrom http://www.junit.org.\nGetting Started with jUnit Tests\nAs we noted in the introduction of this chapter, we want to test isValid, a method of\ntheclass Accounts. Accounts is called the class under test. In jUnit 4, tests are put in a\ndifferent class, the test class. By convention, the name of the test class is the name of\nthe class under test with the suffix Test added. In this case, that would mean the\nname of the test class is AccountsTest. It must be a public class, but apart from that,\nthere are no other requirements for a test class. In particular, it does not need to\nextend any other class. It is convenient, but not required, to place the test class in the\nsame package as the class under test. That way, the test class has access to all members\nof the test class under test that have package (but not public) access.\nIn jUnit 4, a test itself is any method that has the @Test annotation. By convention,\nthe name of a test method starts with test, but this is just a convention, not a\nrequirement. To test isValid, you can use the following jUnit 4 test class:\npackage eu.sig.training.ch04.v1;\nimport static org.junit.Assert.assertFalse;\nimport static org.junit.Assert.assertTrue;\nimport org.junit.Test;\npublic class AccountsTest {\n@Test\npublic void testIsValidNormalCases() {\nassertTrue(Accounts.isValid(\"123456789\"));\nassertFalse(Accounts.isValid(\"123456788\"));\n}\n}\nThis test handles two cases:\n\u2022 Bank account number 123456789: We know this is a valid bank account number\n(see \u201cThe 11-Check for Bank Account Numbers\u201d on page 12), so isValid should\nreturn true. The call of assertTrue tests this.\n\u2022 Bank account number 123456788: We know this is an invalid bank account num\u2010\nber (because it differs from a valid account number by one digit), so isValid"}
{"135": "Ant. Figure 10-1 shows the result of running the preceding test in Eclipse. The darker\nbar indicates that all tests have succeeded.\nFigure 10-1. All tests succeeded!\nThe test in the preceding test class only tests normal cases: two bank account num\u2010\nbers of the expected format (exactly nine characters, all digits). How about corner\ncases? One obvious special case is the empty string. The empty string is, of course,\nnot a valid bank account number, so we test it by calling assertFalse:\n@Test\npublic void testEmptyString() {\nassertFalse(Accounts.isValid(\"\"));\n}\nAs Figure 10-2 shows, it turns out that this test fails! While the call to isValid should\nreturn false, it actually returned something else (which, of course, must be true, as\nthere is no other option).\nFigure 10-2. One test failed\nThe failed test points us to a flaw in isValid. In case the argument to isValid is the\nempty string, the for loop does not run at all. So the only lines executed are:"}
{"136": "This indeed returns true, while it should return false. This reminds us to add code\nto isValid that checks the length of the bank account number.1\nThe jUnit 4 runner reports this as a test failure and not as a test error. A test failure\nmeans that the test itself (the method testEmptyString) is executed perfectly, but the\nassertion failed. A test error means that the test method itself did not execute cor\u2010\nrectly. The following code snippet illustrates this: the showError method raises a\ndivision-by-zero exception and never even executes assertTrue:\n@Test\npublic void showError() {\nint dummy = 1 / 0;\n// Next line is never executed because the previous one raises an\n// exception.\n// If it were executed, you'll never see the assert message because\n// the test always succeeds.\nassertTrue(\"You will never see this text.\", true);\n}\nNext, we present some basic principles that will help you write good unit tests. We\nstart with the most basic principles and then progress to more advanced ones that\napply when your test efforts become more mature.\nGeneral Principles for Writing Good Unit Tests\nWhen writing tests, it is important to keep in mind the following general principles:\nTest both normal and special cases\nAs in the examples given in this chapter, test two kinds of cases. Write tests that\nconfirm that a unit indeed behaves as expected on normal input (called happy\nflow or sunny-side testing). Also write tests that confirm that a unit behaves sensi\u2010\nbly on non-normal input and circumstances (called unhappy flow or rainy-side\ntesting). For instance, in jUnit it is possible to write tests to confirm that a\nmethod under test indeed throws a certain exception.\nMaintain tests just like nontest (production) code\nWhen you adjust code in the system, the changes should be reflected in the unit\ntests as well. This is most relevant for unit tests, though it applies to all tests. In\nparticular, when adding new methods or enhancing the behavior of existing\nmethods, be sure to add new test cases that cover that new code."}
{"137": "Write tests that are isolated: their outcomes should reflect only the behavior of the sub\u2010\nject being tested\nThat is, each test should act independently of all other tests. For unit testing, this\nmeans that each test case should test only one functionality. No unit test should\ndepend on state, such as files written by other tests. That is why a unit test that,\nsay, causes the class under test to access the filesystem or a database server is not\na good unit test.\nConsequently, in unit testing you should simulate the state/input of other classes\nwhen those are needed (e.g., as arguments). Otherwise, the test is not isolated and\nwould test more than one unit. This was easy for the test of isValid, because isValid\ntakes a string as an argument, and it does not call other methods of our system. For\nother situations, you may need a technique like stubbing or mocking.\nIn Chapter 6, we introduced a Java interface for a simple digital camera, which is\nrepeated here for ease of reference:\npublic interface SimpleDigitalCamera {\npublic Image takeSnapshot();\npublic void flashLightOn();\npublic void flashLightOff();\n}\nSuppose this interface is used in an application that ensures people never forget to\nturn on the flash at night:\npublic final static int DAYLIGHT_START = 6;\npublic Image takePerfectPicture(int currentHour) {\nImage image;\nif (currentHour < PerfectPicture.DAYLIGHT_START) {\ncamera.flashLightOn();\nimage = camera.takeSnapshot();\ncamera.flashLightOff();\n} else {\nimage = camera.takeSnapshot();\n}\nreturn image;\n}\nAlthough the logic is simple (takePerfectPicture simply assumes that if the hour of\nthe day on a 24-hour clock is lower than 6 p.m., it is night), it needs testing. For a\nproper unit test for takePerfectPicture to be written, taking a picture needs to be\nautomatic and independent. That means that the normal implementation of the digi\u2010"}
{"138": "button. The picture taken can be any picture, so it is hard to test whether the (suppos\u2010\nedly perfect) picture taken is the one expected.\nThe solution is to use an implementation of the camera interface that has been made\nespecially for testing. This implementation is a fake object, called a test stub or simply\na stub.2 In this case, we want this fake object to behave in a preprogrammed (and\ntherefore predictable) way. We write a test stub like this:\nclass DigitalCameraStub implements SimpleDigitalCamera {\npublic Image testImage;\npublic Image takeSnapshot() {\nreturn this.testImage;\n}\npublic void flashLightOn() {}\npublic void flashLightOff() {}\n}\nIn this stub, takeSnapshot always returns the same image, which we can set simply\nby assigning to testImage (for reasons of simplicity, we have made testImage a pub\u2010\nlic field and do not provide a setter). This stub can now be used in a test:\n@Test\npublic void testDayPicture() throws IOException {\nBufferedImage image =\nImageIO.read(new File(\"src/test/resources/VanGoghSunflowers.jpg\"));\nDigitalCameraStub cameraStub = new DigitalCameraStub();\ncameraStub.testImage = image;\nPerfectPicture.camera = cameraStub;\nassertEquals(image, new PerfectPicture().takePerfectPicture(12));\n}\nIn this test, we create a stub camera and supply it with an image to return. We then\ncall takePerfectPicture(12) and test whether it returns the correct image. The\nvalue of the call, 12, means that takePerfectPicture assumes it is between noon\nand 1 p.m.\nNow suppose we want to test takePerfectPicture for nighttime behavior; that is, we\nwant to ensure that if takePerfectPicture is called with a value lower than Perfect\nPicture.DAYLIGHT_START, it indeed switches on the flash. So, we want to test whether\ntakePerfectPicture indeed calls flashLightOn. However, flashLightOn does not\nreturn any value, and the SimpleDigitalCamera interface also does not provide any\nother way to know whether the flash has been switched on. So what to check?"}
{"139": "The solution is to provide the fake digital camera implementation with some mecha\u2010\nnism to record whether the method we are interested in gets called. A fake object that\nrecords whether expected calls have taken place is called a mock object. So, a mock\nobject is a stub object with added test-specific behavior. The digital camera mock\nobject looks like this:\nclass DigitalCameraMock implements SimpleDigitalCamera {\npublic Image testImage;\npublic int flashOnCounter = 0;\npublic Image takeSnapshot() {\nreturn this.testImage;\n}\npublic void flashLightOn() {\nthis.flashOnCounter++;\n}\npublic void flashLightOff() {}\n}\nCompared to DigitalCameraStub, DigitalCameraMock additionally keeps track of\nthe number of times flashLightOn has been called, in a public field. DigitalCamera\nMock still contains preprogrammed behavior, so it is both a stub and a mock. We can\ncheck that flashLightOn is called in the unit test like so:\n@Test\npublic void testNightPicture() throws IOException {\nBufferedImage image =\nImageIO.read(new File(\"src/test/resources/VanGoghStarryNight.jpg\"));\nDigitalCameraMock cameraMock = new DigitalCameraMock();\ncameraMock.testImage = image;\nPerfectPicture.camera = cameraMock;\nassertEquals(image, new PerfectPicture().takePerfectPicture(0));\nassertEquals(1, cameraMock.flashOnCounter);\n}\nIn these examples, we have written our own stub and mock objects. This leads to a lot\nof code. Generally, it is most efficient to use a mocking framework such as Mockito or\nEasyMock. Mocking frameworks use features of the Java Virtual Machine to automat\u2010\nically create mock objects from normal interfaces or classes. They also provide meth\u2010\nods to test whether methods of a mock object have been called, and with which\narguments. Some mocking frameworks also provide ways to specify preprogrammed\nbehavior of mock objects, giving them the characteristics of both stubs and mocks.\nIndeed, using Mockito as an example, you can write testNightPicture without any"}
{"140": "@Test\npublic void testNightPictureMockito() throws IOException {\nBufferedImage image =\nImageIO.read(new File(\"src/test/resources/VanGoghStarryNight.jpg\"));\nSimpleDigitalCamera cameraMock = mock(SimpleDigitalCamera.class);\nPerfectPicture.camera = cameraMock;\nwhen(cameraMock.takeSnapshot()).thenReturn(image);\nassertEquals(image, new PerfectPicture().takePerfectPicture(0));\nverify(cameraMock).flashLightOn();\n}\nIn this test, Mockito\u2019s mock method is called to create cameraMock, the mock object\nused in this test. With Mockito\u2019s when and thenReturn methods, the desired behavior\nis specified. Mockito\u2019s verify method is used to verify whether flashLightOn has\nbeen called.\nMeasure Coverage to Determine Whether There Are Enough Tests\nHow many unit tests are needed? One way to assess whether you have written enough\nunit tests is to measure coverage of your unit tests. Coverage, or more precisely, line\ncoverage, is the percentage of lines of code in your codebase that actually get executed\nwhen all unit tests are executed. As a rule of thumb, you should aim for at least 80%\nline coverage with a sufficient number of tests\u2014that is, as many lines of test code as pro\u2010\nduction code.\nWhy 80% coverage (and not 100%)? Any codebase contains fragments of trivial code\nthat technically can be tested, but are so trivial that testing them makes little sense.\nTake the following typical Java getter method:\npublic String getName() {\nreturn this.name;\n}\nIt is possible to test this getter (with something like assertEquals(myObj.get\nName(),\"John Smith\")), but with this test, you are mostly testing that the Java com\u2010\npiler and the Java Virtual Machine work as expected. But it is not true that you should\nnever test getters. Take a typical class that represents postal mail addresses. It typically\nhas two or three string fields that represent (additional) address lines. It is easy to\nmake a mistake like this one:\npublic String getAddressLine3() {\nreturn this.addressLine2;\n}\nA minimum of 80% coverage alone is not enough to ensure high-quality unit tests."}
{"141": "You can measure coverage using a code coverage tool. Well-known examples are the\nMaven plugin Cobertura, and the Eclipse plugin EclEmma. Figure 10-3 shows cover\u2010\nage of the Joda codebase, an open source Java library with date and time classes that\ncomes with many unit tests.\nFigure 10-3. Joda time coverage report in Eclipse, as provided by the EclEmma plugin.\n10.3 Common Objections to Automating Tests\nThis section discusses typical objections and limitations regarding automation. They\ndeal with the reasons and considerations to invest in test automation.\nObjection: We Still Need Manual Testing\n\u201cWhy should we invest in automated tests at all if we still need manual testing?\u201d\nThe answer to this question is simply because test automation frees up time to man\u2010\nually test those things that cannot be automated.\nConsider the downsides of the alternative to automated tests. Manual testing has clear\nlimitations. It is slow, expensive, and hard to repeat in a consistent manner. In fact,\ntechnical verification of the system needs to take place anyway, since you cannot man\u2010\nually test code that does not work. Because manual tests are not easily repeatable,\neven with small code changes a full retest may be needed to be sure that the system\nworks as intended.\nManual acceptance testing can largely be automated with automa\u2010\nted regression tests. With those, the scope of remaining manual\ntests decreases. You may still need manual review or acceptance\ntests to verify that business logic is correct. This typically concerns\nthe process flow of a functionality."}
{"142": "Objection: I Am Not Allowed to Write Unit Tests\n\u201cI am not allowed to write unit tests because they lower productivity according to my\nmanager.\u201d\nWriting unit tests during development actually improves productivity. It improves\nsystem code by shifting the focus from \u201cwhat code should do\u201d toward \u201cwhat it should\nnot do.\u201d If you never take into account how the code may fail, you cannot be sure\nwhether your code is resilient to unexpected situations.\nThe disadvantages of not having unit tests are mainly in uncertainty and rework.\nEvery time a piece of code is changed, it requires painstaking review to verify whether\nthe code does what it is supposed to do.\nObjection: Why Should We Invest in Unit Tests When the Current\nCoverage Is Low?\n\u201cThe current unit test coverage of my system is very low. Why should I invest time now\nin writing unit tests?\u201d\nWe have elaborated on the reasons why unit tests are useful and help you develop\ncode that works predictably. However, when a very large system has little to no unit\ntest code, this may be a burden. After all, it would be a significant investment to start\nwriting unit tests from scratch for an existing system because you would need to ana\u2010\nlyze all units again. Therefore, you should make a significant investment in unit tests\nonly if the added certainty is worth the effort. This especially applies to critical, cen\u2010\ntral functionality and when there is reason to believe that units are behaving in an\nunintended manner. Otherwise, add unit tests incrementally each time you change\nexisting code or add new code.\nIn general, when the unit test coverage of a system is much below\nthe industry best practice of 80%, a good strategy is to apply the\n\u201cBoy Scout rule.\u201d This rule says to leave code in a better state than\nyou found it (see also Chapter 12 on applying this principle). Thus,\nwhen you are adjusting code, you have the opportunity to (re)write\nunit tests to ensure that in the new state, the code is still working as\nexpected."}
{"143": "10.4 See Also\nStandardization and consistency in applying it are important in achieving a well-\nautomated development environment. For elaboration, see Chapter 11.\nHow SIG Rates Testability\nTestability is one of the five subcharacteristics of maintainability according to ISO\n25010. SIG rates testability by aggregating the ratings of system properties unit com\u2010\nplexity (see Chapter 3), component independence (see Chapter 7), and system volume\n(see Chapter 9), using an aggregation mechanism as explained in Appendix A.\nThe rationale for this is that complex units are especially hard to test, poor compo\u2010\nnent independence increases the need for mocking and stubbing, and higher volumes\nof production code require higher volumes of test code."}
{"144": ""}
{"145": "CHAPTER 11\nWrite Clean Code\nWriting clean code is what you must do in order to call yourself a professional.\n\u2014Robert C. Martin\nGuideline:\n\u2022 Write clean code.\n\u2022 Do this by not leaving code smells behind after development\nwork.\n\u2022 This improves maintainability because clean code is main\u2010\ntainable code.\nCode smells are coding patterns that hint that a problem is present. Introducing or\nnot removing such patterns is bad practice, as they decrease the maintainability of\ncode. In this chapter we discuss guidelines for keeping the codebase clean from code\nsmells to achieve a \u201chygienic environment.\u201d\n11.1 Leave No Trace\nBoy Scouts have a rule that says, \u201cleave the campground cleaner than you found it.\u201d\nApplying the Boy Scout rule to software development means that once you are writ\u2010\ning or modifying a piece of code, you have the opportunity to make small improve\u2010\nments as well. The result is that you leave the code cleaner and more maintainable\nthan you found it. If you are adjusting a piece of code now, apparently there is a need"}
{"146": "11.2 How to Apply the Guideline\nTrying to be a clean coder is an ambitious goal, and there are many best practices that\nyou can follow. From our consultancy experience we have distilled seven developer\n\u201cBoy Scout rules\u201d that will help you to prevent code smells that impact maintainabil\u2010\nity most:\n1. Leave no unit-level code smells behind.\n2. Leave no bad comments behind.\n3. Leave no code in comments behind.\n4. Leave no dead code behind.\n5. Leave no long identifier names behind.\n6. Leave no magic constants behind.\n7. Leave no badly handled exceptions behind.\nThese seven rules are explained in the following sections.\nRule 1: Leave No-Unit Level Code Smells Behind\nAt this point in the book you are familiar with nine guidelines for building maintain\u2010\nable software, discussed in the previous nine chapters. Of those nine guidelines, three\ndeal with smells at the unit level: long units (Chapter 2), complex units (Chapter 3),\nand units with large interfaces (Chapter 5). For modern programming languages,\nthere is really no good reason why any of these guidelines should be violated when\nyou are writing new code.\nTo follow this rule is to refactor \u201csmelly\u201d code in time. By \u201cin time,\u201d we mean as soon\nas possible but certainly before the code is committed to the version control system.\nOf course, it is OK to have small violations when you are working on a development\nticket\u2014for example, a method of 20 lines of code or a method with 5 parameters. But\nthese violations should be refactored out before you commit your changes.\nOf course, the other guidelines, such as avoiding duplicated code and preventing tight\ncoupling, are equally important to building a maintainable system. However, as a\nresponsible software developer, you will find the first three guidelines are easy to inte\u2010\ngrate with your daily way of working. Violations of unit length, complexity, and\nparameters are easy to detect. It is very common to have these checks available in\nmodern integrated development environments. We actually advise you to turn on this\nfeature and make sure your code is free from unit-level code smells before each"}
{"147": "Rule 2: Leave No Bad Comments Behind\nComments are sometimes considered the anti-pattern of good code. From our expe\u2010\nrience we can confirm that inline comments typically indicate a lack of elegant engi\u2010\nneering solutions. Consider the following method taken from the Jenkins codebase:\npublic HttpResponse doUploadPlugin(StaplerRequest req)\nthrows IOException, ServletException {\ntry {\nJenkins.getInstance().checkPermission(UPLOAD_PLUGINS);\nServletFileUpload upload = new ServletFileUpload(\nnew DiskFileItemFactory());\n// Parse the request\nFileItem fileItem = (FileItem)upload.parseRequest(req).get(0);\nString fileName = Util.getFileName(fileItem.getName());\nif (\"\".equals(fileName)) {\nreturn new HttpRedirect(\"advanced\");\n}\n// we allow the upload of the new jpi's and the legacy hpi's\nif (!fileName.endsWith(\".jpi\") && !fileName.endsWith(\".hpi\")) {\nthrow new Failure(\"Not a plugin: \" + fileName);\n}\n// first copy into a temporary file name\nFile t = File.createTempFile(\"uploaded\", \".jpi\");\nt.deleteOnExit();\nfileItem.write(t);\nfileItem.delete();\nfinal String baseName = identifyPluginShortName(t);\npluginUploaded = true;\n// Now create a dummy plugin that we can dynamically load\n// (the InstallationJob will force a restart if one is needed):\nJSONObject cfg = new JSONObject().element(\"name\", baseName)\n.element(\"version\", \"0\"). // unused but mandatory\nelement(\"url\", t.toURI().toString())\n.element(\"dependencies\", new JSONArray());\nnew UpdateSite(UpdateCenter.ID_UPLOAD, null).new Plugin(\nUpdateCenter.ID_UPLOAD, cfg).deploy(true);\nreturn new HttpRedirect(\"../updateCenter\");\n} catch (IOException e) {\nthrow e;\n} catch (Exception e) {// grrr. fileItem.write throws this\nthrow new ServletException(e);"}
{"148": "Although the doUploadPlugin is not very hard to maintain (it has only 1 parameter,\n31 lines of code, and a McCabe index of 6), the inline comments indicate separate\nconcerns that could easily be addressed outside this method. For example, copying\nthe fileItem to a temporary file and creating the plugin configuration are tasks that\ndeserve their own methods (where they can be tested and potentially reused).\nComments in code may reveal many different problems:\n\u2022 Lack of understanding of the code itself\n// I don't know what is happening here, but if I remove this line\n// an infinite loop occurs\n\u2022 Issue tracking systems not properly used\n// JIRA-1234: Fixes a bug when summing negative numbers\n\u2022 Conventions or tooling are being bypassed\n// CHECKSTYLE:OFF\n// NOPMD\n\u2022 Good intentions\n// TODO: Make this method a lot faster some day\nComments are valuable in only a small number of cases. Helpful API documentation\ncan be such a case, but always be cautious to avoid dogmatic boilerplate commentary.\nIn general, the best advice we can give is to keep your code free of comments.\nRule 3: Leave No Code in Comments Behind\nAlthough there might be rare occasions where there is a good reason to use com\u2010\nments in your code, there is never an excuse for checking in code that is commented\nout. The version control system will always keep a record of old code, so it is perfectly\nsafe to delete it. Take a look at the following example, taken from the Apache Tomcat\ncodebase:\nprivate void validateFilterMap(FilterMap filterMap) {\n// Validate the proposed filter mapping\nString filterName = filterMap.getFilterName();\nString[] servletNames = filterMap.getServletNames();\nString[] urlPatterns = filterMap.getURLPatterns();\nif (findFilterDef(filterName) == null)\nthrow new IllegalArgumentException(\nsm.getString(\"standardContext.filterMap.name\", filterName));\nif (!filterMap.getMatchAllServletNames() &&"}
{"149": "// FIXME: Older spec revisions may still check this\n/*\nif ((servletNames.length != 0) && (urlPatterns.length != 0))\nthrow new IllegalArgumentException\n(sm.getString(\"standardContext.filterMap.either\"));\n*/\nfor (int i = 0; i < urlPatterns.length; i++) {\nif (!validateURLPattern(urlPatterns[i])) {\nthrow new IllegalArgumentException(\nsm.getString(\"standardContext.filterMap.pattern\",\nurlPatterns[i]));\n}\n}\n}\nThe FIXME note and accompanying code are understandable from the original develo\u2010\nper\u2019s perspective, but to a new developer they act as a distractor. The original devel\u2010\noper had to make a decision before leaving this commented-out code: either fix\nit at the spot, create a new ticket to fix it at some other time, or reject this corner case\naltogether.\nRule 4: Leave No Dead Code Behind\nDead code comes in different shapes. Dead code is code that is not executed at all or\nits output is \u201cdead\u201d: the code may be executed, but its output is not used elsewhere in\nthe system. Code in comments, as discussed in the previous section, is an example of\ndead code, but there are many other forms of dead code. In this section, we give three\nmore examples of dead code.\nUnreachable code in methods\npublic Transaction getTransaction(long uid) {\nTransaction result = new Transaction(uid);\nif (result != null) {\nreturn result;\n} else {\nreturn lookupTransaction(uid);\n}\n}\nUnreachable code\nUnused private methods\nPrivate methods can be called only from other code in the same class. If they are not,"}
{"150": "Code in comments\nThis is not to be confused with commented-out code. Sometimes it can be useful to\nuse short code snippets in API documentation (such as in Javadoc), but remember\nthat keeping those snippets in sync with the actual code is a task that is quickly over\u2010\nlooked. Avoid code in comments if possible.\nRule 5: Leave No Long Identifiers Behind\nGood identifiers make all the difference between code that is a pleasure to read and\ncode that is hard to wrap your head around. A famous saying by Phil Karlton is\n\u201cThere are only two hard problems in computer science: cache invalidation and nam\u2010\ning things.\u201d In this book we won\u2019t discuss the first, but we do want to say a few things\nabout long identifiers.\nIdentifiers name the items in your codebase, from units to modules to components to\neven the system itself. It is important to choose good names so that developers can\nfind their way through the codebase without great effort. The names of most of the\nidentifiers in a codebase will be dependent on the domain in which the system oper\u2010\nates. It is typical for teams to have a formal naming convention or an informal, but\nconsistent, use of domain-specific terminology.\nIt is not easy to choose the right identifiers in your code, and unfortunately there are\nno guidelines for what is and what isn\u2019t a good identifier. Sometimes it may even take\nyou a couple of iterations to find the right name for a method or class.\nAs a general rule, long identifiers must be avoided. A maximum length for an identi\u2010\nfier is hard to define (some domains have longer terminology than others), but in\nmost cases there is little debate within a development team when an identifier is con\u2010\nsidered too long. Identifiers that express multiple responsibilities (such as generate\nConsoleAnnotationScriptAndStylesheet) or contain too many technical terms\n(e.g., GlobalProjectNamingStrategyConfiguration) are always a violation of this\nrule.\nRule 6: Leave No Magic Constants Behind\nMagic constants are number or literal values that are used in code without a clear def\u2010\ninition of their meaning (hence the name magic constant). Consider the following\ncode example:\nfloat calculateFare(Customer c, long distance) {\nfloat travelledDistanceFare = distance * 0.10f;\nif (c.getAge() < 12) {"}
{"151": "}\nreturn 3.00f + travelledDistanceFare;\n}\nAll the numbers in this code example could be considered magic numbers. For\ninstance, the age thresholds for children and the elderly may seem like familiar num\u2010\nbers, but remember they could be used at many other places in the codebase. The fare\nrates are constants that are likely to change over time by business demands.\nThe next snippet shows what the code looks like if we define all magic constants\nexplicitly. The code volume increased with six extra lines of code, which is a lot com\u2010\npared to the original source, but remember that these constants can be reused in\nmany other places in the code:\nprivate static final float BASE_RATE = 3.00f;\nprivate static final float FARE_PER_KM = 0.10f;\nprivate static final float DISCOUNT_RATE_CHILDREN = 0.25f;\nprivate static final float DISCOUNT_RATE_ELDERLY = 0.5f;\nprivate static final int MAXIMUM_AGE_CHILDREN = 12;\nprivate static final int MINIMUM_AGE_ELDERLY = 65;\nfloat calculateFare(Customer c, long distance) {\nfloat travelledDistanceFare = distance * FARE_PER_KM;\nif (c.getAge() < MAXIMUM_AGE_CHILDREN) {\ntravelledDistanceFare *= DISCOUNT_RATE_CHILDREN;\n} else\nif (c.getAge() >= MINIMUM_AGE_ELDERLY) {\ntravelledDistanceFare *= DISCOUNT_RATE_ELDERLY;\n}\nreturn BASE_RATE + travelledDistanceFare;\n}\nRule 7: Leave No Badly Handled Exception Behind\nThree guidelines for good exception handling are discussed here specifically because\nin our practice we see many flaws in implementing exception handling:\n\u2022 Always catch exceptions. You are logging failures of the system to help you\nunderstand these failures and then improve the system\u2019s reaction to them. That\nmeans that exceptions must always be caught. Also, in some cases an empty\ncatch block compiles, but it is bad practice since it does not provide information\nabout the context of the exception.\n\u2022 Catch specific exceptions. To make exceptions traceable to a specific event,\nyou should catch specific exceptions. General exceptions that do not provide\ninformation specific to the state or event that triggered it fail to provide that"}
{"152": "\u2022 Translate specific exceptions to general messages before showing them to end\nusers. End users should not be \u201cbothered\u201d with detailed exceptions, since they\nare mostly confusing and a security bad practice (i.e., providing more informa\u2010\ntion than necessary about the inner workings of the system).\n11.3 Common Objections to Writing Clean Code\nThis section discusses typical objections regarding clean code. The most common\nobjections are reasons why commenting would be a good way to document code and\nwhether corners can be cut for doing exception handling.\nObjection: Comments Are Our Documentation\n\u201cWe use comments to document the workings of the code.\u201d\nComments that tell the truth can be a valuable aid to less experienced developers who\nwant to understand how a piece of code works. In practice, however, most comments\nin code lie\u2014not on purpose, of course, but comments often tell an outdated version\nof the truth. Outdated comments become more and more common as the system gets\nolder. Keeping comments in sync with code is a task that requires precision and a lot\nof times is overlooked during maintenance.\nCode that \u201ctells the story\u201d itself does not require lengthy comments to document its\nworkings. By keeping units small and simple, and by using descriptive names for\nidentifiers, using comments for documentation is seldom necessary.\nObjection: Exception Handling Causes Code Additions\n\u201cImplementing exception classes forces me to add a lot of extra code without visible\nbenefits.\u201d\nException handling is an important part of defensive programming: coding to prevent\nunstable situations and unpredictable system behavior. Anticipating unstable situa\u2010\ntions means trying to foresee what can go wrong. This does indeed add to the burden\nof analysis and coding. However, this is an investment. The benefits of exception han\u2010\ndling may not be visible now, but they definitely will prove valuable in preventing and\nabsorbing unstable situations in the future.\nBy defining exceptions, you are documenting and safeguarding your assumptions.\nThey can later be adjusted when circumstances change."}
{"153": "Objection: Why Only These Coding Guidelines?\n\u201cWe use a much longer list of coding conventions and quality checks in our team. This\nlist of seven seems like an arbitrary selection with many important omissions.\u201d\nHaving more guidelines and checks than the seven in this chapter is of course not a\nproblem. These seven rules are the ones we consider the most important for writing\nmaintainable code and the ones that should be adhered to by every member on the\ndevelopment team. A risk of having many guidelines and checks is that developers\ncan be overwhelmed by them and focus their efforts on the less critical issues. How\u2010\never, teams are obviously allowed to extend this list with items that they find indis\u2010\npensable for building a maintainable system."}
{"154": ""}
{"155": "CHAPTER 12\nNext Steps\nAt this point, you know a lot more about what maintainable code is, why it is impor\u2010\ntant, and how to apply the 10 guidelines in this book. But writing maintainable code\nis not something you learn from a book. You learn it by doing it! Therefore, here we\nwill discuss simple advice on practicing the 10 guidelines for achieving maintainable\nsoftware.\n12.1 Turning the Guidelines into Practice\nEnsuring that your code is easy to maintain depends on two behaviors in your daily\nroutine: discipline and setting priorities. Discipline helps you to constantly keep\nimproving your coding techniques, up to a point where any new code you write will\nalready be maintainable. As for priorities, some of the presented guidelines can seem\nto contradict each other. It takes consideration on your side about which guideline\nhas the most impact on the actual maintainability of your system. Be sure to take\nsome time to deliberate and ask your team for their opinion.\n12.2 Lower-Level (Unit) Guidelines Take Precedence Over\nHigher-Level (Component) Guidelines\nKeep in mind that the aggregated higher-level guidelines are effects of the application\napplying the lower-level principles. For example, when units of code are long and\nduplicated throughout the system (see Chapters 2 and 4), the codebase will likely be\nlarge as well (see Chapter 9). This is because one of the causes of having a large code\u2010\nbase is that long units are being duplicated."}
{"156": "units into multiple smaller units slightly grows the total codebase. But the advantage\nof small units in terms of reusability will have a huge pay-off when more functionality\nis added to the system.\nThe same applies to the architecture-level guidelines (see Chapters 7 and 8): it makes\nno sense to reorganize your code structure when it makes your components highly\ndependent on each other. To put it succinctly: fix your dependencies before trying to\nbalance your components.\n12.3 Remember That Every Commit Counts\nThe hardest part of applying the guidelines in this book may be keeping the discipline\nto apply them. It is tempting to violate the guidelines when a \u201cquick fix\u201d seems more\nefficient. To keep this discipline, follow the Boy Scout rule presented in Chapter 11.\nThe Boy Scout rule is especially effective on large codebases. Unless you have the time\nto sort out your whole system and improve maintainability, you will have to do it\nstep-by-step while doing your regular work. This gradually improves maintainability\nand hones your refactoring skills. So, in the long run, you also have the skill to write\nhighly maintainable software.\n12.4 Development Process Best Practices Are Discussed in\nthe Follow-Up Book\nAs discussed in the preface, the process part of developing high-quality software is\ndiscussed in detail in the follow-up book in this series: Building Software Teams. It\nprovides 10 guidelines for managing and measuring the software development pro\u2010\ncess. It focuses on how to measure and manage best practices for software develop\u2010\nment (e.g., development tool support, automation, standardization)."}
{"157": "APPENDIX A\nHow SIG Measures Maintainability\nSIG measures system maintainability based on eight metrics. Those eight metrics are\ndiscussed in Chapters 2 through 9. Those chapters include sidebars explaining how\nSIG rates source code properties relevant to those guidelines. These ratings are\nderived from the SIG/T\u00dcViT1 Evaluation Criteria for Trusted Product Maintainabil\u2010\nity. In this appendix, we provide you with additional background.\nTogether with T\u00dcViT, SIG has determined eight properties of source code that can be\nmeasured automatically. See \u201cWhy These Ten Specific Guidelines?\u201d on page xi for\nhow these properties have been chosen.\nTo assess maintainability of a system, we measure these eight source code properties\nand summarize these measurements either in a single number (for instance, the per\u2010\ncentage of code duplication) or a couple of numbers (for instance, the percentage of\ncode in four categories of complexity, which we call a quality profile; see \u201cRating\nMaintainability\u201d).\nWe then compare these numbers against a benchmark containing several hundreds of\nsystems, using Table A-1 to determine the quality level on each property. So, if the\nmeasurement for a system is among the top 5% of all systems in the benchmark, the\nsystem is rated at 5 stars for this property. If it is among the next best 30%, it rates 4\nstars, and so forth. This process of comparing quality profiles for each system prop\u2010\nerty against the benchmark results in eight star ratings, one for each system property."}
{"158": "Table A-1. SIG maintainability ratings\nRating Maintainability\n5 stars Top 5% of the systems in the benchmark\n4 stars Next 30% of the systems in the benchmark (above-average systems)\n3 stars Next 30% of the systems in the benchmark (average systems)\n2 stars Next 30% of the systems in the benchmark (below-average systems)\n1 star Bottom 5% least maintainable systems\nWe then aggregate the ratings to arrive at one overall rating. We do this in two steps.\nFirst, we determine the ratings for the subcharacteristics of maintainability as defined\nby ISO 25010 (i.e., analyzability, modifiability, etc.) by taking the weighted averages\naccording to the rows of Table A-2. Each cross in a given row indicates that the corre\u2010\nsponding system property (column) contributes to this subcharacteristic. Second, we\ntake a weighted average of the five subcharacteristics to determine an overall rating\nfor maintainability.\nTable A-2. Relation of subcharacteristics and system properties\nVolume Duplication Unit Unit Unit Module Component Component\nsize complexity interfacing coupling balance independence\nAnalyzability X X X X\nModifiability X X X\nTestability X X X\nModularity X X X\nReusability X X\nThis describes the SIG maintainability model in a nutshell, since there is more detail\nto it than what we can cover in this appendix. If you would like to learn more about\nthe details of the maintainability model, a good start for elaboration is the following\npublication:"}
{"159": "\u2022 Visser, Joost. SIG/T\u00dcViT Evaluation Criteria Trusted Product Maintainability.\nhttp://bit.ly/eval_criteria\nBackground on the development of the model and its application is provided in the\nfollowing publications:\n\u2022 Heitlager, Ilja, Tobias Kuipers, and Joost Visser. \u201cA Practical Model for Measuring\nMaintainability.\u201d In Proceedings of the 6th International Conference on the Quality\nof Information and Communications Technology (QUATIC 2007), 30\u201339. IEEE\nComputer Society Press, 2007.\n\u2022 Baggen, Robert, Jos\u00e9 Pedro Correia, Katrin Schill, and Joost Visser. \u201cStandardized\ncode quality benchmarking for improving software maintainability.\u201d Software\nQuality Journal 20, no. 2 (2012): 287\u2013307.\n\u2022 Bijlsma, Dennis, Miguel Alexandre Ferreira, Bart Luijten, and Joost Visser.\n\u201cFaster issue resolution with higher technical quality of software.\u201d Software Qual\u2010\nity Journal 20, no. 2 (2012): 265\u2013285.\nDoes Maintainability Improve Over Time?\nA question we often get at SIG is whether maintainability improves over time across\nall systems we see. The answer is yes, but very slowly. The recalibration that we carry\nout every year consistently shows that the thresholds become stricter over time. This\nmeans that for one system to get a high maintainability rating, over time it must have\nfewer units that are overly long or complex, must have less duplication, lower cou\u2010\npling, and so on. Given the structure of our model, the reason for this must be that\nsystems in our benchmark over time have less duplication, less tight coupling, and so\non. One could argue that this means that maintainability across the systems we\nacquire for our benchmark is improving. We are not talking about big changes. In\nbroad terms, we can say this: it is about a tenth of a star per year.\nThe selection of systems within the SIG benchmark is a representative cross-cut of the\nsoftware industry, including both proprietary and open source systems, developed in\na variety of languages, functional domains, platforms, and so on. Therefore, the tenth\nof a star improvement per year means that the industry as a whole is slowly but con\u2010\nstantly improving."}
{"160": ""}
{"161": "Index\nSymbols C\n11-check, 12 calls, 79\nchains, conditional, 34-35\nA change\nand duplication, 51\nAbstract Factory design pattern, 82-84\npossible reasons for, 51\nacceptance tests\nchecksum, 12\nautomation of, 119\nclasses, splitting, 70\ncharacteristics, 110\nclean coding\nadaptive maintenance, 2\nand badly-handled exceptions, 129\nanalysis\nand Boy Scout rule, 120\nand component balance, 91\nand commented-out code, 126\nand duplication, 45\nand comments as documentation, 130\narchitecture components, balance of (see com\u2010\nand dead code, 127\nponent balance)\nand exception class implementation, 130\nautomation of tests (see test automation)\nand number of coding guidelines, 131\nB and unit-level code, 124\napplying guidelines for, 124-130\nbackups, code duplication and, 52\navoiding long identifiers, 128\nbalance of components (see component bal\u2010\navoiding magic constants, 128\nance)\ncommon objections to, 130\nbank account numbers, checksum for, 12\nimportance of, 123-131\nbenchmarking, 7\nminimizing inline comments, 125\nBoy Scout rules\nrefactoring for improved maintainability,\nand improvement of modified code, 123\n123\ndefined, 120\nclone detection tools, 45\nfor preventing code smells, 124-130\nclones (see code clones)\nwith large codebases, 134\ncode\nbranch coverage, 28\ncomplexity as quality characteristic, 27\nbranch points\ndead, 127\ndefined, 27"}
{"162": "reading when spread out over multiple and component independence, 80\nunits, 22 and deceptive acceptability of imbalance, 94\nreplacing custom code with libraries/frame\u2010 and entanglements, 94\nworks, 73 applying guidelines for, 92\ntest automation and predictability of, 109 clarifying domains for, 93\nunreachable, 127 common objections to, 94\nwriting clean (see clean coding) deciding on proper conceptual level for\ncode clones grouping functionality, 93\nand SIG duplication ratings, 53 importance of, 89-95\ndefined, 43 isolation of maintenance effects, 91\ncodebase separation of maintenance responsibilities,\nBoy Scout rule with large, 134 92\ndefined, 97 SIG rating thresholds, 95\nnavigation of, 69 when finding/analyzing code, 91\nvolume (see codebase volume) component coupling, module coupling vs., 78\nworking on isolated parts of, 69 component dependence, 78\ncodebase volume component guidelines, unit guidelines vs., 133\nadvantages of minimizing, 98-100 component imbalance, 89, 94\nand defect density, 100 component independence\napplying guidelines for, 101-103 advantages of, 78-82\ncommon objections to minimizing of, and Abstract Factory design pattern, 82-84\n103-106 and entangled components, 84\nduplication as impediment to reducing, 105 and isolated maintenance, 81\nfunctionality-related measures to minimize, and separation of maintenance responsibili\u2010\n101 ties, 81\nimportance of minimizing, 97-106 and testing, 82\nmaintenance and, 100 applying guidelines for, 82-84\nplatform architecture and, 105 common objections to, 84\nproductivity measures and, 104 defined, 78\nprogramming languages and, 104 importance of, 77-86\nproject failures and, 98 SIG rating thresholds, 86\nSIG rating thresholds, 106 time as factor in achieving, 85\nsystem complexity as impediment to reduc\u2010 when throughput is a requirement, 85\ning, 104 components, packages vs., xvii\ntechnical measures to minimize, 102 concepts, generic names for, xvi\ntight coupling as impediment to reducing, concerns, separation of, 68-71, 81, 91, 94\n105 common objections to, 73\ncomments splitting classes for, 70\nand commented-out code, 126 conditional chains, 34-35\nas documentation, 130 conditionals, nested, 36-37\ncode in, 128 constants, magic, 128\nminimizing inline comments, 125 conventions, 23, 91, 126, 128, 131\ncomplexity, 27 copying and pasting\n(see also unit complexity) avoiding, 102\nas quality characteristic, 27 from another codebase, 50"}
{"163": "corrective maintenance, 2 common objections to avoiding, 50-52\ncoupling, 68 guidelines for avoiding, 45-50\n(see also module coupling) SIG ratings, 53\nbetween classes, 68 types of, 43\ndefined, 68\nloose (see component independence) (see E\nloose coupling)\nEasymock, 117\ntight (see tight coupling)\nembedded software, 6\ncoverage\nencapsulation, 69, 72, 74, 79, 83, 85\nmeasuring for test adequacy, 118\nend-to-end tests, 110\ntest automation in low-coverage situation,\nentangled components\n120\nand component balance, 94\nCPD (clone detection tool), 45\nand component independence, 84\ncyclomatic complexity, 29\nevolution over time, 80\n(see also McCabe complexity)\nerrors, test, 114\nexception handling\nD\nand clean coding, 129\ndata transfer objects, 60 and exception class implementation, 130\ndead code, 127 execution paths, 28\ndeadlines, component independence and, 85 Extract Method refactoring technique, 17, 45\ndefect density, codebase volume and, 100 Extract Superclass refactoring technique, 47-50,\ndefensive programming, exception handling 48\nand, 130\ndependency injection, 82 F\ndiscipline, coding and, 133\nfailures, 114\ndocumentation\n(see also exception handling)\nand test automation, 110\ncodebase volume and, 98\ncomments as, 130\ntest errors vs. test failures, 114\ndomains\nfluent interface, 62\nclarifying, 93\nfor loops, 13\ncomplex, 37\nformatting, unit size guidelines and, 22\nduplicates (code clones), 43\nframeworks\nduplication (code)\nfor reducing codebase, 102\nadvantages of avoiding, 45\nreplacing custom code with, 73\nand analysis, 45\nwith long parameter lists, 63\nand assumption that code will never change,\nfunctionality\n51\nand codebase volume, 101\nand backups, 52\nextending unit with new, 16\nand copying from another codebase, 50\ngrouping into components, 93\nand Extract Superclass refactoring techni\u2010\nstandardization of, 102\nque, 47-50\nand modification, 45 G\nand slight variations of common functional\u2010\nguidelines, maintainability\nity, 51\nand quality profiles, 5\nand string literals, 52"}
{"164": "overview, 9 modules, 65, 69-71, 73, 74, 110\npracticing, 133 systems, 103\nprinciples of, 4 lower-level guidelines, 133\nH M\nhappy flow testing, 114 magic constants, 128\nhidden code, 86 maintainability\nhigher-level guidelines, 133 and discipline during development, 5\nas enabler for other quality characteristics, 4\nI as industry-independent, 6\nas language-independent, 6\nidentifiers, long, 128\nas nonbinary quantity, 7\nif-then-else statements, 34-35\nbusiness impact of, 3\nimplementations, specialized, 71-73\ndefined, 1\nincoming calls, 79\nguidelines (see guidelines, maintainability)\nindustry-dependent software development, 6\nimportance of, 3-4\nintegration tests, 110\nimportance of simple guidelines, 4\ninterface code, 86\nimprovement over time, 137\ninterfaces, unit (see unit interfaces)\nmetrics for, xi\ninternal calls, 79\nmisunderstandings about, 6\nIntroduce Parameter Object refactoring pat\u2010\nperformance vs., 21\ntern, 57\nrating, 7-9\ninversion of control (IoC), 74\nmaintenance, four types of, 2\nisolated maintenance, 81\nman-months/years, 106\nJ manual testing\nand test automation, 119\nJava\nlimitations of, 108\nconcept names in, xvi\nunit test, 108\npackages vs. components, xvii\nMcCabe complexity\nJava interfaces, 74\nas SIG/T\u00dcViT rating criteria, 39\nJMeter, 111\ndefined, 29\nJPacman, 15\nmethod invocations, 21\njUnit tests, 108, 112-114\nmethod modification, 58\nmethod splitting, 38\nL\nmethods\nlarge class smell, 47, 68, 70 unreachable code in, 127\nlarge systems, splitting up of, 103 unused private, 127\nlibraries metrics, maintainability, xi, 135-136\ndangers of modifying source code in, 103 mock object, 117\nfor reducing codebase, 102 mocking framework, 117\nreplacing custom code with, 73 Mockito, 117\nwith long parameter lists, 63 modifiability, 73\nlocal variables, 18 modification\nloose coupling, 69 and duplication, 45\n(see also component independence) and unit complexity, 32"}
{"165": "and Java interfaces, 74 programming languages\nand navigation of codebase, 69 as impediment to reducing codebase vol\u2010\nand reuse, 73 ume, 104\nand utility code, 74 as maintainability factor, 6\napplying guidelines for, 70-73 project failures, codebase volume and, 98\ncommon objections to separating concerns,\n73 Q\ncomponent coupling vs., 78\nquality profiles, 5, 8\nhiding specialized implementations behind\nquality, maintainability as enabler for, 4\ninterfaces, 71-73\nquick fixes, 134\nloose (see loose coupling)\nreplacing custom code with libraries/frame\u2010 R\nworks, 73\nrainy-side testing, 114\nSIG rating thresholds, 75\nrefactoring\nsplitting classes to separate concerns, 70\nand unit complexity, 38\ntight (see tight coupling)\nand unit interfaces, 63\nto prevent no-go areas for new developers,\nand unit size, 17-21\n70\ndifficulties as maintainability issue, 24\nwhen working on isolated parts of codebase,\nExtract Method technique, 17, 45\n69\nExtract Superclass technique, 47-50, 48\nmutual dependencies, 84\nfor improved maintainability, 123\nN Introduce Parameter Object pattern, 57\nReplace Method with Method Object tech\u2010\nnested conditionals, 36-37\nnique, 18-21, 61\nto reduce codebase, 102\nO\nreferral, reuse by, 102\nobservers, 15 regression, 91, 105, 108-111, 119\noutgoing calls, 79 automated testing to identify, 109\nbugs, 45\nP regression tests\ncharacteristics, 110\npackages, components vs., xvii\nfor automation of manual acceptance test\u2010\nparameter lists, 63\ning, 119\n(see also unit interfaces)\nrepeatability, test automation and, 109\nparameter objects, 60, 62\nReplace Conditional with Polymorphism pat\u2010\nperfective maintenance, 2\ntern, 34\nperformance\nReplace Method with Method Object refactor\u2010\nand quantity of units, 21\ning technique, 18-21, 61\nmaintainability vs., 21\nReplace Nested Conditional with Guard Clau\u2010\nplatform architecture, codebase volume and,\nses pattern, 36\n105\nreuse\npreventive maintenance, 2\nby referral, 102\npriorities, setting, 133\ncopying and pasting vs., 14\nprivate methods, unused, 127\nloose coupling and, 73\nproductivity measures\nof unit interfaces, 58"}
{"166": "S and productivity measures, 120\nscientific software, 6 and repeatability, 109\nscope creep, 101 and SIG testability rating thresholds, 121\nSelenium, 111 applying guidelines for, 110-119\nself-taught developers, xi common objections to, 119\nseparation of concerns (see concerns, separa\u2010 general principles for writing good unit\ntion of) tests, 114-118\nSIG (Software Improvement Group), xiii in low-coverage situation, 120\nSIG/T\u00dcViT Evaluation Criteria Trusted Prod\u2010 measuring coverage of, 118\nuct Maintainability test stub, 116\ncodebase volume rating thresholds, 106 test-driven development (TDD), 110\ncomponent balance rating thresholds, 95 testability\ncomponent independence rating thresholds, and unit size, 13\n86 SIG rating thresholds, 121\nduplication rating thresholds, 53 tests/testing\nmetrics, xi, 135-136 and unit complexity, 33\nmodule coupling rating thresholds, 75 automation of (see test automation)\nstar ratings, 7-9 component independence and ease of, 82\ntestability rating thresholds, 121 dangers of postponing, 109\nunit complexity rating thresholds, 39 errors vs. failures, 114\nunit interface rating thresholds, 64 types of, 110\nunit size rating thresholds, 25 third-party libraries/frameworks (see frame\u2010\nsimple units (see unit complexity) works) (see libraries)\nsingle responsibility principle throughput code, 80\nclasses that violate, 70 throughput, component independence and, 85\ndefined, 69 tight coupling\nsmells, Boy Scout rules for preventing, 124-130 as impediment to reducing codebase vol\u2010\n(see also large class smell) ume, 105\nSoapUI, 111 as risk when removing clones, 47\nSoftware Improvement Group (see SIG) maintenance consequences of, 69\nSoftware Risk Monitoring service, xiii maintenance problems with, 66-69\nSQL queries, 23 of classes, 68\nstandardization of functionality, 102 Type 1 clones, 43\nstar rating system, 8 Type 2 clones, 44\nstring literals, 52\nU\nstub (test stub), 116\nsunny-side testing, 114 unhappy flow testing, 114\nsystem complexity, codebase volume and, 104 unit\nsystem properties, xi complexity of (see unit complexity)\ndefined, 11\nT extending with new functionality, 16\ntest automation, 107-121 maximum recommended length, 14\nadvantages of, 109-110 unit complexity\nand code improvement, 110 advantages of minimizing, 32\nand code predictability, 109 and conditional chains, 34-35"}
{"167": "applying guidelines for, 33-37 difficulty of optimizing by splitting units, 23\ncommon objections to minimizing, 37 in real-world systems, 22\nin complex domains, 37 minimizing, 11-25\nminimizing, 27, 39 perceived lack of advantage in splitting\nSIG rating of, 39 units, 24\nunit guidelines quantity of units and performance, 21\ncomponent guidelines vs., 133 reading code when spread out over multiple\ndefined, 65 units, 22\nunit interfaces refactoring techniques for guideline applica\u2010\nadvantages of minimizing size of, 57 tions, 17-21\nand libraries/frameworks with long parame\u2010 SIG thresholds for, 25\nter lists, 63 when extending unit with new functionality,\nand method modification, 58 16\nand parameter objects, 62 when writing new unit, 15\napplying guidelines for, 58-62 unit tests\ncommon objections to minimizing size of, and Boy Scout rule, 120\n62 and duplication, 52\nease of understanding, 58 characteristics, 110\nhiding specialized implementations behind, common objections to automating tests, 119\n71-73 failures vs. errors, 114\nJava interfaces and loose coupling, 74 general principles for writing, 114-118\nminimizing size of, 55-64 jUnit tests, 112-114\nrefactoring, 63 measuring coverage to determine proper\nreuse, 58 number of, 120\nSIG rating thresholds, 64 unit-level code, clean coding and, 124\nunit size unreachable code, 127\nadvantages of minimizing, 13 unused private methods, 127\nand ease of analysis, 14 utility code, module coupling and, 74\nand improper formatting, 22\nand reuse, 14 V\nand testability, 13\nviolations\napplying guidelines to, 14-21\nprioritizing, 5\ncommon objections to writing short units,\nwith frameworks/libraries, 63\n21-24"}
{"168": "Colophon\nThe animal on the cover of Building Maintainable Software is a grey-headed wood\u2010\npecker (Picus canus). Like all woodpeckers, which consitute about half of the Pici\u2010\nformes order, grey-headed woodpeckers use strong bills to puncture the surface of\ntrees and seek small insects that inhabit the wood. Very long, bristly tongues coated\nwith an adhesive extend into deep cracks, holes, and crevices to gather food in the\nbird\u2019s bill. A membrane that closes over the woodpecker\u2019s eye protects it from the\ndebris that may result from each blow at the tree. Slit-like nostrils provide a similar\nprotection, as do feathers that cover them. Adaptations to the brain like small size\nand a position that maximizes its contact with the skull\u2014permitting optimal shock\nabsorption\u2014represent further guards against the violence of the woodpecker\u2019s drill\u2010\ning. The zygodactyl arrangement of the feet, putting two toes forward and two back,\nallow the woodpecker to maintain its position on the tree\u2019s trunk during this activity,\nas well as to traverse vertically up and down it.\nGrey-headed woodpeckers maintain a vast range across Eurasia, though individual\nmembers of the species tend to be homebodies to particular forest and woodland\nhabitats. As such, they rarely travel overseas and switch to a seed-based diet in winter.\nMating calls that begin with high-pitched whistles lead to monogamous pairs roost\u2010\ning with clutches of 5 to 10 eggs in the holes that males bore into the trunks of trees,\nwhere both parents remain to incubate eggs and nurse the hatchlings for the three to\nfour weeks in which the hatchlings progress to juveniles. At this point, the young can\nfly from the nest and gather their own food.\nIn their greenish back and tail plumage, grey-headed woodpeckers very much resem\u2010\nble the closely related green woodpecker, and males of the species will develop on\ntheir foreheads the red patch that appears on many other species of woodpecker.\nMany of the animals on O\u2019Reilly covers are endangered; all of them are important to\nthe world. To learn more about how you can help, go to animals.oreilly.com.\nThe cover image is from Lydekker\u2019s Royal Natural History. The cover fonts are URW\nTypewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\nis Adobe Myriad Condensed; and the code font is Dalton Maag\u2019s Ubuntu Mono."}
{"45": "gates and seeing that it's past 1 P.M. I'm slowing down to make the\nturn through the gate, when\u2014I don't know how else to say it\u2014it\njust doesn't feel right. I look at the plant. And I put my foot down\non the gas and keep going. I'm hungry; I'm thinking maybe I\nshould get some lunch.\nBut I guess the real reason is I just don't want to be found\nyet. I need to think and I'll never be able to do it if I go back to\nthe office now.\nUp the road about a mile is a little pizza place. I see they're\nopen, so I stop and go in. I'm conservative; I get a medium pizza\nwith double cheese, pepperoni, sausage, mushrooms, green pep-\npers, hot peppers, black olives and onion, and\u2014mmmmmmmm\n\u2014a sprinkling of anchovies. While I'm waiting, I can't resist the\nMunchos on the stand by the cash register, and I tell the Sicilian\nwho runs the place to put me down for a couple of bags of beer\nnuts, some taco chips, and\u2014for later\u2014some pretzels. Trauma\nwhets my appetite.\nBut there's one problem. You just can't wash down beer nuts\nwith soda. You need beer. And guess what I see in the cooler. Of\ncourse, I don't usually drink during the day . . . but I look at\nthe way the light is hitting those frosty cold cans. . . .\n\"Screw it.\"\nI pull out a six of Bud.\nTwenty-three dollars and sixty-two cents and I'm out of there.\nJust before the plant, on the opposite side of the highway,\nthere is a gravel road leading up a low hillside. It's an access road\nto a substation about half a mile away. So on impulse, I turn the\nwheel sharply. The Mazda goes bouncing off the highway onto the"}
{"46": "I pop the top on one of the beers and go to work on the\npizza.\nThe plant has the look of a landmark. It's as if it has always\nbeen there, as if it will always be there. I happen to know the\nplant is only about fifteen years old. And it may not be here as\nmany years from now.\nSo what is the goal?\nWhat are we supposed to be doing here?\nWhat keeps this place working?\nJonah said there was only one goal. Well, I don't see how that\ncan be. We do a lot of things in the course of daily operations, and\nthey're all important. Most of them anyway ... or we wouldn't\ndo them. What the hell, they all could be goals.\nI mean, for instance, one of the things a manufacturing orga-\nnization must do is buy raw materials. We need these materials in\norder to manufacture, and we have to obtain them at the best\ncost, and so purchasing in a cost-effective manner is very impor-\ntant to us.\nThe pizza, by the way, is primo. I'm chowing down on my\nsecond piece when some tiny voice inside my head asks me, But is\nthis the goal? Is cost-effective purchasing the reason for the\nplant's existence?\nI have to laugh. I almost choke.\nYeah, right. Some of the brilliant idiots in Purchasing sure do\nact as if that's the goal. They're out there renting warehouses to\nstore all the crap they're buying so cost-effectively. What is it we\nhave now? A thirty-two-month supply of copper wire? A seven-\nmonth inventory of stainless steel sheet? All kinds of stuff."}
{"47": "why the plant exists. After all, how many people have we laid off\nso far?\nAnd anyway, even if UniCo offered lifetime employment like\nsome of the Japanese companies, I still couldn't say the goal is\njobs. A lot of people seem to think and act as if that were the goal\n(empire-building department managers and politicians just to\nname two), but the plant wasn't built for the purpose of paying\nwages and giving people something to do.\nOkay, so why was the plant built in the first place?\nIt was built to produce products. Why can't that be the goal?\nJonah said it wasn't. But I don't see why it isn't the goal. We're a\nmanufacturing company. That means we have to manufacture\nsomething, doesn't it? Isn't that the whole point, to produce\nproducts? Why else are we here?\nI think about some of the buzzwords I've been hearing lately.\nWhat about quality?\nMaybe that's it. If you don't manufacture a quality product\nall you've got at the end is a bunch of expensive mistakes. You\nhave to meet the customer's requirements with a quality product,\nor before long you won't have a business. UniCo learned its les-\nson on that point.\nBut we've already learned that lesson. We've implemented a\nmajor effort to improve quality. Why isn't the plant's future se-\ncure? And if quality were truly the goal, then how come a com-\npany like Rolls Royce very nearly went bankrupt?\nQuality alone cannot be the goal. It's important. But it's not\nthe goal. Why? Because of costs?\nIf low-cost production is essential, then efficiency would"}
{"48": "sounds like a good goal. But can that goal keep the plant work-\ning?\nI'm bothered by some of the examples that come to mind. If\nthe goal is to produce a quality product efficiently, then how\ncome Volkswagen isn't still making Bugs? That was a quality\nproduct that could be produced at low cost. Or, going back a\nways, how come Douglas didn't keep making DC-3's? From ev-\nerything I've heard, the DC-3 was a fine aircraft. I'll bet if they\nhad kept making them, they could turn them out today a lot\nmore efficiently than DC-10's.\nIt's not enough to turn out a quality product on an efficient\nbasis. The goal has to be something else.\nBut what?\nAs I drink my beer, I find myself contemplating the smooth\nfinish of the aluminum beer can I hold in my hand. Mass produc-\ntion technology really is something. To think that this can until\nrecently was a rock in the ground. Then we come along with\nsome know-how and some tools and turn the rock into a light-\nweight, workable metal that you can use over and over again. It's\npretty amazing\u2014\nWait a minute, I'm thinking. That's it!\nTechnology: that's really what it's all about. We have to stay\non the leading edge of technology. It's essential to the company.\nIf we don't keep pace with technology, we're finished. So that's\nthe goal.\nWell, on second thought . . . that isn't right. If technology\nis the real goal of a manufacturing organization, then how come\nthe most responsible positions aren't in research and develop-"}
{"49": "it, I can almost see the stack of phone messages my secretary is\nbringing in my wheelbarrow.\nOh well. I lift my beer for a good long slug. And as I tilt my\nhead back, I see them.\nOut beyond the plant are two other long, narrow buildings.\nThey're our warehouses. They're filled to the roof with spare\nparts and unsold merchandise we haven't been able to unload\nyet. Twenty million dollars in finished-goods inventory: quality\nproducts of the most current technology, all produced efficiently,\nall sitting in their boxes, all sealed in plastic with the warranty\ncards and a whiff of the original factory air\u2014and all waiting for\nsomeone to buy them.\nSo that's it. UniCo obviously doesn't run this plant just to fill\na warehouse. The goal is sales.\nBut if the goal is sales, why didn't Jonah accept market share\nas the goal? Market share is even more important as a goal than\nsales. If you have the highest market share, you've got the best\nsales in your industry. Capture the market and you've got it\nmade. Don't you?\nMaybe not. I remember the old line, \"We're losing money,\nbut we're going to make it up with volume.\" A company will\nsometimes sell at a loss or at a small amount over cost\u2014as UniCo\nhas been known to do\u2014just to unload inventories. You can have a\nbig share of the market, but if you're not making money, who\ncares?\nMoney. Well, of course . . . money is the big thing. Peach is\ngoing to shut us down because the plant is costing the company\ntoo much money. So I have to find ways to reduce the money that"}
{"50": "gem of a product in its day. And then investors gave him more\nmoney so they could make a bundle and J. Bart could make an\neven bigger one.\nBut is making money the only goal? What are all these other\nthings I've been worrying about?\nI reach for my briefcase, take out a yellow legal pad and take\na pen from my coat pocket. Then I make a list of all the items\npeople think of as being goals: cost-effective purchasing, employ-\ning good people, high technology, producing products, produc-\ning quality products, selling quality products, capturing market\nshare. I even add some others like communications and customer\nsatisfaction.\nAll of those are essential to running the business successfully.\nWhat do they all do? They enable the company to make money.\nBut they are not the goals themselves; they're just the means of\nachieving the goal.\nHow do I know for sure?\nWell, I don't. Not absolutely. But adopting \"making money\"\nas the goal of a manufacturing organization looks like a pretty\ngood assumption. Because, for one thing, there isn't one item on\nthat list that's worth a damn if the company isn't making money.\nBecause what happens if a company doesn't make money? If\nthe company doesn't make money by producing and selling\nproducts, or by maintenance contracts, or by selling some of its\nassets, or by some other means . . . the company is finished. It\nwill cease to function. Money must be the goal. Nothing else\nworks in its place. Anyway, it's the one assumption I have to\nmake."}
{"51": "6\nBy my watch, it's about 4:30 when I park the Mazda in the\nplant lot. One thing I've effectively managed today is to evade the\noffice. I reach for my briefcase and get out of the car. The glass\nbox of the office in front of me is silent as death. Like an ambush.\nI know they're all inside waiting for me, waiting to pounce. I\ndecide to disappoint everyone. I decide to take a detour through\nthe plant. I just want to take a fresh look at things.\nI walk down to a door into the plant and go inside. From my\nbriefcase, I get the safety glasses I always carry. There is a rack of\nhard hats by one of the desks over by the wall. I steal one from\nthere, put it on, and walk inside.\nAs I round a corner and enter one of the work areas, I hap-\npen to surprise three guys sitting on a bench in one of the open\nbays. They're sharing a newspaper, reading and talking with each\nother. One of them sees me. He nudges the others. The newspa-\nper is folded away with the grace of a snake disappearing in the\ngrass. All three of them nonchalantly become purposeful and go\noff in three separate directions.\nI might have walked on by another time. But today it makes\nme mad. Dammit, the hourly people know this plant is in trouble.\nWith the layoffs we've had, they have to know. You'd think they'd\nall try to work harder to save this place. But here we've got three\nguys, all of them making probably ten or twelve bucks an hour,\nsitting on their asses. I go and find their supervisor.\nAfter I tell him that three of his people are sitting around\nwith nothing to do, he gives me some excuse about how they're"}
{"52": "Then it occurs to me: those three guys are doing something\nnow, but is that going to help us make money? They might be\nworking, but are they productive?\nFor a moment, I consider going back and telling the supervi-\nsor to make those guys actually produce. But, well . . . maybe\nthere really isn't anything for them to work on right now. And\neven though I could perhaps have those guys shifted to some-\nplace where they could produce, how would I know if that work\nis helping us make money?\nThat's a weird thought.\nCan I assume that making people work and making money\nare the same thing? We've tended to do that in the past. The basic\nrule has been just keep everybody and everything out here work-\ning all the time; keep pushing that product out the door. And\nwhen there isn't any work to do, make some. And when we can't\nmake work, shift people around. And when you still can't make\nthem work, lay them off.\nI look around and most people are working. Idle people in\nhere are the exception. Just about everybody is working nearly all\nthe time. And we're not making money.\nSome stairs zig-zag up one of the walls, access to one of the\noverhead cranes. I climb them until I am halfway to the roof and\ncan look out over the plant from one of the landings.\nEvery moment, lots and lots of things are happening down\nthere. Practically everything I'm seeing is a variable. The com-\nplexity in this plant\u2014in any manufacturing plant\u2014is mind-bog-\ngling if you contemplate it. Situations on the floor are always\nchanging. How can I possibly control what goes on? How the hell"}
{"53": "whether we're just playing accounting games? There must be a\nconnection, but how do I define it?\nI shuffle back down the stairs.\nMaybe I should just dash off a blistering memo on the evil of\nreading newspapers on the job. Think that'll put us back in the\nblack?\nBy the time I finally set foot inside my office, it is past five\no'clock and most of the people who might have been waiting for\nme are gone. Fran was probably one of the first ones out the\ndoor. But she has left me all their messages. I can barely see the\nphone under them. Half of the messages seem to be from Bill\nPeach. I guess he caught my disappearing act.\nWith reluctance, I pick up the phone and dial his number.\nBut God is merciful. It rings for a straight two minutes; no an-\nswer. I breathe quietly and hang up.\nSitting back in my chair, looking out at the reddish-gold of\nlate afternoon, I keep thinking about measurements, about all the\nways we use to evaluate performance: meeting schedules and due\ndates, inventory turns, total sales, total expenses. Is there a sim-\nplified way to know if we're making money?\nThere is a soft knock at the door.\nI turn. It's Lou.\nAs I mentioned earlier, Lou is the plant controller. He's a\npaunchy, older man who is about two years away from retire-\nment. In the best accountants' tradition, he wears horn-rimmed\nbifocal glasses. Even though he dresses in expensive suits, some-\nhow he always seems to look a little frumpled. He came here from\ncorporate about twenty years ago. His hair is snow white. I think"}
{"54": "\"Yeah, most of it,\" Lou says. \"I sent it to him; he should get it\nin the morning. Most of it was like the stuff I gave you.\"\n\"What about the rest?\"\n\"Just a few things I have to pull together,\" he says. \"I should\nhave it sometime tomorrow.\"\n\"Let me see it before it goes, okay?\" I say. \"Just so I know.\"\n\"Oh, sure,\" says Lou.\n\"Hey, you got a minute?\"\n\"Yeah, what's up?\" he asks, probably expecting me to give\nhim the rundown on what's going on between me and Peach.\n\"Sit down,\" I tell him.\nLou pulls up a chair.\nI think for a second, trying to phrase this correctly. Lou waits\nrxpectantly.\n\"This is just a simple, fundamental question,\" I say.\nLou smiles. \"Those are the kind I like.\"\n\"Would you say the goal of this company is to make money?\"\nHe bursts out laughing.\n\"Are you kidding?\" he asks. \"Is this a trick question?\"\n\"No, just tell me.\"\n\"Of course it's to make money!\" he says.\nI repeat it to him: \"So the goal of the company is to make\nmoney, right?\"\n\"Yeah,\" he says. \"We have to produce products, too.\"\n\"Okay, now wait a minute,\" I tell him. \"Producing products\na just a means to achieve the goal.\"\nI run through the basic line of reasoning with him. He lis-\ntens. He's a fairly bright guy, Lou. You don't have to explain"}
{"55": "Lou puts a finger alongside his face and squints through his\nbifocals at his shoe.\n\"Well, you'd have to have some kind of absolute measure-\nment,\" he says. \"Something to tell you in dollars or yen or what-\never just how much money you've made.\"\n\"Something like net profit, right?\" I ask.\n\"Yeah, net profit,\" he says. \"But you'd need more than just\nthat. Because an absolute measurement isn't going to tell you\nmuch.\"\n\"Oh yeah?\" I say. \"If I know how much money I've made,\nwhy do I need to know anything else? You follow me? If I add up\nwhat I've made, and I subtract my expenses, and I get my net\nprofit\u2014what else do I need to know? I've made, say, $10 million,\nor $20 million, or whatever.\"\nFor a fraction of a second, Lou gets a glint in his eye like I'm\nreal dumb.\n\"All right,\" he says. \"Let's say you figure it out and you come\nup with $10 million net profit ... an absolute measurement.\nOffhand, that sounds like a lot of money, like you really raked it\nin. But how much did you start with?\"\nHe pauses for effect.\n\"You see? How much did it take to make that $10 million?\nWas it just a million dollars? Then you made ten times more\nmoney than you invested. Ten to one. That's pretty goddamned\ngood. But let's say you invested a billion dollars. And you only\nmade a lousy ten million bucks? That's pretty bad.\"\n\"Okay, okay,\" I say. \"I was just asking to be sure.\"\n\"So you need a relative measurement, too,\" Lou continues."}
{"56": "He nods.\n\"Yeah, but suppose you've got enough cash coming in every\nmonth to meet expenses for a year,\" I tell him. \"If you've got\nenough of it, then cash flow doesn't matter.\"\n\"But if you don't, nothing else matters,\" says Lou. \"It's a\nmeasure of survival: stay above the line and you're okay; go below\nand you're dead.\"\nWe look each other in the eye.\n\"It's happening to us, isn't it?\" Lou asks.\nI nod.\nLou looks away. He's quiet.\nThen he says, \"I knew it was coming. Just a matter of time.\"\nHe pauses. He looks back to me.\n\"What about us?\" he asks. \"Did Peach say anything?\"\n\"They're thinking about closing us down.\"\n\"Will there be a consolidation?\" he asks.\nWhat he's really asking is whether he'll have a job.\n\"I honestly don't know, Lou,\" I tell him. \"I imagine some\npeople might be transferred to other plants or other divisions,\nbut we didn't get into those kinds of specifics.\"\nLou takes a cigarette out of the pack in his shirt pocket. I\nwatch him stamp the end of it repeatedly on the arm of his chair.\n\"Two lousy years to go before retirement,\" he mutters.\n\"Hey, Lou,\" I say, trying to lift him out of despair, \"the worst\nit would probably mean for you would be an early retirement.\"\n\"Dammit!\" he says. \"I don't want an early retirement!\"\nWe're both quiet for some time. Lou lights his cigarette. We\nsit there."}
{"57": "With resignation in his voice, he says, \"Okay, Al. I'll give you\nall the help I can. But. ...\"\nHe leaves the sentence unfinished, waves his hand in the air.\n\"I'm going to need that help, Lou,\" I tell him. \"And the first\nthing I need from you is to keep all this to yourself for the time\nbeing. If the word gets out, we won't be able to get anyone to lift a\nfinger around here.\"\n\"Okay, but you know this won't stay a secret for long,\" he\nsays.\nI know he's right.\n\"So how do you plan on saving this place?\" Lou asks.\n\"The first thing I'm trying to do is get a clear picture of what\nwe have to do to stay in business,\" I say.\n\"Oh, so that's what all this stuff with the measurements is\nabout,\" he says. \"Listen, Al, don't waste your time with all that.\nThe system is the system. You want to know what's wrong? I'll tell\nyou what the problem is.\"\nAnd he does. For about an hour. Most of it I've heard before,\nit's the kind of thing everybody's heard: It's all the union's fault;\nif everybody would just work harder; nobody gives a damn about\nquality; look at foreign labor\u2014we can't compete on costs alone;\nand so on, and so on. He even tells me what sorts of self-\nflagellation we should administer in order to chasten our-\nselves. Mostly Lou is blowing off steam. That's why I let him\ntalk.\nBut I sit there wondering. Lou actually is a bright guy. We're\nall fairly bright; UniCo has lots of bright, well-educated people on\nthe payroll. And I sit here listening to Lou pronounce his opin-"}
{"58": "the organization deliver a bigger net profit this year at the ex-\npense of net profit in years to come (don't fund any R&D, for\ninstance; that kind of thing). They can make a bunch of no-risk\ndecisions and have any one of those measurements look great\nwhile the others stink. Aside from that, the ratios between the\nthree might have to vary according to the needs of the business.\nBut then I sit back.\nIf I were J. Bart Granby III sitting high atop my company's\ncorporate tower, and if my control over the company were se-\ncure, I wouldn't want to play any of those games. I wouldn't want\nto see one measurement increase while the other two were ig-\nnored. I would want to see increases in net profit and return on\ninvestment and cash flow\u2014all three of them. And I would want to\nsee all three of them increase all the time.\nMan, think of it. We'd really be making money if we could\nhave all of the measurements go up simultaneously and forever.\nSo this is the goal:\nTo make money by increasing net profit, while simultane-\nously increasing return on investment, and simultaneously in-\ncreasing cash flow.\nI write that down in front of me.\nI feel like I'm on a roll now. The pieces seem to be fitting\ntogether. I have found one clear-cut goal. I've worked out three\nrelated measurements to evaluate progress toward the goal. And\nI have come to the conclusion that simultaneous increases in all\nthree measurements are what we ought to be trying to achieve.\nNot bad for a day's work. I think Jonah would be proud of me.\nNow then, I ask myself, how do I build a direct connection"}
{"59": "same conclusions as everyone else and that means I'll have no\ntruer understanding of what's going on than I do now.\nI'm stuck.\nI turn away from the window. Behind my desk is a bookcase;\nI pull out a textbook, flip through it, put it back, pull out an-\nother, flip through it, put it back.\nFinally, I've had it. It's late.\nI check my watch\u2014and I'm shocked. It's past ten o'clock. All\nof a sudden, I realize I never called Julie to let her know I wasn't\ngoing to be home for dinner. She's really going to be pissed off at\nme; she always is when I don't call.\nI pick up the phone and dial. Julie answers.\n\"Hi,\" I say. \"Guess who had a rotten day.\"\n\"Oh? So what else is new?\" she says. \"It so happens my day\nwasn't too hot either.\"\n\"Okay, then we both had rotten days,\" I tell her. \"Sorry I\ndidn't call before. I got wrapped up in something.\"\nLong pause.\n\"Well, I couldn't get a babysitter anyway,\" she says.\nThen it dawns on me; our postponed night out was sup-\nposed to be tonight.\n\"I'm sorry, Julie. I really am. It just completely slipped my\nmind,\" I tell her.\n\"I made dinner,\" she says. \"When you hadn't shown up after\ntwo hours, we ate without you. Yours is in the microwave if you\nwant it.\"\n\"Thanks.\"\n\"Remember your daughter? The little girl who's in love with"}
{"60": "plant to pay a visit to Eddie, my second shift supervisor, and see\nhow everything is going.\nWhen I get there, Eddie is not in his office; he's out dealing\nwith something on the floor. I have him paged. Finally, I see him\ncoming from way down at the other end of the plant. I watch him\nas he walks down. It's a five-minute wait.\nSomething about Eddie has always irritated me. He's a com-\npetent supervisor. Not outstanding, but he's okay. His work is not\nwhat bothers me. It's something else.\nI watch Eddie's steady gait. Each step is very regular.\nThen it hits me. That's what irritates me about Eddie: it's the\nway he walks. Well, it's more than that; Eddie's walk is symbolic of\nthe kind of person he is. He walks a little bit pigeon-toed. It's as if\nhe's literally walking a straight and narrow line. His hands cross\nstiffly in front of him, seeming to point at each foot. And he does\nall this like he read in a manual someplace that this is how walk-\ning is supposed to be done.\nAs he approaches, I'm thinking that Eddie has probably\nnever done anything improper in his entire life\u2014unless it was\nexpected of him. Call him Mr. Regularity.\nWe talk about some of the orders going through. As usual,\neverything is out of control. Eddie, of course, doesn't realize this.\nTo him, everything is normal. And if it's normal, it must be right.\nHe's telling me\u2014in elaborate detail\u2014about what is running\ntonight. Just for the hell of it, I feel like asking Eddie to define\nwhat he's doing tonight in terms of something like net profit.\nI want to ask him, \"Say, Eddie, how's our impact on ROI\nbeen in the last hour? By the way, what's your shift done to im-"}
{"61": "the plant floor and the values on the many floors of UniCo head-\nquarters. They're too different.\nIn the middle of a sentence, Eddie notices I'm looking at him\nfunny.\n\"Something wrong?\" asks Eddie."}
{"62": "7\nWhen I get home, the house is dark except for one light. I\ntry to keep it quiet as I come in. True to her word, Julie has left\nme some dinner in the microwave. As I open the door to see what\ndelectable treat awaits me (it seems to be some variety of mystery\nmeat) I hear a rustling behind me. I turn around, and there\nstands my little girl, Sharon, at the edge of the kitchen.\n\"Well! If it isn't Miz Muffet!\" I exclaim. \"How is the tuffet\nthese days?\"\nShe smiles. \"Oh . . . not bad.\"\n\"How come you're up so late?\" I ask.\nShe comes forward holding a manila envelope. I sit down at\nthe kitchen table and put her on my knee. She hands the enve-\nlope to me to open.\n\"It's my report card,\" she says.\n\"No kidding?\"\n\"You have to look at it,\" she tells me.\nAnd I do.\n\"You got all A's!\" I say.\nI give her a squeeze and big kiss.\n\"That's terrific!\" I tell her. \"That's very good, Sharon. I'm\nreally proud of you. And Til bet you were the only kid in your\nclass to do this well.\"\nShe nods. Then she has to tell me everything. I let her go on,\nand half an hour later, she's barely able to keep her eyes open. I\ncarry her up to her bed.\nBut tired as I am, I can't sleep. It's past midnight now. I sit in"}
{"63": "exactly been stellar.) What turns me against the idea of looking\nfor another job is I'd feel I were running away. And I just can't do\nthat.\nIt's not that I feel I owe my life to the plant or the town or\nthe company, but I do feel some responsibility. And aside from\nthat, I've invested a big chunk of my life in UniCo. I want that\ninvestment to pay off. Three months is better than nothing for a\nlast chance.\nMy decision is, I'm going to do everything I can for the three\nmonths.\nBut that decided, the big question arises: what the hell can I\nreally do? I've already done the best I can with what I know.\nMore of the same is not going to do any good.\nUnfortunately, I don't have a year to go back to school and\nre-study a lot of theory. I don't even have the time to read the\nmagazines, papers, and reports piling up in my office. I don't\nhave the time or the budget to screw around with consultants,\nmaking studies and all that crap. And anyway, even if I did have\nthe time and money, I'm not sure any of those would give me a\nmuch better insight than what I've got now.\nI have the feeling there are some things I'm not taking into\naccount. If I'm ever going to get us out of this hole, I can't take\nanything for granted; Tm going to have to watch closely and\nthink carefully about what is basically going on ... take it one\nstep at a time.\nI slowly realize that the only tools I have\u2014limited as they\nmay be\u2014are my own eyes and ears, my own hands, my own\nvoice, my own mind. That's about it. I am all I have. And the"}
{"64": "8\nTwo steps after rolling out of bed in the morning, I don't like\nmoving at all. But in the midst of a morning shower, memory of\nmy predicament returns. When you've only got three months to\nwork with, you don't have much time to waste feeling tired. I\nrush past Julie\u2014who doesn't have much to say to me\u2014and the\nkids, who already seem to sense that something is wrong, and\nhead for the plant.\nThe whole way there I'm thinking about how to get in touch\nwith Jonah. That's the problem. Before I can ask for his help, I've\ngot to find him.\nThe first thing I do when I get to the office is have Fran\nbarricade the door against the hordes massing outside for frontal\nattack. Just as I reach my desk, Fran buzzes me; Bill Peach is on\nthe line.\n\"Great,\" I mutter.\nI pick up the phone.\n\"Yes, Bill.\"\n\"Don't you ever walk out of one of my meetings again,\" rum-\nbles Peach. \"Do you understand me?\"\n\"Yes, Bill.\"\n\"Now, because of your untimely absence yesterday, we've got\nsome things to go over,\" he says.\nA few minutes later, I've pulled Lou into the office to help\nme with the answers. Then Peach has dragged in Ethan Frost and\nwe're having a four-way conversation.\nAnd that's the last chance I have to think about Jonah for the"}
{"65": "I pull over at a gas station and use the pay phone to call\nJulie.\n\"Hello,\" she answers.\n\"Hi, it's me,\" I say. \"Listen, I've got to go over to my\nmother's for something. I'm not sure how long I'll be, so why\ndon't you go ahead and eat without me.\"\n\"The next time you want dinner\u2014\"\n\"Look, don't give me any grief, Julie; this is important.\"\nThere is a second of silence before I hear the click.\nIt's always a little strange going back to the old neighbor-\nhood, because everywhere I look is some kind of memory waiting\njust out of sight in my mind's eye. I pass the corner where I had\nthe fight with Bruno Krebsky. I drive down the street where we\nplayed ball summer after summer. I see the alley where I made\nout for the first time with Angelina. I go past the utility pole upon\nwhich I grazed the fender of my old man's Chevy (and subse-\nquently had to work two months in the store for free to pay for\nthe repair). All that stuff. The closer I get to the house, the more\nmemories come crowding in, and the more I get this feeling that's\nkind of warm and uncomfortably tense.\nJulie hates to come here. When we first moved to town, we\nused to come down every Sunday to see my mother and Danny\nand his wife, Nicole. But there got to be too many fights about it,\nso we don't make the trip much anymore.\nI park the Mazda by the curb in front of the steps to my\nmother's house. It's a narrow, brick row house, about the same as\nany other on the street. Down at the corner is my old man's store,\nthe one my brother owns today. The lights are off down there;"}
{"66": "\"Look for something? Look for what?\" she asks, turning to\nlet me in. \"Come in, come in. You're letting all the cold inside.\nBoy, you gave me a scare. Here you are in town and you never\ncome to see me anymore. What's the matter? You too important\nnow for your old mother?\"\n\"No, of course not, Mom. I've been very busy at the plant,\" I\nsay. ^\n\"Busy, busy,\" she says leading the way to the kitchen. \"You\nhungry?\"\n\"No, listen, I don't want to put you to any trouble,\" I say.\nShe says, \"Oh, it's no trouble. I got some ziti I can heat up.\nYou want a salad too?\"\n\"No, listen, a cup of coffee will be fine. I just need to find my\nold address book,\" I tell her. \"It's the one I had when I was in\ncollege. Do you know where it might be?\"\nWe step into the kitchen.\n\"Your old address book . . .\" she muses as she pours a cup\nof coffee from the percolator. \"How about some cake? Danny\nbrought some day-old over last night from the store.\"\n\"No thanks, Mom. I'm fine,\" I say. \"It's probably in with all\nmy old notebooks and stuff from school.\"\nShe hands me the cup of coffee. \"Notebooks . . .\"\n\"Yeah, you know where they might be?\"\nHer eyes blink. She's thinking.\n\"Well . . . no. But I put all that stuff up in the attic,\" she\nsays.\n\"Okay, I'll go look there,\" I say.\nCoffee in hand, I head for the stairs leading to the second"}
{"67": "\"No, but here's a picture of your Uncle Paul before he was\narrested for embezzlement. Did I ever tell you that story?\"\nAfter another hour, we've gone through everything, and I've\nhad a refresher course in all there is to know about Uncle Paul.\nWhere the hell could it be?\n\"Well, I don't know,\" says my mother. \"Unless it could be in\nyour old room.\"\nWe go upstairs to the room I used to share with Danny. Over\nin the corner is the old desk where I used to study when I was a\nkid. I open the top drawer. And, of course, there it is.\n\"Mom, I need to use your phone.\"\nMy mother's phone is located on the landing of the stairs\nbetween the floors of the house. It's the same phone that was\ninstalled in 1936 after my father began to make enough money\nfrom the store to afford one. I sit down on the steps, a pad of\npaper on my lap, briefcase at my feet. I pick up the receiver,\nwhich is heavy enough to bludgeon a burglar into submission. I\ndial the number, the first of many.\nIt's one o'clock by now. But I'm calling Israel, which happens\nto be on the other side of the world from us. And vice versa.\nWhich roughly means their days are our nights, our nights are\ntheir mornings, and consequently, one in the morning is not such\na bad time to call.\nBefore long, I've reached a friend I made at the university,\nsomeone who knows what's become of Jonah. He finds me an-\nother number to call. By two o'clock, I've got the tablet of paper\non my lap covered with numbers I've scribbled down, and I'm\ntalking to some people who work with Jonah. I convince one of"}
{"68": "I freeze for a moment. Then I realize he's referring to his\nquestion, what is the goal?\n\"Right,\" I say.\n\"Well?\"\nI hesitate. My answer seems so ludicrously simple I am sud-\ndenly afraid that it must be wrong, that he will laugh at me. But I\nblurt it out.\n\"The goal of a manufacturing organization is to make\nmoney,\" I say to him. \"And everything else we do is a means to\nachieve the goal.\"\nBut Jonah doesn't laugh at me.\n\"Very good, Alex. Very good,\" he says quietly.\n\"Thanks,\" I tell him. \"But, see, the reason I called was to ask\nyou a question that's kind of related to the discussion we had at\nO'Hare.\"\n\"What's the problem?\" he asks.\n\"Well, in order to know if my plant is helping the company\nmake money, I have to have some kind of measurements,\" I say.\n\"Right?\"\n\"That's correct,\" he says.\n\"And I know that up in the executive suite at company head-\nquarters, they've got measurements like net profit and return on\ninvestment and cash flow, which they apply to the overall organi-\nzation to check on progress toward the goal.\"\n\"Yes, go on,\" says Jonah.\n\"But where I am, down at the plant level, those measure-\nments don't mean very much. And the measurements I use inside\nthe plant . . . well, I'm not absolutely sure, but I don't think"}
{"69": "ent ways, ways which mean the same thing as those two words,\n'making money.' '\n\"Okay,\" I answer, \"so I can say the goal is to increase net\nprofit, while simultaneously increasing both ROI and cash flow,\nand that's the equivalent of saying the goal is to make money.\"\n\"Exactly,\" he says. \"One expression is the equivalent of the\nother. But as you have discovered, those conventional measure-\nments you use to express the goal do not lend themselves very\nwell to the daily operations of the manufacturing organization. In\nfact, that's why I developed a different set of measurements.\"\n\"What kind of measurements are those?\" I ask.\n\"They're measurements which express the goal of making\nmoney perfectly well, but which also permit you to develop oper-\national rules for running your plant,\" he says. \"There are three\nof them. Their names are throughput, inventory and operational\nexpense.\"\n\"Those all sound familiar,\" I say.\n\"Yes, but their definitions are not,\" says Jonah. \"In fact, you\nwill probably want to write them down,\"\nPen in hand, I flip ahead to a clean sheet of paper on my\ntablet and tell him to go ahead.\n\"Throughput,\" he says, \"is the rate at which the system gen-\nerates money through sales.\"\nI write it down word for word.\nThen I ask, \"But what about production? Wouldn't it be\nmore correct to say\u2014\"\n\"No,\" he says. \"Through sales\u2014not production. If you pro-\nduce something, but don't sell it, it's not throughput. Got it?\""}
{"70": "I write it down, but I'm wondering about it, because it's very\ndifferent from the traditional definition of inventory.\n\"And the last measurement?\" I ask.\n\"Operational expense,\" he says. \"Operational expense is all\nthe money the system spends in order to turn inventory into\nthroughput.\"\n\"Okay,\" I say as I write. \"But what about the labor invested\nin inventory? You make it sound as though labor is operational\nexpense?\"\n\"Judge it according to the definitions,\" he says.\n\"But the value added to the product by direct labor has to be\na part of inventory, doesn't it?\"\n\"It might be, but it doesn't have to be,\" he says.\n\"Why do you say that?\"\n\"Very simply, I decided to define it this way because I believe\nit's better not to take the value added into account,\" he says. \"It\neliminates the confusion over whether a dollar spent is an invest-\nment or an expense. That's why I defined inventory and opera-\ntional expense the way I just gave you.\"\n\"Oh,\" I say. \"Okay. But how do I relate these measurements\nto my plant?\"\n\"Everything you manage in your plant is covered by those\nmeasurements,\" he says.\n\"Everything?\" I say. I don't quite believe him. \"But going\nback to our original conversation, how do I use these measure-\nments to evaluate productivity?\"\n\"Well, obviously you have to express the goal in terms of the\nmeasurements,\" he says, adding, \"Hold on a second, Alex.\" Then"}
{"71": "the goal with these measurements, how do I go about deriving\noperational rules for running my plant?\"\n\"Give me a phone number where you can be reached,\" he\nsays.\nI give him my office number.\n\"Okay, Alex, I really do have to go now,\" he says.\n\"Right,\" I say. \"Thanks for\u2014\"\nI hear the click from far away.\n\"\u2014talking to me.\"\nI sit there on the steps for some time staring at the three\ndefinitions. At some point, I close my eyes. When I open them\nagain, I see beams of sunlight below me on the living room rug. I\nhaul myself upstairs to my old room and the bed I had when I\nwas a kid. I sleep the rest of the morning with my torso and limbs\npainstakingly arranged around the lumps in the mattress.\nFive hours later, I wake up feeling like a waffle."}
{"72": "9\nIt's eleven o'clock when I wake up. Startled by what time it is,\nI fall onto my feet and head for the phone to call Fran, so she can\nlet everyone know I haven't gone AWOL.\n\"Mr. Rogo's office,\" Fran answers.\n\"Hi, it's me,\" I say.\n\"Well, hello stranger,\" she says. \"We were just about ready to\nstart checking the hospitals for you. Think you'll make it in to-\nday?\"\n\"Uh, yeah, I just had something unexpected come up with\nmy mother, kind of an emergency,\" I say.\n\"Oh, well, I hope everything's all right.\"\n\"Yeah, it's, ah, taken care of now. More or less. Anything\ngoing on that I should know about?\"\n\"Well . . . let's see,\" she says, checking (I suppose) my mes-\nsage slips. \"Two of the testing machines in G-aisle are down, and\nBob Donovan wants to know if we can ship without testing.\"\n\"Tell him absolutely not,\" I say.\n\"Okay,\" says Fran. \"And somebody from marketing is calling\nabout a late shipment.\"\nMy eyes roll over.\n\"And there was a fist fight last night on second shift . . .\nLou still needs to talk to you about some numbers for Bill Peach\n... a reporter called this morning asking when the plant was\ngoing to close; I told him he'd have to talk to you . . . and a\nwoman from corporate communications called about shooting a\nvideo tape here about productivity and robots with Mr. Granby,\""}
{"73": "must be some mistake. I mean, by the time Granby's limo pulls\nup to the gate, the whole plant might be closed.\nBut the woman confirms it; they want to shoot Granby here\nsometime in the middle of next month.\n\"We need a robot as a suitable background for Mr. Granby's\nremarks,\" says the woman.\n\"So why did you pick Bearington?\" I ask her.\n\"The director saw a slide of one of yours and he likes the\ncolor. He thinks Mr. Granby will look good standing in front of\nit,\" she says.\n\"Oh, I see,\" I tell her. \"Have you talked to Bill Peach about\nthis?\"\n\"No, I didn't think there was any need for that,\" she says.\n\"Why? Is there a problem?\"\n\"You might want to run this past Bill in case he has any other\nsuggestions,\" I tell her. \"But it's up to you. Just let me know\nwhen you have an exact date so I can notify the union and have\nthe area cleaned up.\"\n\"Fine. I'll be in touch,\" she says.\nI hang up and sit there on the steps muttering, \"So ... he\nlikes the color.\"\n\"What was that all about on the phone just now?\" my mother\nasks. We're sitting together at the table. She's obliged me to have\nsomething to eat before I leave.\nI tell her about Granby coming.\n\"Well that sounds like a feather in your cap, the head man\u2014\nwhat's his name again?\" asks my mother.\n\"Granby.\""}
{"74": "They're run by computer and you can program them to do dif-\nferent jobs,\" I explain.\nMom nods, still trying to picture what these robots are.\n\"So why's this Granby guy want to have his picture taken\nwith a bunch of robots who don't even have faces?\" she asks.\n\"I guess because they're the latest thing, and he wants to tell\neverybody in the corporation that we ought to be using more of\nthem so that\u2014\"\nI stop and glance away for a second, and see Jonah sitting\nthere smoking his cigar.\n\"So that what?\" asks my mother.\n\"Uh ... so that we can increase productivity,\" I mumble,\nwaving my hand in the air.\nAnd Jonah says, have they really increased productivity at\n\\ our plant? Sure they have, I say. We had\u2014what?\u2014a thirty-six\npercent improvement in one area. Jonah puffs his cigar.\n\"Is something the matter?\" my mother asks.\n\"I just remembered something, that's all.\"\n\"What? Something bad?\" she asks.\n\"No, an earlier conversation I had with the man I talked to\nlast night,\" I say.\nMy mother puts her hand on my shoulder.\n\"Alex, what's wrong?\" she's asking. \"Come on, you can tell\nme. I know something's wrong. You show up out of the blue on\nmy doorstep, you're calling people all over the place in the mid-\ndle of the night. What is it?\"\n\"See, Mom, the plant isn't doing so well . . . and, ah ...\nwell, we're not making any money.\""}
{"75": "everything. Okay? I've got to get going now. I've really got a lot\nof work to do.\"\nI stand up and go to get my briefcase. My mother follows.\nDid I get enough to eat? Would I like a snack to take with me for\nlater in the day? Finally, she takes my sleeve and holds me in one\nplace.\n\"Listen to me, Al. Maybe you've got some problems. I know\nyou do, but this running all over the place, staying up all night\nisn't good for you. You've got to stop worrying. It's not going to\nhelp you. Look what worrying did to your father,\" she says. \"It\nkilled him.\"\n\"But, Mom, he was run over by a bus.\"\n\"So if he hadn't been so busy worrying he would have looked\nbefore he crossed the street.\"\nI sigh. \"Yeah, well, Mom, you may have a point. But it's more\ncomplicated than you think.\"\n\"I mean it! No worrying!\" she says. \"And this Granby fellow,\nif he's making trouble for you, you let me know. I'll call him and\ntell him what a worker you are. And who should know better than\na mother? You leave him to me. I'll straighten him out.\"\nI smile. I put my arm around her shoulders.\n\"I bet you would, Mom.\"\n\"You know I would.\"\nI tell Mom to call me as soon as her phone bill arrives in the\nmail, and I'll come over and pay it. I give her a hug and a kiss\ngood-bye, and I'm out of there. I walk out into the daylight and\nget into the Mazda. For a moment, I consider going straight to the\noffice. But a glance at the wrinkles in my suit and a rub of the"}
{"76": "But I wonder how Jonah knew? He seemed to know right\naway that productivity hadn't increased. There were those ques-\ntions he asked.\nOne of them, I remember as I'm driving, was whether we\nhad been able to sell any more products as a result of having the\nrobots. Another one was whether we had reduced the number of\npeople on the payroll. Then he had wanted to know if inventories\nhad gone down. Three basic questions.\nWhen I get home, Julie's car is gone. She's out some place,\nwhich is just as well. She's probably furious at me. And I simply\ndo not have time to explain right now.\nAfter I'm inside, I open my briefcase to make a note of those\nquestions, and I see the list of measurements Jonah gave me last\nnight. From the second I glance at those definitions again, it's\nobvious. The questions match the measurements.\nThat's how Jonah knew. He was using the measurements in\nthe crude form of simple questions to see if his hunch about the\nrobots was correct: did we sell any more products (i.e., did our\nthroughput go up?); did we lay off anybody (did our operational\nexpense go down?); and the last, exactly what he said: did our\ninventories go down?\nWith that observation, it doesn't take me long to see how to\nexpress the goal through Jonah's measurements. I'm still a little\npuzzled by the way he worded the definitions. But aside from\nthat, it's clear that every company would want to have its\nthroughput go up. Every company would also want the other\ntwo, inventory and operational expense, to go down, if at all pos-\nsible. And certainly it's best if they all occur simultaneously\u2014just"}
{"77": "people around. Which means the robots had to increase opera-\ntional expense.\nOkay, but efficiencies have gone up because of the robots. So\nmaybe that's been our salvation. When efficiencies go up, the\ncost-per-part has to come down.\nBut did the cost really come down? How could the cost-per-\npart go down if operational expense went up?\nBy the time I make it to the plant, it's one o'clock, and I still\nhaven't thought of a satisfactory answer. I'm still thinking about it\nas I walk through the office doors. The first thing I do is stop by\nLou's office.\n\"Have you got a couple minutes?\" I ask.\n\"Are you kidding?\" he says. \"I've been looking for you all\nmorning.\"\nHe reaches for a pile of paper on the corner of his desk. I\nknow it's got to be the report he has to send up to division.\n\"No, I don't want to talk about that right now,\" I tell him.\n\"I've got something more important on my mind.\"\nI watch his eyebrows go up.\n\"More important than this report for Peach?\"\n\"Infinitely more important than that,\" I tell him.\nLou shakes his head as he leans back in his swivel chair and\ngestures for me to have a seat.\n\"What can I do for you?\"\n\"After those robots out on the floor came on line, and we got\nmost of the bugs out and all that,\" I say, \"what happened to our\nsales?\"\nLou's eyebrows come back down again; he's leaning forward"}
{"78": "files, starts printing out handfuls of reports, charts, and graphs.\nWe both start leafing through. But we find that in every case\nwhere a robot came on line, there was no increase in sales for any\nproduct for which they made parts, not even the slightest blip in\nthe curve. For the heck of it, we also check the shipments made\nfrom the plant, but there was no increase there either. In fact, the\nonly increase is in overdue shipments\u2014they've grown rapidly\nover the last nine months.\nLou looks up at me from the graphs.\n\"Al, I don't know what you're trying to prove,\" he says. \"But\nif you want to broadcast some success story on how the robots are\ngoing to save the plant with increased sales, the evidence just\ndoesn't exist. The data practically say the opposite.\"\n\"That's exactly what I was afraid of,\" I say.\n\"What do you mean?\"\n\"I'll explain it in a minute. Let's look at inventories,\" I tell\nhim. \"I want to find out what happened to our work-in-process\non parts produced by the robots.\"\nLou gives up.\n\"I can't help you there,\" he says. \"I don't have anything on\ninventories by part number.\"\n\"Okay, let's get Stacey in on this.\"\nStacey Potazenik manages inventory control for the plant.\nLou makes a call and pulls her out of another meeting.\nStacey is a woman in her early 40's. She's tall, thin, and brisk\nin her manner. Her hair is black with strands of gray and she\nwears big, round glasses. She is always dressed in jackets and\nskirts; never have I seen her in a blouse with any kind of lace,"}
{"79": "end of the third quarter,\" she says. \"And you can't blame me for\nit\u2014even though everyone always does\u2014because I fought it every\nstep of the way.\"\n\"What do you mean?\"\n\"You remember, don't you? Or maybe you weren't here\nthen. But when the reports came in, we found the robots in weld-\ning were only running at something like thirty percent efficiency.\nAnd the other robots weren't much better. Nobody would stand\nfor that.\"\nI look over at Lou.\n\"We had to do something,\" he says. \"Frost would have had\nmy head if I hadn't spoken up. Those things were brand new and\nvery expensive. They'd never pay for themselves in the projected\ntime if we kept them at thirty percent.\"\n\"Okay, hold on a minute,\" I tell him. I turn back to Stacey.\n\"What did you do then?\"\nShe says, \"What could I do? I had to release more materials to\nthe floor in all the areas feeding the robots. Giving the robots\nmore to produce increased their efficiencies. But ever since then,\nwe've been ending each month with a surplus of those parts.\"\n\"But the important thing was that efficiencies did go up,\"\nsays Lou, trying to add a bright note. \"Nobody can find fault with\nus on that.\"\n\"I'm not sure of that at all any more,\" I say. \"Stacey, why are\nwe getting that surplus? How come we aren't consuming those\nparts?\"\n\"Well, in a lot of cases, we don't have any orders to fill at\npresent which would call for those parts,\" she says. \"And in the"}
{"80": "\"Yes, there is. We've just been talking about our local celebri-\nties, the robots,\" I say.\nBob glances from side to side, wondering, I suppose, what\nwe've been saying.\n\"What are you worried about them for?\" he asks. \"The ro-\nbots work pretty good now.\"\n\"We're not so sure about that,\" I say. \"Stacey tells me we've\ngot an excess of parts built by the robots. But in some instances\nwe can't get enough of certain other parts to assemble and ship\nour orders.\"\nBob says, \"It isn't that we can't get enough parts\u2014it's more\nthat we can't seem to get them when we need them. That's true\neven with a lot of the robot parts. We'll have a pile of something\nlike, say, a CD-50 sit around for months waiting for control boxes.\nThen we'll get the control boxes, but we won't have something\nelse. Finally we get the something else, and we build the order\nand ship it. Next thing you know, you're looking around for a\nCD-50 and you can't find any. We'll have tons of CD-45's and\n80's, but no 50's. So we wait. And by the time we get the 50's\nagain, all the control boxes are gone.\"\n\"And so on, and so on, and so on,\" says Stacey.\n\"But, Stacey, you said the robots were producing a lot of\nparts for which we don't have product orders,\" I say. \"That\nmeans we're producing parts we don't need.\"\n\"Everybody tells me we'll use them eventually,\" she says.\nThen she adds, \"Look, it's the same game everybody plays.\nWhenever efficiencies take a drop, everybody draws against the\nfuture forecast to keep busy. We build inventory. If the forecast"}
{"81": "ventory? That's operational expense. And if that went up, how\ncould the cost of parts go down?\"\n\"Look, it depends on volume,\" says Lou.\n\"Exactly,\" I say. \"Sales volume . . . that's what matters. And\nwhen we've got parts that can't be assembled into a product and\nsold because we don't have the other components, or because we\ndon't have the orders, then we're increasing our costs.\"\n\"Al,\" says Bob, \"are you trying to tell us we got screwed by\nthe robots?\"\nI sit down again.\n\"We haven't been managing according to the goal,\" I mut-\nter.\nLou squints. \"The goal? You mean our objectives for the\nmonth?\"\nI look around at them.\n\"I think I need to explain a few things.\""}
{"82": "10\nAn hour and a half later, I've gone over it all with them.\nWe're in the conference room, which I've commandeered be-\ncause it has a whiteboard. On that whiteboard, I've drawn a dia-\ngram of the goal. Just now I've written out the definitions of the\nthree measurements.\nAll of them are quiet. Finally, Lou speaks up and says,\n\"Where the heck did you get these definitions anyway?\"\n\"My old physics teacher gave them to me.\"\n\"Who?\" asks Bob.\n\"Your old physics teacher?\" asks Lou.\n\"Yeah,\" I say defensively. \"What about it?\"\n\"So what's his name?\" asks Bob.\n\"Or what's 'her' name,\" says Stacey.\n\"His name is Jonah. He's from Israel.\"\nBob says, \"Well, what I want to know is, how come in\nthroughput he says 'sales'? We're manufacturing. We've got noth-\ning to do with sales; that's marketing.\"\nI shrug. After all, I asked the same question over the phone.\nJonah said the definitions were precise, but I don't know how to\nanswer Bob. I turn toward the window. Then I see what I should\nhave remembered.\n\"Come here,\" I say to Bob.\nHe lumbers over. I put a hand on his shoulder and point out\nthe window. \"What are those?\" I ask him.\n\"Warehouses,\" he says.\n\"For what?\""}
{"83": "throughput happen. One measurement for the incoming money,\none for the money still stuck inside, and one for the money going\nout.\"\n\"Well, if you think about all the investment represented by\nwhat we've got sitting out there on the floor, you know for sure\nthat inventory is money,\" says Stacey. \"But what bothers me is\nthat I don't see how he's treating value added to materials by\ndirect labor.\"\n\"I wondered the same thing, and I can only tell you what he\ntold me,\" I say.\n\"Which is?\"\n\"He said he thinks that it's just better if value added isn't\ntaken into account. He said that it gets rid of the 'confusion'\nabout what's an investment and what's an expense, I say.\nStacey and the rest of us think about this for a minute. The\nroom gets quiet again.\nThen Stacey says, \"Maybe Jonah feels direct labor shouldn't\nbe a part of inventory because the time of the employees isn't\nwhat we're really selling. We 'buy' time from our employees, in a\nsense, but we don't sell that time to a customer\u2014unless we're\ntalking about service.\"\n\"Hey, hold it,\" says Bob. \"Now look here: if we're selling the\nproduct, aren't we also selling the time invested in that product?\"\n\"Okay, but what about idle time?\" I ask.\nLou butts in to settle it, saying, \"All this is, if I understand it\ncorrectly, is a different way of doing the accounting. All employee\ntime\u2014whether it's direct or indirect, idle time or operating time,\nor whatever\u2014is operational expense, according to Jonah. You're still"}
{"84": "\"But how do we know the value of our finished goods?\" she\nasks.\n\"First of all, the market determines the value of the prod-\nuct,\" says Lou. \"And in order for the corporation to make money,\nthe value of the product\u2014and the price we're charging\u2014has to\nbe greater than the combination of the investment in inventory\nand the total operational expense per unit of what we sell.\"\nI see by the look on Bob's face that he's very skeptical. I ask\nhim what's bothering him.\n\"Hey, man, this is crazy,\" Bob grumbles.\n\"Why?\" asks Lou.\n\"It won't work!\" says Bob. \"How can you account for every-\nthing in the whole damn system with three lousy measurements?\"\n\"Well,\" says Lou as he ponders the board. \"Name something\nthat won't fit in one of those three.\"\n\"Tooling, machines . . .\" Bob counts them on with his fin-\ngers. \"This building, the whole plant!\"\n\"Those are in there,\" says Lou.\n\"Where?\" asks Bob.\nLou turns to him. \"Look, those things are part one and part\nthe other. If you've got a machine, the depreciation on that ma-\nchine is operational expense. Whatever portion of the investment\nstill remains in the machine, which could be sold, is inventory.\"\n\"Inventory? I thought inventory was products, and parts\nand so on,\" says Bob. \"You know, the stuff we're going to sell.\"\nLou smiles. \"Bob, the whole plant is an investment which can\nbe sold\u2014for the right price and under the right circumstances.\"\nAnd maybe sooner than we'd like, I think."}
{"85": "Then I think about the \"soft\" things in business, things like\nknowledge\u2014knowledge from consultants, knowledge gained\nfrom our own research and development. I throw it out to them\nto see how they think those things should be classified.\nMoney for knowledge has us stumped for a while. Then we\ndecide it depends, quite simply, upon what the knowledge is used\nfor. If it's knowledge, say, which gives us a new manufacturing\nprocess, something that helps turn inventory into throughput,\nthen the knowledge is operational expense. If we intend to sell\nthe knowledge, as in the case of a patent or a technology license,\nthen it's inventory. But if the knowledge pertains to a product\nwhich UniCo itself will build, it's like a machine\u2014an investment\nto make money which will depreciate in value as time goes on.\nAnd, again, the investment that can be sold is inventory; the de-\npreciation is operational expense.\n\"I got one for you,\" says Bob. \"Here's one that doesn't fit:\nGranby's chauffeur.\"\n\"What?\"\n\"You know, the old boy in the black suit who drives J. Bart\nGranby's limo for him,\" says Bob.\n\"He's operational expense,\" says Lou.\n\"Like hell he is! You tell me how Granby's chauffeur turns\ninventory into throughput,\" says Bob, and looks around as if he's\nreally got us on this one. \"I bet his chauffeur doesn't even know\nthat inventory and throughput exist.\"\n\"Unfortunately, neither do some of our secretaries,\" says\nStacey.\nI say, \"You don't have to have your hands on the product in"}
{"86": "morning that Granby may be coming here to make a video tape\non robots.\"\n\"Granby's coming here?\" asks Bob.\n\"And if Granby's coming, you can bet Bill Peach and all the\nothers will be tagging along,\" says Stacey.\n\"Just what we need,\" grumbles Lou.\nStacey turns to Bob. \"You see now why Al's asking questions\nabout the robots. We've got to look good for Granby.\"\n\"We do look good,\" says Lou. \"The efficiencies there are\nquite acceptable; Granby will not be embarrassed by appearing\nwith the robots on tape.\"\nBut I say, \"Dammit, I don't care about Granby and his video-\ntape. In fact, I will lay odds that the tape will never be shot here\nanyway, but that's beside the point. The problem is that every-\nbody\u2014including me until now\u2014has thought these robots have\nbeen a big productivity improvement. And we just learned that\nthey're not productive in terms of the goal. The way we've been\nusing them, they're actually counter-productive.\"\nEveryone is silent.\nFinally, Stacey has the courage to say, \"Okay, so somehow\nwe've got to make the robots productive in terms of the goal.\"\n\"We've got to do more than that,\" I say. I turn to Bob and\nStacey. \"Listen, I've already told Lou, and I guess this is as good a\ntime as any to tell the both of you. I know you'll hear it eventually\nanyhow.\"\n\"Hear what?\" asks Bob.\n\"We've been given an ultimatum by Peach\u2014three months to\nturn the plant around or he closes us down for good,\" I say."}
{"87": "\"I honestly don't know,\" I say. \"But at least now we can see\nsome of what we're doing wrong,\"\n\"So what can we do that's different?\" asks Bob.\n\"Why don't we stop pushing materials through the robots\nand try to reduce inventories?\" suggests Stacey.\n\"Hey, I'm all for lower inventory,\" says Bob. \"But if we don't\nproduce, our efficiencies go down. Then we're right back where\nwe started.\"\n\"Peach isn't going to give us a second chance if all we give\nhim is lower efficiencies,\" says Lou. \"He wants higher efficiencies,\nnot lower.\"\nI run my fingers through my hair.\nThen Stacey says, \"Maybe you should try calling this guy,\nJonah, again. He seems like he's got a good handle on what's\nwhat.\"\n\"Yeah, at least we could find out what he has to say,\" says\nLou.\n\"Well, I talked to him last night. That's when he gave me all\nthis stuff,\" I say, waving to the definitions on the board. \"He was\nsupposed to call me . . .\"\nI look at their faces.\n\"Well, okay, I'll try him again,\" I say and reach for my brief-\ncase to get the London number.\nI put through a call from the phone in the conference room\nwith the three of them listening expectantly around the table. But\nhe isn't there anymore. Instead I end up talking to some secre-\ntary.\n\"Ah, yes, Mr. Rogo,\" she says. \"Jonah tried to call you, but"}
{"88": "\"Oh, I'm sorry\u2014I'll try not to keep you long. But I really\nneed to talk to you at greater length about what we were discuss-\ning last night,\" I tell him.\n\"Last night?\" he asks. \"Yes, I suppose it was 'last night' your\ntime.\"\n\"Maybe we could make arrangements for you to come to my\nplant and meet with me and my staff,\" I suggest.\n\"Well, the problem is I have commitments lined up for the\nnext three weeks, and then I'm going back to Israel,\" he says.\n\"But, you see, I can't wait that long,\" I say. \"I've got some\nmajor problems I have to solve and not a lot of time. I under-\nstand now what you meant about the robots and productivity.\nBut my staff and I don't know what the next step should be and\n... uh, well, maybe if I explained a few things to you\u2014\"\n\"Alex, I would like to help you, but I also need to get some\nsleep. I'm exhausted,\" he says. \"But I have a suggestion: if your\nschedule permits, why don't I meet with you here tomorrow\nmorning at seven for breakfast at my hotel.\"\n\"Tomorrow?\"\n\"That's right,\" he says. \"We'll have about an hour and we\ncan talk. Otherwise . . .\"\nI look around at the others, all of them watching me anx-\niously. I tell Jonah to hold on for a second.\n\"He wants me to come to New York tomorrow,\" I tell them.\n\"Can anybody think of a reason why I shouldn't go?\"\n\"Are you kidding?\" says Stacey.\n\"Go for it,\" says Bob.\n\"What have you got to lose?\" says Lou."}
{"89": "11\nBut Julie does not understand.\n\"Thanks for the advance notice,\" she says.\n\"If I'd known earlier, I'd have told you,\" I say.\n\"Everything is unexpected with you lately,\" she says.\n\"Don't I always tell you when I know I've got trips coming\nup?\"\nShe fidgets next to the bedroom door. I'm packing an over-\nnight bag which lies open on the bed. We're alone; Sharon is\ndown the street at a friend's house, and Davey is at band practice.\n\"When is this going to end?\" she asks.\nI stop midway through taking some underwear from a\ndrawer. I'm getting irritated by the questions because we just\nwent over the whole thing five minutes ago. Why is it so hard for\nher to understand?\n\"Julie, I don't know.\" I say. \"I've got a lot of problems to\nsolve.\"\nMore fidgeting. She doesn't like it. It occurs to me that\nmaybe she doesn't trust me or something.\n\"Hey, I'll call you as soon as I get to New York,\" I tell her.\n\"Okay?\"\nShe turns as if she might walk out of the room.\n\"Fine. Call,\" she says, \"but I might not be here.\"\nI stop again.\n\"What do you mean by that?\"\n\"I might be out someplace,\" she says.\n\"Oh,\" I say. \"Well, I guess I'll have to take my chances.\""}
{"90": "\"Everything is for your job,\" she says. \"It's all you think\nabout. I can't even count on you for dinner. And the kids are\nasking me why you're like this\u2014\"\nThere is a tear forming in the corner of her eye. I reach to\nwipe it away, but she brushes my hand aside.\n\"No!\" she says. \"Just go catch your plane to wherever it is\nyou're going.\"\n\"Julie-\"\nShe walks past me.\n\"Julie, this is not fair!\" I yell at her.\nShe turns to me.\n\"That's right,\" she says. \"You are not being fair. To me or to\nyour children.\"\nShe goes upstairs without looking back. And I don't even\nhave time to settle this; I'm already late for my flight, I pick up\nmy bag in the hall, sling it over my shoulder, and grab my brief-\ncase on my way out the door.\nAt 7:10 the next morning, I'm waiting in the hotel lobby for\nJonah. He's a few minutes late, but that's not what's on my mind\nas I pace the carpeted floor. I'm thinking about Julie. I'm wor-\nried about her . . . about us. After I checked into my room last\nnight, I tried to call home. No answer. Not even one of the kids\npicked up the phone. I walked around the room for half an hour,\nkicked a few things, and tried calling again. Still no answer. From\nthen until two in the morning, I dialed the number every fifteen\nminutes. Nobody home. At one point I tried the airlines to see if I\ncould get on a plane back, but nothing was flying in that direction\nat that hour. I finally fell asleep. My wake-up call got me out of"}
{"91": "I walk with him into the restaurant and the maitre d' leads us\nto a table with a white linen cloth.\n\"How did you do with the measurements I defined for you\nover the telephone?\" he asks after we've sat down.\nI switch my mind to business, and tell him how I expressed\nthe goal with his measurements. Jonah seemed very pleased.\n\"Excellent,\" he says. \"You have done very well.\"\n\"Well, thanks, but I'm afraid I need more than a goal and\nsome measurements to save my plant.\"\n\"To save your plant?\" he asks.\nI say, \"Well . . . yes, that's why I'm here. I mean, I didn't\njust call you to talk philosophy.\"\nHe smiles. \"No, I didn't think you tracked me down purely\nfor the love of truth. Okay, Alex, tell me what's going on.\"\n\"This is confidential,\" I say to him. Then I explain the situa-\ntion with the plant and the three-month deadline before it gets\nclosed. Jonah listens attentively. When I've finished, he sits back.\n\"What do you expect from me?\" he asks.\n\"I don't know if there is one, but I'd like you to help me find\nthe answer that will let me keep my plant alive and my people\nworking,\" I say.\nJonah looks away for a moment.\n\"I'll tell you my problem,\" he says. \"I have an unbelievable\nschedule. That's why we're meeting at this ungodly hour, inci-\ndentally. With the commitments I already have, there is no way I\ncan spend the time to do all the things you probably would ex-\npect from a consultant.\"\nI sigh, very disappointed. I say, \"Okay, if you're too busy\u2014\""}
{"92": "you are diligent, that is. And if you aren't, then nothing I say\ncould save you anyway.\"\n\"Oh, you can count on our diligence, for sure,\" I say.\n\"Shall we try it then?\" he asks.\n\"Frankly, I don't know what else to do,\" I say. Then I smile.\n\"I guess I'd better ask what this is going to cost me. Do you have\nsome kind of standard rate or something?\"\n\"No, I don't,\" he says. \"But I'll make a deal with you. Just\npay me the value of what you learn from me.\"\n\"How will I know what that is?\"\n\"You should have a reasonable idea after we've finished. If\nyour plant folds, then obviously the value of your learning won't\nhave been much; you won't owe me anything. If, on the other\nhand, you learn enough from me to make billions, then you\nshould pay me accordingly,\" he says.\nI laugh. What have I got to lose?\n\"Okay, fair enough,\" I say finally.\nWe shake hands across the table.\nA waiter interrupts to ask if we're ready to order. Neither of\nus have opened the menus, but it turns out we both want coffee.\nThe waiter informs us there's a ten-dollar minimum for sitting in\nthe dining room. So Jonah tells him to bring us both our own\npots of coffee and a quart of milk. He gives us a dirty look and\nvanishes.\n\"Now then,\" Jonah says. \"Where shall we begin . . .\"\n\"I thought maybe first we could focus on the robots,\" I tell\nhim.\nJonah shakes his head."}
{"93": "\"Alex, you told me in our first meeting that your plant has\nvery good efficiencies overall. If your efficiencies are so good,\nthen why is your plant in trouble?\"\nHe takes a cigar out of his shirt pocket and bites the end off\nof it.\n\"Okay, look, I have to care about efficiencies if only for the\nreason that my management cares about them,\" I tell him.\n\"What's more important to your management, Alex: efficien-\ncies or money?\" he asks.\n\"Money, of course. But isn't high efficiency essential to mak-\ning money?\" I ask him.\n\"Most of the time, your struggle for high efficiencies is taking\nyou in the opposite direction of your goal.\"\n\"I don't understand,\" I say. \"And even if I did, my manage-\nment wouldn't.\"\nBut Jonah lights his cigar and says between puffs, \"Okay,\nlet's see if I can help you understand with some basic questions\nand answers. First tell me this: when you see one of your workers\nstanding idle with nothing to do, is that good or bad for the\ncompany?\"\n\"It's bad, of course,\" I say.\n\"Always?\"\nI feel this is a trick question.\n\"Well, we have to do maintenance\u2014\"\n\"No, no, no, I'm talking about a production employee who is\nidle because there is no product to be worked on.\"\n\"Yes, that's always bad,\" I say.\n\"Why?\""}
{"94": "He says, \"You've already proven it in your own plant. It's\nright in front of your eyes. But you don't see it.\"\nNow I shake my head. I say, \"Jonah, I don't think we're\ncommunicating. You see, in my plant, I don't have extra people.\nThe only way we can get products out the door is to keep every-\none working constantly.\"\n\"Tell me, Alex, do you have excess inventories in your\nplant?\" he asks.\n\"Yes, we do,\" I say.\n\"Do you have a lot of excess inventories?\"\n\"Well . . . yes.\"\n\"Do you have a lot of a lot of excess inventories?\"\n\"Yeah, okay, we do have a lot of a lot of excess, but what's the\npoint?\"\n\"Do you realize that the only way you can create excess in-\nventories is by having excess manpower?\" he says.\nI think about it. After a minute, I have to conclude he's right;\nmachines don't set up and run themselves. People had to create\nthe excess inventory.\n\"What are you suggesting I do?\" I ask. \"Lay off more peo-\nple? I'm practically down to a skeleton force now.\"\n\"No, I'm not suggesting that you lay off more people. But I\nam suggesting that you question how you are managing the ca-\npacity of your plant. And let me tell you, it is not according to the\ngoal.\"\nBetween us, the waiter sets down two elegant silver pots with\nsteam coming out of their spouts. He puts out a pitcher of cream\nand pours the coffee. While he does this, I find myself staring"}
{"95": "is balanced exactly with demand from the market. Do you know\nwhy managers try to do this?\"\nI tell him, \"Well, because if we don't have enough capacity,\nwe're cheating ourselves out of potential throughput. And if we\nhave more than enough capacity, we're wasting money. We're\nmissing an opportunity to reduce operational expense.\"\n\"Yes, that's exactly what everybody thinks,\" says Jonah. \"And\nthe tendency for most managers is to trim capacity wherever they\ncan, so no resource is idle, and everybody has something to work\non.\"\n\"Yeah, sure, I know what you're talking about,\" I say. \"We\ndo that at our plant. In fact, it's done at every plant I've ever\nseen.\"\n\"Do you run a balanced plant?\" he asks.\n\"Well, it's as balanced as we can make it. Of course, we've got\nsome machines sitting idle, but generally that's just outdated\nequipment. As for people, we've trimmed our capacity as much as\nwe can,\" I explain. \"But nobody ever runs a perfectly balanced\nplant.\"\n\"Funny, I don't know of any balanced plants either,\" he says.\n\"Why do you think it is that nobody after all this time and effort\nhas ever succeeded in running a balanced plant?\"\n\"I can give you a lot of reasons. The number one reason is\nthat conditions are always changing on us,\" I say.\n\"No, actually that isn't the number one reason,\" he says.\n\"Sure it is! Look at the things I have to contend with\u2014my\nvendors, for example. We'll be in the middle of a hot order and\ndiscover that the vendor sent us a bad batch of parts. Or look at"}
{"96": "the goal,\" he says. \"When you lay off people, do you increase\nsales?\"\n\"No, of course not,\" I say.\n\"Do you reduce your inventory?\" he asks.\n\"No, not by cutting people,\" I say. \"What we do by laying off\nworkers is cut our expenses.\"\n\"Yes, exactly,\" Jonah says. \"You improve only one measure-\nment, operational expense.\"\n\"Isn't that enough?\"\n\"Alex, the goal is not to reduce operational expense by itself.\nThe goal is not to improve one measurement in isolation. The\ngoal is to reduce operational expense and reduce inventory while\nsimultaneously increasing throughput,\" says Jonah.\n\"Fine. I agree with that,\" I say. \"But if we reduce expenses,\nand inventory and throughput stay the same, aren't we better\noff?\"\n\"Yes, if you do not increase inventory and/or reduce\nthroughput,\" he says.\n\"Okay, right. But balancing capacity doesn't affect either\none,\" I say.\n\"Oh? It doesn't? How do you know that?\"\n\"We just said\u2014\"\n\"I didn't say anything of the sort. I asked you. And you as-\nsumed that if you trim capacity to balance with market demand\nyou won't affect throughput or inventory,\" he says. \"But, in fact,\nthat assumption\u2014which is practically universal in the western\nbusiness world\u2014is totally wrong.\"\n\"How do you know it's wrong?\""}
{"97": "other can begin . . . the subsequent event depends upon the ones\nprior to it. You follow?\"\n\"Yeah, sure,\" I say. \"But what's the big deal about that?\"\n\"The big deal occurs when dependent events are in combi-\nnation with another phenomenon called 'statistical fluctuations,''\nhe says. \"Do you know what those are?\"\nI shrug. \"Fluctuations in statistics, right?\"\n\"Let me put it this way,\" he says. \"You know that some types\nof information can be determined precisely. For instance, if we\nneed to know the seating capacity in this restaurant, we can de-\ntermine it precisely by counting the number of chairs at each\ntable.\"\nHe points around the room.\n\"But there are other kinds of information we cannot pre-\ncisely predict. Like how long it will take the waiter to bring us our\ncheck. Or how long it will take the chef to make an omelet. Or\nhow many eggs the kitchen will need today. These types of infor-\nmation vary from one instance to the next. They are subject to\nstatistical fluctuations.\"\n\"Yeah, but you can generally get an idea of what all those are\ngoing to be based on experience,\" I say.\n\"But only within a range. Last time, the waiter brought the\ncheck in five minutes and 42 seconds. The time before it only\ntook two minutes. And today? Who knows? Could be three, four\nhours,\" he says, looking around. \"Where the hell is he?\"\n\"Yeah, but if the chef is doing a banquet and he knows how\nmany people are coming and he knows they're all having om-\nelets, then he knows how many eggs he's going to need,\" I say."}
{"98": "\"but about the effect of the two of them together. Which is what I\nwant you to think about, because I have to go.\"\n\"You're leaving?\" I ask.\n\"I have to,\" he says.\n\"Jonah, you can't just run off like this.\"\n\"There are clients waiting for me,\" he says.\n\"Jonah, I don't have time for riddles. I need answers,\" I tell\nhim.\nHe puts his hand on my arm.\n\"Alex, if I simply told you what to do, ultimately you would\nfail. You have to gain the understanding for yourself in order to\nmake the rules work,\" he says.\nHe shakes my hand.\n\"Until next time, Alex. Call me when you can tell me what\nthe combination of the two phenomena mean to your plant.\"\nThen he hurries away. Fuming inside, I flag down the waiter\nand hand him the check and some money. Without waiting for\nthe change, I follow in the direction of Jonah out to the lobby.\nI claim my overnight bag from the bellhop at the desk where\nI checked it, and sling it over my shoulder. As I turn, I see Jonah,\nstill without jacket or tie, talking to a handsome man in a blue\npinstripe suit over by the doors to the street. They go through\nthe doors together, and I trudge along a few steps behind them.\nThe man leads Jonah to a black limousine waiting at the curb. As\nthey approach, a chauffeur hops out to open the rear door for\nthem.\nI hear the handsome man in the blue pinstripe saying as he\ngets into the limo behind Jonah, \"After the facilities tour, we're"}
{"99": "12\nThere is a guy I heard about in UniCo who came home from\nwork one night, walked in, and said, \"Hi, honey, I'm home!\" And\nhis greeting echoed back to him from the empty rooms of his\nhouse. His wife had taken everything: the kids, the dog, the gold-\nfish, the furniture, the carpets, the appliances, the curtains, the\npictures on the wall, the toothpaste, everything. Well, just about\neverything\u2014actually, she left him two things: his clothes (which\nwere in a heap on the floor of the bedroom by the closet; she had\neven taken the hangers), and a note written in lipstick on the\nbathroom mirror which said, \"Good-bye, you bastard!\"\nAs I drive down the street to my house, that kind of vision is\nrunning through my mind, and has been periodically since last\nnight. Before I pull into the driveway, I look at the lawn for the\ntelltale signs of tracks left by the wheels of a moving van, but the\nlawn is unmarred.\nI park the Mazda in front of the garage. On my way inside, I\npeek through the glass, Julie's Accord is parked inside, and I look\nat the sky and silently say, \"Thank You.\"\nShe's sitting at the kitchen table, her back to me as I come in.\nI startle her. She stands up right away and turns around. We\nstare at each other for a second. I can see that the rims of her eyes\nare red.\n\"Hi,\" I say.\n\"What are you doing home?\" Julie asks.\nI laugh\u2014not a nice laugh, an exasperated laugh.\n\"What am / doing home? I'm looking for you!\" I say."}
{"100": "you. I tried it again this morning and nobody answered. So I\nknow you were gone all night,\" I say, \"And, by the way, where\nwere the kids?\"\n\"They stayed with friends,\" she says.\n\"On a school night?\" I ask. \"And what about you? Did you\nstay with a friend?\"\nShe puts her hands on her hips.\n\"Yes, as a matter of fact, I did stay with a friend,\" she says.\n\"Man or woman?\"\nHer eyes get hard on me. She takes a step forward.\n\"You don't care if I'm home with the kids night after night,\"\nshe says. \"But if I go away for one night, all of a sudden you have\nto know where I've been, what I've done.\"\n\"I just feel you owe me some explanation,\" I say.\n\"How many times have you been late, or out of town, or who\nknows where?\" she asks.\n\"But that's business,\" I say. \"And I always tell you where I've\nbeen if you ask. Now I'm asking.\"\n\"There's nothing to tell,\" she says. \"All that happened was I\nwent out with Jane.\"\n\"Jane?\" It takes me a minute to remember her. \"You mean\nyour friend from where we used to live? You drove all the way\nback there?\"\n\"I just had to talk to someone,\" she says. \"By the time we'd\nfinished talking, I'd had too much to drink to drive home. Any-\nway, I knew the kids were okay until morning. So I just stayed at\nJane's.\"\n\"Okay, but why? How did this come over you all of a sud-"}
{"101": "She still doesn't say anything.\n\"All right, look: I promise I'll make more time for you and\nthe kids,\" I say. \"Honest, I'll spend more time at home.\"\n\"Al, it's not going to work. Even when you're home, you're at\nthe office. Sometimes I've seen the kids tell you something two or\nthree times before you hear them.\"\n\"It won't be like that when I get out of the jam I'm in right\nnow,\" I say.\n\"Do you hear what you're saying? 'When I get out of the jam\nI'm in right now.' Do you think it's going to change? You've said\nall that before, Al. Do you know how many times we've been over\nthis?\"\n\"Okay, you're right. We have been over it a lot of times. But,\nright now, there's nothing I can do,\" I say.\nShe looks up at the sky and says, \"Your job has always been\non the line. Always. So if you're such a marginal employee, why\ndo they keep giving you promotions and more money?\"\nI pinch the bridge of my nose.\n\"How do I make you understand this,\" I say. \"I'm not up for\nanother promotion or pay raise this time. This time it's different.\nJulie, you have no idea what kind of problems I've got at the\nplant.\"\n\"And you have no idea what it's like here at home,\" she says.\nI say, \"Okay, look, I'd like to spend more time at home, but\nthe problem is getting the time.\"\n\"I don't need all your time,\" she says. \"But I do need some\nof it, and so do the kids.\"\n\"I know that. But to save this plant, I'm going to have to give"}
{"102": "She smiles. \"You mean it?\"\n\"Sure, if it doesn't work, we can talk about it,\" I say. \"Deal?\"\n\"Deal,\" she says.\nI lean toward her and ask, \"Want to seal it with a handshake\nor a kiss?\"\nShe comes around the table and sits on my lap and kisses me.\n\"You know, I sure missed you last night,\" I tell her.\n\"Did you?\" she says. \"I really missed you too. I had no idea\nsingles bars could be so depressing.\"\n\"Singles bars?\"\n\"It was Jane's idea,\" she says. \"Honest.\"\nI shake my head. \"I don't want to hear about it.\"\n\"But Jane showed me some new dance steps,\" she says. \"And\nmaybe this weekend\u2014\"\nI give her a squeeze. \"If you want to do something this week-\nend, baby, I'm all yours.\"\n\"Great,\" she says and whispers in my ear, \"You know, it's\nFriday, so ... why don't we start early?\"\nShe kissed me again.\nAnd I say, \"Julie, I'd really love to, but . . .\"\n\"But?\"\n\"I really should check in at the plant,\" I say.\nShe stands up. \"Okay, but promise me you'll hurry home\ntonight.\"\n\"Promise,\" I tell her. \"Really, it's going to be a great week-\nend.\""}
{"103": "13\nI open my eyes Saturday morning to see a drab green blur.\nThe blur turns out to be my son, Dave, dressed in his Boy Scout\nuniform. He is shaking my arm.\n\"Davey, what are you doing here?\" I ask.\nHe says, \"Dad, it's seven o'clock!\"\n\"Seven o'clock? I'm trying to sleep. Aren't you supposed to\nbe watching television or something?\"\n\"We'll be late,\" he says.\n\"We will be late? For what?\"\n\"For the overnight hike!\" he says. \"Remember? You prom-\nised me I could volunteer you to go along and help the troop-\nmaster.\"\nI mutter something no Boy Scout should ever hear. But\nDave isn't fazed.\n\"Come on. Just get in the shower,\" he says, as he pulls me\nout of bed. \"I packed your gear last night. Everything's in the car\nalready. We just have to get there by eight.\"\nI manage a last look at Julie, her eyes still shut, and the warm\nsoft mattress as Davey drags me through the door.\nAn hour and ten minutes later, my son and I arrive at the\nedge of some forest. Waiting for us is the troop: fifteen boys out-\nfitted in caps, neckerchiefs, merit badges, the works.\nBefore I have time to say, \"Where's the troopmaster?\", the\nother few parents who happen to be lingering with the boys take\noff in their cars, all pedals to the metal. Looking around, I see\nthat I am the only adult in sight."}
{"104": "The plan, I learn, is for the troop to hike through the forest\nfollowing a blazed trail to someplace called \"Devil's Gulch.\"\nThere we are to bivouac for the evening. In the morning we are\nto break camp and make our way back to the point of departure,\nwhere Mom and Dad are supposed to be waiting for little Freddy\nand Johnny and friends to walk out of the woods.\nFirst, we have to get to Devil's Gulch, which happens to be\nabout ten miles away. So I line up the troop. They've all got their\nrucksacks on their backs. Map in hand, I put myself at the front\nof the line in order to lead the way, and off we go.\nThe weather is fantastic. The sun is shining through the\ntrees. The skies are blue. It's breezy and the temperature is a little\non the cool side, but once we get into the woods, it's just right for\nwalking.\nThe trail is easy to follow because there are blazes (splotches\nof yellow paint) on the tree trunks every 10 yards or so. On either\nside, the undergrowth is thick. We have to hike in single file.\nI suppose I'm walking at about two miles per hour, which is\nabout how fast the average person walks. At this rate, I think to\nmyself, we should cover ten miles in about five hours. My watch\ntells me it's almost 8:30 now. Allowing an hour and a half for\nbreaks and for lunch, we should arrive at Devil's Gulch by three\no'clock, no sweat.\nAfter a few minutes, I turn and look back. The column of\nscouts has spread out to some degree from the close spacing we\nstarted with. Instead of a yard or so between boys, there are now\nlarger gaps, some a little larger than others. I keep walking.\nBut I look back again after a few hundred yards, and the"}
{"105": "\"Everybody stay behind Ron!\" I call back to the others. \"No-\nbody passes Ron, because he's got the map. Understand?\"\nEverybody nods, waves. Everybody understands.\nI wait by the side of the trail as the troop passes. My son,\nDavey, goes by talking with a friend who walks close behind him.\nNow that he's with his buddies, Dave doesn't want to know me.\nHe's too cool for that. Five or six more come along, all of them\nkeeping up without any problems. Then there is a gap, followed\nby a couple more scouts. After them, another, even larger gap has\noccurred. I look down the trail. And I see this fat kid. He already\nlooks a little winded. Behind him is the rest of the troop.\n\"What's your name?\" I ask as the fat kid draws closer.\n\"Herbie,\" says the fat kid.\n\"You okay, Herbie?\"\n\"Oh, sure, Mr. Rogo,\" says Herbie. \"Boy, it's hot out, isn't\nit?\"\nHerbie continues up the trail and the others follow. Some of\nthem look as if they'd like to go faster, but they can't get around\nHerbie. I fall in behind the last boy. The line stretches out in\nfront of me, and most of the time, unless we're going over a hill\nor around a sharp bend in the trail, I can see everybody. The\ncolumn seems to settle into a comfortable rhythm.\nNot that the scenery is boring, but after a while I begin to\nthink about other things. Like Julie, for instance. I really had\nwanted to spend this weekend with her. But I'd forgotten all\nabout this hiking business with Dave. \"Typical of you,\" I guess\nshe'd say. I don't know how I'm ever going to get the time I need\nto spend with her. The only saving grace about this hike is that"}
{"106": "assemble the product. The product has to be assembled before we\ncan ship it. And so on.\nBut you find dependent events in any process, and not just\nthose in a factory. Driving a car requires a sequence of dependent\nevents. So does the hike we're taking now. In order to arrive at\nDevil's Gulch, a trail has to be walked. Up front, Ron has to walk\nthe trail before Davey can walk it. Davey has to walk the trail\nbefore Herbie can walk it. In order for me to walk the trail, the\nboy in front of me has to walk it first. It's a simple case of depen-\ndent events.\nAnd statistical fluctuations?\nI look up and notice that the boy in front of me is going a\nlittle faster than I have been. He's a few feet farther ahead of me\nthan he was a minute ago. So I take some bigger steps to catch\nup. Then, for a second, I'm too close to him, so I slow down.\nThere: if I'd been measuring my stride, I would have re-\ncorded statistical fluctuations. But, again, what's the big deal?\nIf I say that I'm walking at the rate of \"two miles per hour,\" I\ndon't mean I'm walking exactly at a constant rate of two miles per\nhour every instant. Sometimes I'll be going 2.5 miles per hour;\nsometimes maybe I'll be walking at only 1.2 miles per hour. The\nrate is going to fluctuate according to the length and speed of\neach step. But over time and distance, I should be averaging about\ntwo miles per hour, more or less.\nThe same thing happens in the plant. How long does it take\nto solder the wire leads on a transformer? Well, if you get out\nyour stopwatch and time the operation over and over again, you\nmight find that it takes, let's say, 4.3 minutes on the average. But"}
{"107": "\"Okay, enough of that,\" I say to the persecutors.\nThen Herbie reaches the top. He turns around. His face is\nred from the climb.\n\"Atta boy, Herbie!\" I say to encourage him. \"Let's keep it\nmoving!\"\nHerbie disappears over the crest. The others continue the\nclimb, and I trudge behind them until I get to the top. Pausing\nthere, I look down the trail.\nHoly cow! Where's Ron? He must be half a mile ahead of us.\nI can see a couple of boys in front of Herbie, and everyone else is\nlost in the distance. I cup my hands over my mouth.\n\"HEY! LET'S GO UP THERE! LET'S CLOSE RANKS!\" I\nyell. \"DOUBLE TIME! DOUBLE TIME!\"\nHerbie eases into a trot. The kids behind him start to run. I\njog after them. Rucksacks and canteens and sleeping bags are\nbouncing and shaking with every step. And Herbie\u2014I don't\nknow what this kid is carrying, but it sounds like he's got a junk-\nyard on his back with all the clattering and clanking he makes\nwhen he runs. After a couple hundred yards, we still haven't\ncaught up. Herbie is slowing down. The kids are yelling at him to\nhurry up. I'm huffing and puffing along. Finally I can see Ron off\nin the distance.\n\"HEY RON!\" I shout. \"HOLD UP!\"\nThe call is relayed up the trail by the other boys. Ron, who\nprobably heard the call the first time, turns and looks back.\nHerbie, seeing relief in sight, slows to a fast walk. And so do the\nrest of us. As we approach, all heads are turned our way.\n\"Ron, I thought I told you to set a moderate pace,\" I say."}
{"108": "\"Not according to the map Ron has,\" he says.\n\"Oh,\" I say. \"Well, I guess we'd better get a move on.\"\nThe boys are already lining up.\n\"All right, let's go,\" I say.\nWe start out again. The trail is straight here, so I can see\neveryone. We haven't gone thirty yards before I notice it starting\nall over again. The line is spreading out; gaps between the boys\nare widening. Dammit, we're going to be running and stopping\nall day long if this keeps up. Half the troop is liable to get lost if\nwe can't stay together.\nI've got to put an end to this.\nThe first one I check is Ron. But Ron, indeed, is setting a\nsteady, \"average\" pace for the troop\u2014a pace nobody should have\nany trouble with. I look back down the line, and all of the boys\nare walking at about the same rate as Ron. And Herbie? He's not\nthe problem anymore. Maybe he felt responsible for the last de-\nlay, because now he seems to be making a special effort to keep\nup. He's right on the ass of the kid in front of him.\nIf we're all walking at about the same pace, why is the dis-\ntance between Ron, at the front of the line, and me, at the end of\nthe line, increasing?\nStatistical fluctuations?\nNah, couldn't be. The fluctuations should be averaging out.\nWe're all moving at about the same speed, so that should mean\nthe distance between any of us will vary somewhat, but will even\nout over a period of time. The distance between Ron and me\nshould also expand and contract within a certain range, but\nshould average about the same throughout the hike."}
{"109": "packstraps. In front of him, Ron continues onward, oblivious. A\ngap of ten . . . fifteen . . . twenty feet opens up. Which means\nthe entire line has grown by 20 feet.\nThat's when I begin to understand what's happening.\nRon is setting the pace. Every time someone moves slower\nthan Ron, the line lengthens. It wouldn't even have to be as obvi-\nous as when Dave slowed down. If one of the boys takes a step\nthat's half an inch shorter than the one Ron took, the length of\nthe whole line could be affected.\nBut what happens when someone moves faster than Ron?\nAren't the longer or faster steps supposed to make up for the\nspreading? Don't the differences average out?\nSuppose I walk faster. Can I shorten the length of the line?\nWell, between me and the kid ahead of me is a gap of about five\nfeet. If he continues walking at the same rate, and if I speed up, I\ncan reduce the gap\u2014and maybe reduce the total length of the\ncolumn, depending upon what's happening up ahead. But I can\nonly do that until I'm bumping the kid's rucksack (and if I did\nthat he'd sure as hell tell his mother). So I have to slow down to\nhis rate.\nOnce I've closed the gap between us, I can't go any faster\nthan the rate at which the kid in front of me is going. And he\nultimately can't go any faster than the kid in front of him. And so\non up the line to Ron. Which means that, except for Ron, each of\nour speeds depends upon the speeds of those in front of us in the\nline.\nIt's starting to make sense. Our hike is a set of dependent\nevents ... in combination with statistical fluctuations. Each of"}
{"110": "in our various speeds, but an accumulation of the fluctuations. And\nmostly it's an accumulation of slowness\u2014because dependency limits\nthe opportunities for higher fluctuations. And that's why the line is\nspreading. We can make the line shrink only by having everyone\nin the back of the line move much faster than Ron's average over\nsome distance.\nLooking ahead, I can see that how much distance each of us\nhas to make up tends to be a matter of where we are in the line.\nDavey only has to make up for his own slower than average fluc-\ntuations relative to Ron\u2014that twenty feet or so which is the gap in\nfront of him. But for Herbie to keep the length of the line from\ngrowing, he would have to make up for his own fluctuations plus\nthose of all the kids in front of him. And here I am at the end of\nthe line. To make the total length of the line contract, I have to\nmove faster than average for a distance equal to all the excess\nspace between all the boys. I have to make up for the accumula-\ntion of all their slowness.\nThen I start to wonder what this could mean to me on the\njob. In the plant, we've definitely got both dependent events and\nstatistical fluctuations. And here on the trail we've got both of\nthem. What if I were to say that this troop of boys is analogous to\na manufacturing system . . . sort of a model. In fact, the troop\ndoes produce a product; we produce \"walk trail.\" Ron begins\nproduction by consuming the unwalked trail before him, which is\nthe equivalent of raw materials. So Ron processes the trail first by\nwalking over it, then Davey has to process it next, followed by the\nboy behind him, and so on back to Herbie and the others and on\nto me."}
{"111": "And what is operational expense? It's whatever lets us turn\ninventory into throughput, which in our case would be the en-\nergy the boys need to walk. I can't really quantify that for the\nmodel, except that I know when I'm getting tired.\nIf the distance between Ron and me is expanding, it can only\nmean that inventory is increasing. Throughput is my rate of\nwalking. Which is influenced by the fluctuating rates of the oth-\ners. Hmmm. So as the slower than average fluctuations accumu-\nlate, they work their way back to me. Which means I have to slow\ndown. Which means that, relative to the growth of inventory,\nthroughput for the entire system goes down.\nAnd operational expense? I'm not sure. For UniCo, when-\never inventory goes up, carrying costs on the inventory go up as\nwell. Carrying costs are a part of operational expense, so that\nmeasurement also must be going up. In terms of the hike, opera-\ntional expense is increasing any time we hurry to catch up, be-\ncause we expend more energy than we otherwise would.\nInventory is going up. Throughput is going down. And op-\nerational expense is probably increasing.\nIs that what's happening in my plant?\nYes, I think it is.\nJust then, I look up and see that I'm nearly running into the\nkid in front of me.\nAh ha! Okay! Here's proof I must have overlooked some-\nthing in the analogy. The line in front of me is contracting rather\nthan expanding. Everything must be averaging out after all. I'm\ngoing to lean to the side and see Ron walking his average two-\nmile-an-hour pace."}
{"112": "14\n\"But we're not supposed to be having lunch here,\" says one\nof the kids. \"We're not supposed to eat until we're farther down\nthe trail, when we reach the Rampage River.\"\n\"According to the schedule the troopmaster gave us, we're\nsupposed to eat lunch at 12:00 noon,\" says Ron.\n\"And it is now 12:00 noon,\" Herbie says, pointing to his\nwatch. \"So we have to eat lunch.\"\n\"But we're supposed to be at Rampage River by now and\nwe're not.\"\n\"Who cares?\" says Ron. \"This is a great spot for lunch. Look\naround.\"\nRon has a point. The trail is taking us through a park, and it\nso happens that we're passing through a picnic area. There are\ntables, a water pump, garbage cans, barbecue grills\u2014all the ne-\ncessities. (This is my kind of wilderness I'll have you know.)\n\"Okay,\" I say. \"Let's just take a vote to see who wants to eat\nnow. Anyone who's hungry, raise your hand.\"\nEveryone raises his hand; it's unanimous. We stop for lunch.\nI sit down at one of the tables and ponder a few thoughts as I\neat a sandwich. What's bothering me now is that, first of all, there\nis no real way I could operate a manufacturing plant without\nhaving dependent events and statistical fluctuations. I can't get\naway from that combination. But there must be a way to over-\ncome the effects. I mean, obviously, we'd all go out of business if\ninventory was always increasing, and throughput was always de-\ncreasing."}
{"113": "me off. I mean, sure, it shows me the effect of statistical fluctua-\ntions and dependent events in combination. But is it a balanced\nsystem? Let's say the demand on us is to walk two miles every\nhour\u2014no more, no less. Could I adjust the capacity of each kid so\nhe would be able to walk two miles per hour and no faster? If I\ncould, I'd simply keep everyone moving constantly at the pace he\nshould go\u2014by yelling, whip-cracking, money, whatever\u2014and ev-\nerything would be perfectly balanced.\nThe problem is how can I realistically trim the capacity of\nfifteen kids? Maybe I could tie each one's ankles with pieces of\nrope so that each would only take the same size step. But that's a\nlittle kinky. Or maybe I could clone myself fifteen times so I have\na troop of Alex Rogos with exactly the same trail-walking capac-\nity. But that isn't practical until we get some advancements in\ncloning technology. Or maybe I could set up some other kind of\nmodel, a more controllable one, to let me see beyond any doubt\nwhat goes on.\nI'm puzzling over how to do this when I notice a kid sitting at\none of the other tables, rolling a pair of dice. I guess he's practic-\ning for his next trip to Vegas or something. I don't mind\u2014al-\nthough I'm sure he won't get any merit badges for shooting craps\n\u2014but the dice give me an idea. I get up and go over to him.\n\"Say, mind if I borrow those for a while?\" I ask.\nThe kid shrugs, then hands them over.\nI go back to the table again and roll the dice a couple of\ntimes. Yes, indeed: statistical fluctuations. Every time I roll the\ndice, I get a random number that is predictable only within a\ncertain range, specifically numbers one to six on each die. Now"}
{"114": "Why not?\n\"Sure you can,\" I say.\nAll of a sudden Dave is interested.\n\"Hey, can I play too?\" he asks.\n\"Yeah, I guess I'll let you in,\" I tell him. \"In fact, why don't\nyou round up a couple more of the guys to help us do this.\"\nWhile they go get the others, I figure out the details. The\nsystem I've set up is intended to \"process\" matches. It does this\nby moving a quantity of match sticks out of their box, and\nthrough each of the bowls in succession. The dice determine how\nmany matches can be moved from one bowl to the next. The dice\nrepresent the capacity of each resource, each bowl; the set of\nbowls are my dependent events, my stages of production. Each\nhas exactly the same capacity as the others, but its actual yield will\nfluctuate somewhat.\nIn order to keep those fluctuations minimal, however, I de-\ncide to use only one of the dice. This allows the fluctuations to\nrange from one to six. So from the first bowl, I can move to the\nnext bowls in line any quantity of matches ranging from a mini-\nmum of one to a maximum of six.\nThroughput in this system is the speed at which matches\ncome out of the last bowl. Inventory consists of the total number\nof matches in all of the bowls at any time. And I'm going to\nassume that market demand is exactly equal to the average num-\nber of matches that the system can process. Production capacity\nof each resource and market demand are perfectly in balance. So\nthat means I now have a model of a perfectly balanced manufac-\nturing plant."}
{"115": "They nod again.\n\"How many matches do you think we can move through the\nline each time we go through the cycle?\" I ask them.\nPerplexity descends over their faces.\n\"Well, if you're able to move a maximum of six and a mini-\nmum of one when it's your turn, what's the average number you\nought to be moving?\" I ask them.\n\"Three,\" says Andy.\n\"No, it won't be three,\" I tell them. \"The mid-point between\none and six isn't three.\"\nI draw some numbers on my paper.\n\"Here, look,\" I say, and I show them this:\n123456\nAnd I explain that 3.5 is really the average of those six num-\nbers.\n\"So how many matches do you think each of you should\nhave moved on the average after we've gone through the cycle a\nnumber of times?\" I ask.\n\"Three and a half per turn,\" says Andy.\n\"And after ten cycles?\"\n\"Thirty-five,\" says Chuck.\n\"And after twenty cycles?\"\n\"Seventy,\" says Ben.\n\"Okay, let's see if we can do it,\" I say.\nThen I hear a long sigh from the end of the table. Evan looks\nat me.\n\"Would you mind if I don't play this game, Mr. Rogo?\" he\nasks."}
{"116": "They're all excited now. They're practicing rolling the die.\nMeanwhile, I set up a grid on a sheet of paper. What I plan to do\nis record the amount that each of them deviates from the average.\nThey all start at zero. If the roll of the die is a 4, 5, or 6 then I'll\nrecord\u2014respectively\u2014a gain of .5, 1.5, or 2.5. And if the roll is a\n1, 2, or 3 then I'll record a loss of-2.5, -1.5, or -.5 respectively.\nThe deviations, of course, have to be cumulative; if someone is\n2.5 above, for example, his starting point on the next turn is 2.5,\nnot zero. That's the way it would happen in the plant.\n\"Okay, everybody ready?\" I ask.\n\"All set.\"\nI give the die to Andy.\nHe rolls a two. So he takes two matches from the box and\nputs them in Ben's bowl. By rolling a two, Andy is down 1.5 from\nhis quota of 3.5 and I note the deviation on the chart.\nBen rolls next and the die comes up as a four.\n\"Hey, Andy,\" he says. \"I need a couple more matches.\"\n\"No, no, no, no,\" I say. \"The game does not work that way.\nYou can only pass the matches that are in your bowl.\"\n\"But I've only got two,\" says Ben.\n\"Then you can only pass two.\"\n\"Oh,\" says Ben.\nAnd he passes his two matches to Chuck. I record a deviation\nof-1.5 for him too.\nChuck rolls next. He gets a five. But, again, there are only\ntwo matches he can move.\n\"Hey, this isn't fair!\" says Chuck.\n\"Sure it is,\" I tell him. \"The name of the game is to move"}
{"117": "Andy shakes the die in his hand for what seems like an hour.\nEveryone is yelling at him to roll. The die goes spinning onto the\ntable. We all look. It's a six.\n\"All right!\"\n\"Way to go, Andy!\"\nHe takes six match sticks out of the box and hands them to\nBen. I record a gain of+2.5 for him, which puts his score at 1.0\non the grid.\nBen takes the die and he too rolls a six. More cheers. He\npasses all six matches to Chuck. I record the same score for Ben\nas for Andy.\nBut Chuck rolls a three. So after he passes three matches to\nDave, he still has three left in his bowl. And I note a loss of-0.5\non the chart.\nNow Dave rolls the die; it comes up as a six. But he only has\nfour matches to pass\u2014the three that Chuck just passed to him\nand one from the last round. So he passes four to Evan. I write\ndown a gain of +0.5 for him.\nEvan gets a three on the die. So the lone match on the end of\nthe table is joined by three more. Evan still has one left in his\nbowl. And I record a loss of-0.5 for Evan.\nAt the end of two rounds, this is what the chart looks like."}
{"118": "We keep going. The die spins on the table and passes from\nhand to hand. Matches come out of the box and move from bowl\nto bowl. Andy's rolls are\u2014what else?\u2014very average, no steady\nrun of high or low numbers. He is able to meet the quota and\nthen some. At the other end of the table, it's a different story.\n\"Hey, let's keep those matches coming.\"\n\"Yeah, we need more down here.\"\n\"Keep rolling sixes, Andy.\"\n\"It isn't Andy, it's Chuck. Look at him, he's got five.\"\nAfter four turns, I have to add more numbers\u2014negative\nnumbers\u2014to the bottom of the chart. Not for Andy or for Ben or\nfor Chuck, but for Dave and Evan. For them, it looks like there is\nno bottom deep enough.\nAfter five rounds, the chart looks like this:"}
{"119": "\"How am I doing, Mr. Rogo?\" Evan asks me.\n\"Well, Evan . . . ever hear the story of the Titanic?\"\nHe looks depressed.\n\"You've got five rounds left,\" I .tell him. \"Maybe you can pull\nthrough.\"\n\"Yeah, remember the law of averages,\" says Chuck.\n\"If I have to wash dishes because you guys didn't give me\nenough matches . . .\" says Evan, letting vague implications of\nthreat hang in the air.\n\"I'm doing my job up here,\" says Andy.\n\"Yeah, what's wrong with you guys down there?\" asks Ben.\n\"Hey, I just now got enough of them to pass,\" says Dave.\n\"I've hardly had any before.\"\nIndeed, some of the inventory which had been stuck in the\nfirst three bowls had finally moved to Dave. But now it gets stuck\nin Dave's bowl. The couple of higher rolls he had in the first five\nrounds are averaging out. Now he's getting low rolls just when he\nhas inventory to move.\n\"C'mon, Dave, gimme some matches,\" says Evan.\nDave rolls a one.\n\"Aw, Dave! One match!\"\n\"Andy, you hear what we're having for dinner tonight?\" asks\nBen.\n\"I think it's spaghetti,\" says Andy.\n\"Ah, man, that'll be a mess to dean up.\"\n\"Yeah, glad I won't have to do it,\" says Andy.\n\"You just wait,\" says Evan. \"You just wait 'til Dave gets some\ngood numbers for a change.\""}
{"120": ""}
{"121": "the maximum potential of each station. If this had been an actual\nplant, half of our orders\u2014or more\u2014would have been late. We'd\nnever be able to promise specific delivery dates. And if we did,\nour credibility with customers would drop through the floor.\nAll of that sounds familiar, doesn't it?\n\"Hey, we can't stop now!\" Evan is clamoring.\n\"Yea, let's keep playing,\" says Dave.\n\"Okay,\" says Andy. \"What do you want to bet this time? I'll\ntake you on.\"\n\"Let's play for who cooks dinner,\" says Ben.\n\"Great,\" says Dave.\n\"You're on,\" says Evan.\nThey roll the die for another twenty rounds, but I run out of\npaper at the bottom of the page while tracking Dave and Evan.\nWhat was I expecting? My initial chart ranged from +6 to -6. I\nguess I was expecting some fairly regular highs and lows, a nor-\nmal sine curve. But I didn't get that. Instead, the chart looks like\nI'm tracing a cross-section of the Grand Canyon. Inventory\nmoves through the system not in manageable flow, but in waves.\nThe mound of matches in Dave's bowl passes to Evan's and onto\nthe table finally\u2014only to be replaced by another accumulating\nwave. And the system gets further and further behind schedule.\n\"Want to play again?\" asks Andy.\n\"Yeah, only this time I get your seat,\" says Evan.\n\"No way!\" says Andy.\nChuck is in the middle shaking his head, already resigned to\ndefeat. Anyway, it's time to head on up the trail again.\n\"Some game that turned out to be,\" says Evan."}
{"122": "15\nFor a while, I watch the line ahead of me. As usual, the gaps\nare widening. I shake my head. If I can't even deal with this in a\nsimple hike, how am I going to deal with it in the plant?\nWhat went wrong back there? Why didn't the balanced\nmodel work? For about an hour or so, I keep thinking about what\nhappened. Twice I have to stop the troop to let us catch up.\nSometime after the second stop, I've fairly well sorted out what\nhappened.\nThere was no reserve. When the kids downstream in the\nbalanced model got behind, they had no extra capacity to make\nup for the loss. And as the negative deviations accumulated, they\ngot deeper and deeper in the hole.\nThen a long-lost memory from way back in some math class\nin school comes to mind. It has to do with something called a\ncovariance, the impact of one variable upon others in the same\ngroup. A mathematical principle says that in a linear dependency\nof two or more variables, the fluctuations of the variables down\nthe line will fluctuate around the maximum deviation established\nby any preceding variables. That explains what happened in the\nbalanced model.\nFine, but what do I do about it?\nOn the trail, when I see how far behind we are, I can tell\neveryone to hurry up. Or I can tell Ron to slow down or stop.\nAnd we close ranks. Inside a plant, when the departments get\nbehind and work-in-process inventory starts building up, people\nare shifted around, they're put on overtime, managers start to"}
{"123": "I lean to the side so I can see the line better. Ron i's no longer\nleading the troop; he's a third of the way back now. And Davey is\nahead of him. I don't know who's leading. I can't see that far.\nWell, son of a gun. The little bastards changed their marching\norder on me.\n\"Herbie, how come you're all the way back here?\" I ask.\n\"Oh, hi, Mr. Rogo,\" says Herbie as he turns around. \"I just\nthought I'd stay back here with you. This way I won't hold any-\nbody up.\"\nHe's walking backwards as he says this.\n\"Hu-huh, well, that's thoughtful of you. Watch out!\"\nHerbie trips on a tree root and goes flying onto his backside.\nI help him up.\n\"Are you okay?\" I ask.\n\"Yeah, but I guess I'd better walk forwards, huh?\" he says.\n\"Kind of hard to talk that way though.\"\n\"That's okay, Herbie,\" I tell him as we start walking again.\n\"You just enjoy the hike. I've got lots to think about.\"\nAnd that's no lie. Because I think Herbie may have just put\nme onto something. My guess is that Herbie, unless he's trying\nvery hard, as he was before lunch, is the slowest one in the troop.\nI mean, he seems like a good kid and everything. He's clearly\nvery conscientious\u2014but he's slower than all the others. (Some-\nbody's got to be, right?) So when Herbie is walking at what I'll\nloosely call his \"optimal\" pace\u2014a pace that's comfortable to him\n\u2014he's going to be moving slower than anybody who happens to\nbe behind him. Like me.\nAt the moment, Herbie isn't limiting the progress of anyone"}
{"124": "You can look at it this way, too: Herbie is advancing at his\nown speed, which happens to be slower than my potential speed.\nBut because of dependency, my maximum speed is the rate at\nwhich Herbie is walking. My rate is throughput. Herbie's rate\ngoverns mine. So Herbie really is determining the maximum\nthroughput.\nMy head feels as though it's going to take off.\nBecause, see, it really doesn't matter how fast any one of us\ncan go, or does go. Somebody up there, whoever is leading right\nnow, is walking faster than average, say, three miles per hour. So\nwhat! Is his speed helping the troop as a whole to move faster, to\ngain more throughput? No way. Each of the other boys down the\nline is walking a little bit faster than the kid directly behind him.\nAre any of them helping to move the troop faster? Absolutely not.\nHerbie is walking at his own slower speed. He is the one who is\ngoverning throughput for the troop as a whole.\nIn fact, whoever is moving the slowest in the troop is the one\nwho will govern throughput. And that person may not always be\nHerbie. Before lunch, Herbie was walking faster. It really wasn't\nobvious who was the slowest in the troop. So the role of Herbie\u2014\nthe greatest limit on throughput\u2014was actually floating through\nthe troop; it depended upon who was moving the slowest at a\nparticular time. But overall, Herbie has the least capacity for\nwalking. His rate ultimately determines the troop's rate. Which\nmeans\u2014\n\"Hey, look at this, Mr. Rogo,\" says Herbie.\nHe's pointing at a marker made of concrete next to the trail.\nI take a look. Well, I'll be ... it's a milestone! A genuine, hon-"}
{"125": "8:30 A.M. So subtracting the hour we took for lunch, that means\nwe've covered five miles ... in five hours?\nWe aren't moving at two miles per hour. We are moving at\nthe rate of one mile per hour. So with five hours to go ...\nIt's going to be DARK by the time we get there.\nAnd Herbie is standing here next to me delaying the\nthroughput of the entire troop.\n\"Okay, let's go! Let's go!\" I tell him.\n\"All right! All right!\" says Herbie, jumping.\nWhat am I going to do?\nRogo, (I'm telling myself in my head), you loser! You can't\neven manage a troop of Boy Scouts! Up front, you've got some\nkid who wants to set a speed record, and here you are stuck\nbehind Fat Herbie, the slowest kid in the woods. After an hour,\nthe kid in front\u2014if he's really moving at three miles per hour\u2014is\ngoing to be two miles ahead. Which means you're going to have\nto run two miles to catch up with him.\nIf this were my plant, Peach wouldn't even give me three\nmonths. I'd already be on the street by now. The demand was for\nus to cover ten miles in five hours, and we've only done half of\nthat. Inventory is racing out of sight. The carrying costs on that\ninventory would be rising. We'd be ruining the company.\nBut there really isn't much I can do about Herbie. Maybe I\ncould put him someplace else in the line, but he's not going to\nmove any faster. So it wouldn't make any difference.\nOr would it?\n\"HEY!\" I yell forward. \"TELL THE KID AT THE FRONT\nTO STOP WHERE HE IS!\""}
{"126": "And when I'm twice the distance of the line-up, I stop. What I've\ndone is turn the entire troop around so that the boys have exactly\nthe opposite order they had before.\n\"Now listen up!\" I say. \"This is the order you're going to stay\nin until we reach where we're going. Understood? Nobody passes\nanybody. Everybody just tries to keep up with the person in front\nof him. Herbie will lead.\"\nHerbie looks shocked and amazed. \"Me?\"\nEveryone else looks aghast too.\n\"You want him to lead?\" asks Andy.\n\"But he's the slowest one!\" says another kid.\nAnd I say, \"The idea of this hike is not to see who can get\nthere the fastest. The idea is to get there together. We're not a\nbunch of individuals out here. We're a team. And the team does\nnot arrive in camp until all of us arrive in camp.\"\nSo we start off again. And it works. No kidding. Everybody\nstays together behind Herbie. I've gone to the back of the line so\nI can keep tabs, and I keep waiting for the gaps to appear, but\nthey don't. In the middle of the line I see someone pause to\nadjust his pack straps. But as soon as he starts again, we all walk\njust a little faster and we're caught up. Nobody's out of breath.\nWhat a difference!\nOf course, it isn't long before the fast kids in the back of the\nline start their grumbling.\n\"Hey, Herpes!\" yells one of them. \"I'm going to sleep back\nhere. Can't you speed it up a little?\"\n\"He's doing the best he can,\" says the kid behind Herbie, \"so\nlay off him!\""}
{"127": "\"Herbie, this thing weighs a ton,\" I say. \"What have you got\nin here?\"\n\"Nothing much,\" says Herbie.\nI open it up and reach in. Out comes a six-pack of soda. Next\nare some cans of spaghetti. Then come a box of candy bars, a jar\nof pickles, and two cans of tuna fish. Beneath a rain coat and\nrubber boots and a bag of tent stakes, I pull out a large iron\nskillet. And off to the side is an army-surplus collapsible steel\nshovel.\n\"Herbie, why did you ever decide to bring all this along?\" I\nask.\nHe looks abashed. \"We're supposed to be prepared, you\nknow.\"\n\"Okay, let's divide this stuff up,\" I say.\n\"I can carry it!\" Herbie insists.\n\"Herbie, look, you've done a great job of lugging this stuff so\nfar. But we have to make you able to move faster,\" I say. \"If we\ntake some of the load off you, you'll be able to do a better job at\nthe front of the line.\"\nHerbie finally seems to understand. Andy takes the iron skil-\nlet, and a few of the others pick up a couple of the items I've\npulled out of the pack. I take most of it and put it into my own\npack, because I'm the biggest. Herbie goes back to the head of the\nline.\nAgain we start walking. But this time, Herbie can really\nmove. Relieved of most of the weight in his pack, it's as if he's\nwalking on air. We're flying now, doing twice the speed as a troop\nthat we did before. And we still stay together. Inventory is down."}
{"128": "tell him. \"Look, we'll have to settle for what we've got. Let's make\ncamp.\"\nThe time is now five o'clock. This means that after relieving\nHerbie of his pack, we covered about four miles in two hours.\nHerbie was the key to controlling the entire troop.\nTents are erected. A spaghetti dinner is prepared by Dave\nand Evan. Feeling somewhat guilty because I set up the rules that\ndrove them into their servitude, I give them a hand with cleaning\nup afterwards.\nDave and I share the same tent that night. We're lying inside\nit, both of us tired. Dave is quiet for a while. Then he speaks up.\nHe says, \"You know, Dad, I was really proud of you today.\"\n\"You were? How come?\"\n\"The way you figured out what was going on and kept every-\none together, and put Herbie in front\u2014we'd probably have been\non that trail forever if it hadn't been for you,\" he says. \"None of\nthe other guys' parents took any responsibility for anything. But\nyou did.\"\n\"Thanks,\" I tell him. \"Actually, I learned a lot of things to-\nday.\"\n\"You did?\"\n\"Yeah, stuff that I think is going to help me straighten out\nthe plant,\" I say.\n\"Really? Like what?\"\n\"Are you sure you want to hear about it?\"\n\"Sure I am,\" he claims.\nWe're awake for some time talking about everything. He\nhangs in there, even asks some questions. By the time we're fin-"}
{"129": "16\nDavey and I get home around 4:30 on Sunday afternoon.\nBoth of us are tired, but we're feeling pretty good in spite of the\nmiles. After I pull into the driveway, Dave hops out to open the\ngarage door. I ease the Mazda in and go around to open the trunk\nso we can get our packs.\n\"I wonder where Mom went,\" says Dave.\nI look over and notice that her car is gone.\n\"She's probably out shopping or something,\" I tell Dave.\nInside, Dave stows the camping gear while I go into the bed-\nroom to change clothes. A hot shower is going to feel absolutely\nterrific. After I wash off the great outdoors, I'm thinking, maybe\nI'll take everybody out to dinner, get us a good meal as kind of a\ncelebration of the triumphant return of father and son.\nA closet door is open in the bedroom. When I reach to shut\nit, I see that most of Julie's clothes are gone. I stand there for a\nminute looking at the empty space. Dave comes up behind me.\n\"Dad?\"\nI turn.\n\"This was on the kitchen table. I guess Mom left it.\"\nHe hands me a sealed envelope.\n\"Thanks Dave.\"\nI wait until he's gone to open it. Inside is just a short hand-\nwritten note. It says:\nAl,\nI can't handle always being last in line for you. I need\nmore of you and it's clear now that you won't change."}
{"130": "When I'm able to move, I put the note in my pocket and go\nfind Davey. I tell him I have to go across town to pick up Sharon,\nand that he's to stay here. If his mother calls, he's to ask her\nwhere she's calling from and get a number where I can call her\nback. He wants to know if something is wrong. I tell him not to\nworry and promise to explain when I get back.\nI go rocketing to my mother's house. When she opens the\ndoor, she starts talking about Julie before I can even say hello.\n\"Alex, do you know your wife did the strangest thing,\" she\nsays. \"I was making lunch yesterday when the doorbell rang, and\nwhen I opened the door Sharon was standing here on the step\nwith her little suitcase. And your wife was in the car at the curb\nthere, but she wouldn't get out and when I went down to talk to\nher, she drove away.\"\nBy now I'm in the door. Sharon runs to greet me from the\nliving room where she is watching television. I pick her up and\nshe gives me a long hug. My mother is still talking.\n\"What on earth could be wrong with her?\" my mother asks\nme.\n\"We'll talk about it later,\" I tell her.\n\"I just don't understand what\u2014\"\n\"Later, okay?\"\nThen I look at Sharon. Her face is rigid. Her eyes are frozen\nbig. She's terrified.\n\"So . . . did you have a nice visit with Grandma?\" I ask her.\nShe nods but doesn't say anything.\n\"What do you say we go home now?\"\nShe looks down at the floor."}
{"131": "\"Yes she is. She wouldn't talk to me.\"\n\"No, no, no, Sharon,\" I say. \"Your mother isn't upset with\nyou. You didn't do anything wrong.\"\n\"Then why?\" she asks.\nI say, \"Why don't we wait until we get home. I'll explain it to\nboth you and your brother then.\"\nI think that explaining the situation to both of the kids at the\nsame time turns out to be easier on me than on them. I've always\nbeen reasonably adept at maintaining the outward illusion of con-\ntrol in the midst of chaos. I tell them Julie has simply gone away\nfor a little while, maybe only a day or so. She'll be back. She just\nhas to get over a few things that are upsetting and confusing her.\nI give them all the standard reassurances: your mom still loves\nyou; I still love you; there was nothing that either of you could\nhave done; everything will work out for the best. For the most\npart, both of them sit there like little rocks. Maybe they're reflect-\ning back what I'm giving them.\nWe go out and get a pizza for dinner. That normally would\nbe kind of a fun thing. Tonight, it's very quiet. Nobody has any-\nthing to say. We mechanically chew and then leave.\nWhen we get back, I make both of the kids do homework for\nschool. I don't know if they do it or not. I go to the phone, and\nafter a long debate with myself; I try to make a couple of calls.\nJulie doesn't have any friends in Bearington. None that I\nknow of. So it would be useless to try to call the neighbors. They\nwouldn't know anything, and the story about us having problems\nwould spread instantly.\nInstead, I try calling Jane, the friend from the last place we"}
{"132": "a camping trip with Dave. I was wondering if you had heard from\nher.\"\nImmediately he's spreading the alarm to Julie's mother. She\ngets on the phone.\n\"Why did she leave?\" she asks.\n\"I don't know.\"\n\"Well, I know the daughter we raised, and she wouldn't just\nleave without a very good reason,\" says Julie's mother.\n\"She just left me a note saying she had to get away for\nawhile.\"\n\"What did you do to her?\" yells her mother.\n\"Nothing!\" I plead, feeling like a liar in the onslaught.\nThen her father gets back on the phone and asks if I've\ntalked to the police. He suggests that maybe she was kidnapped. I\ntell him that's highly unlikely, because my mother saw her drive\naway and nobody had a gun to her head.\nFinally I say, \"If you hear from her, would you please have\nher give me a call? I'm very worried about her.\"\nAn hour later, I do call the police. But, as I expected, they\nwon't help unless I have some evidence that something criminal\nhas taken place. I go and put the kids to bed.\nSometime after midnight, I'm staring at the dark bedroom\nceiling and I hear a car turning into the driveway. I leap out of\nbed and run to the window. By the time I get there, the head-\nlights are arcing back toward the street. It's just a stranger turn-\ning around. The car drives away."}
{"133": "17\nMonday morning is a disaster.\nIt starts with Davey trying to make breakfast for himself and\nSharon and me. Which is a nice, responsible thing to do, but he\ntotally screws it up. While I'm in the shower, he attempts pan-\ncakes. I'm midway through shaving when I hear the fight from\nthe kitchen. I rush down to find Dave and Sharon pushing each\nother. There is a skillet on the floor with lumps of batter, black on\none side and raw on the other, splattered.\n\"Hey! What's going on?\" I shout.\n\"It's all her fault!\" yells Dave pointing at his sister.\n\"You were burning them!\" Sharon says.\n\"I was not!\"\nSmoke is fuming off the stove where something spilled. I step\nover to shut it off.\nSharon appeals to me. \"I was just trying to help. But he\nwouldn't let me.\" Then she turns to Dave. \"Even / know how to\nmake pancakes.\"\n\"Okay, because both of you want to help, you can help clean\nup,\" I say.\nWhen everything is back in some semblance of order, I feed\nthem cold cereal. We eat another meal in silence.\nWith all the disruption and delay. Sharon misses her school\nbus. I get Davey out the door, and go looking for her so I can\ndrive her to school. She's lying down on her bed.\n\"Ready, whenever you are, Miz Rogo.\"\n\"I can't go to school,\" she says."}
{"134": "After a minute, she says, \"I guess I'll go to school.\"\nI give her a squeeze and say, \"Atta way, kid. I knew you'd do\nthe right thing.\"\nBy the time I get both kids to school and myself to work, it's\npast nine o'clock. As I walk in, Fran waves a message slip at me. I\ngrab it and read it. It's from Hilton Smyth, marked \"urgent\" and\ndouble underlined.\nI call him.\n\"Well, it's about time,\" says Hilton. \"I tried to reach you an\nhour ago.\"\nI roll my eyes. \"What's the problem, Hilton?\"\n\"Your people are sitting on a hundred sub-assemblies I\nneed,\" says Smyth.\n\"Hilton, we're not sitting on anything,\" I say.\nHe raises his voice. \"Then why aren't they here? I've got a\ncustomer order we can't ship because your people dropped the\nball!\"\n\"Just give me the particulars, and I'll have somebody look\ninto it,\" I tell him.\nHe gives some reference numbers and I write them down.\n\"Okay, I'll have somebody get back to you.\"\n\"You'd better do more than that, pal,\" says Hilton. \"You'd\nbetter make sure we get those sub-assemblies by the end of the\nday\u2014and I mean all 100 pieces, not 87, not 99, but all of them.\nBecause I'm not going to have my people do two setups for final\nassembly on account of your lateness.\"\n\"Look, we'll do our best,\" I tell him, \"but I'm not going to\nmake promises.\""}
{"135": "I hang up.\n\"Weird,\" I mumble.\nI talk to Fran. She calls Bob Donovan for me and then noti-\nfies the staff that there will be a meeting at ten o'clock. Donovan\ncomes in and I ask him to have an expediter see what's holding\nup the job for Smyth's plant. Almost gritting my teeth as I say it, I\ntell him to make sure the sub-assemblies go out today. After he's\ngone, I try to forget about the call, but I can't. Finally, I go ask\nFran if anything has come in recently that mentions Hilton\nSmyth. She thinks for a minute, then reaches for a folder.\n\"This memo just came in on Friday,\" she says. \"It looks like\nMr. Smyth got a promotion.\"\nI take the memo she hands me. It's from Bill Peach. It's an\nannouncement that he's named Smyth to the newly-created posi-\ntion of division productivity manager. The appointment is effec-\ntive at the end of this week. The job description says that all plant\nmanagers will now report on a dotted line to Smyth, who will\n\"give special attention to manufacturing-productivity improve-\nment with emphasis on cost reduction.\"\nAnd I start to sing, \"Oh, what a beautiful morning. . . !\"\nWhatever enthusiasm I expected from the staff with regard\nto my education over the weekend . . . well, I don't get it.\nMaybe I thought all I had to do was walk in and open my mouth\nto reveal my discoveries, and they'd all be instantly converted by\nthe obvious Tightness. But it doesn't work that way. We\u2014Lou,\nBob, Stacey, and Ralph Nakamura, who runs data processing for\nthe plant\u2014are in the conference room. I'm standing in front next\nto an easel which holds a big pad of paper, sheet after sheet of"}
{"136": "They glance at each other.\n\"Come on,\" I say. \"This is like I just proved two and two\nequals four and you don't believe me.\" I look straight at Lou.\n\"What's the problem you're having?\"\nLou sits back and shakes his head. \"I don't know, Al. It's just\nthat . . . well, you said how you figured this out by watching a\nbunch of kids on a hike in the woods.\"\n\"So what's wrong with that?\"\n\"Nothing. But how do you know these things are really go-\ning on out there in the plant?\"\nI flip back a few sheets on the easel until I find the one with\nthe names of Jonah's two phenomena written on it.\n\"Look at this: do we have statistical fluctuations in our opera-\ntions?\" I ask, pointing to the words.\n\"Yes, we do,\" he says.\n\"And do we have dependent events in our plant?\" I ask.\n\"Yes,\" he says again.\n\"Then what I've told you has to be right,\" I say.\n\"Now hold on a minute,\" says Bob. \"Robots don't have statis-\ntical fluctuations. They always work at the same pace. That's one\nof the reasons we bought the damn things\u2014consistency. And I\nthought the main reason you went to see this Jonah guy was to\nfind out what to do about the robots.\"\n\"It's okay to say that fluctuations in cycle time for a robot\nwould be almost flat while it was working,\" I tell him. \"But we're\nnot dealing just with a robotic operation. Our other operations\ndo have both phenomena. And, remember, the goal isn't to make\nthe robots productive; it's to make the whole system productive."}
{"137": "through two more departments before the sub-assemblies are\ncomplete and ready for shipment.\n\"Can we get them out today?\" I ask.\n\"It's going to be close, but we can try,\" says Fred. \"The truck\nshuttle leaves at five o'clock.\"\nThe shuttle is a private trucking service that all the plants in\nthe division use to move parts back and forth.\n\"Five o'clock is the last run of the day that we can use to\nreach Smyth's plant,\" says Bob. \"If we don't make that trip, the\nnext shuttle won't be until tomorrow afternoon.\"\n\"What has to be done?\" I ask.\n\"Peter Schnell's department has to do some fabricating.\nThen the pieces have to be welded,\" says Fred. \"We're going to\nset up one of the robots to do the welds.\"\n\"Ah, yes, the robots,\" I say. \"You think we can do it?\"\n\"According to the quotas, Pete's people are supposed to give\nus the parts for twenty-five units every hour,\" says Fred. \"And I\nknow the robot is capable of welding twenty-five units of this sub-\nassembly per hour.\"\nBob asks about moving the pieces to the robot. In a normal\nsituation, the pieces finished by Pete's people probably would be\nmoved to the robot only once a day, or maybe not until the entire\nbatch was finished. We can't wait that long. The robot has to\nbegin its work as soon as possible.\n\"I'll make arrangements to have a materials handler stop at\nPete's department every hour on the hour,\" says Fred.\n\"Okay,\" says Bob. \"How soon can Pete start?\"\nFred says, \"Pete can start on the job at noon, so we've got five"}
{"138": "now. They went at eleven-thirty. So they'll start at twelve. And the\nrobot will be set up by one o'clock, when the materials handler\nwill make the first transfer.\"\nI take some paper and a pencil and start sketching a simple\nschedule.\n\"The output has to be one hundred pieces by five o'clock\u2014\nno less than that. Hilton says he won't accept a partial shipment.\nSo if we can't do the whole job, then I don't want us to ship\nanything,\" I say. \"Now Pete's people are supposed to produce at\nthe rate of twenty-five pieces per hour. But that doesn't mean\nthey'll always have twenty-five at the end of every hour. Some-\ntimes they'll be a few pieces short, sometimes they'll be a few\nahead.\"\nI look around; everyone is with me.\n\"So we've got statistical fluctuations going on,\" I say. \"But\nwe're planning that from noon until four o'clock, Pete's depart-\nment should have averaged an output of one hundred pieces.\nThe robot, on the other hand, is supposed to be more precise in\nits output. It will be set up to work at the rate of twenty-five pieces\nper hour\u2014no more, no less. We also have dependent events, be-\ncause the robot cannot begin its welding until the materials han-\ndler has delivered the pieces from Pete's department.\"\n\"The robot can't start until one o'clock,\" I say, \"but by five\no'clock when the truck is ready to leave, we want to be loading\nthe last piece into the back. So, expressed in a diagram, this is\nwhat is supposed to happen . . .\"\nI show them the finished schedule, which looks like this:"}
{"139": "\"Okay, I want Pete to keep a log of exactly how many parts\nare actually completed by his department hour by hour,\" I say.\n\"And I want Fred to keep the same type of log for the robot. And\nremember: no cheating. We need the real numbers. Okay?\"\n\"Sure, no problem,\" says Fred.\n\"By the way, do you actually think we'll be able to ship one\nhundred pieces today?\" I ask.\n\"I guess it's up to Pete,\" says Bob. \"If he says he can do it, I\ndon't see why not.\"\n\"Tell you what,\" I say to Bob. \"I'll bet you ten bucks we don't\nship today.\"\n\"You serious?\" asks Bob.\n\"Sure I am.\"\n\"Okay, you're on,\" says Bob. \"Ten bucks.\"\nWhile everyone else is at lunch, I call Hilton Smyth. Hilton is\nat lunch as well, but I leave a message for him. I tell his secretary\nthe sub-assemblies will definitely arrive at his plant tomorrow, but\nthat's the best we can do\u2014unless Hilton wants to pay for a special\nshipment tonight. (Knowing his concern for holding down costs,\nI'm sure Hilton won't want to shell out anything extra.)\nAfter that call, I sit back and try to think about my marriage\nand what to do. Obviously, there has been no news from Julie.\nI'm mad as hell that she took off\u2014I'm also very worried about\nher. But what can I do? I can't cruise the streets looking for her.\nShe could be anywhere; I just have to be patient. Eventually I\nshould hear from her. Or her lawyer. Meanwhile, there are two\nkids who have to be taken care of. Well, for all practical purposes,\nwe'd better make that three kids."}
{"140": "When she leaves, I pick up the phone again.\n\"Hello, Mom? It's Alex.\"\n\"Have you heard from Julie yet?\" she asks.\n\"No, I haven't,\" I say. \"Listen, Mom, would you mind stay-\ning with me and the kids until Julie gets back?\"\nAt two o'clock I slip out to pick up my mother and take her\nto the house before the kids get home from school. When I arrive\nat her house, she's at the door with two suitcases and four card-\nboard boxes filled with half of her kitchen.\n\"Mom, we've already got pots and pans at my house,\" I tell\nher.\n\"They're just not the same as mine,\" she says.\nSo we load the trunk. I take her and her pots and pans over\nto the house and unload. She waits for the kids to come home\nfrom school, and I race back to the plant.\nAround four o'clock, at the end of first shift, I go down to\nBob Donovan's office to find out what the story is on Smyth's\nshipment. He's waiting for me.\n\"Well, well, well. Good afternoon!\" says Bob as I open the\ndoor and walk in. \"How nice of you to drop by!\"\n\"What are you so happy about?\" I ask him.\n\"I'm always happy when people who owe me money drop\nby,\" says Bob.\n\"Oh, is that right?\" I ask him. \"What makes you think any-\nbody owes you money?\"\nBob holds out his hand and wiggles his fingers. \"Come on!\nDon't tell me you forgot about the bet we made! Ten bucks, re-\nmember? I just talked to Pete and his people are indeed going to"}
{"141": "flashes. Coming the other way are two guys. Just as they pass the\nwelding area, they stop and give a little cheer.\n\"We beat the robot! We beat the robot!\" they say.\n\"Must be from Pete's department,\" says Bob.\nWe smile as we pass them. They didn't really beat anything,\nof course, but what the hell. They look happy. Bob and I con-\ntinue on to Pete's office, which is a little steel-sided shack among\nthe machines.\n\"Hello there,\" says Pete as we walk in. \"We got that rush job\ndone for you today.\"\n\"Good, Pete. But do you have that log sheet you were sup-\nposed to keep,\" I ask him.\n\"Yes, I do,\" says Pete. \"Now where did I put it?\"\nHe sorts through the papers on his desk, talking as he hunts\nfor it.\n\"You should have seen my people this afternoon. I mean,\nthey really moved. I went around and told them how important\nthis shipment is, and they really put themselves into it. You know\nhow things usually slow down a little at the end of a shift. But\ntoday they hustled. They were proud when they walked out of\nhere today.\"\n\"Yeah, we noticed,\" says Bob.\nHe puts the log sheet down on top of a table in front of us.\n\"There you are,\" he says.\nWe read it."}
{"142": "\"Okay, so you only got nineteen pieces done in the first\nhour,\" I say.\n\"Well, it took us a little longer to get organized, and one guy\nwas late coming back from lunch,\" says Pete. \"But at one o'clock\nwe had a materials handler take the nineteen over to the robot so\nit could get started.\"\n\"Then from one to two, you still missed the quota by four\npieces,\" says Bob.\n\"Yeah, but so what?\" says Pete. \"Look what happened from\ntwo o'clock to three: we beat the quota by three pieces. Then\nwhen I saw we were still behind, I went around and told every-\none how important it was for us to get those hundred pieces done\nby the end of the shift.\"\n\"So everyone went a little faster,\" I say.\n\"That's right,\" says Pete. \"And we made up for the slow\nstart.\"\n\"Yeah, thirty-two pieces in the last hour,\" says Bob. \"So what\ndo you say, Al?\"\n\"Let's go see what's happening with the robot,\" I say.\nAt five minutes past five o'clock, the robot is still turning out\nwelded sub-assemblies. Donovan is pacing. Fred walks up.\n\"Is that truck going to wait?\" asks Bob.\n\"I asked the driver, and he says he can't. He's got other stops\nto make and if he waits for us, he'll be late all night,\" says Fred.\nBob turns to the machine. \"Well, what the heck is wrong with\nthis stupid robot? It's got all the parts it needs.\"\nI tap him on the shoulder.\n\"Here,\" I say. \"Look at this.\""}
{"143": "Output = 90 pcs.\n\"Every time Pete's area got behind, it was passed on to the\nrobot,\" I say. \"But when Pete delivered 28 pieces, the robot could\nstill only do twenty-five. That meant that when the final delivery\nof thirty-two pieces arrived at four o'clock, the robot still had\nthree pieces to work on from the last batch. So it couldn't start on\nthe final batch right away.\"\n\"Okay, I see now,\" says Bob.\nFred says, \"You know, the most Pete was ever behind was ten\npieces. Kind of funny how that's exactly the number of pieces we\nended up short.\"\n\"That's the effect of the mathematical principle I was trying\nto explain this morning,\" I say. \"The maximum deviation of a\npreceding operation will become the starting point of a subse-\nquent operation.\"\nBob reaches for his wallet.\n\"Well, I guess I owe you ten bucks,\" he says to me.\n\"Tell you what,\" I say. \"Instead of paying me, why don't you\ngive the money to Pete so he can spring for a round of coffee or\nsomething for the people in his department\u2014just a little way to\nsay thanks for the extra effort this afternoon.\""}
{"144": "\"Yeah, right, that's a good idea,\" says Bob. \"Listen, sorry we\ncouldn't ship today. Hope it doesn't get us in trouble.\"\n\"We can't worry about it now,\" I tell him. \"The gain we\nmade today is that we learned something. But I'll tell you one\nthing: we've got to take a close look at our incentives here.\"\n\"How come?\" asks Bob.\n\"Don't you see? It didn't matter that Pete got his hundred\npieces done, because we still couldn't ship,\" I say. \"But Pete and\nhis people thought they were heroes. Ordinarily, we might have\nthought the same thing. That isn't right.\""}
{"145": "18\nWhen I get home that evening, both of the kids greet me at\nthe door. My mother is in the background, with steam pouring\nout of the kitchen. I presume it has something to do with dinner\nand that she has everything under control. In front of me,\nSharon's face is beaming up at me.\n\"Guess what!\" she says.\n\"I give up,\" I say.\n\"Mommy called on the phone,\" Sharon says.\n\"She did!\" I say.\nI glance up at my mother. She shakes her head.\n\"Davey answered the phone,\" she says. \"I didn't talk to her.\"\nI look down at Sharon. \"So what did Mommy say?\"\n\"She said she loved Davey and me,\" says Sharon.\n\"And she said she would be away for a while,\" adds Davey.\n\"But that we shouldn't worry about her.\"\n\"Did she say when she would be coming back?\" I ask.\n\"I asked her that,\" says Davey. \"But she said she couldn't say\nright now.\"\n\"Did you get a phone number so I can call her back?\" I ask\nhim.\nHe looks down at the floor.\n\"David! You were supposed to ask her for the number if she\ncalled!\"\nHe mumbles, \"I did, but . . . she didn't want to give it to\nme.\"\n\"Oh,\" I say."}
{"146": "work on time. By 8:30, Bob, Stacey, Lou, and Ralph are in my\noffice, and we're talking about what happened yesterday. Today,\nI find them much more attentive. Maybe it's because they've seen\nthe proof of the idea take place on their own turf, so to speak.\n\"This combination of dependency and fluctuations is what\nwe're up against every day,\" I tell them. \"I think it explains why\nwe have so many late orders.\"\nLou and Ralph are examining the two charts we made yes-\nterday. \"What would have happened if the second operation\nhadn't been a robot, if it had been some kind of job with people?\"\nasks Lou.\n\"We would have had another set of statistical fluctuations to\ncomplicate things,\" I say. \"Don't forget we only had two opera-\ntions here. You can imagine what happens when we've got de-\npendency running through ten or fifteen operations, each with\nits own set of fluctuations, just to make one part. And some of our\nproducts involve hundreds of parts.\"\nStacey is troubled. She asks, \"Then how can we ever control\nwhat's going on out there?\"\nI say, \"That's the billion-dollar question: how can we control\nthe fifty-thousand or\u2014who knows?\u2014maybe it's fifty-million vari-\nables which exist in this plant?\"\n\"We'd have to buy a new super computer just to keep track\nof all of them,\" says Ralph.\nI say, \"A new computer wouldn't save us. Data management\nalone isn't going to give us more control.\"\n\"What about longer lead times?\" asks Bob.\n\"Oh, you really think longer lead time would have guaran-"}
{"147": "pacity of a resource in isolation. Its true productive capacity de-\npends upon where it is in the plant. And trying to level capacity\nwith demand to minimize expenses has really screwed us up. We\nshouldn't be trying to do that at all.\"\n\"But that's what everybody else does,\" says Bob.\n\"Yes, everybody does. Or claims to. As we now can see, it's a\nstupid thing to try,\" I say.\n\"So how do other manufacturers survive?\" asks Lou.\nI tell him I was wondering that myself. What I suspect is that\nas a plant comes close to being balanced through the efforts of\nengineers and managers doing the wrong things, events head\ntoward a crisis and the plant is very quickly un balanced by shift-\ning workers or by overtime or by calling back some people from\nlayoff. The survival incentive overrides false beliefs.\n\"Okay, but again, what are we going to do?\" asks Bob. \"We\ncan't hire without division approval. And we've even got a policy\nagainst overtime.\"\n\"Maybe it's time to call Jonah again,\" says Stacey.\nAnd I say, \"I think maybe you're right.\"\nIt takes Fran half an hour to locate the area of the world\nwhere Jonah happens to be today, and another hour passes be-\nfore Jonah can get to the phone to talk to us. As soon as he's on\nthe line, I have another secretary round up the staff again and\ncorral them in my office so we can hear him on a speaker phone.\nWhile they're coming in, I tell Jonah about the hike with Herbie\nwhere I discovered the meaning of what he was telling me, and\nwhat we've learned about the effects of the two phenomena in the\nplant."}
{"148": "I whisper to everybody to start taking some notes on this.\n\"A bottleneck,\" Jonah continues, \"is any resource whose ca-\npacity is equal to or less than the demand placed upon it. And a\nnon-bottleneck is any resource whose capacity is greater than the\ndemand placed on it. Got that?\"\n\"Right,\" I tell him.\n\"Once you have recognized these two types of resources,\"\nsays Jonah, \"you will begin to see vast implications.\"\n\"But, Jonah, where does market demand come in?\" Stacey\nasks. \"There has to be some relationship between demand and\ncapacity.\"\nHe says, \"Yes, but as you already know, you should not bal-\nance capacity with demand. What you need to do instead is bal-\nance the flow of product through the plant with demand from the\nmarket. This, in fact, is the first of nine rules that express the\nrelationships between bottlenecks and non-bottlenecks and how\nyou should manage your plant. So let me repeat it for you: Bal-\nance flow, not capacity.\"\nStacey is still puzzled. She says, \"I'm not sure I understand.\nWhere do the bottlenecks and non-bottlenecks come into the pic-\nture?\"\nJonah says, \"Let me ask you: which of the two types of re-\nsources determines the effective capacity of the plant?\"\n\"It would have to be the bottleneck,\" she says.\nI say, \"That's right. It's like the kid on that hike last weekend\n\u2014Herbie. He had the least capacity and he was the one who\nactually determined how fast the troop as a whole could move.\"\n\"So where should you balance the floor?\" asks Jonah."}
{"149": "\"No, bottlenecks are not necessarily bad\u2014or good,\" says Jo-\nnah, \"they are simply a reality. What I am suggesting is that\nwhere they exist, you must then use them to control the flow\nthrough the system and into the market.\"\nThat makes sense to me as I'm listening, because I'm remem-\nbering how I used Herbie to control the troop during the hike.\n\"Now I have to run,\" says Jonah, \"because you caught me\nduring a ten-minute break in a presentation.\"\nI jump in. \"Jonah, before you go\u2014!\"\n\"Yes?\"\n\"What's our next step?\"\nHe says, \"Well, first of all, does your plant have any bottle-\nnecks?\"\n\"We don't know,\" I tell him.\n\"Then that's your next step,\" he says. \"You have to find this\nout, because it makes an enormous difference in how you manage\nyour resources.\"\n\"How do we find the bottlenecks?\" says Stacey.\n\"It's very simple, but it would take a few minutes to explain.\nLook, try to figure that out for yourselves,\" says Jonah. \"It's re-\nally easy to do if you think about it first.\"\nI say, \"Okay, but. . . .\"\n\"Good-bye for now,\" he says. \"Call me when you know if you\nhave a bottleneck.\"\nThe speaker phone issues a click, followed by a fuzzy hum.\n\"Well . . . what now?\" asks Lou.\n\"I guess we look at all our resources,\" I say, \"and compare\nthem against market demand. If we find one in which demand is"}
{"150": "\"And if that's the case?\" asks Lou.\n\"I don't know,\" I tell him. \"I guess the first thing to do is\nfind out if we've got a bottleneck.\"\n\"So we go look for Herbie,\" says Ralph. \"If he's out there.\"\n\"Yeah, quick, before we talk ourselves to death,\" says Bob.\nI walk into the conference room a few days later and there's\npaper everywhere. The main table is covered with computer\nprint-outs and binders. Over in the corner, a data terminal has\nbeen installed; next to it, a printer is churning out even more\npaper. The wastebaskets are full. So are all the ashtrays. The litter\nof white styrofoam coffee cups, empty sugar packets and creamer\ncontainers, napkins, candy bar and cracker wrappers, and so on\nis scattered about. What has happened is the place has been\nturned into our headquarters in the search for Herbie. We have\nnot found him yet. And we're getting tired.\nSitting at the far end of the main table is Ralph Nakamura.\nHe and his data processing people, and the system data base they\nmanage, are essential to the search.\nRalph does not look happy as I come in. He's running his\nskinny fingers through his thinning black hair.\n\"This isn't the way it's supposed to be,\" he's saying to Stacey\nand Bob.\n\"Ahh, perfect timing,\" says Ralph when he sees me. \"Do you\nknow what we just did?\"\n\"You found Herbie?\" I say.\nRalph says, \"No, we just spent two and a half hours calculat-\ning the demand for machines that don't exist.\"\n\"Why'd you do that?\""}
{"151": "ket demand placed on it. To find out if we've got one then, we\nconcluded we first would have to know the total market demand\nfor products coming out of this plant. And, second, we would\nhave to find out how much time each resource has to contribute\ntoward filling the demand. If the number of available hours for\nproduction (discounting maintenance time for machines, lunch\nand breaks for people, and so on) for the resource is equal to or\nless than the hours demanded, then we know we've found our\nHerbie.\nGetting a fix on the total market demand is a matter of pull-\ning together data which we have on hand anyway\u2014the existing\nbacklog of customer orders, and the forecast for new product and\nspare parts. It's the complete product mix for the entire plant,\nincluding what we \"sell\" to other plants and divisions in the com-\npany.\nHaving done that, we're now in the process of calculating the\nhours each \"work center\" has to contribute. We're defining a\nwork center as any group of the same resources. Ten welders with\nthe same skills constitute a work center. Four identical machines\nconstitute another. The four machinists who set up and run the\nmachines are still another, and so on. Dividing the total of work\ncenter hours needed, by the number of resources in it, gives us\nthe relative effort per resource, a standard we can use for com-\nparison.\nYesterday, for instance, we found the demand for injection\nmolding machines is about 260 hours a month for all the injec-\ntion molded parts that they have to process. The available time\nfor those machines is about 280 hours per month, per resource."}
{"152": "\"Or years,\" mumbles Bob.\nI sit down and close my eyes for a second. When I open my\neyes, they're all looking at me.\n\"Obviously, we're not g\u00b0ing to have time for that,\" I say.\n\"We've only got ten weeks now to make something happen be-\nfore Peach blows the whistle. I know we're on the right track, but\nwe're still just limping along here. We've got to accept the fact\nwe're not going to have perfect data to work with.\"\nRalph says, \"Then I have to remind you of the old data\nprocessing aphorism: Garbage in, garbage out.\"\n\"Wait a minute,\" I say. \"Maybe we're being a little too\nmethodical. Searching a data base isn't the only way to find an-\nswers. Can't we come up with some other faster way to isolate the\nbottleneck\u2014or at least identify the candidates? When I think\nback to the model of the boys on the hike, it was obvious who the\nslower kids were on the trail. Doesn't anybody have any hunches\nwhere the Herbie might be in the plant?\"\n\"But we don't even know if we've got one yet,\" says Stacey.\nBob has his hands on his hips. His mouth is half open as if he\nmight say something. Finally, he does.\n\"Hell, I've been at this plant for more than twenty years.\nAfter that much time, I know where the problems usually seem to\nstart,\" he says. \"I think I could put together a list of areas where\nwe might be short on capacity; at least that would narrow the\nfocus for us. It might save some time.\"\nStacey turns to him. \"You know, you just gave me an idea. If\nwe talk to the expediters. They could probably tell us which parts\nthey're missing most of the time, and in which departments they"}
{"153": "Bob, Ralph, and Stacey stare at me.\n\"Don't you see?\" I ask them. \"If we've got a Herbie, it's\nprobably going to have a huge pile of work-in-process sitting in\nfront of it.\"\n\"Yeah, but we got huge piles all over the place out there,\"\nsays Bob.\n\"Then we find the biggest one,\" I say.\n\"Right! That's got to be another sure sign,\" says Stacey.\nI turn and ask, \"What do you think, Ralph?\"\n\"Well, it all sounds worth a try,\" says Ralph. \"Once you've\nnarrowed the field to maybe three of four work centers, it won't\ntake long for us to check your findings against the historical data\njust to be sure.\"\nBob looks at Ralph and says in a kidding voice, \"Yeah, well,\nwe've all seen how good that is.\"\nBut Ralph doesn't take it in a kidding way. He looks embar-\nrassed.\n\"Hey, I can only work with what I've got,\" he says. \"What do\nyou want me to do?\"\n\"Okay, the important thing is that we have new methods to\ntry,\" I say. \"Let's not waste time pinning the blame on bad data.\nLet's get to work.\"\nFueled by the energy of new ideas, we go to work, and the\nsearch goes quickly ... so quickly, in fact, that what we discover\nmakes me feel as though we've run ourselves straight into a wall.\n\"This is it. Hello, Herbie,\" says Bob.\nIn front of us is the NCX-10.\n\"Are you sure this is a bottleneck?\" I ask."}
{"154": "\"It is,\" says Bob. \"It's the lowest-cost, highest-rate means we\nhave of producing these particular parts.\"\n\"So why is this a bottleneck?\"\n\"This is the only one like it we've got,\" he says.\n\"Yes, I know that,\" I say, and I stare at him until he explains.\n\"See, this machine here is only about two years old. Before\nwe installed it, we used other machines to do what it does. But\nthis machine can do all the operations that used to take three\ndifferent machines,\" says Bob.\nHe tells me about how they used to process these parts using\nthe three separate types of machines. In one typical instance, the\nprocess times per part were something like two minutes on the\nfirst machine, eight minutes on the second, and four minutes on\nthe third\u2014a grand total of fourteen minutes per part. But the\nnew NCX-10 machine can do all three processes in ten minutes\nper part.\nI say, \"You're telling me we're saving four minutes per part.\nDoesn't that mean we're producing more parts per hour than we\nwere? How come we've got so much inventory stacked up for this\nthing?\"\n\"With the old way, we had more machines,\" he says. \"We\nhad two of the first type, five of the second type, and three of the\nthird type.\"\nI nod, understanding now. \"So you could do more parts,\nen though it took you longer per part. Then why did we buy\ne NCX-10?\"\n\"Each of the other machines had to have a machinist to run\nBob says. \"The NCX-10 only needs two guys on it for setups."}
{"155": "dollars more with somebody else,\" says Bob. \"And we can't seem\nto attract anybody good with the wages we offer.\"\n\"Well why don't we pay more for people on this equipment?\"\n\"The union,\" says Bob. \"We'd get complaints, and the union\nwould want us to up the pay-grade for all the setup people.\"\nI take a last look.\n\"Okay, so much for this,\" I say.\nBut that isn't all. The two of us walk to the other side of the\nplant where Bob gives me a second introduction.\n\"Meet Herbie Number Two: the heat-treat department,\" says\nBob.\nThis one looks more like what you might think of in terms of\nan industrial Herbie. It's dirty. It's hot. It's ugly. It's dull. And it's\nindispensable.\nHeat-treat basically is a pair of furnaces ... a couple of\ngrimy, dingy, steel boxes, the insides of which are lined with ce-\nramic blocks. Gas burners raise the internal temperatures to the\n1500-degree-Fahrenheit range.\nCertain parts, after they've been machined or cold-worked\nor whatever at ordinary temperatures, can't be worked on any-\nmore until they've been treated with heat for an extended period\nof time. Most often, we need to soften the metal, which becomes\nvery hard and brittle during processing, so it can have more\nmachining done to it.\nSo the furnace operators put in the parts, from a dozen or\nless to a couple of hundred, then they fire up the thing and cook\nthe parts in there for a long time\u2014anywhere from six hours to\nsixteen hours. And afterwards, the parts always have to go"}
{"156": "\"Yeah, sometimes we are. But sometimes even if we do a full\nbatch in number, it's not enough to fill the furnace.\"\n\"The batches are too small?\"\n\"Or too big in size, and we have to run a second heat to\nhandle the pieces that wouldn't fit in the first. It just never seems\nto work out,\" says Bob. \"You know, a couple of years ago, there\nwas a proposal to add a third furnace, on account of the prob-\nlems.\"\n\"What happened to it?\"\n\"It was killed at the division level. They wouldn't authorize\nthe funds because of low efficiencies. They told us to use the\ncapacity we've got. Then maybe they'd talk expansion. Besides,\nthere was all kinds of noise about how we've got to save energy\nand how another furnace would burn twice as much fuel and all\nthat.\"\n\"Okay, but if we filled the furnace every time, would we have\nenough capacity to meet demand?\" I ask.\nBob laughs.\n\"I don't know. We've never done it that way before.\"\nOnce upon a time, I had an idea for doing to the plant essen-\ntially what I did with the boys on the hike. I thought the best\nthing to do would be to reorganize everything so the resource\nwith the least capacity would be first in the routings. All other\nresources would have gradual increases in capacity to make up\nfor the statistical fluctuations passed on through dependency.\nWell, the staff and I meet right after Bob and I get back to\nthe office, and it's pretty obvious, awfully damn quick, that my\ngrand plan for the perfect un balanced plant with Herbie in front"}
{"157": "in the sequence, then maybe we can increase their capacities.\nWe'll make them into non-bottlenecks.\"\nStacey asks, \"But what about the step-up in capacity from\nbeginning to end?\"\n\"We'll reorganize . . . we'll decrease capacity at the head of\nproduction and increase it each stage on through,\" I suggest.\n\"Al, we're not just talking about moving people around. How\ncan we add capacity without adding equipment?\" asks Bob. \"And\nif we're talking about equipment, we're getting ourselves into\nsome major capital. A second furnace on heat-treat, and possibly\na second n/c machine . . . brother, you're talking megabucks.\"\n\"The bottom line,\" says Lou, \"is that we don't have the\nmoney. If we think we can go to Peach and ask him for excess\ncapacity for a plant that currently isn't making money in the mid-\ndle of one of the worst years in the company's history . . . well,\nexcuse my French, but we're out of our goddamned minds.\""}
{"158": "19\nMy mother and the kids and I are having dinner that eve-\nning when Mom says to me, \"Aren't you going to eat your peas,\nAlex?\"\nI tell her, \"Mom, I'm an adult now. It's my option whether\nor not to eat my peas.\"\nShe looks hurt.\nI say, \"Sorry. I'm a little depressed tonight.\"\n\"What's wrong, Dad?\" asks Davey.\n\"Well . . . it's kind of complicated,\" I say. \"Let's just finish\ndinner. I've got to leave for the airport in a few minutes.\"\n\"Are you going away?\" asks Sharon.\n\"No, I'm just going to pick up somebody,\" I say.\n\"Is it Mommy?\" asks Sharon.\n\"No, not Mommy. I wish it could be.\"\n\"Alex, tell your children what's bothering you,\" says my\nmother. \"It affects them, too.\"\nI look at the kids and realize my mother's right. I say, \"We\nfound out we've got some problems at the plant which we might\nnot be able to solve.\"\n\"What about the man you called?\" she asks. \"Can't you talk\nto him?\"\n\"You mean Jonah? That's who I'm picking up at the air-\nport,\" I say. \"But I'm not sure even Jonah's help will do any\ngood.\"\nHearing this, Dave is shocked. He says, \"You mean ... all\nthat stuff we learned about on the hike, about Herbie setting the"}
{"159": "\"It's not people; it's equipment,\" I explain. \"We can't fire\nmachines. And, anyway, what they do is essential. We couldn't\nproduce most of our products without these two operations.\"\n\"So why don't you make them go faster?\" asks Sharon.\n\"Sure, Dad,\" says Davey. \"Remember what happened on the\nhike when you took Herbie's pack from him? Maybe you could\ndo something kind of like that in the plant.\"\n\"Yeah, but it's not quite that simple,\" I say.\nMom says, \"Alex, I know you'll do the best you can. If you've\ngot these two slow pokes holding everything up, you'll just have\nto keep after them and make sure they don't waste any more\ntime.\"\nI say, \"Yeah, well, I've got to run. Don't wait up for me. I'll\nsee you in the morning.\"\nWaiting at the gate, I watch Jonah's plane taxi up to the\nterminal. I talked to him in Boston this afternoon just before he\nwas leaving for Los Angeles. I told him I wanted to thank him for\nhis advice, but that the situation at the plant was impossible so far\nas we could see.\n\"Alex, how do you know it's impossible?\" he asked.\nI told him, \"We've only got two months left before my boss\ngoes to the board of directors with his recommendation. If we\nhad more time, maybe we could do something, but with only two\nmonths. . . .\"\n\"Two months is still enough time to show an improvement,\"\nhe said. \"But you have to learn how to run your plant by its\nconstraints.\"\n\"Jonah, we've analyzed the situation thoroughly\u2014'"}
{"160": "\"Okay then. Have you tried to take some of the load off the\nbottlenecks by using other resources?\" he asked.\n\"You mean offloading? We can't. These are the only two re-\nsources of their type in the plant.\"\nHe paused for a moment and finally he said, \"All right, one\nmore question: Does Bearington have an airport?\"\nAnd so here he is tonight, walking out of Gate Two. He\nchanged his flight to Los Angeles to make a stop here for the\nevening. I walk up to him and shake his hand.\n\"How was your flight?\" I ask him.\n\"Have you ever spent time in a sardine can?\" he says, then\nadds, \"I shouldn't complain. I'm still breathing.\"\n\"Well, thanks for coming,\" I tell him. \"I appreciate you\nchanging your plans, although I'm still not sure you can help us.\"\n\"Alex, having a bottleneck\u2014\"\n\"Two bottlenecks,\" I remind him.\n\"Having two bottlenecks doesn't mean you can't make\nmoney,\" he says. \"Quite the contrary, in fact. Most manufactur-\ning plants do not have bottlenecks. They have enormous excess\ncapacity. But they should have them\u2014one on every part they\nmake.\"\nHe reads the puzzled look on my face.\n\"You don't understand, but you will,\" he said. \"Now I want\nyou to give me as much background on your plant as you can.\"\nAll the way from the airport, I talk non-stop about our pre-\ndicament. When we reach the plant, I park the Mazda in front of\nthe offices. Waiting for us inside are Bob, Lou, Stacey and Ralph.\nThey're standing around the vacant receptionist's desk. Everyone"}
{"161": "\"That sure would be a big help,\" says Lou. \"How do you\nthink we might be able to do that?\"\n\"Your bottlenecks are not maintaining a flow sufficient to\nmeet demand and make money,\" he says. \"So there is only one\nthing to do. We have to find more capacity.\"\n\"But we don't have the money for more capacity,\" says Lou.\n\"Or the time to install it,\" says Bob.\n\"I'm not talking about more capacity from one end of the\nplant to the other,\" says Jonah. \"To increase the capacity of the\nplant is to increase the capacity of only the bottlenecks.\"\n\"You mean make them into non-bottlenecks,\" says Stacey.\n\"No,\" he says. \"Absolutely not. The bottlenecks stay bottle-\nnecks. What we must do is find enough capacity for the bottle-\nnecks to become more equal to demand.\"\n\"Where're we going to find it?\" asks Bob. \"You mean it's just\nlayin' around out there?\"\n\"In effect, yes,\" says Jonah. \"If you are like most manufac-\nturers, you will have capacity that is hidden from you because\nsome of your thinking is incorrect. And I suggest that first of all\nwe go into your plant and see for ourselves exactly how you are\nmanaging your two bottlenecks.\"\n\"Why not,\" I say. \"After all, no one visits this plant and es-\ncapes without a tour.\"\nThe six of us put on the safety glasses and hats and go into\nthe plant. Jonah and I head the column as we walk through the\ndouble doors into the orange light. It's about halfway into second\nshift now and somewhat quieter than it is on day turn. That's\ngood because it lets us hear each other better when we talk. I"}
{"162": "\"Probably because the set-up people went on break about ten\nminutes ago,\" says Bob. \"They should be back in about twenty\nminutes.\"\n\"There is a clause in our union contract which stipulates\nthere must be a half-hour break after every four hours of work,\"\nI explain to Jonah.\nHe asks, \"But why should they take their break now instead\nof when the machine is running?\"\nBob says, \"Because it was eight o'clock and\u2014\"\nJonah holds up his hands and says, \"Wait a minute. On any\nnon-bottleneck machine in your plant, no problem. Because, after\nall, some percentage of a non-bottleneck's time should be idle. So\nwho cares when those people take their breaks? It's no big deal.\nBut on a bottleneck? It's exactly the opposite.\"\nHe points to the NCX-10 and says, \"You have on this ma-\nchine only so many hours available for production\u2014what is it\n. . . 600, 700 hours?\"\n\"It's around 585 hours a month,\" says Ralph.\n\"Whatever is available, the demand is even greater,\" says\nJonah. \"If you lose one of those hours, or even half of it, you have\nlost it forever. You cannot recover it someplace else in the system.\nYour throughput for the entire plant will be lower by whatever\namount the bottleneck produces in that time. And that makes an\nenormously expensive lunch break.\"\n\"But we have a union to deal with,\" says Bob.\nJonah says, \"So talk to them. They have a stake in this plant.\nThey're not stupid. But you have to make them understand.\"\nYeah, I'm thinking; that's easier said than done. On the"}
{"163": "Lou edges in and and says, \"Excuse me, but you're not actu-\nally suggesting we use that old equipment, are you?\"\n\"If it's still operational, then yes, I might suggest it,\" says\nJonah.\nLou's eyes blink.\nHe says, \"Well, I'm not sure what that would do to our cost\nprofile. But I have to tell you that those old machines are going to\nbe much more expensive to operate.\"\nJonah says, \"We'll deal with that directly. First, I just want to\nknow if you have the machines or not.\"\nFor the answer, we turn to Bob\u2014who chuckles.\n\"Sorry to disappoint you all,\" he says, \"but we got rid of an\nentire class of machine that we'd need to supplement the\nNCX-10.\"\n\"Why did we go do a dumb thing like that?\" I ask.\nBob says, \"We needed the floor space for that new pen to\nhold inventory.\"\nI say, \"Oh.\"\n\"It seemed like a good idea at the time,\" says Stacey.\nMoving right along to heat-treat, we gather in front of the\nfurnaces.\nThe first thing Jonah does is look at the stacks of parts and\nask, \"Are you sure all this inventory requires heat-treat?\"\n\"Oh, absolutely,\" says Bob.\n\"There are no alternatives in the processing ahead of this\ndepartment that would prevent the need for heat-treat on at least\nsome of these parts?\" he asks.\nWe all look at each other."}
{"164": "\"There are,\" says Stacey, \"but going outside would increase\nour cost-per-part.\"\nThe expression on Jonah's face says he's getting a little bored\nwith this stonewalling. He points at the mountains of parts.\n\"How much money is represented in that pile?\" he asks.\nLou says, \"I don't know . . . maybe ten or fifteen thousand\ndollars in parts.\"\n\"No, it isn't thousands of dollars, not if this is a bottleneck,\"\nsays Jonah, \"Think again. It's considerably more.\"\nStacey says, \"I can go dig up the records if you like, but the\ncost won't be much more than what Lou said. At the most, I'd\nguess we've got about twenty thousands dollars in material\u2014\"\n\"No, no,\" says Jonah. \"I'm not just talking about the cost of\nmaterials. How many products are you going to sell to customers\nas soon as you can process this entire pile?\"\nThe staff and I talk among ourselves for a moment.\n\"It's kind of hard to say,\" says Bob.\n\"We're not sure all the parts in that pile would translate into\nimmediate sales,\" says Stacey.\n\"Oh really? You are making your bottlenecks work on parts\nthat will not contribute to throughput?\" asks Jonah.\n\"Well . . . some of them become spare parts or they go into\nfinished goods inventory. Eventually it becomes throughput,\"\nsays Lou.\n\"Eventually,\" says Jonah. \"And, meanwhile, how big did you\nsay your backlog of overdue orders is?\"\nI explain to him that sometimes we inflate the batch quanti-\nties to improve efficiency."}
{"165": "\"And if you could finish the parts in that pile, you could\nassemble and ship the product?\" he asks.\n\"Sure, no problem,\" says Bob.\n\"And what is the selling price of each unit?\"\n\"About a thousand dollars a unit on the average,\" says Lou,\n\"although it varies, of course.\"\n\"Then we are not dealing with ten or fifteen or even twenty\nthousand dollars here,\" says Jonah. \"Because we are dealing with\nhow many parts in that pile?\"\n\"Perhaps, a thousand,\" says Stacey.\n\"And each part means you can ship a product?\"\n\"Generally, yes,\" she says.\n\"And each product shipped means a thousand dollars,\" says\nJonah. \"A thousand units times a thousand dollars is how much\nmoney?\"\nIn unison, our faces turn toward the mountain.\n\"One million dollars,\" I say with awe.\n\"On one condition!\" says Jonah. \"That you get these parts in\nand out of heat-treat and shipped as a finished product before\nyour customers get tired of waiting and go elsewhere!\"\nHe looks at us, his eyes shifting from face to face.\n\"Can you afford to rule out any possibility,\" he asks, \"espe-\ncially one that is as easy to invoke as a change in policy?\"\nEveryone is quiet.\n\"By the way, I'll tell you more about how to look at the costs\nin a moment. But one more thing,\" says Jonah. \"I want to know\nwhere you do quality inspection on bottleneck parts.\"\nI explain to him that most inspection is done prior to final"}
{"166": "\"Do you realize what the rejection by Q.C. has done to you?\"\nasks Jonah.\n\"It means we have to scrap about a hundred parts,\" says\nBob.\n\"No, think again,\" says Jonah. \"These are bottleneck parts.\"\nIt dawns on me what he's getting at.\n\"We lost the time on the bottleneck,\" I say.\nJonah whirls toward me.\n\"Exactly right!\" he says. \"And what does lost time on a bot-\ntleneck mean? It means you have lost throughput.\"\n\"But you're not saying we should ignore quality, are you?\"\nasks Bob.\n\"Absolutely not. You can't make money for long without a\nquality product,\" says Jonah. \"But I am suggesting you use qual-\nity control in a different way.\"\nI ask, \"You mean we should put Q.C. in front of the bottle-\nnecks?\"\nJonah raises a finger and says, \"Very perceptive of you. Make\nsure the bottleneck works only on good parts by weeding out the\nones that are defective. If you scrap a part before it reaches the\nbottleneck, all you have lost is a scrapped part. But if you scrap\nthe part after it's passed the bottleneck, you have lost time that\ncannot be recovered.\"\n\"Suppose we get sub-standard quality downstream from the\nbottleneck?\" says Stacey.\n\"That's another aspect of the same idea,\" says Jonah. \"Be\nsure the process controls on bottleneck parts are very good, so\nthese parts don't become defective in later processing. Are you"}
{"167": "\"It averages around a thousand dollars a unit,\" says Lou.\n\"And you're worried about spending a dollar or two at the\nbottlenecks to make them more productive?\" he asks. \"First of all,\nwhat do you think the cost of, let's say, the X machine is for one\nhour?\"\nLou says, \"That's well established. It costs us $32.50 per\nhour.\"\n\"And heat-treat?\"\n\"That's $21 per hour,\" says Lou.\n\"Both of those amounts are incorrect,\" says Jonah.\n\"But our cost data\u2014\"\n\"The numbers are wrong, not because you have made a cal-\nculating error, but because the costs were determined as if these\nwork centers existed in isolation,\" says Jonah. \"Let me explain:\nwhen I was a physicist, people would come to me from time to\ntime with problems in mathematics they couldn't solve. Thev\nwanted me to check their numbers for them. But after a while I\nlearned not to waste my time checking the numbers\u2014because the\nnumbers were almost always right. However, if I checked the\nassumptions, they were almost always wrong.\"\nJonah pulls a cigar out of his pocket and lights it with a\nmatch.\n\"That's what's going on here,\" he says between puffs. \"You\nhave calculated the cost of operating these two works centers ac-\ncording to standard accounting procedures . . . without consid-\nering the fact that both are bottlenecks.\"\n\"How does that change their costs?\" asks Lou.\n\"What you have learned is that the capacity of the plant is"}
{"168": "\"And let's just take the X machine as an example,\" he says.\n\"How many hours a month did you say it's available for produc-\ntion?\"\n\"About 585,\" says Ralph.\n\"The actual cost of a bottleneck is the total expense of the\nsystem divided by the number of hours the bottleneck produces,\"\nsays Jonah. \"What does this make it?\"\nLou takes out his calculator from his coat pocket and\npunches in the numbers.\n\"That's $2,735,\" says Lou. \"Now wait a minute. Is that\nright?\"\n\"Yes, it's right,\" says Jonah. \"If your bottlenecks are not\nworking, you haven't just lost $32 or $21. The true cost is the cost\nof an hour of the entire system. And that's twenty seven hundred\ndollars.\"\nLou is flabbergasted.\n\"That puts a different perspective on it,\" says Stacey.\n\"Of course it does,\" says Jonah. \"And with that in mind, how\ndo we optimize the use of the bottlenecks? There are two princi-\npal themes on which you need to concentrate . . .\n\"First, make sure the bottlenecks' time is not wasted,\" he\nsays. \"How is the time of a bottleneck wasted? One way is for it to\nbe sitting idle during a lunch break. Another is for it to be pro-\ncessing parts which are already defective\u2014or which will become\ndefective through a careless worker or poor process control. A\nthird way to waste a bottleneck's time is to make it work on parts\nyou don't need.\"\n\"You mean spare parts?\" asks Bob."}
{"169": "\"That's why I was asking those questions when we were out\nin the plant,\" he says. \"Do all of the parts have to be processed by\nthe bottleneck? If not, the ones which don't can be shifted to non-\nbottlenecks for processing. And the result is you gain capacity on\nyour bottleneck. A second question: do you have other machines\nto do the same process? If you have the machines, or if you have a\nvendor with the right equipment, you can offload from the bottle-\nneck. And, again, you gain capacity which enables you to increase\nthroughput.\"\nI come into the kitchen for breakfast the next morning and\nsit down to a big steaming bowl of my mother's oatmeal . . .\nwhich I have hated ever since I was a kid. I'm staring at the\noatmeal (and the oatmeal is staring back) when Mom/Grandma\nasks, \"So how did everything go last night?\"\nI say, \"Well, actually, you and the kids were on the right\ntrack at dinner.\"\n\"We were?\" asks Dave.\n\"We need to make the Herbies go faster,\" I say. \"And last\nnight Jonah pointed out some ways to do that. So we learned a\nlot.\"\n\"Well, now, isn't that good news,\" says my mother.\nShe pours a cup of coffee for herself and sits down at the\ntable. It's quiet for a moment. Then I notice that Mom and the\nkids are eyeing each other.\n\"Something wrong?\" I ask.\n\"Their mother called again last night while you were gone,\"\nsays my mother.\nJulie has been calling the kids regularly since she left. But for"}
{"170": "\"Right, the violins,\" says Sharon. \"Well, when Mom wasn't\ntalking, I heard that on the phone last night.\"\n\"I heard 'em too,\" says Dave.\n\"Really?\" I say. \"That's very interesting. Thank you both for\nnoticing that. Maybe I'll give Grandma and Grandpa Barnett an-\nother call today.\"\nI finish my coffee and stand up.\n\"Alex, you haven't even touched your oatmeal,\" says Mom.\nI lean down and kiss her on the cheek. \"Sorry, I'm late for\nschool.\"\nI wave to the kids and hurry to grab my briefcase.\n\"Well, I'll just have to save it so you can eat it tomorrow,\"\nsays my mother."}
{"171": "20\nDriving to the plant, I pass the motel where Jonah stayed last\nnight. I know he's long gone\u2014he had a 6:30 A.M. flight to catch. I\noffered to pick him up this morning and drive him to the airport,\nbut (lucky for me) he refused and said he'd take a cab.\nAs soon as I get to the office, I tell Fran to set up a meeting\nwith the staff. Meanwhile, I start to write down a list of the actions\nJonah suggested last night. But Julie comes to mind and won't\nleave. I close my office door and sit down at my desk. I find the\nnumber for Julie's parents and dial it.\nThe first day after Julie left, her parents called to ask me if I\nhad heard anything. They haven't called back since. A day or two\nago, I tried getting in touch with them to find out if they had\nheard anything. I called in the afternoon and I talked to Julie's\nmother, Ada. She said she didn't know where Julie was. Even\nthen, I didn't quite believe her.\nNow Ada answers again.\n\"Hi, this is Alex,\" I tell her. \"Let me talk to Julie.\"\nAda is flustered. \"Well, um, ah ... she isn't here.\"\n\"Yes, she is.\"\nI hear Ada sigh.\n\"She is there, isn't she,\" I say.\nFinally Ada says, \"She does not want to talk to you.\"\n\"How long, Ada? How long has she been there? Were you\nlying to me even that Sunday night when I called?\"\n\"No, we were not lying to you,\" she says indignantly. \"We\nhad no idea where she was. She was with her friend, Jane, for a"}
{"172": "\"She says she'll call you when she's ready,\" says Ada.\n\"What does that mean?\"\n\"If you hadn't neglected her all these years, you wouldn't be\nin this situation,\" she says.\n\"Ada\u2014\"\n\"Good-bye,\" she says.\nShe hangs up the phone. I try calling back right away, but\nthere is no answer. After a few minutes, I force my mind back to\ngetting ready to talk to the staff.\nAt ten o'clock, the meeting starts in my office.\n\"I'd like to know what you think about what you heard last\nnight,\" I say. \"Lou, what was your reaction?\"\nLou says, \"Well . . . I just couldn't believe what he was say-\ning about an hour of a bottleneck. I went home last night and\nthought it over to see if it all made sense. And, actually, we were\nwrong about a lost hour of a bottleneck costing $2,700.\"\n\"We were?\" I ask.\n\"Only eighty percent of our products flow through the bot-\ntlenecks,\" says Lou as he takes a piece of note paper from his\nshirt pocket. \"So the truer cost ought to be eighty percent of our\noperating expense, and that comes to $2,188 an hour\u2014not\n$2,735.\"\n\"Oh,\" I say. \"I suppose you're right.\"\nThen Lou smiles.\n\"Nevertheless,\" he says, \"I have to admit it was quite an eye-\nopener to look at the situation from that perspective.\"\n\"I agree,\" I say. \"What about the rest of you?\"\nI go from person to person around the office asking for reac-"}
{"173": "\"Okay, but some of what Jonah talked about will be easier\nand faster to make happen than the rest,\" he says. \"Why don't we\ngo ahead with the easier things right away and see what kind of\neffect they have while we're developing the others.\"\nI tell him, \"That sounds reasonable. What would you do\nfirst?\"\n\"I think I'd wanna move the Q.C. inspection points first, to\ncheck parts going into the bottlenecks,\" says Bob. \"The other\nQ.C. measures will take a little time, but we can have an inspector\nchecking pre-bottleneck parts in no time\u2014by the end of today if\nyou want.\"\nI nod. \"Good. What about new rules for lunch breaks?\"\n\"We might have a squawk or two from the union,\" he says.\nI shake my head. \"I think they'll go along with it. Work out\nthe details and I'll talk to O'Donnell.\"\nBob makes a note on the paper pad on his lap. I stand up\nand step around the desk to emphasize what I'm about to say.\n\"One of the questions Jonah raised last night really struck\nhome for me,\" I tell them. \"Why are we making the bottlenecks\nwork on inventory that won't increase throughput?\"\nBob looks at Stacey, and she looks back at him.\n\"That's a good question,\" she says.\nBob says, \"We made the decision\u2014\"\n\"I know the decision,\" I say. \"Build inventory to maintain\nefficiencies.\" But our problem is not efficiencies. Our problem is\nour backlog of overdue orders. And it's very visible to our cus-\ntomers and to division management. We positively must do some-\nthing to improve our due-date performance, and Jonah has given"}
{"174": "up in a warehouse. So here's what we need to do,\" I say. \"Ralph,\nI want you to make us a list of all the overdue orders. Have them\nranked in priority ranging from the most days overdue to the\nleast days overdue. How soon can you have that for us?\"\n\"Well, that in itself won't take very long,\" he says. \"The prob-\nlem is we've got the monthlies to run.\"\nI shake my head. \"Nothing is more important to us right\nnow than making the bottlenecks more productive. We need that\nlist as soon as possible, because once you've got it, I want you to\nwork with Stacey and her people in inventory control\u2014find out\nwhat parts still have to be processed by either of the bottlenecks\nto complete those orders.\"\nI turn to Stacey.\n\"After you know which parts are missing, get together with\nBob and schedule the bottlenecks to start working on the parts\nfor the latest order first, the next latest, and so on.\"\n\"What about the parts that don't go through either one of\nthe bottlenecks?\" asks Bob.\n\"I'm not going to worry about those at the moment,\" I tell\nhim. \"Let's work on the assumption that anything not needing to\ngo through a bottleneck is either waiting in front of assembly\nalready, or will be by the time the bottleneck parts arrive.\"\nBob nods.\n\"Everybody got it?\" I ask. \"Nothing else takes priority over\nthis. We don't have time to take a step back and do some kind of\nheadquarters number where everyone takes six months to think\nabout it. We know what we have to do. Let's get it done.\"\nThat evening, I'm driving along the Interstate. Around sun-"}
{"175": "with trees just getting the new leaves of spring. They are brilliant\ngreen in the golden setting sun.\nI see the house halfway down the street. It's the two-story\nbrick colonial painted white. It has shutters. The shutters are\nmade of aluminum and have no hinges; they are non-functional\nbut traditional. This is where Julie grew up.\nI park the Mazda by the curb in front of the house. I look up\nthe driveway, and sure enough, there is Julie's Accord in front of\nthe garage.\nBefore I have reached the front door, it opens. Ada Barnett\nis standing behind the screen. I see her hand reach down and\nclick the screen door lock as I approach.\n\"Hello,\" I say.\n\"I told you she doesn't want to talk to you,\" says Ada.\n\"Will you just ask her please?\" I ask. \"She is my wife.\"\n\"If you want to talk to Julie, you can do it through her law-\nyer,\" says Ada.\nShe starts to close the door.\nI say, \"Ada, I am not leaving until I talk to your daughter.\"\n\"If you don't leave, I will call the police to have you removed\nfrom our property,\" says Ada Barnett.\n\"Then I will wait in my car,\" I say. \"You don't own the\nstreet.\"\nThe door closes. I walk across the lawn and over the side-\nwalk, and get in the Mazda. I sit there and stare at the house.\nEvery so often, I notice the curtains move behind the window\nglass of the Barnett house. After about forty five minutes, the sun\nhas set and I'm seriously wondering how long I can sit here when"}
{"176": "She glances away. I slap the roof of the Mazda.\n\"Let's go for a ride,\" I say.\n\"No, I can't,\" she says.\n\"How about a walk then?\" I ask.\n\"Alex, just tell me what you want, okay?\" she says.\n\"I want to know why you're doing this!\"\n\"Because I don't know if I want to be married to you any-\nmore,\" she says. \"Isn't that obvious?\"\n\"Okay, can't we talk about it?\"\nShe says nothing.\n\"Come on,\" I say. \"Let's take that walk\u2014just once around\nthe block. Unless you want to give the neighbors lots to talk\nabout.\"\nJulie looks around at the houses and realizes we're a specta-\ncle. Awkwardly, she steps toward me. I hold out my hand. She\ndoesn't take it, but we turn together and begin a stroll down the\nsidewalk. I wave to the Barnett house and note the flurry of a\ncurtain. Julie and I walk a hundred feet or so in the twilight\nbefore we say anything. At last I break the silence.\n\"Look, I'm sorry about what happened that weekend,\" I tell\nher. \"But what else could I do? Davey expected me\u2014\"\n\"It wasn't because you went on the hike with Davey,\" she\nsays. \"That was just the last straw. All of a sudden, I just couldn't\nstand it anymore. I had to get away.\"\n\"Julie, why didn't you at least let me know where you were?\"\n\"Listen,\" she says. \"I went away from you so I could be\nalone.\"\nHesitantly, I ask, \"So ... do you want a divorce?\""}
{"177": "\"All I'm suggesting is that we talk about what's bothering\nyou.\"\nShe sighs in exasperation and says, \"Al, we've been over it a\nmillion times already!\"\n\"Okay, look, just tell me this: are you having an affair?\"\nJulie stops. We have reached the corner.\nShe says coldly, \"I think I've gone far enough with you.\"\nI stand there for a moment as she turns and heads back\ntoward her parents' house. I catch up with her.\nI say, \"Well? Are you or aren't you?\"\n\"Of course I'm not having an affair!\" she yells. \"Do you think\nI'd be staying with my parents if I were having an affair?\"\nA man who is walking his dog turns and stares at us. Julie\nand I stride past him in stiff silence.\nI whisper to Julie, \"I just had to know . . . that's all.\"\n\"If you think I'd leave my children just to go have a fling\nwith some stranger, you have no understanding of who I am,''\nshe says.\nI feel as if she'd slapped my face.\n\"Julie, I'm sorry,\" I tell her. \"That kind of thing sometimes\nhappens, and I just needed to make sure of what's going on.\"\nShe slows her walk. I put my hand on her shoulder. She\nbrushes it off.\n\"Al, I've been unhappy for a long time,\" she says. \"And I'll\ntell you something: I feel guilty about it. I feel as though I don't\nhave a right to be unhappy. I just know I am.\"\nWith irritation, I see we're back in front of her parents'\nhouse. The walk was too short. Ada is standing in plain view at"}
{"178": "She shakes her head. \"I can't, Al. I've heard too many prom-\nises before.\"\nI say, \"Then you want a divorce?\"\nJulie says, \"I told you, I don't know!\"\n\"Okay,\" I say finally. \"I can't make up your mind for you.\nMaybe it is your decision. All I can say is I want you back. I'm\nsure that's what the kids want too. Give me a call when you know\nwhat you want.\"\n\"That was exactly what I planned to do, Al.\"\nI get into the Mazda and start the engine. Rolling down the\nwindow, I look up at her as she stands on the sidewalk next to the\ncar.\n\"You know, I do happen to love you,\" I tell her.\nThis finally melts her. She comes to the car and leans down.\nReaching through the window, I take her hand for a moment.\nShe kisses me. Then without a word she stands up and walks\naway; halfway across the lawn, she breaks into a run. I watch her\nuntil she's disappeared through the door. Then I shake my head,\nput the car into gear, and drive away."}
{"179": "21\nI'm home by ten o'clock that night. Depressed, but home.\nRummaging through the refrigerator, I attempt to find dinner,\nbut have to settle for cold spaghetti and some leftover peas. Wash-\ning it down with some leftover vodka, I dine in dejection.\nI'm wondering while I'm eating what I'm going to do if Julie\ndoesn't come back. If I don't have a wife, do I start to date\nwomen again? Where would I meet them? I have a sudden vision\nof myself standing in the bar of the Bearington Holiday Inn,\nattempting to be sexy while asking strange females, \"What's your\nsign?\"\nIs that my fate? My God. And anyway, do lines like that even\nwork these days? Did they ever?\nI must know somebody to go out with.\nFor a while, I sit there thinking of all the available women I\nknow. Who would go out with me? Whom would I want to go out\nwith? It doesn't take long to exhaust the list. Then one woman\ncomes to mind. Getting up from my chair, I go to the phone and\nspend about five minutes staring at it.\nShould I?\nNervously, I dial the number. I hang up before it rings. I\nstare at the phone some more. Oh, what the hell! All she can do is\nsay no, right? I dial the number again. It rings about ten times\nbefore anyone answers.\n\"Hello.\" It's her father.\n\"May I speak to Julie please.\"\nPause. \"Just a minute.\""}
{"180": "She says, \"Well ... I guess not.\"\n\"Good. What are you doing Saturday night?\" I ask.\nThere is a moment of silence as the smile forms on her face.\nAmused, she asks, \"Are you asking me for a date?\"\n\"Yes, I am.\"\nLong pause.\nI say, \"So would you like to go out with me?\"\n\"Yes, I'd like that a lot,\" she says finally.\n\"Great. How about I see you at 7:30?\"\n\"I'll be ready,\" she says.\nThe next morning in the conference room, we've got the two\nsupervisors of the bottlenecks with us. By \"us,\" I mean Stacey,\nBob, Ralph and me. Ted Spencer is the supervisor responsible for\nthe heat-treat furnaces. He's an older guy with hair that looks like\nsteel wool and a body like a steel file. We've got him and Mario\nDeMonte, supervisor of the machining center with the NCX-10.\nMario is as old as Ted, but plumper.\nStacey and Ralph both have red eyes. Before we sat down,\nthey told me about the work that went into this morning's meet-\ning.\nGetting the list of overdue orders was easy. The computer\nlisted them and sorted them according to lateness. Nothing to it,\ndidn't even take a minute. But then they had to go over the bills of\nmaterial for each of the orders and find out which parts are done\nby the bottlenecks. And they had to establish whether there was\ninventory to make those parts. That took most of the night.\nWe all have our own photocopies of a hand-written list Ralph\nhas had prepared. Listed in the print-out is a grand total of sixty"}
{"181": "\"So it's obvious those parts get first priority,\" I explain to the\ntwo supervisors.\nThen Ralph says, \"We went ahead and made a list for both\nheat-treat and the NCX-10 as to which parts they each have to\nprocess and in what order\u2014again, the same sequence of latest\norder to least late. In a day or two we can generate the list by\ncomputer and stop burning the midnight oil.\"\n\"Fantastic, Ralph. I think both you and Stacey have done a\nsuper job,\" I tell him. Then I turn to Ted and Mario. \"Now, all\nyou gentlemen have to do is have your foremen start at the top of\nthe list and work their way down.\"\n\"That sounds easy enough,\" says Ted. \"I think we can han-\ndle that.\"\n\"You know, we may have to go track some of these down,\"\nsays Mario.\n\"So you'll have to do some digging through the inventory,\"\nsays Stacey. \"What's the problem?\"\nMario frowns and says, \"No problem. You just want us to do\nwhat's on this list, right?\"\n\"Yep, it's that simple,\" I say. \"I don't want to see either of\nyou working on something not on that list. If the expediters give\nyou any problem, tell them to come see me. And be sure you stick\nto the sequence we've given you.\"\nTed and Mario both nod.\nI turn to Stacey and say, \"You do understand how important\nit is for the expediters not to interfere with this priority list, don't\nyou?\"\nStacey says, \"Okay, but you have to promise me you won't"}
{"182": "gripping the armrests of his chair with white knuckles, while\nO'Donnell is talking at the top of his voice.\n\"What's the problem here?\" I ask.\n\"You know very well what the problem is: your new lunch\nrules in heat-treat and n/c machining,\" says O'Donnell. \"They're\nin violation of the contract. I refer you to Section Seven, Para-\ngraph Four . . .\"\nI say, \"Okay, wait a minute, Mike. It's time we gave the\nunion an update on the situation of the plant.\"\nFor the rest of the morning I describe for him the situation\nthe plant is in. Then I tell him some of what we've discovered and\nexplain why the changes are necessary.\nWrapping up, I say, \"You understand, don't you, that it's\nprobably only going to affect about twenty people at the most?\"\nHe shakes his head.\n\"Look, I appreciate you trying to explain all this,\" he says.\n\"But we got a contract. Now if we look the other way on one\nthing, what's to say you won't start changing whatever else you\ndon't like?\"\nI say, \"Mike, in all honesty, I can't tell you that down the\nroad aways, we won't need to make other changes. But we're\nultimately talking about jobs. I'm not asking for cuts in wages or\nconcessions on benefits. But I am asking for flexibility. We have\nto have the leeway necessary to make changes that will allow the\nplant to make money. Or, very simply, there may not be a plant\nin a few months.\"\n\"Sounds like scare tactics to me,\" he says finally.\n\"Mike, all I can say is, if you want to wait a couple of months"}
{"183": "I go find Mario.\n\"Why the hell isn't that machine working?\" I ask him.\nHe checks with the foreman. Finally he walks back to me.\n\"We don't have the materials,\" he says.\n\"What do you mean, you don't have materials,\" I shout. \"What\ndo you call these stacks of steel everywhere?\"\n\"But you told us to work according to what's on the list,\" says\nMario.\n\"You mean you finished all the late parts?\"\n\"No, they did the first two batches of parts,\" says Mario.\n\"When they got to the third part on the list, they looked all\naround and couldn't find the materials for it in the queue. So\nwe're shut down until they turn up.\"\nI'm ready to strangle him.\n\"That's what you wanted us to do, right?\" says Mario. \"You\nwanted us to do only what was on the list and in the same order\nas listed, didn't you? Isn't that what you said?\"\nFinally I say, \"Yes, that is what I said. But didn't it occur to\nyou that if you couldn't do one item on the list you should go on\nto the next?\"\nMario looks helpless.\n\"Well, where the hell are the materials you need?\" I ask him.\n\"I have no idea,\" he says. \"They could be any of half-a-dozen\nplaces. But I think Bob Donovan might have somebody looking\nfor them already.\"\n\"Okay, look,\" I tell him. \"You have the setup people get this\nmachine ready for whatever is the next part on that list for which\nyou do have the materials. And keep this hunk of junk running.\""}
{"184": "It turns out, as Bob explains to me, that the parts they were\nwaiting for at the NCX-10 have been sitting there for about a\nweek. Otto has been running other batches of parts. He didn't\nknow about the importance of the parts destined for the NCX-10.\nTo him they looked like any other batch\u2014and a rather unimpor-\ntant one judging from the size. When Bob got here, they were in\nthe middle of a big, long run. Otto didn't want to stop . . . until\nDonovan explained it to him, that is.\n\"Dammit, Al, it's just like before,\" Bob says. \"They get set up\nand they start running one thing, and then they have to break in\nthe middle so we can finish something else. It's the same damn\nthing!\"\n\"Now hold on,\" I say. \"Let's think about this for a second.\"\nBob shakes his head. \"What is there to think about?\"\n\"Let's just try to reason this through,\" I say. \"What was the\nproblem?\"\n\"The parts didn't arrive at the NCX-10, which meant the\noperators couldn't run the batch they were supposed to be run-\nning,\" says Bob in kind of a sing-song way.\n\"And the cause was that the bottleneck parts were held up by\nthis non-bottleneck machine running non-bottleneck parts,\" I\nsay. \"Now we've got to ask ourselves why that happened.\"\n\"The guy in charge here was just trying to stay busy, that's\nall,\" says Bob.\n\"Right. Because if he didn't stay busy, someone like you\nwould come along and jump all over him,\" I say.\n\"Yeah, and if I didn't, then someone like you would jump all\nover me,\" says Bob."}
{"185": "\"Until you came along,\" I say. \"But you can't be everywhere,\nand this same kind of thing is going to happen again. So how do\nwe communicate to everybody in the plant which parts are im-\nportant?\"\n\"I guess we need some kind of system,\" says Bob.\n\"Fine. Let's go work on one right away so we don't have to\nkeep putting up with this crap,\" I say. \"And before we do any-\nthing else, let's make sure that people at both of the bottlenecks\nknow to keep working on the order with the highest priority\nnumber on the list.\"\nBob has a final chat with Otto to make sure he knows what to\ndo with the parts. Then the two of us head for the bottlenecks.\nFinally we're walking back to the office. Glancing at Bob's\nface, I can tell he's still bothered by what happened.\n\"What's wrong? You look unconvinced about all this,\" I say.\n\"Al, what's going to happen if we repeatedly have people\nbreak up process runs to run parts for the bottlenecks?\" he asks.\n\"We should be able to avoid idle time on the bottlenecks,\" I\nsay.\n\"But what's going to happen to our costs on the other 98\npercent of the work centers we got here?\" he asks.\n\"Right now, don't worry about it. Let's just keep the bottle-\nnecks busy,\" I say. \"Look, I'm convinced you did the right thing\nback there. Aren't you?\"\n\"Maybe I did the right thing,\" he says, \"but I had to break all\nthe rules to do it.\"\n\"Then the rules had to be broken,\" I say. \"And maybe they\nweren't good rules to begin with. You know we've always had to"}
{"186": "with everyone working in the plant, both foremen and hourly\npeople. This afternoon, we'll do the same thing with people\nworking second shift, and I'll come in late tonight to talk to the\nthird shift as well. When we've got everybody this morning, I get\nup in front of them and talk.\n\"All of you know that this plant has been in a downward slide\nfor some time. What you don't know is that we're in the position\nto begin to change that,\" I tell them. \"You're here in this meeting\nbecause we're introducing a new system today ... a system\nwhich we think will make the plant more productive than it's\nbeen in the past. In the next few minutes, I'm going to explain\nbriefly some of the background that made us develop this new\nsystem. And then Bob Donovan is going to tell you how it works.\"\nTrying to keep meetings to fifteen minutes doesn't give us\nthe time to tell them very much. But using the analogy of an\nhourglass, I do explain briefly about the bottlenecks and why we\nhave to give priority to parts on the heat-treat and NCX-10 rout-\nings. For the things I can't take time to tell them, there is going to\nbe a newsletter, which will replace the old plant employee paper,\nand which will report developments and progress in the plant.\nAnyway, I turn over the microphone to Donovan and he tells\nthem how we're going to prioritize all materials in the plant so\neverybody knows what to work on.\n\"By the end of today, all work-in-process on the floor will be\nmarked by a tag with a number on it,\" he says and holds up some\nsamples. \"The tag will be one of two colors: red or green.\n\"A red marker means the work attached to it has first prior-\nity. The red tags go on any materials needing to be processed by a"}
{"187": "most of the work-in-process out there will be marked by green.\nEven so, you work on green orders only if you don't have any red\nones in queue.\n\"That explains the priority of the colors. But what happens\nwhen you've got two batches of the same color? Each tag will have\na number marked on it. You should always work on the materials\nwith the lowest number.\"\nDonovan explains some of the details and answers a couple\nof questions, after which I wrap it up.\nI tell them, \"This meeting was my idea. I decided to take you\naway from your jobs, mostly because I wanted everyone to hear\nthe same message at the same time, so that\u2014I hope\u2014you'll have\na better understanding of what's going on. But another reason is\nthat I know it's been a long time since most of you have heard any\ngood news about the plant. What you've just heard about is a\nbeginning. Even so, the future of this plant and the security of\nyour jobs will only be assured when we start making money\nagain. The most important thing you can do is to work with us\n. . . and, together, we'll all be working to keep this plant work-\ning.\"\nLate that afternoon, my phone rings.\n\"Hi, this is O'Donnell. Go ahead with the new policy on\nlunch and coffee breaks. We won't challenge it.\"\nI relay the news to Donovan. And with these small victories,\nthe week ends.\nAt 7:29 on Saturday evening, I park the washed, waxed,\nbuffed and vacuumed Mazda in the Barnett driveway. I reach for\nthe bouquet of flowers beside me on the seat, and step out onto"}
{"188": "\"Shall we go?\" suggests Julie.\nJoking, I tell Julie's mother, \"I'll have her home by ten\no'clock.\"\n\"Good,\" says Mrs. Barnett. \"We'll be waiting.\""}
{"189": "22\n\"There you have it,\" says Ralph.\n\"Not bad,\" says Stacey.\n\"Not bad? It's a lot better than not bad,\" says Bob.\n\"We must be doing something right,\" says Stacey.\n\"Yeah, but it isn't enough,\" I mutter.\nA week has passed. We're grouped around a computer ter-\nminal in the conference room. Ralph has extracted from the com-\nputer a list of overdue orders that we shipped last week.\n\"Isn't enough? At least it's progress,\" says Stacey. \"We\nshipped twelve orders last week. For this plant, that's not bad.\nAnd they were our twelve most overdue orders.\"\n\"By the way, our worst overdue order is now only forty four\ndays late,\" says Ralph. \"As you may recall, the worst one used to\nbe fifty eight days.\"\n\"All right!\" says Donovan.\nI step back to the table and sit down.\nTheir enthusiasm is somewhat justified. The new system of\ntagging all the batches according to priority and routing has been\nworking fairly well. The bottlenecks are getting their parts\npromptly. In fact, the piles of inventory in front of them have\ngrown. Following bottleneck processing, the red-tagged parts\nhave been getting to final assembly faster. It's as if we've created\nan \"express lane\" through the plant for bottleneck parts.\nAfter putting Q.C. in front of the bottlenecks, we discovered\nthat about five percent of the parts going into the NCX-10 and\nabout seven percent going into heat-treat did not conform to"}
{"190": "The combination of these has allowed us to ship our most\ncritical orders and to ship a few more of them than normal. But I\nknow we're not going fast enough. A few weeks ago we were\nlimping along; now we're walking, but we ought to be jogging.\nGlancing back toward the monitor, I see the eyes are upon\nme.\n\"Listen ... I know we've taken a step in the right direc-\ntion,\" I explain. \"But we have to accelerate the progress. It's\ngood that we got twelve shipments out last week. But we're still\nhaving some customer orders become past due. It's not as many,\nI'll grant you, but we still have to do better. We really shouldn't\nhave any late orders.\"\nEveryone walks away from the computer and joins me around\nthe table. Bob Donovan starts telling me how they're planning\nsome refinements on what we've already done.\nI say, \"Bob, those are fine, but they're minor. How are we\ncoming on the other suggestions Jonah made?\"\nBob glances away.\n\"Well . . . we're looking into them,\" he says.\nI say, \"I want recommendations on offloading the bottle-\nnecks ready for our Wednesday staff meeting.\"\nBob nods, but says nothing.\n\"You'll have them for us?\" I ask.\n\"Whatever it takes,\" he says.\nThat afternoon in my office, I have a meeting with Elroy\nLangston, our Q.C. manager, and Barbara Penn, who handles\nemployee communications. Barbara writes the newsletters, which\nare now explaining the background and reasons for the changes"}
{"191": "the part is on a bottleneck routing. What we need is a simple way\nto show people the parts they need to treat with special attention\n\u2014the ones they need to treat like gold.\"\n\"That's a suitable comparison,\" I tell her.\nShe says, \"So what if we simply mark the tags with pieces of\nyellow tape after the parts are finished by the bottlenecks. The\ntape would tell people on sight that these are the parts you treat\nlike gold. In conjunction with this, I'll do an internal promotion\nto spread the word about what the tape means. For media, we\nmight use some sort of bulletin board poster, an announcement\nthat the foremen would read to the hourly people, maybe a ban-\nner which would hang in the plant\u2014those kinds of things.\"\n\"As long as the tape can be added without slowing down the\nbottlenecks, that sounds fine,\" I say.\n\"I'm sure we can find a way to do it so it doesn't interfere,\"\nsays Langston.\n\"Good,\" I say. \"One other concern of mine is that I don't\nwant this to be just a lot of promotion.\"\n\"That's perfectly understood,\" says Langston with a smile.\n\"Right now, we're systematically identifying the causes of quality\nproblems on the bottlenecks and in subsequent processing. Once\nwe know where to aim, we'll be having specific procedures devel-\noped for bottleneck-routed parts and processes. And once they're\nestablished, we'll set up training sessions so people can learn\nthose procedures. But that's obviously going to take some time.\nFor the short term, we're specifying that the existing procedures\nbe double-checked for accuracy on the bottleneck routes.\"\nWe talk that over for a few minutes, but basically all of it"}
{"192": "That doesn't faze him.\n\"Al, have I got something to show you!\" says Bob. \"Got time\nto take a little walk?\"\n\"Yeah, I guess so. What's this all about?\"\n\"Well . . . I'll tell you when you get here,\" says Bob. \"Meet\nme on the receiving dock.\"\nI walk down to the dock, where I see Bob; he's standing\nthere waving to me as if I might miss him. Which would be im-\npossible. There is a flat-bed truck backed up to the dock, and in\nthe middle of the bed is a large object on a skid. The object is\ncovered by a gray canvas tarp which has ropes tying it down. A\ncouple of guys are working with an overhead crane to move the\nthing off of the truck. They're raising it into the air as I walk up\nto Bob. He cups his hands around his mouth.\n\"Easy there,\" Bob calls as he watches the big gray thing sway\nback and forth.\nSlowly, the crane maneuvers the cargo back from the truck\nand lowers it safely to the concrete floor. The workers release the\nhoist chains. Bob walks over and has them untie the ropes hold-\ning down the canvas.\n\"We'll have it off in a minute,\" Bob assures me.\nI stand there patiently, but Bob can't refrain from helping.\nWhen all the ropes are untied, Donovan takes hold of the tarp\nand, with a flair of gusto, flings it off of what it's concealing.\n\"Ta-da!\" he says as he stands back and gestures to what has\nto be one of the oldest pieces of equipment I've ever seen.\n\"What the hell is it?\" I ask.\n\"It's a Zmegma,\" he says."}
{"193": "\"So this must be one of the machines you told Jonah we sold\nto make way for the inventory holding pen,\" I say.\n\"You got it,\" he says.\n\"It's practically an antique. All of them are,\" I say, referring\nto the other machines. \"Are you sure they can give us acceptable\nquality?\"\n\"It isn't automated equipment, so with human error we\nmight have a few more mistakes,\" says Bob. \"But if you want\ncapacity, this is a quick way to get it.\"\nI smile. \"It's looking better and better. Where did you find\nthis thing?\"\n\"I called a buddy of mine this morning up at our South End\nplant,\" he says. \"He told me he still had a couple of these sitting\naround and he'd have no problem parting with one of them. So I\ngrabbed a guy from maintenance and we took a ride up to have a\nlook.\"\nI ask him, \"What did it cost us?\"\n\"The rental fee on the truck to haul it down here,\" says Bob.\n\"The guy at South End told us just to go ahead and take it. He'll\nwrite it off as scrap. With all the paperwork he'd have to do, it\nwas too much trouble to sell it to us.\"\n\"Does it still work?\"\n\"It did before we left,\" says Bob. \"Let's find out.\"\nThe maintenance man connects the power cable to an outlet\non a nearby steel column. Bob reaches for the power switch and\nhits the ON button. For a second, nothing happens. Then we\nhear the slow, gathering whirr from somewhere in the guts of the\nold machine. Poofs of dust blow out of the antique fan housing."}
{"194": "23\nRain is beating at the windows of my office. Outside, the\nworld is gray and blurred. It's the middle of a middle-of-the-week\nmorning. In front of me are some so-called \"Productivity Bulle-\ntins\" put out by Hilton Smyth which I've come across in my in-\nbasket. I haven't been able to make myself read past the first\nparagraph of the one on top. Instead, I'm gazing at the rain and\npondering the situation with my wife.\nJulie and I went out on our \"date\" that Saturday night, and\nwe actually had a good time. It was nothing exotic. We went to a\nmovie, we got a bite to eat afterwards, and for the heck of it we\ntook a drive through the park on the way home. Very tame. But it\nwas exactly what we needed. It was good just to relax with her. I\nadmit that at first I felt kind of like we were back in high school or\nsomething. But, after a while, I decided that wasn't such a bad\nfeeling. I brought her back to her parents at two in the morning,\nand we made out in the driveway until her old man turned on\nthe porch light.\nSince that night, we've continued to see each other. A couple\nof times last week, I made the drive up to see her. Once, we met\nhalfway at a restaurant. I've been dragging myself to work in the\nmorning, but with no complaints. We've had fun together.\nBy some unspoken agreement, neither of us talk about di-\nvorce or marriage. The subject has only come up once, which\nhappened when we talked about the kids and agreed they should\nstay with Julie and her folks as soon as school ends. I tried then to\npush us into some answers, but the old argument syndrome be-"}
{"195": "\"What about?\"\nFran steps into the office and closes the door behind her. She\nquickly comes over to my desk and whispers to me.\n\"I don't know, but I heard on the grapevine that he had an\nargument with Ralph Nakamura about an hour ago,\" she says.\n\"Oh,\" I say. \"Okay, thanks for the warning. Send him in.\"\nA moment later Ted Spencer comes in. He looks mad. I ask\nhim what's happening down in heat-treat.\nHe says, \"Al, you've got to get that computer guy off my\nback.\"\n\"You mean Ralph? What have you got against him?\"\n\"He's trying to turn me into some kind of clerk or some-\nthing,\" says Ted. \"He's been coming around and asking all kinds\nof dumb questions. Now he wants me to keep some kind of spe-\ncial records on what happens in heat-treat.\"\n\"What kind of records?\" I ask.\n\"I don't know ... he wants me to keep a detailed log of\neverything that goes in and out of the furnaces . . . the times we\nput 'em in, the times we take 'em out, how much time between\nheats, all that stuff,\" says Ted. \"And I've got too much to do to be\nbothered with all that. In addition to heat-treat, I've got three\nother work centers I'm responsible for.\"\n\"Why does he want this time log?\" I ask.\n\"How should I know? I mean, we've already got enough\npaperwork to satisfy anybody, as far as I'm concerned,\" says Ted.\n\"I think Ralph just wants to play games with numbers. If he's got\nthe time for it, then fine, let him do it in his own department. I've\ngot the productivity of my department to worry about.\""}
{"196": "\"So tell me what you did to light Ted Spencer's fuse,\" I say to\nhim.\nRalph rolls his eyes and says, \"All I wanted from him was to\nkeep an accurate record of the actual times for each heat of parts\nin the furnace. I thought it was a simple enough request.\"\n\"What prompted you to ask him?\"\n\"I had a couple of reasons,\" says Ralph. \"One of them is that\nthe data we have on heat-treat seems to be very inaccurate. And if\nwhat you say is true, that this operation is so vital to the plant,\nthen it seems to me we ought to have valid statistics on it.\"\n\"What makes you think our data is so inaccurate?\" I ask.\n\"Because after I saw the total on last week's shipments I was\nkind of bothered by something. A few days ago on my own, I did\nsome projections of how many shipments we would actually be\nable to make last week based on the output of parts from the\nbottlenecks. According to those projections, we should have been\nable to do about eighteen to twenty shipments instead of twelve.\nThe projections were so far off that I figured at first I must have\nmade a big mistake. So I took a closer look, double-checked my\nmath and couldn't find anything wrong. Then I saw that the\nestimates for the NCX-10 were within the ballpark. But for heat-\ntreat, there was a big difference.\"\n\"And that's what made you think that the data base must be\nin error,\" I say.\n\"Right,\" he says. \"So I went down to talk to Spencer. And,\nah. . . .\"\n\"And what?\"\n\"Well, I noticed some funny things were happening,\" he"}
{"197": "\"Two-and-a-half hours after they could have come out, they\nhadn't been unloaded?\" I ask.\n\"That's right,\" says Ralph. \"So I found Sammy, the second-\nshift foreman down there, and asked him what was going on. He\ntold me he was short-handed that night, and they'd get to it later.\nHe said it didn't hurt the parts to stay in the furnace. While I was\nthere, he shut off the burners, but I found out later that the parts\ndidn't come out until about eight o'clock. I didn't mean to start\ntrouble, but I'd thought if we recorded the actual times per heat,\nwe'd at least have some realistic figures to use for estimating. You\nsee, I asked some of the hourly people down there and they told\nme those kinds of delays happen a lot in heat-treat.\"\n\"No kidding,\" I say. \"Ralph ... I want you to take all the\nmeasurements down there that you need. Don't worry about Ted.\nAnd do the same thing on the NCX-10.\"\n\"Well, I'd like to, but it's kind of a chore,\" he says. \"That's\nwhy I wanted Ted and the others just to jot down the times and\nall.\"\nI say, \"Okay, we'll take care of that. And, ah ... thanks\nvery much.\"\n\"You're welcome,\" he says.\n\"By the way, what was the other reason?\" I ask him. \"You\nmentioned you had more than one.\"\n\"Oh, well, it's probably not that important.\"\n\"No, tell me,\" I say.\n\"I don't really know if we can do it or not,\" says Ralph, \"but\nit occurred to me we might find a way to use the bottlenecks to\npredict when we'll be able to ship an order.\""}
{"198": "are they supposed to do? Stand around and twiddle their\nthumbs?\"\n\"I don't care what they do between times as long as they get\nthe parts in and out of the furnace pronto,\" I say. \"We could have\ndone almost another batch of parts in the five hours of waiting for\npeople to finish what they were doing elsewhere and change\nloads.\"\n\"All right,\" says Bob. \"How about this: we loan the people to\nother areas while the parts cook, but as soon as the time is up, we\nmake sure we call them back immediately so\u2014\"\n\"No, because what's going to happen is everybody will be\nvery conscientious about it for two days, and then it'll slip back to\nthe way it is now,\" I say. \"I want people at those furnaces stand-\ning by, ready to load and unload twenty-four hours a day, seven\ndays a week. The first ones I want assigned there are foremen\nwho are responsible full-time for what happens down there. And\ntell Ted Spencer that the next time I see him, he'd better know\nwhat's going on in heat-treat or I'll kick his ass.\"\n\"You bet,\" says Bob. \"But you know you're talking about\ntwo, maybe three people per shift.\"\n\"Is that all?\" I ask. \"Don't you remember what lost time on a\nbottleneck costs us?\"\n\"Okay, I'm with you,\" he says. \"Tell you the truth, what\nRalph found out about heat-treat is a lot like what I found out on\nmy own about those rumors of idle time on the NCX-10.\"\n\"What's going on there?\"\nBob tells me that, indeed, it's true the NCX-10 is sitting idle\nfor as much as half an hour or more at a time. But the problem is"}
{"199": "\"That's just dandy with me,\" says Bob. \"But you know how\nit's going to look on paper. It's going to seem like we increased\nthe direct labor content of the parts coming out of heat-treat and\nthe NCX-10.\"\nI slump into the chair behind my desk.\n\"Let's fight one battle at a time,\" I say.\nThe next morning, Bob comes to the staff meeting with his\nrecommendations. They basically consist of four actions. The first\ntwo concern what he and I talked about the day before\u2014dedicat-\ning a machinist and helper to the NCX-10, and stationing a fore-\nman and two workers at the heat-treat furnaces. The assignments\nwould apply to all three shifts. The other two recommendations\nconcern offloading the bottlenecks. Bob has determined if we\ncould activate one each of these old machines\u2014the Zmegma and\nthe two others\u2014just one shift a day, we could add eighteen per-\ncent to the output of parts of the type produced by the NCX-10.\nLast of all, is that we take some of the parts queued at heat-treat\nand send them out to the vendor across town.\nAs he's presenting these, I'm wondering what Lou is going to\nsay. As it happens, Lou offers little resistance.\n\"Knowing what we know now,\" says Lou, \"it's perfectly legit-\nimate for us to assign people to the bottlenecks if it will increase\nour throughput. We can certainly justify the cost if it increases\nsales\u2014and thereby increases cash flow. My question is, where are\nyou going to get the people?\"\nBob says we could call them back from layoff.\n\"No, you can't. See, the problem we have,\" says Lou, \"is that\nthe division has a recall freeze in effect. We can't recall without"}
{"200": "\"Who's going to set up the other machines?\" he asks.\n\"The helpers on the other machines know enough to set up\ntheir own equipment,\" I say.\n\"Well, I guess we can try it,\" says Bob. \"But what happens if\nstealing people turns non-bottlenecks into bottlenecks?\"\nI tell him, \"The important thing is to maintain the flow. If we\ntake a worker away, and we can't maintain the flow, then we'll put\nthe worker back and steal a body from someplace else. And if we\nstill can't keep the flow going, then we'll have no choice but to go\nto a division and insist that we either go to overtime or call a few\npeople back from layoff.\"\n\"Okay,\" says Bob. \"I'll go for it.\"\nLou gives us his blessing.\n\"Good. Let's do it,\" I say. \"And, Bob, make sure the people\nyou pick are good. From now on, we put only our best people to\nwork on the bottlenecks.\"\nAnd so it is done.\nThe NCX-10 gets a dedicated setup crew. The Zmegma and\nthe other machines go to work. The outfit across town is only too\nglad to take our surplus parts for heat-treating. And in our own\nheat-treat department, two people per shift are assigned to stand\nby, ready to load and unload parts from the furnaces. Donovan\njuggles the work-center responsibilities so heat-treat has a fore-\nman there at all times.\nFor a foreman, heat-treat seems like a very small kingdom,\nnot much of a prize. There is nothing intrinsically attractive about\nrunning that operation, and having only two people to manage\nmakes it seem like no big deal. To prevent it from seeming like a"}
{"201": "doing the trick. Anyway, I go down there to try to learn what he's\ndoing.\nAs I walk up, I see the two helpers are not just standing\naround with nothing to do. They're moving parts. In front of the\nfurnaces are two tightly organized stacks of work-in-process,\nwhich the helpers are building. I call Mike over and ask him what\nthey're doing.\n\"They're getting ready,\" he says.\n\"What do you mean?\"\n\"They're getting ready for when we have to load one of the\nfurnaces again,\" he says. \"The parts in each stack are all treated\nat the same temperature.\"\n\"So you're splitting and overlapping some batches,\" I say.\n\"Sure,\" he says. \"I know we're not really supposed to do\nthat, but you need the parts, right?\"\n\"Sure, no problem. You're still doing the treating according\nto the priority system?\" I ask.\n\"Oh, yeah,\" he says. \"Come here. Let me show you.\"\nMike leads me past the control console for the furnaces to a\nworn old battleship of a desk. He finds the computer print-out\nfor the week's most important overdue orders.\n\"See, look at number 22,\" he says pointing to it. \"We need\nfifty of the high stress RB-dash-11's. They get treated at a 1200-\ndegree temperature cycle. But fifty of them won't fill up the fur-\nnace. So we look down and what do we see here but item number\n31, which calls for 300 fitted retaining rings. Those also take a\n1200-degree cycle.\"\n\"So you'll fill up the furnace with as many of the retaining"}
{"202": "ing, we could make those tables interchangeable. That way we\ncould stack a load of parts in advance and switch loads with the\nuse of a forklift. If it saves us a couple of hours a day, that means\nwe can do an extra heat of parts over the course of a week.\"\nI look from the furnaces back to Mike. I say, \"Mike, I want\nyou to take tomorrow night off. We'll get one of the other fore-\nmen to cover for you.\"\n\"Sounds good to me,\" he says with a grin. \"How come?\"\n\"Because the day after tomorrow, I want you on day turn.\nI'm going to have Bob Donovan put you together with an I.E. to\nwrite up these procedures formally, so we can start using them\nround the clock,\" I tell him. \"You keep that mind of yours work-\ning. We need it.\"\nLater that morning, Donovan happens by my office.\n\"Hi, there,\" he says.\n\"Well, hello,\" I tell him. \"Did you get my note on Haley?\"\n\"It's being taken care of,\" says Bob.\n\"Good. And let's make sure he gets some more money out of\nthis whenever the wage freeze is lifted,\" I say.\n\"Okay,\" says Bob as a smile spreads across his face. Then he\nleans against the doorway.\n\"Something else?\" I ask.\n\"Got good news for you,\" says Bob.\n\"How good?\"\n\"Remember when Jonah asked us if all the parts going\nthrough heat-treat really needed it?\"\nI tell him I remember.\n\"I just found out that in three cases, it wasn't engineering"}
{"203": "the slower processing, we don't need the heat-treat. Which means\nwe can take about twenty percent of the current load off the\nfurnaces.\"\n\"Sounds fantastic,\" I tell him. \"What about getting it ap-\nproved by engineering?\"\n\"That's the beauty of it,\" says Bob. \"We were the ones who\ninitiated the change five years ago.\"\n\"So if it was our option to begin with,\" I say, \"we can change\nit back any time we want.\"\n\"Right! We don't need to get an engineering change order,\nbecause we already have an approved procedure on the books,\"\nsays Bob.\nHe leaves shortly with my blessing to implement the change\nas soon as possible. I sit there marveling that we're going to reduce\nthe efficiency of some operations and make the entire plant more\nproductive. They'd never believe it on the fifteenth floor."}
{"204": "24\nIt's a Friday afternoon. Out in the parking lot, the people on\nfirst shift are getting into their cars to go home. There is the usual\ncongestion at the gate. I'm in my office\u2014minding my own busi-\nness\u2014when suddenly, from through the half-open door . . .\nBAM!\nSomething ricochets off the ceiling tiles. I jump to my feet,\ncheck myself for wounds and, finding none, search the carpet for\nthe offending missile. It's a champagne cork.\nThere is laughing outside my door. In the next instant, it\nseems as though everyone is in my office. There is Stacey, Bob\nDonovan (who holds the bottle from which the cork came),\nRalph, Fran, a couple of the secretaries, and a swarm of other\npeople\u2014even Lou joins us. Fran hands me one of the styrofoam\ncoffee cups she's dispensing to everyone. Bob fills it from the\nbottle.\n\"What's this all about?\" I ask.\n\"I'll tell you in the toast I'm g\u00b0ing to make as soon as every-\none has something to swallow,\" says Bob.\nMore bottles are opened\u2014there is a case of this stuff\u2014and\nwhen all the cups are filled, Bob lifts his own.\n\"Here's to a new plant record in shipments of product,\" he\nsays. \"Lou went through the records for us and discovered that\nuntil now the best this place has ever done in a month was thirty-\none orders shipped at value of about two million dollars. This\nmonth we topped that. We shipped fifty-seven customer orders\nwith a value of ... well, in round numbers, we'll call it a cool"}
{"205": "\"Keep drinking. It gets better,\" says Donovan.\nI'm just about to hazard a second cup when I notice Fran\nbeside me.\n\"Mr. Rogo?\"\n\"Yes.\"\n\"Bill Peach is on the line,\" says Fran.\nI shake my head wondering what the hell it's going to be this\ntime.\n\"I'll take it at your desk, Fran.\"\nI go out there and punch the blinking button on my phone\nand pick it up.\n\"Yes, Bill, what can I do for you?\"\n\"I was just talking to Johnny Jons,\" says Peach.\nI automatically grab a pencil and pull over a pad of paper to\ntake down the particulars on whatever order is causing us grief. I\nwait for Peach to continue, but he doesn't say anything for a\nsecond.\n\"What's the problem?\" I ask him.\n\"No problem,\" says Peach. \"Actually he was very happy.\"\n\"Really? What about?\"\n\"He mentioned you've been coming through lately for him\non a lot of late customer orders,\" says Peach. \"Some kind of spe-\ncial effort I guess.\"\n\"Well, yes and no. We're doing a few things a little differently\nnow,\" I say.\n\"Well, whatever. The reason I called is I know how I'm al-\nways on your case when things go wrong, Al, so I just wanted to\ntell you thanks from me and Jons for doing something right,\""}
{"206": "Peach's call. About half of the original group went to dinner to-\ngether. Lou and Ralph threw in the towel early. But Donovan,\nStacey and I\u2014along with three or four die-hards\u2014went to a bar\nafter we ate and we had a good time. Now it is 1:30 and I am\nblissfully stinko.\nThe Mazda for safety's sake, it still parked behind the bar.\nStacey, who switched to club soda a couple of hours ago, has\ngenerously played chauffeur to Bob and me. About ten minutes\nago, we nudged Donovan through his kitchen door where he\nstood there bewildered for a moment before bidding us a good\nevening. If he remembers, Donovan is supposed to enlist his wife\nlater today to drive us over to the bar and retrieve our vehicles.\nStacey gets out of the car and comes around and opens my\ndoor so I can spill myself onto the driveway. Standing up on\nuncertain legs, I steady myself against the car.\n\"I've never seen you smile so much,\" says Stacey.\n\"I've got a lot to smile about,\" I tell her.\n\"Wish you could be this happy in staff meetings,\" she says.\n\"Henceforth, I shall smile continuously through all staff\nmeetings,\" I proclaim.\n\"Come on, I'll make sure you get to the door,\" she says.\nWith her hands around my arm to steady me, she guides me\nup the front walk to the door.\nWhen we're at the door, I ask her, \"How about some cof-\nfee?\"\n\"No, thanks,\" she says. \"It's late and I'd better get home.\"\n\"Sure?\"\n\"Absolutely.\""}
{"207": "I look up, my eyes adjusting to the sudden light, and there\nshe is.\n\"Julie? What are you doing here?\"\nWithout answering, she's now stomping through the kitchen.\nAs I get to my feet and stagger after her, the door to the garage\nopens. The light switch in the garage clicks. I see her in silhouette\nfor half a second.\n\"Julie! Wait a minute!\"\nI hear the garage door rumbling open as I attempt to follow\nher. As I go into the garage, she's already getting into her car.\nThe door slams. I zig-zag closer, wildly waving my arms. The\nengine starts.\n\"I sit here waiting for you all night, putting up with your\nmother for six hours,\" she yells through the rolled-down window,\n\"and you come home drunk with some floozy!\"\n\"But Stacey isn't a floozy, she's\u2014\"\nAccelerating to about thirty miles per hours in reverse, Julie\nbacks out of the garage, down the driveway (narrowly missing\nStacey's car) and into the street. I'm left standing there in the\nlight of the garage. The tires of her car chirp upon the asphalt.\nShe's gone.\nOn Saturday morning, I wake up and groan a couple of\ntimes. The first groan is from the hangover. The second groan is\nfrom the memory of what happened.\nWhen I'm able, I get dressed and venture into the kitchen in\nquest of coffee. My mother is there.\n\"You know your wife was here last night,\" says my mother as\nI pour my first cup."}
{"208": "\"Sure. Come on in,\" I say.\nShe seems disturbed about something. She's avoiding my\neyes as she sits down.\nI say, \"Listen, about Friday night, I'm sorry about what hap-\npened when you dropped me off.\"\nStacey says, \"It's okay. Did your wife come back?\"\n\"Uh, well, no, she didn't. She's staying with her parents for a\nlittle while,\" I say.\n\"Was it just because of me?\" she asks.\n\"No, we've been having some problems lately.\"\n\"Al, I still feel kind of responsible,\" she says. \"Look, why\ndon't I talk to her.\"\n\"No, you don't have to do that,\" I say.\n\"Really, I think I ought to talk to her,\" says Stacey. \"What's\nher number?\"\nI finally admit to myself it might be worth a try. So I give the\nBarnett's number to Stacey. She writes it down, and promises to\ncall sometime today. Then she continues to sit there.\n\"Was there something else?\" I ask.\n\"I'm afraid there is,\" she says.\nShe pauses.\n\"So what is it?\"\n\"I don't think you're going to like this,\" she says. \"But I'm\npretty sure about it ...\"\n\"Stacey,\" I say. \"What?\"\n\"The bottlenecks have spread.\"\n\"What do you mean 'the bottlenecks have spread'?\" I ask. \"Is\nthere a disease out there or something?\""}
{"209": "day and was ready by Friday morning. But the others are still\nmissing.\"\nI lean back in my chair and pinch the bridge of my nose.\n\"Dammit, what the hell is going on out there? I had assumed\nthe parts that have to go through a bottleneck would reach as-\nsembly last. Is there a materials shortage on those green-tagged\nparts? Some kind of vendor problem?\" I ask her.\nStacey shakes her head. \"No, I haven't had any problems\nwith purchasing. And none of the parts have any processing by\noutside contractors. The problem is definitely internal. That's\nwhy I really think we have one or more new bottlenecks.\"\nI get up from my desk, walk around the office.\n\"Maybe with the increase in throughput, we've loaded the\nplant to a level that we've run out of capacity on some other\nresources in addition to heat-treat and the NCX-10,\" Stacey sug-\ngests quietly.\nI nod. Yes, that sounds like a possibility. With the bottlenecks\nmore productive now, our throughput has gone up and our\nbacklog is declining. But making the bottlenecks more productive\nhas put more demand on the other work centers. If the demand\non another work center has gone above one hundred percent,\nthen we've created a new bottleneck.\nOf the ceiling, I ask, \"Does this mean we're going to have to\ngo through the whole process of finding the bottlenecks all over\nagain? Just when it seemed like we were on our way out of this\nmess. . . .\"\nStacey folds the print-outs.\nI tell her, \"Okay, look, I want you to find out everything you"}
{"210": "over to the assembly line without having to turn sideways to\nsqueeze between the stacks and bins of inventory. I thought it was\ngood. But now this happens.\n\"Mr. Rogo,\" says Fran through the intercom speaker. \"I've\ngot him on the line.\"\nI pick up the phone. \"Jonah? Hi. Listen, we've got trouble\nhere.\"\n\"What's wrong?\" he asks.\nAfter I tell him the symptoms, Jonah asks what we've done\nsince his visit. So I relate all the history to him\u2014putting Q.C. in\nfront of the bottlenecks, training people to give special care to\nbottleneck parts, activating the three machines to supplement the\nNCX-10, the new lunch rules, assigning certain people to work\nonly at the bottlenecks, increasing the batch sizes going into heat-\ntreat, implementing the new priority system in the plant. . . .\n\"New priority system?\" asks Jonah.\n\"Right,\" I say, and then I explain about the red tags and\ngreen tags, and how the system works.\nJonah says, \"Maybe I'd better come have another look.\"\nI'm at home that night when the phone rings.\n\"Hi,\" says Julie's voice when I answer.\n\"Hi.\"\n\"I owe you an apology. I'm sorry about what happened on\nFriday night,\" she says. \"Stacey called me here. Al, I'm really\nembarrassed. I completely misunderstood.\"\n\"Yeah, well ... it seems to me there's a lot of misunder-\nstanding between us lately,\" I say.\n\"All I can say is I'm sorry. I drove down thinking you'd be"}
{"211": "I can hear her relief.\n\"Yes, I would,\" she says. \"When will I see you?\"\nI suggest we try Friday all over again. She says she can't wait\nthat long. We compromise on Wednesday."}
{"212": "25\nDeja vu. At the airport next morning, I again greet Jonah as\nhe walks out of Gate Two.\nBy ten o'clock, we're in the conference room at the plant.\nSitting around the table are Lou, Bob, Ralph and Stacey. Jonah\npaces in front of us.\n\"Let's start with some basic questions,\" he says. \"First of all,\nhave you determined exactly which parts are giving you the\nproblem?\"\nStacey, who is sitting at the table with a veritable fortress of\npaper around her and looking as if she's ready for a siege, holds\nup a list.\nShe says, \"Yes, we've identified them. In fact, I spent last\nnight tracking them down and double checking the data with\nwhat's on the floor out there. Turns out the problem covers thirty\nparts.\"\nJonah asks, \"Are you sure you released the materials for\nthem?\"\n\"Oh, yes,\" says Stacey. \"No problem there. They've been\nreleased according to schedule. But they're not reaching final\nassembly. They're stuck in front of our new bottleneck.\"\n\"Wait a minute. How do you know it's really a bottleneck?\"\nasks Jonah.\nShe says, \"Well, since the parts are held up, I just figured it\nhad to be . . .\"\n\"Before we jump to conclusions, let's invest half an hour to\ngo into the plant so we can find out what's happening,\" Jonah"}
{"213": "\"But we need them now,\" I say. \"How come they're not\nbeing worked on?\"\nJake shrugs his shoulders. \"You know which ones you want,\nwe'll do 'em right now. But that goes against them rules you set\nup in that there priority system.\"\nHe points to some other skids of materials nearby.\n\"You see over there?\" says Jake. \"They all got red tags. We\ngot to do all of 'em before we touch the stuff with green tags.\nThat's what you told us, right?\"\nUh-huh. It's becoming clear what's been happening.\n\"You mean,\" says Stacey, \"that while the materials with\ngreen tags have been building up, you've been spending all your\ntime on the parts bound for the bottlenecks.\"\n\"Yeah, well, most of it,\" says Jake. \"Hey, like we only got so\nmany hours in a day, you know what I mean?\"\n\"How much of your work is on bottleneck parts?\" asks Jo-\nnah.\n\"Maybe seventy-five or eighty percent,\" says Jake. \"See, ev-\nerything that goes to heat-treat or the NCX-10 has to pass\nthrough here first. As long as the red parts keep coming\u2014and\nthey haven't let up one bit since that new system started\u2014we just\ndon't have the time to work on very many of the green-tag parts.\"\nThere is a moment of silence. I look from the parts to the\nmachines and back to Jake again.\n\"What the hell do we do now?\" asks Donovan in echo to my\nown thoughts. \"Do we switch tags? Make the missing parts red\ninstead of green?\"\nI throw up my hands in frustration and say, \"I guess the only"}
{"214": "a red\" tag. Somewhere behind it all, its own hugeness obscured\nfrom our view, is the NCX-10.\n\"How do we get there from here?\" asks Ralph, looking for a\npath through the inventory.\n\"Here, let me show you,\" says Bob.\nAnd he leads us through the maze of materials until we reach\nthe machine.\nGazing at all the work-in-process around us, Jonah says to\nus, \"You know, I would guess, just from looking at it, that you\nhave at least a month or more of work lined-up here for this\nmachine. And I bet if we went to heat-treat we would find the\nsame situation. Tell me, do you know why you have such a huge\npile of inventory here?\"\n\"Because everyone ahead of this machine is giving first pri-\nority to red parts,\" I suggest.\n\"Yes, that's part of the reason,\" says Jonah. \"But why is so\nmuch inventory coming through the plant to get stuck here?\"\nNobody answers.\n\"Okay, I see I'm going to have to explain some of the basic\nrelationships between bottlenecks and non-bottlenecks,\" says Jo-\nnah. Then he looks at me and says, \"By the way, do you remem-\nber when I told you that a plant in which everyone is working all\nthe time is very in efficient? Now you'll see exactly what I was\ntalking about.\"\nJonah walks over to the nearby Q.C. station and takes a piece\nof chalk the inspectors use to mark defects on the parts they\nreject. He kneels down to the concrete floor and points to the\nNCX-10."}
{"215": "Product parts are what join the two in a relationship with\neach other, Jonah explains, and the arrow obviously indicates the\nflow of parts from one to the other. He adds that we can consider\nany non-bottleneck feeding parts to X, because no matter which\none we choose, its inventory must be processed at some subse-\nquent point in time by X.\n\"By the definition of a non-bottleneck, we know that Y has\nextra capacity. Because of its extra capacity, we also know that Y\nwill be faster in filling the demand than X,\" says Jonah. \"Let's say\nboth X and Y have 600 hours a month available for production.\nBecause it is a bottleneck, you will need all 600 hours of the X\nmachine to meet demand. But let's say you need only 450 hours a\nmonth, or 75 percent, of Y to keep the flow equal to demand.\nWhat happens when Y has worked its 450 hours? Do you let it sit\nidle?\"\nBob says, \"No, we'll find something else for it to do.\"\n\"But Y has already satisfied market demand,\" says Jonah.\nBob says, \"Well, then we let it get a head start on next\nmonth's work.\"\n\"And if there is nothing for it to work on?\" asks Jonah.\nBob says, \"Then we'll have to release more materials.\"\n\"And that is the problem,\" says Jonah. \"Because what hap-\npens to those extra hours of production from Y? Well, that inven-\ntory has to go somewhere. Y is faster than X. And by keeping Y\nactive, the flow of parts to X must be greater than the flow of\nparts leaving X. Which means . . .\"\nHe walks over to the work-in-process mountain and makes a\nsweeping gesture."}
{"216": "\"That's right,\" says Jonah. \"If Y is depending exclusively\nupon X to feed it inventory, the maximum number of hours it\ncan work is determined by the output of X. And 600 hours from\nX equates to 450 hours for Y. After working those hours, Y will\nbe starved for inventory to process. Which, by the way, is quite\nacceptable.\"\n\"Wait a minute,\" I say. \"We have bottlenecks feeding non-\nbottlenecks here in the plant. For instance, whatever leaves the\nNCX-10 will be processed by a non-bottleneck.\"\n\"From other non-bottlenecks you mean. And do you know\nwhat happens when you keep Y active that way?\" asks Jonah.\n\"Look at this.\"\nHe draws a third diagram on the floor with the chalk.\nIn this case, Jonah explains, some parts do not flow through\na bottleneck; their processing is done only by a non-bottleneck\nand the flow is directly from Y to assembly. The other parts do\nflow through a bottleneck, and they are on the X route to assem-\nbly where they are mated to the Y parts into a finished product.\nIn a real situation, the Y route probably would consist of one\nnon-bottleneck feeding another non-bottleneck, feeding yet an-\nother non-bottleneck, and so on, to final assembly. The X route\nmight have a series of non-botjtlenecks feeding a bottleneck,\nwhich in turn feeds a chain of more non-bottlenecks. In our case,\nJonah says, we've got a group of non-bottleneck machines down-\nstream from X which can process parts from either the X or the Y\nroute.\n\"But to keep it simple, I've diagrammed the combination\nwith the fewest number of elements\u2014one X and one Y. No mat-"}
{"217": "activating Y just to keep it busy is the same. So let's say you keep\nboth X and Y working continuously for every available hour.\nHow efficient would the system be?\"\n\"Super efficient,\" says Bob.\n\"No, you're wrong,\" says Jonah. \"Because what happens\nwhen all this inventory from Y reaches final assembly?\"\nBob shrugs and says, \"We build the orders and ship them.\"\n\"How can you?\" asks Jonah. \"Eighty percent of your prod-\nucts require at least one part from a bottleneck. What are you\ngoing to substitute for the bottleneck part that hasn't shown up\nyet?\"\nBob scratches his head and says, \"Oh, yeah ... I forgot.\"\n\"So if we can't assemble,\" says Stacey, \"we get piles of inven-\ntory again. Only this time the excess inventory doesn't accumu-\nlate in front of a bottleneck; it stacks up in front of final assem-\nbly.\"\n\"Yeah,\" says Lou, \"and another million bucks sits still just to\nkeep the wheels turning.\"\nAnd Jonah says, \"You see? Once more, the non-bottleneck\ndoes not determine throughput, even if it works twenty-hour\nhours a day.\"\nBob asks, \"Okay, but what about that twenty percent of\nproducts without any bottleneck parts? We can still get high effi-\nciencies with them.\"\n\"You think so?\" asks Jonah.\nOn the floor he diagrams it like this . . .\nThis time, he says, the X and Y operate independently of\none another. They are each filling separate marketing demands."}
{"218": "inventory. And this time you end up, not with excess work-in-\nprocess, but with excess finished goods. The constraint here is\nnot in production. The constraint is marketing's ability to sell.\"\nAs he says this, I'm thinking to myself about the finished\ngoods we've got crammed into warehouses. At least two-thirds of\nthose inventories are products made entirely with non-bottleneck\nparts. By running non-bottlenecks for \"efficiency,\" we've built\ninventories far in excess of demand. And what about the remain-\ning third of our finished goods? They have bottleneck parts, but\nmost of those products have been sitting on the shelf now for a\ncouple of years. They're obsolete. Out of 1,500 or so units in\nstock, we're lucky if we can sell ten a month. Just about all of the\ncompetitive products with bottleneck parts are sold virtually as\nsoon as they come out of final assembly. A few of them sit in the\nwarehouse a day or two before they go to the customer, but due\nto the backlog, not many.\nI look at Jonah. To the four diagrams on the floor, he has\nnow added numbers so that together they look like this . . .\nJonah says, \"We've examined four linear combinations in-\nvolving X and Y. Now, of course, we can create endless combina-\ntions of X and Y. But the four in front of us are fundamental\nenough that we don't have to go any further. Because if we use\nthese like building blocks, we can represent any manufacturing\nsituation. We don't have to look at trillions of combinations of X\nand Y to find what is universally true in all of them; we can\ngeneralize the truth simply by identifying what happens in each\nof these four cases. Can you tell me what you have noticed to be\nsimilar in all of them?\""}
{"219": "activate Y above the level of X, doing so results only in excess\ninventory, not in greater throughput.\n\"Yes, and if we follow that thought to a logical conclusion,\"\nsays Jonah, \"we can form a simple rule which will be true in every\ncase: the level of utilization of a non-bottleneck is not determined\nby its own potential, but by some other constraint in the system.\"\nHe points to the NCX-10.\n\"A major constraint here in your system is this machine,\"\nsays Jonah. \"When you make a non-bottleneck do more work\nthan this machine, you are not increasing productivity. On the\ncontrary, you are doing exactly the opposite. You are creating\nexcess inventory, which is against the goal.\"\n\"But what are we supposed to do?\" asks Bob. \"If we don't\nkeep our people working, we'll have idle time, and idle time will\nlower our efficiencies.\"\n\"So what?\" asks Jonah.\nDonovan is taken aback. \"Beg pardon, but how the hell can\nyou say that?\"\n\"Just take a look behind you,\" says Jonah. \"Take a look at the\nmonster you've made. It did not create itself. You have created\nthis mountain of inventory with your own decisions. And why?\nBecause of the wrong assumption that you must make the work-\ners produce one hundred percent of the time, or else get rid of\nthem to 'save' money.\"\nLou says, \"Well, granted that maybe one hundred percent is\nunrealistic. We just ask for some acceptable percentage, say,\nninety percent.\"\n\"Why is ninety percent acceptable?\" asks Jonah. \"Why not"}
{"220": "making use of the resource in a way that moves the system toward\nthe goal. \"Activating\" a resource is like pressing the ON switch of\na machine; it runs whether or not there is any benefit to be de-\nrived from the work it's doing. So, really, activating a non-bottle-\nneck to its maximum is an act of maximum stupidity.\n\"And the implication of these rules is that we must not seek to\noptimize every resource in the system,\" says Jonah. \"A system of\nlocal optimums is not an optimum system at all; it is a very ineffi-\ncient system.\"\n\"Okay,\" I say, \"but how does knowing this help us get the\nmissing parts unstuck at the milling machines and moved to final\nassembly?\"\nJonah says, \"Think about the build-up of inventory both\nhere and at your milling machines in terms of these two rules we\njust talked about.\"\n\"I think I see the cause of the problem,\" Stacey says, \"We're\nreleasing material faster than the bottlenecks can process it.\"\n\"Yes,\" says Jonah. \"You are sending work onto the floor\nwhenever non-bottlenecks are running out of work to do.\"\nI say, \"Granted, but the milling machines are a bottleneck.\"\nJonah shakes his head and says, \"No, they are not\u2014as evi-\ndenced by all this excess inventory behind you. You see, the mill-\ning machines are not intrinsically a bottleneck. You have turned\nthem into one.\"\nHe tells us that with an increase in throughput, it is possible\nto create new bottlenecks. But most plants have so much extra\ncapacity that it takes an enormous increase in throughput before\nthis happens. We've only had a twenty percent increase. When I"}
{"221": "our ways. Can you tell us what we should do to correct the prob-\nlem?\"\n\"I want you all to think about it as we walk back to your\nconference room and then we'll talk about what you should do,\"\nsays Jonah. \"The solution is fairly simple.\""}
{"222": "26\nJust how simple the solution is doesn't become apparent to\nme until I'm home that night. I'm sitting at the kitchen table with\na pad of paper and a pencil thinking about what was suggested\ntoday when Sharon comes in.\n\"Hi,\" she says as she sits down.\n\"Hi,\" I say back. \"What's up?\"\n\"Not much,\" she says. \"Just wondered what you were do-\ning.\"\n\"I'm working,\" I tell her.\n\"Can I help?\" she asks.\n\"Well ... I don't know,\" I say. \"It's kind of technical. I\nthink you'll probably be bored by it.\"\n\"Oh,\" she says. \"Does that mean you want me to leave?\"\nGuilt strikes.\n\"No, not if you want to stay,\" I tell her. \"Do you want to try\nto solve a problem?\"\n\"Okay,\" she says, brightening.\nI say, \"All right. Let me think of how to put this to you. Do\nyou know about the scout hike Dave and I were on?\"\n\"She doesn't, but I do!\" says Dave, racing into the kitchen.\nHe skids to a stop on the smooth floor and says, \"Sharon doesn't\nknow anything about the hike. But I can help you.\"\nI say, \"Son, I think there is a career for you in sales.\"\nSharon indignantly says, \"Yes, I do know about the hike.\"\n\"You weren't even there,\" says Dave.\n\"I've heard everybody talk about it,\" she says."}
{"223": "I say, \"All right, now both of you go into the other room. I'll\ngive you ten minutes, and then we'll see which one of you comes\nup with the best idea to keep everyone together in the line.\"\n\"What does the winner get?\" asks Dave.\n\"Well . . . anything within reason.\"\n\"Anything?\" asks Sharon.\n\"Within reason,\" I repeat.\nSo they leave and I get about ten minutes of peace and quiet.\nThen I see the two faces looking around the corner.\n\"Ready?\" I ask.\nThey come in and sit down at the kitchen table with me.\n\"Want to hear my idea?\" asks Sharon.\n\"My idea is better,\" says Dave.\n\"It is not!\" she tells him.\n\"Okay, enough!\" I say. \"What's your idea, Sharon?\"\nSharon says, \"A drummer.\"\n\"Pardon me?\"\n\"You know . . . like in a parade,\" she says.\n\"Oh, I know what you mean,\" I say, realizing what she has in\nmind. \"There aren't any gaps in a parade. Everybody is marching\nin step.\"\nSharon beams. Dave gives her a dirty look.\n\"So everybody's marching in step ... to a beat,\" I say,\nthinking out loud. \"Sure. But how do you keep the people in\nfront of Herbie from setting a faster pace?\"\n\"You have Herbie beat the drum,\" says Sharon.\nI think about it and say, \"Yeah, that's not bad.\"\n\"But my idea is better,\" says Dave."}
{"224": "would have to walk at the same speed. I look at Dave, a little in\nawe of his creativity.\n\"Come to think of it, the rope makes it sound like having\nphysical links between all the equipment,\" I tell him, \"which is\nlike an assembly line.\"\n\"Yeah, an assembly line,\" says Dave. \"Didn't you tell me once\nthat an assembly line is supposed to be the best way to make\nthings?\"\n\"Well, yes, it's the most efficient way to manufacture,\" I say.\n\"In fact, we use that approach when we do the final assembly for\nmost of our products. The problem is that an assembly line won't\nwork throughout the whole plant.\"\n\"Oh,\" says Dave.\n\"But those are both good ideas you two thought up,\" I tell\nthem. \"In fact, if we changed each of your ideas just a little bit\nwe'd almost have the solution suggested to us today.\"\n\"Like how?\" asks Sharon.\n\"See, to keep the line from spreading, it actually wouldn't be\nnecessary to keep everyone marching to exactly the same step or\nto keep everyone tied to the rope,\" I tell them. \"What we really\nhave to do is just keep the kid at the front of the line from walk-\ning faster than Herbie. If we can do that, then everybody will stay\ntogether.\"\n\"So we just tie the rope from Herbie to the kid at the front,\"\nsays Dave.\n\"Or, maybe Herbie and the boy at the front of the line have\nsignals,\" says Sharon. \"When the boy in front goes too fast,\nHerbie tells him to wait or slow down.\""}
{"225": "my marital difficulties, the inventory problem at the plant seems\nsimple\u2014or at least it seems simple now. I guess every problem is\neasy once you've figured it out.\nWe are, in effect, going to do what my two kids came up with.\nThe Herbies (the bottlenecks) are going to tell us when to let\nmore inventory into the system\u2014except we're going to use the\naid of computers instead of drums and ropes.\nAfter we returned to the conference room in the office build-\ning today, we started talking, and we all agreed that we're obvi-\nously releasing too much material. We don't need five or six\nweeks of inventory in front of the bottleneck to keep it produc-\ntive.\n\"If we can withhold materials for red parts, instead of push-\ning them out there as soon as the first non-bottleneck has nothing\nto do,\" said Stacey, \"the milling machines will then have time to\nwork on the green parts. And the parts we're missing will reach\nassembly with no problem.\"\nJonah nodded and said, \"That's right. What you have to do\nis find a way to release the material for the red parts according to\nthe rate at which the bottlenecks need material\u2014and strictly at\nthat rate.\"\nThen I said, \"Fine, but how do we time each release of mate-\nrial so it arrives at the bottleneck when it's needed?\"\nStacey said, \"I'm not sure, but I see what you're worried\nabout. We don't want the opposite problem of no work in front of\nthe bottleneck.\"\n\"Hell, we got at least a month before that happens, even if\nwe released no more red tags from today on,\" said Bob. \"But I"}
{"226": "the average setup and process times for each type of part, and\nI'm able to calculate when each batch should clear the bottleneck.\nBecause we're only dealing with one work center, with much less\ndependency, we can average the statistical fluctuations and get a\nbetter degree of accuracy.\"\nRalph went on to say that he knows from observation it takes\nabout two weeks, plus or minus a day or two, for material to\nreach the bottlenecks from the first operations.\n\"So by adding two weeks to the setup and process times of\nwhat's in queue at the bottleneck,\" said Ralph, \"I know how long\nit will take until the bottleneck is actually working on material we\nrelease. And as each batch leaves the bottleneck, we can update\nour information and calculate a date when Stacey should release\nmore red-tag material.\"\nJonah looked at Ralph and said, \"that's excellent!\"\n\"Ralph,\" I said, \"that's terrific. How accurate do you really\nthink we can be with this?\"\n\"I'd say we'd be accurate to within plus or minus a day,\" he\nsaid. \"So if we keep, say, a three-day stock of work-in-process in\nfront of each bottleneck, we should be safe.\"\nEveryone was telling Ralph how impressed they were when\nJonah said, \"But, in fact, Ralph, you can do much more than that\nwith the same information.\"\n\"Like what?\" asked Ralph.\nJonah said, \"You can also attack the inventory problems in\nfront of assembly.\"\n\"You mean we not only can do something about excess in-\nventory on the bottleneck parts, but on the non-bottleneck parts"}
{"227": "moving the bottlenecks to the head of production, which is what\nI'd intended for us to do.\"\n\"Yeah, it sounds good,\" said Ralph. \"But I have to warn you,\nI can't say how long it'll take before I can do all that. I mean, I can\nhave schedule for the red-tagged materials worked out in a fairly\nshort order. The rest of it will take awhile.\"\n\"Aw, come on, Ralphie,\" said Bob, \"a computer wiz like you\nought to be able to crank that out in no time.\"\n\"I can crank something out in no time,\" said Ralph, \"but I'm\nnot going to promise it'll work.\"\nI told him, \"Relax; as long as we ease the load on the milling\nmachines, we'll be okay for the short haul. That'll give you the\ntime to get something basic in place.\"\n\"You may feel you have the time now to relax,\" said Jonah,\n\"but I have to catch a plane for Chicago in thirty-five minutes.\"\n\"Oh, shit,\" I muttered, automatically glancing at my watch.\n\"I guess we'd better move.\"\nIt was not a graceful parting. Jonah and I ran out of the\nbuilding, and I broke numerous speed limits\u2014without incident\u2014\ngetting him to the airport.\n\"I have, shall we say, a special interest in plants like yours,\"\nsaid Jonah. \"So I'd appreciate it if you'd keep me informed of\nwhat happens.\"\n\"Sure,\" I told him. \"No problem. In fact, I'd planned on it.\"\n\"Good,\" said Jonah. \"I'll be talking to you.\"\nAnd with that he was out of the car and, with a wave, was\nsprinting through the terminal doors. I didn't get a call, so I\nsuppose he made it."}
{"228": "\"Yeah, but it sounds like we're going to have a lot of people\nidle around here if we do this,\" says Bob.\n\"Yeah, we might have some people idle from time to time,\" I\nadmit.\n\"So are we just supposed to let everyone stand around out\nthere?\" asks Bob.\n\"Why not?\" asks Stacey. \"Once the somebody is already on\nthe payroll, it doesn't cost us any more to have him be idle.\nWhether somebody produces parts or waits a few minutes doesn't\nincrease our operating expense. But excess inventory . . . now\nthat ties up a lot of money.\"\n\"Okay,\" says Bob, \"but what about the reporting system?\nSeems to me that at the end of the month, when old Bill Peach is\nready to decide if we stay open or if we close down, he's not going\nto be awfully positive about us if he sees our efficiencies have\ntaken a dive. I hear they do tend to frown upon that at headquar-\nters.\"\nThere is quiet in the room. Then Lou says, \"He does have a\npoint, Al.\"\nI listen to the hum of the air conditioning for a moment.\n\"All right, look,\" I say finally. \"If we don't go ahead with a\nsystem to withhold inventory and release it according to the bot-\ntlenecks, we'll be missing a major opportunity to improve perfor-\nmance and save the plant. And I'm not about to stand by and let\nthat happen just to maintain a standard that obviously has more\nimpact on middle management politics than it does on the bot-\ntom line. I say we go ahead with this. And if efficiencies drop, let\nthem.\""}
{"229": "27\n\". . . Let me say in conclusion that had it not been for the in-\ncrease in revenue generated last month by the Bearington plant\nand its products, the UniWare Division's losses would have con-\ntinued for the seventh consecutive month. All of the other manu-\nfacturing operations in the division reported only marginal gains\nin performance or sustained losses. Despite the improvement at\nBearington and the fact that as a result the division recorded its\nfirst operating profit of this year, we have a long way to go before\nwe are back on solid financial footing.\"\nHaving said that, Ethan Frost gets the nod from Bill Peach\nand sits down. I'm sitting halfway down a long table where all the\nplant managers are gathered. On Peach's right is Hilton Smyth,\nwho happens to be glowering at me in the aftermath of Frost's\ntribute to my plant. I relax in my chair and for a moment allow\nmyself to contemplate the view through the broad plateglass win-\ndow, a sunny city on an early summer day.\nMay has ended. Aside from the problem with the shortages\nof non-bottleneck parts, which have now gone away, it's been an\nexcellent month. We're now timing the release of all materials\naccording to a new system Ralph Nakamura developed, which is\nkeyed to the speed of the bottlenecks. He's got a data terminal\nnow at both of the bottlenecks, so as inventory is processed, the\nlatest information can be fed directly into the plant data base.\nWith the new system we're beginning to see excellent results.\nRalph did a little experimenting with the system and soon\ndiscovered we can predict within a day, more or less, when a"}
{"230": "good month. Inventory levels have fallen and are continuing to\nfall rapidly. Withholding some materials has meant we're no\nlonger choking on work-in-process. Parts are reaching the bottle-\nnecks when they're supposed to, and the flow through the plant\nis much smoother than before.\nAnd what happened to efficiencies? Well, they did fall ini-\ntially as we began to withhold raw material from the floor, but not\nas much as we had been afraid they would\u2014it turns out we were\nconsuming excess inventory. But with the rate of shipments up\ndramatically, that excess has melted quickly. And now that we're\nbeginning to resume releases of materials to non-bottlenecks\nagain, efficiencies are on their way back up. Donovan has even\ntold me confidentially he thinks the real numbers in the future\nwill be almost the same as before.\nThe best news is we've wiped out our backlog of overdue\norders. Amazing as it seems, we're completely caught up. So cus-\ntomer service has improved. Throughput is up. We're on our way\nback. It's too bad the standard report we've prepared can't begin\nto tell the full story of what's really going on.\nWhen I've finished, I look up the table and see Hilton Smyth\nwhispering something to Bill Peach. There is quiet around the\ntable for a moment. Then Bill nods to Hilton and talks to me.\n\"Good job, Al,\" Bill says stiffly.\nThrough with me, Bill asks another manager to deliver his\nreport. I sit back, irritated slightly that Peach wasn't more posi-\ntive, that he didn't put more praise on me the way Frost had\nindicated he should. I came in here feeling as though we'd really\nturned the plant around. And I guess I expected a little more"}
{"231": "hard nose and tell me to run the plant by the cost accounting\nmethods he believes in.\nI have to bide my time until I can go to him with a solid case\nthat my way (Jonah's way, really) is the one that truly works. It's\ntoo early for that. We've broken too many rules to tell him the full\nstory now.\nBut will we have the time? That's what I keep asking myself.\nPeach hasn't voluntarily lifted the threat to close the plant. I\nthought he might say something (publicly or privately) after this\nreport, but he hasn't. I look at him at the end of the table. He\nseems distracted, not like himself. The others talk and he seems\nonly half interested. Hilton seems to cue him on what to say.\nWhat's with him?\nThe meeting breaks up about an hour after lunch, and by\nthen I've decided to have a private talk with Peach if I can get it. I\nfollow him out into the corridor from the conference room and\nask him. He invites me into his office.\n\"So when are you going to let us off the hook?\" I ask him\nafter the door is closed.\nBill sits down in a big upholstered chair and I take the one\nopposite him. Without the desk between us, it's a nice little inti-\nmate chat.\nBill looks straight at me and says, \"What makes you think\nI'm going to?\"\n\"Bearington is on its way back,\" I tell him. \"We can make\nthat plant make money for the division.\"\n\"Can you?\" he asks. \"Look, Al, you've had a good month.\nThat's a step in the right direction. But can you give us a second"}
{"232": "He shakes his head. \"It'll have to be a bigger improvement\nthan what you gave us in this past period.\"\n\"How big?\"\n\"Just give me fifteen percent more on the bottom line than\nyou did this month,\" he says.\nI nod. \"I think we can do that,\" I say\u2014and note the split\nsecond of shock blink into Peach's face.\nThen he says, \"Fine. If you can deliver that, and keep deliv-\nering it, we'll keep Bearington open.\"\nI smile. If I do this for you, I'm thinking, you'd be an idiot to\nclose us.\nPeach stands, our chat concluded.\nI fly the Mazda up the entrance ramp to the Interstate with\nthe accelerator floored and the radio turned up loud. The adren-\nalin is pumping. The thoughts in my head are racing faster than\nthe car.\nTwo months ago I figured I might be sending out my resume\nby now. But Peach just said if we turned in another good month\nhe'd let the plant stay open. We're almost there. We just might be\nable to pull this off. Just one more month.\nBut fifteen percent?\nWe've been eating up our backlog of orders at a terrific rate.\nAnd by doing so we've been able to ship a tremendous volume of\nproduct\u2014tremendous by any comparison: last month, last quar-\nter, last year. It's given us a big surge of income, and it's looked\nfantastic on the books. But now that we've shipped all the\noverdues, and we're putting out new orders much faster than\nbefore. . . ."}
{"233": "scheduled for the first week or two of July and ship them in June\ninstead.\nBut what am I going to do after that? I'm going to be putting\nus into a huge hole in which we have nothing else to do. We need\nmore business.\nI wonder where Jonah is these days.\nGlancing down at the speedometer, I find to my surprise\nthat I'm zipping along at eighty. I slow down. I loosen my tie. No\nsense killing myself trying to get back to the plant. It occurs to\nme, in fact, that by the time I get back to the plant it'll be time to\ngo home.\nJust about then, I pass a sign saying I'm two miles from the\ninterchange that would put me on the highway to Forest Grove.\nWell, why not? I haven't seen Julie or the kids in a couple of days.\nSince the end of school, the kids have been staying with Julie and\nher parents.\nI take the interchange and get off at the next exit. At a gas\nstation on the corner, I make a call to the office. Fran answers and\nI tell her two things: First, pass the word to Bob, Stacey, Ralph,\nand Lou that the meeting went well for us. And, second, I tell her\nnot to expect me to come in this afternoon.\nWhen I get to the Barnett's house, I get a nice welcome. I\nspend quite a while just talking to Sharon and Dave. Then Julie\nsuggests we go for a walk together. It's a fine summer afternoon\noutside.\nAs I'm hugging Sharon to say goodbye, she whispers in my\near, \"Daddy, when are we all going to go home together?\"\n\"Real soon, I hope,\" I tell her."}
{"234": "sincerely, \"You've been a lot of fun to be around in the last few\nweeks.\"\n\"Thanks. The feeling is mutual,\" I say.\nShe takes my hand and says, \"But . . . I'm sorry, Al. I'm\nstill worried about coming home.\"\n\"Why? We're getting along a lot better now,\" I say, \"What's\nthe problem?\"\n\"Look, we've had some good times for a change. And that's\nfine. I've really needed this time with you,\" she says. \"But if we\ngo back to living together, you know what's going to happen\ndon't you? Everything will be fine for about two days. But a week\nfrom now we'll be having the same arguments. And a month\nlater, or six months, or a year from now . . . well, you know\nwhat I mean.\"\nI sigh. \"Julie, was it that bad living with me?\"\n\"Al, it wasn't bad,\" she says. \"It was just ... I don't know.\nYou weren't paying any attention to me.\"\n\"But I was having all kinds of problems in my job. I was\nreally in over my head for awhile. What did you expect from\nme?\"\n\"More than what I was getting,\" Julie says. \"You know, when\nI was growing up, my father always came home from work at the\nsame time. The whole family always ate together. He spent the\nevenings at home. With you, I never know what's going on.\"\n\"You can't compare me to your father,\" I say. \"He's a den-\ntist. After the last tooth of the day is filled, he can lock up and go\nhome. My business isn't like that.\"\n\"Alex, the problem is you are not like that,\" she says. \"Other"}
{"235": "\"Do you want another fight?\" she asks.\nI look the other way.\n\"No, I don't want to fight,\" I tell her.\nI hear her sigh. Then she says, \"You see? Nothing has\nchanged . . . has it.\"\nNeither of us says a word for quite awhile. Julie gets up and\nwalks over to the river. It looks for a second as if she might run\naway. She doesn't. She comes back again and sits down on the\nbench.\nShe says to me, \"When I was eighteen, I had everything\nplanned\u2014college, a teaching degree, marriage, a house, chil-\ndren. In that order. All the decisions were made. I knew what\nchina pattern I wanted. I knew the names I wanted for the kids. I\nknew what the house should look like and what color the rug\nshould be. Everything was certain. And it was so important that I\nhave it all. But now ... I have it all, only it's different somehow.\nNone of it seems to matter.\"\n\"Julie, why does your life have to conform to this . . . this\nperfect image you have in your head?\" I ask her. \"Do you even\nknow why you want the things you do?\"\n\"Because that's how I grew up,\" she says. \"And what about\nyou? Why do you have to have this big career? Why do you feel\ncompelled to work twenty-four hours a day?\"\nSilence.\nThen she says, \"I'm sorry. I'm just very confused.\"\n\"No, that's okay,\" I say. \"It was a good question. I have no\nidea why I wouldn't be satisfied being a grocer, or a nine-to-five\noffice worker.\""}
{"236": "love . . . because of all the reasons everybody else does,\" she\nsays. \"Alex, you're asking a lot of dumb questions.\"\n\"Whether they're dumb or smart, I'm asking them because\nwe've been living together for fifteen years and we have no clear\nunderstanding of what our marriage is supposed to do ... or\nbecome ... or anything!\" I sputter. \"We're just coasting along,\ndoing 'what everyone else does.' And it turns out the two of us\nhave some very different assumptions of what our lives are sup-\nposed to be like.\"\n\"My parents have been married for thirty-seven years,\" she\nsays, \"and they never asked any questions. Nobody ever asks\n'What is the goal of a marriage?' People just get married because\nthey're in love.\"\n\"Oh. Well, that explains everything, doesn't it,\" I say.\n\"Al, please don't ask these questions,\" she says. \"They don't\nhave any answers. And if we keep talking this way, we're going to\nruin everything. If this is your way of saying you're having second\nthoughts about us\u2014\"\n\"Julie, I'm not having second thoughts about you. But you're\nthe one who can't figure out what's wrong with us. Maybe if you\ntried to think about this logically instead of simply comparing us\nto the characters in a romance novel\u2014\"\n\"I do not read romance novels,\" she says.\n\"Then where did you get your ideas about how a marriage is\nsupposed to be?\" I ask her.\nShe says nothing.\n\"All I'm saying is we ought to throw away for the moment all\nthe pre-conceptions we have about our marriage, and just take a"}
{"237": "\"Will I see you again on Saturday?\" she asks.\nI smile a little \"Yeah, sure. Sounds good.\"\nShe says, \"I'm sorry about what happened.\"\n\"I guess we'll just have to keep trying until we get it right.\"\nWe both start smiling. Then we do some of that nice stuff\nthat makes an argument almost worth the agony."}
{"238": "28\nI get home just as the sun is starting to set. The sky is rosy\npink. As I'm unlocking the kitchen door, I hear the phone ring-\ning inside. I rush in to grab it.\n\"Good morning,\" says Jonah.\n\"Morning?\" Outside the window, the sun is almost below the\nhorizon. I laugh. \"I'm watching the sun set. Where are you calling\nfrom?\"\n\"Singapore,\" he says.\n\"Oh.\"\n\"By the way, from my hotel I'm watching the sun rise,\" Jonah\nsays. \"Alex, I wouldn't have called you at home, but I'm not\ngoing to be able to talk to you again for a few weeks.\"\n\"Why not?\"\n\"Well, it's a long story and I can't go into it now,\" he says.\n\"But I'm sure we'll have a chance to discuss it some time.\"\n\"I see. ...\" I wonder what's going on, but say, \"That's too\nbad. It puts me in a kind of a bind, because I was just about to ask\nfor your help again.\"\n\"Has something gone wrong?\" he asks.\n\"No,\" I tell him. \"Everything is generally going very well\nfrom an operations standpoint. But I just had a meeting with my\ndivision vice president, and I was told the plant has to show an\neven bigger improvement.\"\n\"You're still not making money?\" he asks.\nI say, \"Yes, we are making money again, but we need to\naccelerate the improvement to save the plant from being shut"}
{"239": "So I do. Then, wondering if we've reached some theoretical\nlimit by now, I ask him if there is anything else we can try.\n\"Anything else?\" he says. \"Believe me, we have only begun.\nNow, here's what I suggest. . . .\"\nEarly the next morning, I'm in my office at the plant consid-\nering what Jonah told me. Outside is the dawn of the day he's\nalready seen in Singapore. Stepping out to get a cup of coffee, I\nfind Stacey at the coffee machine.\n\"Hello there,\" she says. \"I hear everything went fairly well\nfor us at headquarters yesterday.\"\n\"Well, not bad,\" I say. \"I'm afraid we still have a way to go\nbefore we convince Peach we're good for the long term. But I\ntalked to Jonah last night.\"\n\"Did you tell him about our progress?\" she asks.\n\"Yes,\" I say. \"And he suggested we try what he called 'the\nnext logical step.''\nI see her face take on a nervous grin. \"What's that?\"\n\"Cut our batch sizes in half on non-bottlenecks,\" I say.\nStacy takes a step back as she thinks about this. \"But why?\"\nshe asks.\nI say with a smile, \"Because in the end we'll make more\nmoney.\"\n\"I don't understand,\" she says. \"How is that going to help\nus?\"\n\"Hey, Stacey, you're in charge of inventory control,\" I tell\nher. \"You tell me what would happen if we cut our batch sizes in\nhalf.\"\nThinking, she sips her coffee for a moment. Her brow com-"}
{"240": "negotiating through purchasing, and I'm not sure all the vendors\nwill go for it.\"\nI tell her, \"That's something we can work on. Eventually\nthey'll go for it because it's to their advantage as well as ours.\"\n\"But if we go to smaller batch sizes,\" she says, squinting at\nme in cynicism, \"doesn't that mean we'll have to have more set-\nups on equipment?\"\n\"Sure,\" I say, \"don't worry about it.\"\n\"Don't\u2014?\"\n\"Yeah, don't worry about it.\"\n\"But Donovan\u2014\"\n\"Donovan will do just fine, even with more setups,\" I say.\n\"And, meanwhile, there is another set of benefits, aside from what\nyou said, that we can have almost immediately.\"\n\"What's that?\" she asks.\n\"You really want to know?\"\n\"Sure, I do.\"\n\"Good. You set up a meeting with the other functions and I'll\ntell everyone at the same time.\"\nFor dumping that little chore of the meeting arrangements\non her, Stacey pays me back in kind by setting the meeting for\nnoon at the most expensive restaurant in town\u2014with lunch bill-\nable to my expense number, of course.\n\"What could I do?\" she asks as we sit down at the table. \"It\nwas the only time everybody was available, right, Bob?\"\n\"Right,\" says Bob.\nI'm not mad. Given the quality and quantity of work these\npeople have done recently, I can't complain about picking up the"}
{"241": "Another is process time, which is the amount of time the part\nspends being modified into a new, more valuable form.\nA third element is queue time, which is the time the part\nspends in line for a resource while the resource is busy working\non something else ahead of it.\nThe fourth element is wait time, which is the time the part\nwaits, not for a resource, but for another part so they can be\nassembled together.\nAs Jonah pointed out last night, setup and process are a\nsmall portion of the total elapsed time for any part. But queue\nand wait often consume large amounts of time\u2014in fact, the ma-\njority of the elapsed total that the part spends inside the plant.\nFor parts that are going through bottlenecks, queue is the\ndominant portion. The part is stuck in front of the bottleneck for\na long time. For parts that are only going through non-bottlenecks,\nwait is dominant, because they are waiting in front of assembly for\nparts that are coming from the bottlenecks. Which means that in\neach case, the bottlenecks are what dictate this elapsed time.\nWhich, in turn, means the bottlenecks dictate inventory as well as\nthroughput.\nWe have been setting batch sizes according to an economical\nbatch quantity (or EBQ) formula. Last night, Jonah told me that\nalthough he didn't have time over the phone to go into all the\nreasons, EBQ has a number of flawed assumptions underlying it.\nInstead, he asked me to consider what would happen if we cut\nbatch sizes by half from their present quantities.\nIf we reduce batch sizes by half, we also reduce by half the\ntime it will take to process a batch. That means we reduce queue"}
{"242": "\"That means more customers come to us because we can\ndeliver faster,\" says Lou.\n\"Our sales increase!\" I say.\n\"And so do our bonuses!\" says Stacey.\n\"Whoa! Whoa now! Hold up here a minute!\" says Bob.\n\"What's the matter?\" I ask him.\n\"What about setup time?\" he says. \"You can batch sizes in\nhalf, you double the number of setups. What about direct labor?\nWe got to save on setups to keep down costs.\"\n\"Okay, I knew this would come up,\" I tell them. \"Now look,\nit's time we think about this carefully. Jonah told me last night\nthat there was a corresponding rule to the one about an hour lost\nat a bottleneck. You remember that? An hour lost at a bottleneck\nis an hour lost for the entire system.\"\n\"Yeah, I remember,\" Bob says.\nI say, \"The rule he gave me last night is that an hour saved at\na non-bottleneck is a mirage.\"\n\"A mirage!\" he says. \"What do you mean, an hour saved at a\nnon-bottleneck is a mirage? An hour saved is an hour saved!\"\n\"No, it isn't,\" I tell him. \"Since we began withholding materi-\nals from the floor until the bottlenecks are ready for them, the\nnon-bottlenecks now have idle time. It's perfectly okay to have\nmore setups on non-bottlenecks, because all we're doing is cut-\nting into time the machines would spend being idle. Saving set-\nups at a non-bottleneck doesn't make the system one bit more\nproductive. The time and money saved is an illusion. Even if we\ndouble the number of setups, it won't consume all the idle time.\"\n\"Okay, okay,\" says Bob. \"I guess I can see what you mean.\""}
{"243": "Finally, Bob admits, \"Okay, if we cut batch sizes in half, then\nthat means it ought to take half the time it does now. So instead of\nsix to eight weeks, it should take about four weeks . . . maybe\neven three weeks in a lot of cases.\"\n\"Suppose I go to marketing and tell them to promise cus-\ntomers deliveries in three weeks?\" I say.\n\"Whoa! Hold on!\" says Bob.\n\"Yeah, give us a break!\" says Stacey.\n\"All right, four weeks then,\" I say. \"That's reasonable, isn't\nit?\"\n\"Sounds reasonable to me,\" says Ralph.\n\"Well . . . okay,\" says Stacey.\n\"I think we should risk it,\" says Lou.\n\"So are you willing to commit to this with us?\" I ask Bob.\nBob sits back and says, \"Well . . . I'm all for bigger bonuses.\nWhat the hell. Let's try it.\"\nFriday morning finds the Mazda and me again hustling up\nthe Interstate toward headquarters. I hit town just as the sun hits\nthe glass of the UniCo building and reflects a blinding glare.\nKind of pretty actually. For a moment, it takes my mind off my\nnerves. I've got a meeting scheduled with Johnny Jons in his\noffice. When I called, he was quite willing to see me, but sounded\nless than enthusiastic about what I said I'd like to talk about. I feel\nthere's a lot riding on my ability to convince him to go along with\nwhat we want to do. So I've found myself biting a fingernail or\ntwo during the trip.\nJons doesn't really have a desk in his office. He has a sheet of\nglass on chrome legs. I guess that's so that everyone can get a"}
{"244": "Jons nods and says, \"Yes, I've noticed my phone hasn't been\nringing lately with complaints from customers missing their or-\nders.\"\n\"My point,\" I tell him, \"is that we've really turned the plant\naround. Here, look at this.\"\nFrom my breifcase, I take the latest list of customer orders.\nAmong other things, it shows the due dates promised, along with\nthe dates when Ralph expected shipment, and the dates the prod-\nucts were actually shipped.\n\"You see,\" I tell Jons as he studies the list on the glass top of\nhis table, \"we can predict to within twenty-four hours one way or\nthe other when an order will leave the plant.\"\n\"Yes, I've seen something like this floating around,\" says\nJons. \"These are the dates?\"\n\"Of course.\"\n\"This is impressive,\" says Jons.\n\"As you can see by comparing a few recently shipped orders\nwith ones of a month or so before, our production lead times\nhave condensed dramatically. Four months' lead time is no\nlonger a holy number with us. From the day you sign the contract\nwith the customer to the day we ship, the current average is\nabout two months. Now, tell me, do you think that could help us\nin the marketplace?\"\n\"Sure it could,\" says Jons.\n\"Then how about four weeks'?\"\n\"What? Al, don't be ridiculous,\" says Jons. \"Four weeks!\"\n\"We can do it.\"\n\"Come on!\" he says. \"Last winter, when demand for every"}
{"245": "turn around an order of 200 Model 12's or 300 DBD-50's in four\nweeks?\"\n\"Try me,\" I tell him. \"Get me five orders\u2014hell, get me ten\norders\u2014and I'll prove it to you.\"\n\"And what happens to our credibility if you can't come\nthrough?\" he asks.\nFlustered, I look down through the glass table.\n\"Johnny,\" I say, \"I'll make a bet with you. If I don't deliver\nin four weeks, I'll buy you a brand new pair of Guccis.\"\nHe laughs, shakes his head and finally says, \"Okay, you're\non. I'll pass the word to the salespeople that on all your products,\nwe're offering terms of factory shipment in six weeks.\"\nI start to protest. Jons holds up a hand.\n\"I know you're confident,\" he says. \"And if you ship any new\norders in less than five weeks, I'll buy you a new pair of shoes.\""}
{"246": "29\nA full moon is shining through the bedroom window and\ninto my eyes. The night is still. I look at the clock beside me,\nwhich says it's 4:20 A.M. Next to me in bed, Julie is sleeping.\nResting on my elbow, I look down at Julie. With her dark\nhair spilled out on the white pillow, she looks nice sleeping in the\nmoonlight. I watch her for a while. I wonder what her dreams are\nlike.\nWhen I woke up, I was having a nightmare. It was about the\nplant. I was running up and down the aisles and Bill Peach was\nchasing me in his crimson Mercedes. Every time he was about to\nrun me over, I'd duck between a couple of machines or hop on a\npassing forklift. He was yelling at me from the window about my\nbottom line not being good enough. Finally he trapped me in the\nshipping department. I had my back against stacks of cardboard\ncartons, and the Mercedes was racing toward me at a hundred\nmiles an hour. I tried to shield my eyes from the blinding head-\nlights. Just as Peach was about to get me, I woke up and discov-\nered that the headlights were moonbeams on my face.\nNow I'm too much awake, and too aware of the problem I\nwas trying to forget this past evening with Julie for me to fall back\nto sleep. Not wanting to awaken Julie with my restlessness, I slip\nout of bed.\nThe house is all ours tonight. We started out this evening\nwith nothing particular to do, when we remembered we had a\nwhole house in Bearington with nobody in it to bother us. So we\nbought a bottle of wine, some cheese and a loaf of bread, came"}
{"247": "efficiencies have gone up, not down, as a result of what we've\nbeen doing in the plant. After we began withholding the release\nof materials and timing the releases according to the completed\nprocessing of heat-treat and the NCX-10, efficiencies dipped\nsomewhat. But that was because we were consuming excess in-\nventories. When the excess inventories were exhausted\u2014which\nhappened quickly as a result of the increase in throughput\u2014effi-\nciencies came back up again.\nThen, two weeks ago, we implemented the new smaller batch\nsizes. When we cut batch sizes in half for non-bottlenecks, effi-\nciencies stayed solid, and now it seems as though we're keeping\nthe work force even more occupied than before.\nThat's because a really terrific thing has happened. Before\nwe reduced batch sizes, it wasn't uncommon for a work center to\nbe forced idle because it didn't have anything to process\u2014even\nthough we were wading through excess inventory. It was usually\nbecause the idle work center had to wait for the one preceding it\nto finish a large batch of some item. Unless told otherwise by an\nexpediter, the materials handlers would wait until an entire batch\nwas completed before moving it. In fact, that's still the case. But\nnow that the batches are smaller, the parts are ready to be moved\nto the next work station sooner than they were before.\nWhat we had been doing many times was turning a non-\nbottleneck into a temporary bottleneck. This was forcing other\nwork centers downstream from it to be idle, which reflected\npoorly on efficiencies. Now, even though we've recognized that\nnon-bottlenecks have to be idle periodically, there is actually less\nidle time than before. Since we cut batch sizes, work is flowing"}
{"248": "filled the plant back up again by dumping new work-in-process\non the floor. The only work-in-process out there now is for cur-\nrent demand.\nBut then there's the bad news. Which is what I'm thinking\nabout when I hear footsteps on the carpet behind me in the dark\n\"Al?\"\n\"Yeah.\"\n\"How come you're out here in the dark?\"\n\"Can't sleep.\"\n\"What's wrong?\"\n\"Nothing.\"\n\"Then why don't you come back to bed?\"\n\"I'm just thinking about some things.\"\nIt's quiet for a second. For a moment, I think she's gone\naway. Then I feel her beside me.\n\"Is it the plant?\" she asks.\n\"Yeah.\"\n\"But I thought everything was getting better,\" she says.\n\"What's wrong?\"\n\"It has to do with our cost measurement,\" I tell her.\nShe sits down beside me.\n\"Why don't you tell me about it,\" she says.\n\"Sure you want to hear about it?\" I ask.\n\"Yes, I do.\"\nSo I tell her: the cost of parts looks as though it's gone up\nbecause of the additional setups necessitated by the smaller batch\nsizes.\n\"Oh,\" she says. \"I guess that's bad, right?\""}
{"249": "countants, the cost of the part is based upon direct labor of 6.2\nminutes.\n\"Now if we cut the batch in half, we still have the same\namount of set-up time. But it's spread over 50 parts instead of\n100. So now we've got 5 minutes of process time, plus 2.4 minutes\nof set-up for a grand total of 7.4 minutes of direct labor. And the\ncalculations are all based on the cost of direct labor.\"\nThen I explain the way costs are calculated. First, there is the\nraw material cost. Then there is the cost of direct labor. And\nfinally there is \"burden,\" which essentially works out to be cost of\nthe direct labor multiplied by a factor, in our case, of about three.\nSo on paper, if the direct labor goes up, the burden also goes up.\n\"So with more set-ups, the cost of making parts goes up,\"\nsays Julie.\n\"It looks that way,\" I tell her, \"but in fact it hasn't really done\nanything to our actual expenses. We haven't added more people\nto the payroll. We haven't added any additional cost by doing\nmore set-ups. In fact, the cost of parts has gone down since we\nbegan the smaller batch sizes.\"\n\"Down? How come?\"\n\"Because we've reduced inventory and increased the amount\nof money we're bringing in through sales,\" I explain. \"So the\nsame burden, the same direct labor cost is now spread over more\nproduct. By making and selling more product for the same cost,\nour operating expense has gone down, not up.\"\n\"How could the measurement be wrong?\" she asks.\nI say, \"The measurement assumes that all of the workers in\nthe plant are always going to be fully occupied, and therefore, in"}
{"250": "\"I can change the base we're using for determining the cost\nof parts. Instead of using the cost factor of the past twelve\nmonths, which is what I'm supposed to be doing, we can use the\npast two months. That will help us, because for the past two\nmonths, we've had big increases in throughput.\"\n\"Yeah,\" I say, sensing the possibilities. \"Yeah, that might\nwork. And actually the past two months are a lot more represen-\ntative of what's really going on here than what happened last\nyear.\"\nLou leans from side to side. He says, \"We-l-l-l, yes, that's\ntrue. But according to accounting policy, it's not valid.\"\n\"Okay, but we have a good excuse,\" I say. \"The plant is\ndifferent now. We're really a hell of a lot better than we were.\"\n\"Al, the problem is Ethan Frost will never buy it,\" says Lou.\n\"Then why did you suggest it?\"\n\"Frost won't buy it if he knows about it,\" says Lou.\nI nod slowly. \"I see.\"\n\"I can give you something that will slide through on the first\nglance,\" says Lou. \"But if Frost and his assistants at division do\nany checking, they'll see through it in no time.\"\n\"You're saying we could end up in very hot water,\" I say.\n\"Yeah, but if you want to take a chance. . . .\" says Lou.\n\"It could give us a couple more months to really show what\nwe can do,\" I say, finishing the thought for him.\nI get up and walk around for a minute turning this over in\nmy mind.\nFinally I look at Lou and say, \"There is no way I can show\nPeach an increase in the cost of parts and convince him the plant"}
{"251": "\"Remember our dear friend Bucky Burnside?\" says Jons.\n\"How could I forget good ole Bucky,\" I say. \"Is he still com-\nplaining about us?\"\n\"No, not anymore,\" says Jons. \"At the moment, in fact, we\ndon't even have a single active contract with Burnside's people.\nThat's the reason I'm calling. For the first time in months, they've\nexpressed interest in buying something from us again.\"\n\"What are they interested in?\"\n\"Model 12's,\" he says. \"They need a thousand units.\"\n\"Terrific!\"\n\"Maybe not,\" says Jons. \"They need the whole order by the\nend of the month.\"\n\"That's only about two weeks away,\" I say.\n\"I know,\" says Jons. \"The sales rep on this already checked\nwith the warehouse. Turns out we've only got about fifty of the\nModel 12's in stock.\"\nHe's telling me, of course, we'll have to manufacture the\nother 950 by the end of the month if we want the business.\n\"Well . . . Johnny, look, I know I told you I wanted busi-\nness, and you've pulled in some nice contracts since I talked to\nyou,\" I say. \"But a thousand Model 12's in two weeks is asking a\nlot.\"\nHe says, \"Al, to tell you the truth, I didn't really think we\ncould do anything with this one when I called. But I thought I'd\nlet you know about it, just in case you knew something I didn't.\nAfter all, a thousand units means a little over a million dollars in\nsales to us.\"\n\"Yes, I realize that,\" I say. \"Look, what's going on that they"}
{"252": "\"Well, I don't know. I'd like that business back again, too,\nbut. . . .\"\n\"The real kick in the head is if we had only had the foresight\nto build a finished goods inventory of Model 12's while we had\nthose slow sales months, we could have made this sale,\" he says.\nI have to smile to myself, because at the beginning of the\nyear I might have agreed with that.\n\"It's too bad,\" Johnny is saying. \"Aside from the initial busi-\nness, it could have been a big opportunity for us.\"\n\"How big?\"\n\"Strong hints have been dropped that if we can come\nthrough on this one, we could become their preferred supplier,\"\nsays Jons.\nI'm quiet for a moment.\n\"All right. You really want this, don't you?\" I ask him.\n\"So bad I can taste it,\" he says. \"But if it's impossible. . . .\"\n\"When do you have to let them know?\" I ask.\n\"Probably sometime today, or tomorrow at the latest,\" he\nsays. \"Why? Do you think we can really do it?\"\n\"Maybe there's a way. Let me see how we stand and I'll give\nyou a call back,\" I tell him.\nAs soon as I get off the phone with Jons, I round up Bob,\nStacey, and Ralph for a meeting in my office, and when we're all\ntogether I tell him what Jons told me.\n\"Ordinarily, I would think this is out of the question,\" I say.\n\"But before we say no, let's think about it.\"\nEverybody looks at me with the certain knowledge this is\ngoing to be a waste of time."}
{"253": "screwing up relations with a dozen customers just to please one.\n\"Let's try something else.\"\n\"Like what?\" asks Bob, who is sitting there with us, looking\nabout as enthusiastic as a bump on a log.\nI say, \"A few weeks ago, we cut our batch sizes by half, and\nthe result was we could condense the time inventory spends in\nthe plant, which also gave us an increase in throughput. What if\nwe cut the batch sizes by half again?\"\nRalph says, \"Gee, I hadn't thought of that.\"\nBob leans forward. \"Cut them again? Sorry, Al, but I don't\nsee how the heck that can help us, not with the volume we're\nalready committed to.\"\n\"You know,\" says Ralph, \"we have quite a few orders we'd\nplanned to ship ahead of their due dates. We could re-schedule\nsome of those in the priority system so they'd ship when prom-\nised instead of early. That could give us more time available on\nthe bottlenecks, and it wouldn't hurt anybody.\"\n\"Good point, Ralph,\" I tell him.\n\"But, hell, we still can't get a thousand units done no-how,\"\ndrawls Bob. \"Not in two weeks.\"\nI say, \"Well, then, if we cut the batch sizes, how many units\ncan we do in two weeks and still ship our current orders on time.\"\nBob pulls on his chin and says, \"I guess we could look into\nit.\"\n\"I'll see what I can find out,\" says Ralph, standing so he can\nleave and go back to his computer.\nHis interest finally piqued, Bob says, \"Maybe I'd better go\nwith you so we can noodle this thing out together.\""}
{"254": "ask her. \"And how soon could they ship the first week's quantity\nto us?\"\n\"I don't know, but doing it that way, we might not be able to\nget a volume discount,\" says Stacey.\n\"Why not?\" I ask. \"We'd be committing to the same thou-\nsand units\u2014it's just that we'd be staggering the shipments.\"\n\"Well, then there's the added shipping cost,\" she says.\n\"Stacey, we're talking a million dollars in business here,\" I\ntell her.\n\"Okay, but they'll take at least three days to a week to get\nhere by truck,\" she says.\n\"So why can't we have them shipped air freight?\" I ask.\n\"They're not very big parts.\"\n\"Well. . . .\" says Stacey.\n\"Look into it, but I doubt if the air freight bill is going to eat\nup the profit on a million-dollar sale,\" I tell her. \"And if we can't\nget these parts, we can't get the sale.\"\n\"All right. I'll see what they can do,\" she says.\nAt the end of the day, the details are still being sweated out,\nbut we know enough for me to place a call to Jons.\n\"I've got a deal on those Model 12's for you to relay to Burn-\nside,\" I say.\n\"Really?\" says Jons excitedly. \"You want to take the busi-\nness?\"\n\"Under certain conditions,\" I tell him. \"First of all, there is\nno way we can deliver the full thousand units in two weeks. But\nwe can ship 250 per week to them for four weeks.\"\n\"Well, okay, they might go for that,\" says Jons, \"but when"}
{"255": "A couple of hours later, my phone rings at home.\n\"Al? We got it! We got the order!\" shouts Jons into my right\near.\nAnd in my left ear, I hear a million bucks rung up on the\ncash register.\n\"You know what?\" Jons is saying. \"They even like the smaller\nshipments better than getting all thousand units at once!\"\nI tell him, \"Okay, great, I'll get the ball rolling right away.\nYou can tell them that two weeks from today, we'll ship the first\n250.\""}
{"256": "30\nAt the beginning of the new month, we have a staff meeting.\nEveryone is present except Lou. Bob tells me he'll be in shortly. I\nsit down and fidget. To get the meeting rolling while we're wait-\ning for Lou, I ask about shipments.\n\"How is Burnside's order coming along?\" I ask.\n\"The first shipment went out as scheduled,\" says Donovan.\n\"How about the rest of it?\" I ask.\n\"No problems to speak of,\" says Stacey. \"The control boxes\nwere a day late, but there was time enough for us to assemble\nwithout delaying the shipment. We got this week's batch from the\nvendor on time.\"\nI say, \"Good. What's the latest on the smaller batches?\"\n\"The flow through the shop is even better now,\" says Bob.\n\"Excellent,\" I say.\nJust then Lou comes into the meeting. He's late because he\nwas finishing the figures for this month. He sits down and looks\nstraight at me.\n\"Well?\" I ask. \"Did we get our fifteen percent?\"\n\"No,\" he says, \"we got seventeen percent, thanks in part to\nBurnside. And the coming month looks just fine.\"\nThen he goes into a wrap-up of how we performed through\nthe second quarter. We're now solidly in the black. Inventories\nare about forty percent of what they were three months ago.\nThroughput has doubled.\n\"Well, we've come a long way, haven't we?\" I ask.\nSitting on my desk when I get back from lunch the next day"}
{"257": "months ago, that second letter would have dunked me into\ndread, because although it doesn't say so directly, I presume the\nreview will be the occasion for determining the future of the\nplant. I was expecting some kind of formal evaluation. And now I\nam no longer dreading it\u2014on the contrary, I welcome it. What\ndo we have to worry about? Hell, this is an opportunity to show\nwhat we've done!\nThroughput is going up as marketing spreads the word\nabout us to other customers. Inventories are a fraction of what\nthey were and still falling. With more business and more parts\nover which to spread the costs, operating expense is down. We're\nmaking money.\nThe following week, I'm away from the plant for two days\nwith my personnel manager, Scott Dolin. We're at an off-site, very\nconfidential meeting in St. Louis with the division's labor rela-\ntions group and the other plant managers. Most of the discussion\nis about winning wage concessions from the various unions. It's a\nfrustrating session for me\u2014at Bearington, we don't particularly\nneed to lower wages. So I'm less than enthusiastic about much of\nthe strategy suggested, knowing it could lead to problems with\nthe union, which could lead to a strike, which could kill the prog-\nress we've been making with customers. Aside from all that, the\nmeeting is poorly run and ends with very little decided. I return\nto Bearington.\nAbout four in the afternoon, I walk through the doors of the\noffice building. The receptionist flags me down as I pass. She tells\nme Bob Donovan has asked to see me the moment I arrive. I\nhave Bob paged and he comes hurrying into my office a few"}
{"258": "come trooping along. By the time I found out what they were\ndoing here, Hilton Smyth is standing at my elbow.\"\n\"Didn't anybody here know they were coming?\" I ask.\nHe tells me Barbara Penn, our employee communicator,\nknew about it.\n\"And she didn't think to tell anybody?\" I say.\n\"See, the whole thing was re-scheduled on short notice,\" says\nBob. \"Since you and Scott weren't around, she went ahead on\nher own, cleared it with the union, and made all the arrange-\nments. She sent around a memo, but nobody got a copy until this\nmorning.\"\n\"Nothing like initiative,\" I mutter.\nHe goes on to tell me about how Hilton's crew proceeded to\nset up in front of one of the robots\u2014not the welding types, but\nanother kind of robot which stacks materials. It soon became ob-\nvious there was a problem, however: the robot didn't have any-\nthing to do. There was no inventory for it, and no work on its\nway.\nIn a videotape about productivity, the robot, of course, could\nnot simply sit there in the background and do nothing. It had to\nbe producing. So for an hour, Donovan and a couple of assistants\nsearched every corner of the plant for something the robot could\nmanipulate. Meanwhile, Smyth became bored with the wait, so he\nstarted wandering around, and it wasn't long before he noticed a\nfew things.\n\"When we got back with the materials, Hilton started asking\nall kinds of things about our batch sizes,\" says Bob. \"I didn't\nknow what to tell him, because I wasn't sure what you've said up"}
{"259": "They march in and take over the conference room. In hardly any\ntime at all, they've found we changed the base for determining\nthe cost of products.\n\"This is highly irregular,\" says Cravitz, peering at us over the\ntops of his glasses as he looks up from the spreadsheets.\nLou stammers that, okay, maybe it wasn't exactly according\nto policy, but we had valid reasons for basing costs on a current\ntwo-month period.\nI added, \"It's actually a more truthful representation this\nway,\"\n\"Sorry, Mr. Rogo,\" says Cravitz. \"We have to observe stan-\ndard policy.\"\n\"But the plant is different now!\"\nAround the table, all five accountants are frowning at Lou\nand me. I finally shake my head. There is no sense attempting to\nappeal to them. All they know are their accounting standards.\nThe audit team recalculates the numbers, and it now looks as\nif our costs have gone up. When they leave, I try to head them off\nby calling Peach before they can return, but Peach is unexpect-\nedly out of town. I try Frost, but he's gone too. One of the secre-\ntaries offers to put me through to Smyth, who seems to be the\nonly manager in the offices, but I ungracefully decline.\nFor a week, I wait for the blast from headquarters. But it\nnever comes. Lou gets a rebuke from Frost in the form of a memo\nwarning him to stick to approved policy, and a formal order to\nredo our quarterly report according to the old cost standards and\nto submit it before the review. From Peach, there is nothing.\nI'm in the middle of a meeting with Lou over our revised"}
{"260": "I look at Lou and he looks at me.\n\"Is that a helicopter?\" I ask.\nLou goes to the window and looks out.\n\"Sure is, and it's landing on our lawn!\" he says.\nI get to the window just as it touches down. Dust and brown\ngrass clippings are whirling in the prop wash around this sleek\nred and white helicopter. With the blades still twirling down to a\nstop, the door opens and two men get out.\n\"That first one looks like Johnny Jons,\" says Lou.\n\"It is Johnny Jons,\" I say.\n\"Who's the other guy?\" asks Lou.\nI'm not sure. I watch them cross the lawn and start to walk\nthrough the parking lot. Something about the girth and the strid-\ning, arrogant swagger of the huge, white-haired second man trig-\ngers the recollection of a distant meeting. It dawns on me who he\nis.\n\"Oh, god,\" I say.\n\"I didn't think He needed a helicopter to get around,\" says\nLou.\n\"It's worse than God,\" I say, \"It's Bucky Burnside!\"\nBefore Lou can utter another word, I'm running for the\ndoor. I dash around the corner and into Stacey's office. She,\nalong with her secretary and some people she's meeting with, are\nall at the window. Everybody is watching the damn helicopter.\n\"Stacey, quick, I need to talk to you right now!\"\nShe comes over to the door and I pull her into the hallway.\n\"What's the status on Burnside's Model 12's?\" I ask her.\n\"The last shipment went out two days ago.\""}
{"261": "\"No,\" says Bob, startled to see me. \"Nothing I know about.\"\n\"Were there any problems on that order?\" I ask him.\nHe reaches for a paper towel and dries his hands. \"No, the\nwhole thing came off like clockwork.\"\nI fall back against the wall. \"Then what the hell is he doing\nhere?\"\n\"Is who doing here?\" asks Bob.\n\"Burnside,\" I tell him. \"He just landed in a helicopter with\nJohnny Jons.\"\n\"What?\"\n\"Come with me,\" I tell him.\nWe go to the receptionist, but nobody is in the waiting area.\n\"Did Mr. Jons come through here just now with a cus-\ntomer?\" I ask her.\nShe says, \"The two men in the helicopter? No I watched\nthem and they went past here and into the plant.\"\nBob and I hustle side by side down the corridor and through\nthe double doors, into the orange light and production din of the\nplant. One of the supervisors sees us from across the aisle and,\nwithout being asked, points in the direction Jons and Burnside\ntook. As we head down the aisle, I spot them ahead of us.\nBurnside is walking up to every employee he sees and he's\nshaking hands with each of them. Honest! He's shaking hands,\nclapping them on the arm, saying things to them. And he's smil-\ning.\nJons is walking with him. He's doing the same thing. As soon\nas Burnside lets go of a hand, Jons shakes it as well. They're\npumping everybody in sight."}
{"262": "Before I can say anything, Jons jumps into the conversation\nand says, \"Bucky and I were having lunch today, and I was telling\nhim how you pulled out all the stops for him, how everybody\ndown here really gave it everything they had.\"\nI say, \"Ah . . . yeah, we just did our best.\"\n\"Mind if I go ahead?\" asks Burnside, intending to continue\ndown the aisle.\n\"No, not at all,\" I say.\n\"Won't hurt your efficiency, will it?\" asks Burnside.\n\"Not one bit,\" I tell him. \"You go right ahead.\"\nI turn to Donovan then and out of the corner of my mouth\nsay, \"Get Barbara Penn down here right away with the camera\nshe uses for the employee news. And tell her to bring lots of film.\"\nDonovan goes trotting off to the offices, and Jons and I fol-\nlow Bucky up and down the aisles, the three of us shaking hands\nwith one and all.\nJohnny, I notice, is virtually atwitter with excitement. When\nBurnside is far enough ahead that he can't hear us, he turns to\nme and asks, \"What's your shoe size?\"\n\"Ten and a half,\" I tell him. \"Why?\"\n\"I owe you a pair of shoes,\" says Jons.\nI say, \"That's okay, Johnny; don't worry about it.\"\n\"Al, I'm telling you, we're meeting with Burnside's people\nnext week on a long-term contract for Model 12's\u201410,000 units a\nyear!\"\nThe number just about sends me reeling backwards.\n\"And I'm calling in my whole department when I get back,\"\nJons continues as we walk. \"We're going to do a new campaign"}
{"263": "tion rehearsed and ten copies of our report in hand, and with\nnothing more to do except imagine what could go wrong, I call\nJulie.\n\"Hi,\" I tell her. \"Listen, I have to be at headquarters for a\nmeeting tomorrow morning. And because Forest Grove is more\nor less on the way, I'd like to come up and be with you tonight.\nWhat do you think?\"\n\"Sure, it sounds great,\" she says.\nSo I leave work a little early and hit the highway.\nAs I head up the Interstate, Bearington is spread out to my\nleft. The \"Buy Me!\" sign on top of the high-rise office building is\nstill in place. Living and breathing within the range of my sight\nare 30,000 people who have no idea that one small but important\npart of the town's economic future will be decided tomorrow.\nMost of them haven't the slightest interest in the plant or what\nwe've done here\u2014except if UniWare closes us, they'll be mad and\nscared. And if we stay open? Nobody will care. Nobody will even\nknow what we went through.\nWell, win or lose, I know I did my best.\nWhen I get to Julie's parents' house, Sharon and Dave run\nup to the car. After getting out of my suit and into some \"off-\nduty\" clothes, I spend about an hour throwing a frisbee to the\ntwo kids. When they've exhausted me, Julie has the idea the two\nof us should go out to dinner. I get the feeling she wants to talk to\nme. I clean up a little and off we go. As we're driving along, we\npass the park.\n\"Al, why don't we stop for awhile,\" says Julie.\n\"How come?\" I ask."}
{"264": "She nods, I shake my head momentarily, still angry at what\nhappened as a result of the audit.\n\"But even with that, we still had a good month. It just\ndoesn't show up as the fantastic month we really had,\" I tell her.\n\"You don't think they'd still close the plant, do you?\" she\nasks.\n\"I don't think so,\" I say. \"A person would have to be an idiot\nto condemn us just because of an increase in cost of products.\nEven with screwed-up measurements, we're making money.\"\nShe reaches over to take my hand and says, \"It was nice of\nyou to take me out to breakfast that morning.\"\nI smile and say, \"After listening to me ramble on at five\no'clock in the morning, you deserved it.\"\n\"When you talked to me then, it made me realize how little I\nknow about what you do,\" she says. \"I wish you had told me\nmore over the years.\"\nI shrug. \"I don't know why I haven't, I guess I thought you\nwouldn't want to hear it. Or I didn't want to burden you with it.\"\n\"Well, I should have asked you more questions,\" she says.\n\"I'm sure I didn't give you many opportunities by working\nthose long hours.\"\n\"When you weren't coming home those days before I left, I\nreally took it personally,\" she says. \"I couldn't believe it didn't\nhave something to do with me. Deep down, I thought you must\nbe using it as an excuse to stay away from me.\"\n\"No, absolutely not, Julie. When all those crises were occur-\nring, I just kept thinking you must know how important they\nwere,\" I tell her. \"I'm sorry. I should have told you more.\""}
{"265": "She says, \"Al, the one thing I definitely know now is that I\nwant more of you, not less. That's always been the problem for\nme.\"\nShe turns to me with her blue eyes, and I get a long-lost\nfeeling about her.\n\"I finally figured out why I haven't wanted to go back to\nBearington with you,\" she says. \"And it isn't just the town, al-\nthough I don't like it very much there. It's that since we've been\nliving apart, we've actually spent more time being together. I\nmean, when we were living in the same house, I felt as though\nyou took me for granted. Now you bring me flowers. You go out\nof your way to be with me. You take time to do things with me\nand the kids. Al, it's been nice. I know it can't go on this way\nforever\u2014I think my parents are getting a little tired of the ar-\nrangement\u2014but I haven't wanted it to end.\"\nI start to feel very good.\nI say, \"At least we're sure we don't want to say good-bye.\"\n\"Al, I don't know exactly what our goal is, or ought to be, but\nI think we know there must be some kind of need between us,\"\nshe says. \"I know I want Sharon and Dave to grow up to be good\npeople. And I want us to give each other what we need.\"\nI put my arm around her.\n\"For starters, that sounds worth shooting for,\" I tell her.\n\"Look, it's probably easier said than done, but I can certainly try\nto keep from taking you for granted. I'd like you to come home,\nbut unfortunately, the pressures that caused all the problems are\nstill going to be there. They're just not going to go away. I can't\nignore my job.\""}
{"266": "ing you must have seemed selfish on my part. I just went crazy\nfor a little while. I'm sorry\u2014\n\"No, you don't have to be sorry,\" I tell her. \"I should have\nbeen paying attention.\"\n\"But I'll try to make it up to you,\" she says. Then she smiles\nbriefly and adds, \"Since we're walking down memory lane,\nmaybe you remember the first fight we had, how we promised\nafterwards we'd always try to look at a situation from the other's\npoint of view as well as our own. Well, I think for the past couple\nof years we haven't been doing that very often. I'm willing to try\nit again if you are.\"\n\"I am too,\" I say.\nThere is a long hug.\n\"So . . . you want to get married?\" I ask her.\nShe leans back in my arms and says, \"I'll try anything twice.\"\n\"You know, don't you, it's not going to be perfect,\" I tell her.\n\"You know we're still going to have fights.\"\n\"And I'll probably be selfish about you from time to time,\"\nshe says.\n\"What the hell,\" I tell her, \"Let's go to Vegas and find a\njustice of the peace.\"\nShe laughs, \"Are you serious?\"\n\"Well, I can't go tonight,\" I say. \"I've got that meeting in the\nmorning. How about tomorrow night?\"\n\"You are serious!\"\n\"All I've been doing since you left is putting my paycheck in\nthe bank. After tomorrow it'll definitely be time to blow some of\nit.\""}
{"267": "31\nThe next morning on the fifteenth floor of the UniCo build-\ning, I walk into the conference room at a few minutes before ten\no'clock. Sitting at the far end of the long table is Hilton Smyth\nand sitting next to him is Neil Cravitz. Flanking them are various\nstaff people.\nI say, \"Good morning.\"\nHilton looks up at me without a smile and says, \"If you close\nthe door, we can begin.\"\n\"Wait a minute. Bill Peach isn't here yet,\" I say. \"We're going\nto wait for him, aren't we?\"\n\"Bill's not coming. He's involved in some negotiations,\" says\nSmyth.\n\"Then I would like this review to be postponed until he's\navailable,\" I tell him.\nSmyth's eyes get steely.\n\"Bill specifically told me to conduct this and to pass along my\nrecommendation to him,\" says Smyth. \"So if you want to make a\ncase for your plant, I suggest you get started. Otherwise, we'll\nhave to draw our own conclusions from your report. And with\nthat increase in cost of products Neil has told me about, it sounds\nto me as if you have a little explaining to do. I, for one, would\nparticularly like to know why you are not observing proper pro-\ncedures for determining economical batch quantities.\"\nI pace in front of them a moment before answering. The fuse\nto my anger has started a slow burn. I try to put it out and think\nabout what this means. I don't like the situation one bit. Peach"}
{"268": "Cravitz sits up in his chair and says, \"That's true.\"\nHilton gives me a tentative nod.\nI say, \"I'm going to demonstrate to you that regardless of\nwhat our costs look like according to standard measurements, my\nplant has never been in a better position to make money.\"\nAnd so it begins.\nAn hour and a half later, I'm midway through an explana-\ntion of the effects of the bottlenecks upon inventory and\nthroughput when Hilton stops me.\n\"Okay, you've taken a lot of time to tell us all this, and I\npersonally can't see the significance,\" says Hilton. \"Maybe at your\nplant you did have a couple of bottlenecks and you discovered\nwhat they were. Well, I mean bravo and all that, but when I was a\nplant manager we dealt with bottlenecks wandering everywhere.\"\n\"Hilton, we're dealing with fundamental assumptions that\nare wrong,\" I tell him.\n\"I can't see that you're dealing with anything fundamental,\"\nsays Hilton. \"It's at best simple common sense, and I'm being\ncharitable at that.\"\n\"No, it's more than just common sense. Because we're doing\nthings every day that are in direct contradiction to the established\nrules most people use in manufacturing,\" I tell him.\n\"Such as?\" asks Cravitz.\n\"According to the cost-accounting rules that everybody has\nused in the past, we're supposed to balance capacity with demand\nfirst, then try to maintain the flow,\" I say. \"But instead we\nshouldn't be trying to balance capacity at all; we need excess ca-\npacity. The rule we should be following is to balance the/low with"}
{"269": "\"We've assumed that utilization and activation are the same. Acti-\nvating a resource and utilizing a resource are not synonymous.\"\nAnd the argument goes on.\n/ say an hour lost at a bottleneck is an hour out of the entire\nsystem. Hilton says an hour lost at a bottleneck is just an hour lost\nof that resource.\nI say an hour saved at a non-bottleneck is worthless. Hilton\nsays an hour saved at a non-bottleneck is an hour saved at that\nresource.\n\"All this talk about bottlenecks,\" says Hilton. \"Bottlenecks\ntemporarily limit throughput. Maybe your plant is proof of that.\nBut they have little impact upon inventory.\"\n\"It's completely the opposite, Hilton,\" I say. \"Bottlenecks\ngovern both throughput and inventory. And I'll tell you what my\nplant really has shown: it's proved our performance measure-\nments are wrong.\"\nCravitz drops the pen he's holding and it rolls noisily on the\ntable.\n\"Then how are we to evaluate the performance of our opera-\ntions?\" asks Cravitz.\n\"By the bottom line,\" I tell him. \"And based upon that evalu-\nation, my plant has now become the best in the UniWare Divi-\nsion, and possibly the best in its industry. We're making money\nwhen none of the others are.\"\n\"Temporarily you may be making money. But if you're really\nrunning your plant this way, I can't possibly see how your plant\ncan be profitable for very long,\" says Hilton.\nI start to speak, but Hilton raises his voice and talks over me."}
{"270": "Bill's secretary, Meg, watches me approach. I stride up to her\ndesk, where she's sorting paper clips.\n\"I need to see Bill,\" I tell her.\n\"Go right in. He's waiting for you,\" she says.\n\"Hello, Al,\" he greets me as I enter his office. \"I knew you\nwouldn't leave without seeing me. Take a seat.\"\nAs I approach his desk I start to talk, \"Hilton Smyth is going\nto submit a negative report about my plant, and I feel that as my\nmanager you should hear me out before you come to any conclu-\nsions.\"\n\"Go ahead, tell me all about it. Sit down, we're not in a\nrush.\"\nI continue to talk. Bill puts his elbows on the desktop and his\nfingers together in front of his face. When I finally stop he says,\n\"And you explained all of this to Hilton?\"\n\"In great detail.\"\n\"And what was his response?\" he asks.\n\"He basically refused to listen. He continues to claim that as\nlong as cost of products increase, profits eventually have to go\ndown.\"\nBill looks straight into my eyes and asks, \"Don't you think he\nhas a point?\"\n\"No, I don't. As long as I keep my operating expenses under\ncontrol and Johnny Jons is happy, I don't see how profits can\nhelp but continue to go up.\"\n\"Fine,\" he says, and buzzes Meg. \"Can you call Hilton, Na-\nthan, and Johnny Jons in here please.\"\n\"What's going on?\" I ask him."}
{"271": "he says, \"And what about the fact that in the last two months that\nplant has turned profits rather than losses, while releasing a lot of\ncash for the division?\"\n\"That is only a temporary phenomenon,\" Hilton states. \"We\nmust expect big losses in the very near future.\"\n\"Johnny, do you have anything to add?\" Bill asks.\n\"Yes, certainly. Alex's plant is the only one that can produce\nmiracles\u2014to deliver what the client needs in a surprisingly short\ntime. You've all heard about Burnside's visit. With such a plant\nbacking up sales, they can really go out and blast the market.\"\n\"Yes, but at what price?\" Hilton reacts. \"Cutting batches to\nfar below optimum size. Devoting the entire plant to one order.\nDo you know the long-term ramifications?\"\n\"But I haven't devoted the plant to one order!\" I can't con-\ntain my anger. \"As a matter of fact, I haven't got any past-due\norders. All my clients are pleased.\"\n\"Miracles exist only in fairy tales,\" Hilton says cynically.\nNobody says a word. At last I cannot hold back, \"So what's\nthe verdict\u2014is my plant going to be closed?\"\n\"No,\" says Bill. \"Not at all. Do you think we're such bad\nmanagers that we would close a gold mine?\"\nI sigh in relief. Only now do I notice I've been holding my\nbreath.\n\"As manager of productivity of the division,\" Hilton says\nwith a red face, \"I feel it's my duty to protest.\"\nBill ignores him, and turning to Ethan and Johnny he asks,\n\"Shall we tell them now, or wait until Monday?\"\nThey both laugh."}
{"272": "At last I'm able to reach Jonah in New York and fill him in on\nthe latest developments. Although pleased for me, he does not\nseem surprised.\n\"And all this time I just worried about saving my one plant,\"\nI tell him. \"Now it seems that I'm ending up with three.\"\n\"Good luck,\" says Jonah. \"Keep up the good work.\"\nHurriedly, before he hangs up I ask in a desperate voice,\n\"I'm afraid that luck will not be enough; I'm out of my depth.\nCan't you come down and help me?\" I haven't spent two hours\ntracking down Jonah just to hear his congratulations. Frankly,\nI'm terrified at the prospect of my new job. It's one thing to\nhandle a production plant, but handling a division of three plants\ndoes not mean just three times the work, it also means responsi-\nbility for product design and marketing.\n\"Even if I had the time, I don't think it's a good idea,\" I hear\nhis disappointing answer.\n\"Why not? It seemed to work fine so far.\"\n\"Alex,\" he says in a stern voice, \"as you climb up the ladder\nand your responsibilities grow, you should learn to rely more and\nmore on yourself. Asking me to come now will lead to the oppo-\nsite; it will increase the dependency.\"\nI refuse to see his point. \"Can't you continue to teach me?\"\n\"Yes, I can,\" he answers. \"But first you should find out ex-\nactly what it is that you want to learn. Call me then.\"\nI don't give up easily. \"I want to learn how to run an efficient\ndivision, isn't it obvious?\"\n\"In the past you wanted to learn how to run an efficient\nplant,\" Jonah sounds impatient. \"Now you want to learn how to"}
{"273": "\"What?\" I ask in a choked voice.\n\"Come on, I didn't ask you to develop them, just to deter-\nmine clearly what they should be. Call me when you have the\nanswer. And Alex, congratulations on your promotion.\""}
{"274": "32\n\"I'm really proud of you. Three more steps like that and we\nwill have made it. Shall we drink to it?\"\nJulie's forced enthusiasm strikes a responding chord inside\nme. \"No, I don't think so.\" I refuse the toast, an event which, as\nyou can imagine, is not very common.\nJulie doesn't say a word. She just slowly lowers her drink,\nleans slightly forward, and looks directly into my eyes. It's quite\napparent that she is waiting for some explanation.\nUnder the pressure I start to talk slowly, trying to verbalize\nmy rambling thoughts. \"Julie, I really don't think that we should\ntoast it, at least not in the way you make it sound, like toasting an\nempty victory. Somehow I feel that you were right all along\u2014\nwhat is this promotion if not just winning a point in the rat race?\"\n\"Hmm,\" is her only response.\nMy wife can express herself very clearly without even open-\ning her mouth\u2014which is definitely not the case for me. Here I\nam, rambling all over the place . . . 'Rat race' . . . 'Empty vic-\ntory.' What on earth am I talking about? But still, why do I feel\nit's inappropriate to toast my promotion?\n\"The family paid too big a price for this promotion,\" I finally\nsay.\n\"Alex you're being too hard on yourself. This crisis was\nabout to explode one way or the other.\"\nShe continues, \"I gave it a lot of thought and let's face it, if\nyou had given up, the feeling of failure would have spoiled every\ngood part of our marriage. I think you should be proud of this"}
{"275": "But I didn't give up. Against all odds I continued to fight.\nAnd I was not alone. Jonah introduced me to his common-sense\n(and thus very controversial) approach to managing a company.\nIt made a lot of sense, so my team enthusiastically backed me up.\nAnd it was fun, real fun. Let me tell you, the last few months were\nquite stormy. I think that we violated almost every rule of corpo-\nrate America. But we made it. We turned the plant around. So\nmuch so that it saved the entire division. Now, Julie and I are\nsitting in this fancy restaurant celebrating. I'm going to head the\ndivision, which means relocation\u2014a fact that probably contrib-\nutes a lot to Julie's supportive mood.\nRaising my glass I say confidently, \"Julie, let's drink to my\npromotion. Not as a step toward the tip of the pyramid, but let's\ndrink to what it really means\u2014positive reassurance to our excit-\ning, worthwhile journey.\"\nA broad smile is spreading over Julie's face and our glasses\nmake a clear, gentle sound.\nWe turn to our menus, in a good mood. \"It's your celebra-\ntion as much as it is mine,\" I say generously. After a while, and in\na more somber tone I continue, \"Actually, it's much more Jonah's\nachievement than mine.\"\n\"You know Alex, it's so typical of you,\" Julie says apparently\ndisturbed. \"You worked so hard and now you want to give the\ncredit to somebody else?\"\n\"Julie, I'm serious. Jonah is the one who gave me all the\nanswers, I was just the instrument. As much as I would like to\nthink otherwise, that's the plain, bare truth.\"\n\"No, it's far from the truth.\""}
{"276": "present them in the form of very pointed questions doesn't\nchange a thing.\"\nRather than continuing, Julie calls the waiter and starts to\norder. She's right. This line of discussion will just ruin a pleasant\nevening.\nIt's not until I'm busy with my delicious veal parmesan that\nmy thoughts start to crystallize. What was the nature of the an-\nswers, the solutions, that Jonah caused us to develop? They all\nhad one thing in common. They all made common sense, and at\nthe same time, they flew directly in the face of everything I'd ever\nlearned. Would we have had the courage to try to implement\nthem if it weren't for the fact that we'd had to sweat to construct\nthem? Most probably not. If it weren't for the conviction that we\ngained in the struggle\u2014for the ownership that we developed in\nthe process\u2014I don't think we'd actually have had the guts to put\nour solutions into practice.\nStill deep in thought, I raise my eyes from the plate and\nexamine Julie's face. It's as if she was waiting for me all this time.\n\"How come you didn't think of it yourselves?\" I hear her\nasking. \"To me your answers look like plain, common sense. Why\ncouldn't you do it without Jonah's guiding questions?\"\n\"Good question, very good question. Frankly, I doubt I\nknow the answer.\"\n\"Alex, don't tell me you haven't thought about it.\"\n\"Yes, I have,\" I admit. \"All of us, back in the plant, had the\nsame question. The solutions look trivial, but the fact is that for\nyears we've done the exact opposite. Moreover, the other plants\nstill insist on sticking to the old, devastating ways. Probably Mark"}
{"277": "\"Okay,\" I give up. \"The best that I have come up with so far\nis to recognize that we refer to something as common sense only\nif it is in line with our own intuition.\"\nShe nods her head in approval.\n\"Which only helps to intensify your question,\" I continue. \"It\nonly means that when we recognize something as common sense,\nit must be that, at least intuitively, we knew it all along. Why is\nthere so often the need for an external trigger to help us realize\nsomething that we already knew intuitively?\"\n\"That was my question!\"\n\"Yes, darling, I know. Probably these intuitive conclusions\nare masked by something else, something that's not common\nsense.\"\n\"What could that be?\"\n\"Probably common practice.\"\n\"Makes sense,\" she smiles and turns to finish her dinner.\n\"I must admit,\" I say after a while, \"that Jonah's way of lead-\ning to the answers through asking questions, his 'Socratic ap-\nproach,' is very effective at peeling away the layers\u2014the thick\nlayers\u2014of common practice. I tried to explain the answers to\nothers, who needed them as badly as we did, but got nowhere. As\na matter of fact, if it hadn't been for Ethan Frost's appreciation\nof our improvements to the bottom line, my approach might have\nled to some very undesirable results.\n\"You know,\" I continue, \"it's amazing how deeply ingrained\nthose things are that we've been told and practiced, but never\nspent the time to think about on our own. 'Don't give the an-\nswers, just ask the questions!' I'll have to practice that.\""}
{"278": "\"What is worse than criticism?\" she asks innocently.\n\"Constructive criticism.\" I smile gloomily, remembering the\nharsh responses of Hilton Smyth and that Cravitz fellow. \"You\nhave a point, but it's below the belt. People will never forgive you\nfor that.\"\n\"Alex, you don't have to convince me that when I want to\npersuade somebody\u2014especially my husband\u2014that giving an-\nswers is not the way. I'm simply not convinced that only asking\nquestions is much better.\"\nI think about it. She is right. Whenever I tried just to ask\nquestions it was interpreted as patronizing, or even worse, that I\nwas simply negative.\n\"It looks like one should think twice before charging the tall\nwindmills of common practice.\" I conclude gloomily.\nJulie busies herself with the delicious cheesecake our waiter\nis placing in front of us. I do the same.\nWhen the coffee's served I gather enough stamina to con-\ntinue the conversation. \"Julie, is it really so bad? I don't recall\ngiving you a lot of grief.\"\n\"Are you kidding? Not only are you stubborn like a Southern\nmule, you had to go and pass on these genes to your kids. I bet\nyou gave Jonah a hard time as well.\"\nI think about it for a short while. \"No Julie, with Jonah\nsomehow it was different. You see, whenever I'm talking with\nJonah, I have the distinct feeling that not only is he ready with his\nquestions, he's also ready with my questions. It must be that the\nSocratic method is much more than just asking questions. One\nthing I can tell you, improvising with this method is hazardous,"}
{"279": "about my troubles at the plant, I always felt he anticipated my\nresponse. It actually bothered me for quite some time.\"\n\"Why?\"\n\"When did he have the time to learn so much? I'm not talk-\ning about theories, I'm talking about his intimate understanding\nof how the wheels are really turning in a plant. As far as I know,\nhe never worked one day of his life in industry. He's a physicist. I\ncan't believe that a scientist, sitting in his ivory tower, can know so\nmuch about the detailed realities of the shop floor. Something\ndoesn't match.\n\"Alex, if that's the case, it seems that you should ask Jonah to\nteach you something more than just the Socratic method.\""}
{"280": "33\nLou is my first and most important target. If I'm unable to\npersuade him to join me, I'm basically lost. It's not going to be\neasy. He's very close to retirement and I know to what extent he's\ninvolved in his community. I take a deep breath and walk into his\noffice. \"Hey Lou, is it a good time?\"\n\"Good as any. How can I help you?\"\nPerfect opening, but somehow I don't have the guts to go\nstraight to the point. \"I was just wondering about your forecast\nfor the next two months,\" I say. \"Do you see any problem in us\nreaching and maintaining the fifteen percent net profit? Not that\nit's crucial any more,\" I hurriedly add, \"but I'd hate giving\nHilton Smyth even the slightest opening to hiss, 'I told you so.' '\n\"You can sleep tight. According to my calculations we'll easily\ncross the twenty percent net profit for the next two months.\"\n\"What!\" I can hardly believe my ears. \"Lou, what's the mat-\nter with you? Since when do you believe marketing's chronically\noptimistic outlook?\"\n\"Alex, a lot has happened to me recently, but believing mar-\nketing is not one of them. Actually, my forecast is based on a\nslight decline in incoming orders.\"\n\"So how did you pull this rabbit out of your hat?\"\n\"Have a seat, it'll take me some time to explain. I have some-\nthing important to tell you,\" he says.\nIt's clear that I'm going to hear about another devious ac-\ncounting trick. \"All right, let's hear it.\"\nI make myself comfortable while Lou shuffles papers. After"}
{"281": "\"Maybe I should start with a question,\" he says. \"Do you\nagree that inventory is a liability?\"\n\"Of course, everybody knows that. And even if we didn't\nknow it, the last few months have shown to what extent inventory\nis a liability. Do you think we could have pulled off this fast re-\nsponse to orders if the floor had been as jammed up as before?\nAnd haven't you noticed that quality has improved, and overtime\nhas gone down\u2014not to mention that we hardly ever have to ex-\npedite today!\"\n\"Yeah,\" he says, still looking at his papers. \"Inventory is defi-\nnitely a liability, but under what heading are we forced to report\nit on the balance sheet?\"\n\"Holy cow, Lou!\" I jump to my feet. \"I knew that the finan-\ncial measurements were remote from reality, but to that extent\u2014\nto report liabilities under the heading of assets? I never realized\nthe full implications . . . Tell me, what are the bottom line\nramifications?\"\n\"Bigger than you think, Alex. I've checked and rechecked it,\nbut the numbers do talk. You see, we're evaluating inventory ac-\ncording to the cost to produce the goods. These costs include not\nonly the money we pay for the raw materials, but also the value\nadded in production.\n\"You know what we have done in the last few months. Dono-\nvan has worked only on things that we have orders for. Stacey has\nreleased material accordingly. We've drained about fifty percent\nof the work in process from the plant, and about twenty-five per-\ncent from finished goods. We've saved a lot by not purchasing\nnew materials to replace this excess inventory, and the cash fig-"}
{"282": "I stare at him. I can't believe my ears.\n\"But look at the good side,\" he says sheepishly, \"now that the\ninventory has stabilized at a new, low level, this effect won't dis-\nturb us any longer.\"\n\"Thank you very much,\" I say sarcastically and turn to leave.\nWhen I reach the door I turn around and ask him, \"When\ndid you discover this phenomena? When did you find out that we\nwere turning much more profit than the targeted fifteen per-\ncent?\"\n\"A week ago.\"\n\"So why didn't you tell me? I could have used these facts\nvery effectively in the plant review.\"\n\"No Alex, you couldn't have used them at all, it just would\nhave confused your story. You see, everyone evaluates inventory\nthis way, it's even required by the tax authorities. You didn't\nstand a chance. But I did discuss it at length with Ethan Frost;\nhe understood it perfectly.\"\n\"So that's what happened, you fox. Now I understand why\nEthan became so supportive,\" I say, sitting back down.\nWhen we've finished grinning at each other, Lou says in a\nquiet voice, \"Alex, I have another issue.\"\n\"Another bomb?\"\n\"You might call it that, but it's sort of a personal matter.\nEthan told me that he's going with Bill Peach to the group. I\nknow that you will need a good divisional controller, someone\nwho has experience in the more diverse subjects that are dealt\nwith at the division level. I'm just one year from retirement; ev-\nerything that I know is old-fashioned. So ...\""}
{"283": "\"I've given it a lot of thought lately. We need financial mea-\nsurements for sure\u2014but we don't need them for their own sake.\nWe need them for two different reasons. One is control; knowing\nto what extent a company is achieving its goal of making money.\nThe other reason is probably even more important; measure-\nments should induce the parts to do what's good for the organiza-\ntion as a whole. What's become apparent to me is that neither of\nthese two objectives is being met.\n\"For example, this conversation we just had. We knew very\nwell that the plant had drastically improved, but the distorted\nmeasurements have almost condemned us. I'm submitting effi-\nciency reports, product-cost reports, and now we both know to\nwhat extent they just lead workers and management alike to do\nwhat's bad for the company.\"\nI've never heard Lou talk for so long. I agree with every-\nthing he just said, but I'm totally confused. I don't know what\nhe's getting at.\n\"Alex, I can't stop here. I can't retire now. Do me a personal\nfavor, take me with you. I want the opportunity to devise a new\nmeasurement system, one that'll correct the system we have now,\nso that it will do what we expect it to do. So that a controller can\nbe proud of his job. I don't know if I'll succeed, but at least give\nme the chance.\"\nWhat am I supposed to say? I stand up and stretch out my\nhand. \"It's a deal.\"\nBack at my desk I ask Fran to call Bob Donovan in. With Lou\non one side and Bob on the other, I'll be free to concentrate on\nthe two areas I know the least, engineering and marketing."}
{"284": "I decide to give it to him straight, \"How'd you like to be\nresponsible for all production of the division?\"\nThe only thing he manages to say is a long \"Wow.\" He puts\nhis big body in a chair, looks at me, and doesn't say any more.\n\"Well, Bob, surprised?\"\n\"You bet.\"\nI go to pour us coffee and he starts to talk to my back. \"Alex,\nI don't want that job. Not now. You know, a month ago I would\nhave grabbed the offer with both hands. It's way beyond what I\nexpected.\"\nPuzzled, I turn around, a cup in each hand. \"What's the\nmatter Bob, afraid?\"\n\"You know better than that.\"\n\"So what happened in the past month to change your per-\nspective?\"\n\"Burnside.\"\n\"You mean he made you a better offer?\"\nHe fills the room with his booming laughter. \"No, Alex,\nnothing like that. What gave me a new perspective was the way\nwe handled Burnside's urgent order. I learned so much from\nhow we handled that case that I would rather stay in this plant\nand develop it further.\"\nSurprises all around me. I thought I knew these people. I\nexpected it would be impossible to convince Lou, and he almost\nbegged me for the job. I didn't expect any problems with Bob,\nand he just declined my offer. It's really annoying.\n\"You'd better explain,\" I hand him his cup.\nBob's chair squeaks in protest as he fidgets. If I were staying"}
{"285": "date by a mile, as we used to do. We re-engineered the deal; we\ncame back with a counter-offer that was feasible and that the\nclient liked even more than his original request.\"\n\"Yes,\" I say, \"it was good work. Especially considering what\ncame out after that. But that was a peculiar set of circumstances.\"\n\"It was peculiar because normally we don't take the initiative\n\u2014but maybe there's a way to make it standard. Don't you see? We\nactually engineered a sale. We\u2014in the plant, in production\u2014en-\ngineered a sale.\"\nI think about it. He's right. Now I start to see where he's\nheading.\nBob, probably misinterpreting my silence, says, \"For you it's\nnot a big deal, you always looked at production and sales as two\nlinks in the same chain. But look at me. All the time I'm buried\nout on the shop floor, thinking that my responsibility is to put out\nfires, and viewing the sales department as snake oil salesmen,\nspreading unrealistic promises to our clients. For me, this event\nwas a revelation.\n\"Look, we give sales a rigid lead time for each product. So if\nit's not in finished goods, those are the numbers they should use\nto promise to clients. Yeah, they deviate from it, but not by much.\nMaybe there should be another way. Maybe the quoted lead times\nshould be done case by case, according to the load on the bottle-\nnecks. And maybe we shouldn't regard the quantities required as\nif we have to supply them in one shot.\n\"Alex, I'd like to look into it more. Actually, that's what\nStacey, Ralph, and I are doing right now. We were looking for\nyou, you should join us. It's pretty exciting.\""}
{"286": "\"Since you came and forced us to rethink the way we were\ndoing stuff. Do you think somebody needs better proof than\nwhat's happened here in the past months? Here we were, run-\nning things like we'd always done it\u2014by the seat of our pants,\nslowly but surely sinking. And then we took the time and re-\nexamined it from basic principles. And look at how many sacred\ncows we've had to slaughter! Worker efficiency\u2014whoops, out the\nwindow. Optimum batch sizes\u2014there it goes. Releasing work just\nbecause we have the material and the people\u2014that's gone as well.\nAnd I can go on and on. But look at the result. If I hadn't seen it\nmyself, I wouldn't believe it.\n\"Yeah, Alex, I want to stay here and continue what you've\nstarted. I want to be the new plant manager. You caused us to\nchange almost every rule in production. You forced us to view\nproduction as a means to satisfy sales. I want to change the role\nproduction is playing in getting sales.\"\n\"Fine with me. But Bob, when you nail those procedures,\"\nand to myself I add, 'if/ \"will you consider taking on responsibil-\nity for all the plants in the division?\"\n\"You bet, boss. I'll teach 'em a trick or two.\"\n\"Let's drink to it,\" I say. And we toast with our coffee.\n\"Who do you suggest should take your place?\" I ask him.\n\"Frankly, I'm not impressed with any of your superintendents.\"\n\"Unfortunately, I agree with you. The best would be Stacey,\nbut I don't give it much chance she'd take it.\"\n\"Why don't we ask her. You know what? Let's call both\nStacey and Ralph in and discuss your idea.\"\n\"So, at last you found him,\" Stacey says to Bob, as she and"}
{"287": "sonnel problems that go with being a production manager, but\nwe both think that you'd do a fantastic job.\"\n\"You bet,\" Bob adds his two cents.\nShe looks calmly at me, and says, \"Last night I was lying in\nbed, praying. I was praying that this job would be offered to me.\"\n\"Done,\" Bob shouts quickly.\n\"Now that you've accepted,\" I say to Stacey, \"can you tell us\nwhy you want this job so badly?\"\n\"Looks like being a material manager,\" Bob booms, \"is start-\ning to be boring around this plant\u2014not enough expediting, not\nenough rush calls. ... I didn't know that you liked that type of\nexcitement.\"\n\"No, I didn't, and I don't. That's why I was so happy with\nour new method, timing the release of material according to the\nbottlenecks' consumption. But you know my fear, what happens\nif new bottlenecks pop up?\n\"What my people and I have done is to examine daily the\nqueues in front of the assembly and in front of the bottlenecks\u2014\nwe call them 'buffers.' We check just to be sure that everything\nthat's scheduled to be worked on is there\u2014that there are no\n'holes.' We thought that if a new bottleneck pops up it would\nimmediately show up as a hole in at least one of these buffers. It\ntook us some time to perfect this technique, but now it's working\nsmoothly.\n\"You see, whenever there's a hole in a buffer\u2014and I'm not\ntalking about just the work that's supposed to be done on a given\nday, but the work for two or three days down the road\u2014we go\nand check in which work center the materials are stuck. And"}
{"288": "most dependent on your people to prioritize their work,\" Bob\nsays. \"But Stacey, you're not answering our question.\"\n\"I'm coming to it. See, these holes have become more and\nmore dangerous lately\u2014sometimes to the extent that assembly\nhas to deviate significantly from their scheduled sequence. And\nit's become apparent that the foremen of the CCRs have more\nand more difficulty supplying on time. Ralph was telling me that\nthese work centers still have enough capacity, and maybe on the\naverage he's right, but I'm afraid that any additional increase in\nsales will throw us into chaos.\"\nSo here's a bomb, ticking below our feet, and I didn't even\nrealize it. I'm pressing so hard on marketing to bring more sales,\nand according to what Stacey's just revealed that might blow up\nthe whole plant. I'm still trying to digest it when she continues.\n\"Don't you realize that we've concentrated our improvement\nefforts too narrowly? We tried so hard to improve our bottle-\nnecks, when what we should do is improve the CCRs as well.\nOtherwise we'll run into an 'inter-active' bottleneck situation.\n\"See, the key is not in the hands of the materials people. If\ninteractive bottlenecks emerge, chaos is inevitable; we'll have to\nexpedite all over the place.\"\n\"So what are you suggesting?\" I ask.\n\"The key is in the hands of production. These techniques to\nmanage the buffers should not be used just to track missing parts\nwhile there is still time, they should be used mainly to focus our\nlocal improvement efforts. We must guarantee that the improve-\nments on the CCRs will always be sufficient to prevent them from\nbecoming bottlenecks."}
{"289": "We all nod in approval.\n\"It's not just me and the computer anymore, trying to fiddle\nwith inaccurate or untimely data. People really need me now, and\nI feel like I'm contributing. But you know what? I think that the\nchange, at least as it relates to my function, is very fundamental.\nWhat I'm holding in my files is data. What you are usually asking\nfor is information. I always regarded information as those sec-\ntions of the data which are needed in order to make a decision\u2014\nand for that, let me admit it, for most decisions my data was\nsimply unsuitable. Remember the time we were trying to find the\nbottlenecks?\" He looks at each of us in turn. \"It took me four\ndays to admit that I simply couldn't find the answer. What I\nstarted to realize is that information is something else. Informa-\ntion is the answer to the question asked. The more I am able to\ndo it, the more a part of the team I become.\n\"This bottleneck concept has really helped me to move along\nthese lines. Let's face it, today the plant obeys a schedule that's\n. released from the computer.\n\"What's my wish, you ask? I want to develop a system that'll\nhelp in what Bob wants to do, that will help to shrink drastically\nthe time and effort needed to engineer a sale, as he calls it. I want\nto develop a system to help Stacey manage the buffers, and even\nto help in managing the local improvements. I want to develop a\nsystem to help Lou measure, in a much more beneficial way, the\nlocal performance. You see, like everyone else, I have my\ndreams.\""}
{"290": "34\nIt's quite late, the kids are already fast asleep. Julie and I are\nsitting in the kitchen; we're each holding a warm cup of tea in our\nhands. I tell her about what happened today at the plant. She\nseems to be more than mildly interested; she actually claims that\nshe finds it fascinating.\nI love it. Rehashing the day's events with Julie really helps\nme to digest it all.\n\"So what do you think?\" I ask her at last.\n\"I'm starting to see what Jonah meant when he warned you\nabout increasing the dependency,\" she replies.\nThat makes me think for a while, but I still can't see the\nconnection. \"What do you mean?\"\n\"Maybe I'm wrong, but you gave me the impression that\nyou're not too sure that Lou'll be able to come up with a good,\nnew measurement system.\"\n\"That's right,\" I smile.\n\"Is a new measurement system important for you?\"\n\"Are you kidding? I don't know of another single thing\nwhich is as important as that.\"\n\"So if it weren't for Jonah's refusal to continue giving you\npointed questions, am I right in assuming that you'd be on the\nphone right now, trying to squeeze more hints from him?\"\n\"Most probably,\" I admit. \"It's certainly important enough.\"\n\"And what about Bob's idea,\" she continues. \"Do you regard\nthat as something important?\"\n\"If he pulls it off it'll be a revolution. It'll guarantee that we"}
{"291": "\"You're right, Julie. And Jonah is also right. I felt it today as\nwell. When each one of them spelled out their immediate dream\nin such a tangible form, I wondered what mine is. The only thing\nthat kept popping into my mind is that I must learn how to man-\nage. But where on earth am I going to find the answer to Jonah's\nquestion: What are the techniques needed for management? I\ndon't know, Julie. What do you think I should do now?\"\n\"All the people back at the plant owe you a lot,\" she says,\nstroking my hair. \"They're proud of you, and rightfully so.\nYou've created quite a team. But this team is going to be broken\nup in two months when we go to the division. Why don't you\nspend the time that's left sitting with them and going over your\nquestion. They'll have ample time after you're gone to work on\ntheir problems. Anyhow, it'll be much easier for them to achieve\nwhat they want to achieve if you have the management tech-\nniques.\"\nI look at her in silence. Here is my real, true advisor.\nSo I've done what my advisor suggested. I gathered them all\ntogether and explained that if each of them wants to be free to\nconcentrate on his pet project the division must be well run, and\nin order for the division to be well run the division manager must\nknow what he is doing. And since I, frankly, don't have the foggi-\nest idea of how to run a division they had better put their brains\nto helping me. Thus, we are going to devote the afternoons\u2014\nprovided of course that no special emergency comes up\u2014to help\nme analyze how the division should be run.\nI decide to start the meeting with the most naive questions.\nInitially they might think that I've lost all my self confidence, but"}
{"292": "If this question had been asked under any other circum-\nstances they would have taken it as a clear indication of a total\nlack of managerial knowledge. As it is they play the game.\n\"Basically you should do general fact finding first,\" Lou an-\nswers.\n\"You know,\" Bob adds, \"like where the entrance is, where\nthe toilets are . . .\"\n\"I do think that meeting the people is important,\" Stacey\ninterrupts the laughter. \"Financial numbers only reveal a small\nfraction of the picture. You have to find out what the people\nthink is going on. What do they see as problems? Where do we\nstand vis-a-vis the clients?\"\n\"Who has a grudge against whom?\" Bob contributes, and\nthen in a more serious tone. \"You also have to get a sense of the\nlocal politics.\"\n\"And then?\"\n\"And then,\" Bob continues. \"I'd probably take a tour of the\nvarious production facilities, visit some of the big clients, and\nprobably even some suppliers. You've got to get the full picture.\"\nMaintaining my poker face I ask, \"And then?\"\nAt last I've succeeded to provoke them, since both Stacey and\nBob answer vehemently, \"And then you'll take it from there!\"\nHow easy it is to give advice when the responsibility is on\nsomeone else's shoulders. Okay wise guys, it's time to turn the\ntable, and in a calm voice I say, \"Yes, what you suggested just now\nis the usual line of action one takes when he is told to 'go there\nand fix it.' Let me play it back for you, but in a more schematic\nway. Where are the colored markers?\""}
{"293": "measles. It looks like one of the drawings my kids used to bring\nhome from kindergarten.\nI don't think they got the message, they just seem confused;\nso I decide to continue a little more bluntly. \"It's about time to\ntalk with another manager, we must get a sense of the local poli-\ntics. Oh, this is very interesting, there are also green circles, and\neven some green stars. Here's an unidentified shape\u2014never\nmind, we'll address it later. Now, let's tour the production facili-\nties, visit clients, and even some suppliers. We're bound to reveal\nmany more interesting facts.\" As I talk the board is filled with\noverlapping shapes.\n\"Now that we have the full picture, we can take it from\nhere,\" I finally conclude and put the markers down. \"Well?\"\nThe board looks like a nightmare in Technicolor. I take a\ndeep breath and pick up the phone to order more coffee.\nNobody says a word, not even Bob.\n\"Let's make it less personal,\" I say after a while. \"Suppose\nthat we are a committee that's been given the ungrateful task of\n'find out what's going on.' How do you suggest we should start?\"\nThey all smile. Somehow pretending that we're a committee\nmakes us feel much better. \"The safety of being part of a herd,\" I\nthink to myself; the blame will not be aimed at anyone in particu-\nlar.\n\"Ralph, will you volunteer to describe the committee's ac-\ntions?\"\n\"They would probably start in the same way\u2014fact finding.\nAnd as you so vividly demonstrated, they would end up in the\nsame colorful ditch. But Alex, is there any other way to start?"}
{"294": "They all laugh nervously. I'm really pleased. They've finally\nstarted to realize the problem that I'm facing.\n\"What are they going to do now?\" Stacey muses. \"They'll\nprobably try to arrange this monstrous pile of facts in some or-\nder.\"\n\"Most likely,\" Lou agrees. \"Sooner or later one of the com-\nmittee members will suggest organizing the shapes according to\ntheir relative size.\"\n\"I don't think so,\" Bob disagrees. \"Determining the relative\nsize of different shapes is quite difficult. They will probably de-\ncide to organize them according to the type of shapes.\" Lou\ndoesn't seem to accept this, and so Bob explains, \"They can ar-\nrange the data by circles, rectangles, and stars.\"\n\"What are they going to do with those four arbitrary\nshapes?\" Ralph asks.\n\"Probably they'll be put in a class of their own, the excep-\ntions.\"\n\"Yes, of course,\" Ralph agrees. \"The major reason for the\nconstant reprogramming are those exceptions that keep popping\nup.\"\n\"No, I have a better idea,\" Lou says stubbornly. \"They'll\nprobably arrange them by color; in this way there will be no\nambiguity. Tell you what.\" He continues when he realizes that\nBob is about to object, \"Let's arrange them first by color, within\ncolor by shape, and within each subclass we'll arrange them by\nsize. This way everybody will be happy.\" Count on Lou to find an\nacceptable compromise.\n\"It's a marvelous idea,\" Ralph picks up the ball. \"Now we can"}
{"295": "changing it according to functional capabilities\u2014and vice versa.\nDeciding that the company is wasting too much money on dupli-\ncated efforts and thus moving to a more centralized mode. Ten\nyears later, we want to encourage entrepreneurship and we move\nback to decentralization. Almost every big company is oscillating,\nevery five to ten years from centralization to decentralization, and\nthen back again.\"\n\"Yeah,\" says Bob. \"As a president of a company, when you\ndon't know what to do, when things are not going well, you can\nalways shuffle the cards\u2014reorganize.\" Mockingly he continues,\n\"That will do it! This reorganization will solve all our problems!\"\nWe stare at each other. If it weren't so painfully true, we\nmight laugh.\n\"Bob,\" I say at last. \"This isn't funny. The only somewhat\npractical ideas I had in mind for what I should do as the new\ndivision manager were all based on reorganizing the division.\"\n\"Oh, no,\" they all groan.\n\"O.K. then,\" and I turn back to the white board, which is not\nso white any more. \"What is one supposed to do with this pile of\ncolored shapes, except to arrange them in some order? Dealing\ndirectly with the pile is obviously totally impractical. Arranging\nthe facts according to some order, classification, must be the first\nstep. Maybe we can proceed from there in a different way than\nwriting reports or rearranging the company, but the first step\ndefinitely must be to put some order into the mess.\"\nAs I continue to look at the board, a new question starts to\nbother me; \"In how many ways can one arrange the assembled\nfacts?\""}
{"296": "these methods have just as much meaning. At least my last sug-\ngestion offers some satisfaction.\"\n\"O.K. fellows,\" I say firmly. \"Bob's last suggestion has really\nclarified what we're dealing with here. We're dealing with the fact\nthat we haven't got any idea of what we're doing. If we're just\nlooking for some arbitrary order, and we can choose among so\nmany possibilities, then what's the point in putting so much effort\nin collecting so much data? What do we gain from it, except the\nability to impress people with some thick reports or to throw the\ncompany into another reorganization in order to hide from the\nfact that we don't really understand what we're doing? This ave-\nnue of first collecting data, getting familiar with the facts, seems\nto lead us nowhere. It's nothing more than an exercise in futility.\nCome on, we need another way to attack the issue. Any sugges-\ntions?\"\nWhen nobody answers, I say, \"Enough for today. We'll con-\ntinue tomorrow\u2014same time, same place.\""}
{"297": "35\n\"Well, anybody got anything good, any breakthroughs?\" I\ntry to start the meeting off as cheerfully as possible. It's not ex-\nactly how I feel; I spent the whole night tossing in my bed,\nsearching for any opening, which I never did find.\n\"I think that I have one,\" Stacey speaks up. \"Not exactly a\nbreakthrough, but . . .\"\n\"Wait,\" says Ralph.\nRalph interrupting. That's new.\nIn an apologetic tone he explains, \"Before we go off on a\ndifferent angle, I'd like to return to where we were yesterday. I\nthink we were too hasty in our decision that classification of data\ncan't lead to something good. May I?\"\n\"Sure,\" Stacey says, almost in relief.\n\"Well,\" Ralph fidgets, apparently uncomfortable, \"as you\nknow, or maybe you don't, I minored in chemistry in college. I\ndon't know much about it, but one story stuck in my mind. Last\nnight I looked back at my notes from class and I think you'll find\nit interesting as well. It's a story about a remarkable Russian\nnamed Mendeleev, and it happened less than one hundred fifty\nyears ago.\"\nNoticing that he grabbed our attention, he becomes more\nconfident. Ralph is a family man and has three little children, so\nhe's probably used to telling stories.\n\"Right from the start, in the days of ancient Greece, people\npostulated that underlying the phenomenal variety of materials\nthere must be a simple set of elements from which all other sub-"}
{"298": "not a basic element but actually composed of many different\nmore basic minerals. Air is composed of different types of gases,\nand even water is a composition of more basic elements, hydro-\ngen and oxygen. The kiss of death to the naive Greece approach\ncame at the end of the eighteenth century, when Lavoisier\nshowed that fire is not a substance but rather a process, the pro-\ncess of attachment to oxygen.\"\n\"Over many years, out of the chemists' mammoth work, the\nmore basic elements emerged and by the middle of the nine-\nteenth century, sixty-three elements had been identified. The sit-\nuation actually resembled our colored board. Many circles, rec-\ntangles, stars, and other shapes, in many colors and sizes filled\nthe area with no apparent order. A real mess.\"\n\"Many tried to organize the elements but no one succeeded\nin offering anything that was not immediately dismissed as a fu-\ntile arbitrary exercise. It got to the point that most chemists gave\nup on the possibility of finding any generic order and concen-\ntrated their efforts on finding more hard facts regarding the com-\nbination of the elements to create other, more complicated mate-\nrials.\"\n\"Makes sense,\" Bob remarks. \"I like practical people.\"\n\"Yes Bob,\" Ralph smiles at him, \"But there was one profes-\nsor who claimed that in his eyes it resembled dealing with the\nleaves while nobody had found yet the trunk.\"\n\"Good point,\" says Lou.\n\"So this peculiar Russian professor who, by the way, taught\nin Paris, decided to concentrate on revealing the underlying or-\nder governing the elements. How would you go about it?\""}
{"299": "We all laugh, then responding to Ralph's gesture I give it a\ntry. \"We probably have to look for a more numerical measure.\nThis way we'll be able to arrange the elements without being\ncriticized for subjective preferences.\"\n\"Very good,\" says Ralph. He's probably mistaken us for his\nkids. \"What do you suggest as a suitable measure?\" he asks me.\n\"I didn't take chemistry,\" I reply, \"not even as a minor. How\nwould I know?\" But since I don't want to offend Ralph I con-\ntinue, \"Maybe something like specific gravity, electrical conduc-\ntivity, or something more fancy like the number of calories ab-\nsorbed or released when the element is combining with a\nreference element like oxygen.\"\n\"Not bad, not bad at all. Mendeleev took basically the same\napproach. He chose to use a quantitative measurement that was\nknown for each element and which didn't change as a function of\nthe temperature or the state of the substance. It was the quantity\nknown as atomic weight, which represents the ratio between the\nweight of one atom of the given element and the weight of one\natom of the lightest element, hydrogen. This number provided\nMendeleev with a unique numerical identifier for each element.\"\n\"Big deal,\" Bob can't hold himself. \"Exactly as I suspected,\nnow he could organize all the elements according to their ascend-\ning atomic weights, like soldiers in a line. But what good does it\ndo? What practical things can possibly come out of it? Like I said,\nchildren playing with lead soldiers, pretending that they do very\nimportant work.\"\n\"Not so fast,\" Ralph responds. \"If Mendeleev had stopped\nhere, I would accept your criticism, but he took it a step further."}
{"300": "\"Very nice, but as I suspected it's nothing more than child's\nplay. What are the practical implications?\" Down-to-earth Bob.\n\"There were practical ramifications,\" Ralph answers. \"You\nsee, when Mendeleev constructed his table, not all the elements\nwere already found. This caused some holes in his table that he\nreacted to by 'inventing' the appropriate missing elements. His\nclassification gave him the ability to predict their weight and\nother properties. You must agree that's a real achievement.\"\n\"How was it accepted by the other scientists of his time?\" I\nask, curious. \"Inventing new elements must have been received\nwith some skepticism.\"\n\"Skepticism is an understatement. Mendeleev became the\nlaughing stock of the entire community. Especially when his table\nwas not as neatly arranged as I described it to you. Hydrogen was\nfloating there above the table, not actually in any column, and\nsome rows didn't have one element in their seventh column, but\na hodgepodge of several elements crowded into one spot.\"\n\"So what happened at the end?\" Stacey impatiently asks.\n\"Did his predictions come true?\"\n\"Yes,\" says Ralph, \"and with surprising accuracy. It took\nsome years, but while he was still alive all the elements that\nMendeleev predicted were found. The last of the elements that\nhe 'invented' was found sixteen years later. He had predicted it\nwould be a dark gray metal. It was. He predicted that its atomic\nweight would be about 72; in reality it was 72.32. Its specific\ngravity he thought would be about 5.5, and it was 5.47.\"\n\"I bet nobody laughed at him then.\"\n\"Certainly not. The attitude switched to admiration and his"}
{"301": "the table should have been constructed to have eight columns,\nnot seven.\"\n\"Just as I've said,\" Bob jumps in a triumphant voice. \"Even\nwhen it works you still can't trust it.\"\n\"Calm down, Bob. You must admit that Ralph's story has a\nlot of merit for us. I suggest that we ask ourselves what's the\ndifference between Mendeleev's classification of the chemical ele-\nments and our many attempts to arrange the colored shapes in\norder? Why was his so powerful and ours so arbitrary?\"\n\"That's just it,\" says Ralph, \"Ours were arbitrary, and his\nwas . . .\"\n\"Was what? Not arbitrary?\" Lou completes his sentence.\n\"Forget it.\" Ralph agrees. \"That's not a serious answer. I'm\njust playing with words.\"\n\"What exactly do we mean by arbitrary, and not arbitrary?\" I\nraise the question.\nSince nobody answers I continue, \"Actually, what are we\nlooking for? We're looking to arrange the facts in some order.\nWhat type of order are we seeking? An arbitrary order that we\nsuperimpose externally on the facts, or are we trying to reveal an\nintrinsic order, an order that already exists there?\"\n\"You're absolutely right,\" Ralph is getting excited, \"Mende-\nleev definitely revealed an intrinsic order. He didn't reveal the\nreason for that order, that had to wait for another fifty years,\nwhen the internal structure of the atoms was found, but he defi-\nnitely revealed the intrinsic order. That's why his classification\nwas so powerful. Any other classification that just tries to super-\nimpose some order, any order, on the given facts is useful in only"}
{"302": "would have done with the pile of facts that we suggested he\ngather. Judging by what we've done for so long here in the plant,\nprobably just that\u2014playing a lot of games with numbers and\nwords. The question is what are we going to do differently now?\nAnybody got an answer?\"\nLooking at Ralph sunk in his chair I say, \"If we could reveal\nthe intrinsic order of the events in the division, that would cer-\ntainly be of tremendous help.\"\n\"Yes,\" Lou says, \"But how does one go about revealing the\nintrinsic order?\"\n\"How can one identify an intrinsic order even when he stum-\nbles on it?\" Bob adds.\nAfter a while Lou says, \"Probably in order to answer this\nquestion we should ask a more basic one: What provides the in-\ntrinsic order among various facts? Looking at the elements that\nMendeleev had to deal with, they all seemed different. Some were\nmetals and some gases, some yellow and some black, no two were\nidentical. Yes, there were some that exhibited similarities, but\nthat's also the case for the arbitrary shapes that Alex drew on the\nboard.\"\nThey continue to argue but I'm not listening any more. I'm\nstuck on Lou's question, \"How does one go about revealing the\nintrinsic order?\" He asked it as if it were a rhetorical question, as\nif the obvious answer is that it is impossible. But scientists do\nreveal the intrinsic order of things . . . and Jonah is a scientist.\n\"Suppose that it is possible,\" I break into the conversation,\n\"suppose that a technique to reveal the intrinsic order does exist?\nWouldn't such a technique be a powerful management tool?\""}
{"303": "method as a method to persuade other people. I wouldn't touch\nphilosophy with a ten foot pole, but to learn a method to per-\nsuade my stubborn husband and kids\u2014for that I'm willing to\nsweat.\"\n\"So you started to read philosophy,\" I'm still trying to digest\nit.\n\"You make it sound like a punishment,\" she laughs. \"Alex,\ndid you ever read the dialogues of Socrates?\"\n\"No.\"\n\"They're not too bad. They're actually written like stories.\nThey're quite interesting.\"\n\"How many have you read so far?\" I ask,\n\"I'm still slaving on the first one, Protagoras.\"\n\"It'll be interesting to hear your opinion tomorrow.\" I say\nskeptically. \"If it's still positive, maybe I'll read it, too.\"\n\"Yeah, when pigs fly,\" she says. Before I can answer, she\nstands up, \"Let's hit the sack.\"\nI yawn and join her. ^"}
{"304": "36\nWe're getting started a little late since Stacey and Bob have to\ndeal with some problematic orders. I wonder what's really hap-\npening; are we drifting back into trouble? Is Stacey's warning\nabout her Capacity Constraint Resources starting to materialize?\nShe was concerned about any increase in sales and, for sure, sales\nare slowly but constantly on the rise. I dismiss these thoughts; it's\njust the natural friction that should be expected when your mate-\nrial manager moves her responsibilities to her replacement. I de-\ncided not to interfere; if it evolves into something serious they\nwon't hesitate to tell me.\nThis is not going to be easy. We all are action-oriented and\nsearching for basic procedures is almost against our nature, no\nmatter how much Bob tells me that he's been transformed.\nSo when, at last, they all take seats I remind them about the\nissue on the table. If we want the same movement that we've\nsucceeded in starting here to happen in the entire division, we\nhave to clarify for ourselves what we actually have done\u2014in a\ngeneric sense. Repeating the specific actions won't work. Not only\nare the plants very different from each other; how can one fight\nlocal efficiencies in sales, or cut batches in product design?\nStacey is the only one who has something to offer and her\nidea is simple. If Jonah forced us to start by asking, 'what is the\ngoal of the company', Stacey suggests that we start by asking,\n'what is our goal'\u2014not as individuals, but as managers.\nWe don't like it. It's too theoretical. Bob yawns, looks bored.\nLou responds to my unspoken request and volunteers to play the"}
{"305": "stantly strive toward it. Let me rephrase my initial answer.\" And\nin his punctuating voice, emphasizing each word, he concludes,\n\"A good job will be to start our division on a process of on-going\nimprovement.\"\nTurning to me, Stacey says, \"You asked for an idea of how to\ntackle the subject? I think that we should proceed from here.\"\n\"How?\" Donovan echoes the question that everybody is\nthinking.\n\"I don't know,\" is Stacey's answer. When she sees Bob's ex-\npression she says defensively, \"I didn't claim to have a break-\nthrough, just an idea.\"\n\"Thank you Stacey,\" I say, and turning to the rest I point to\nthe white board that nobody has bothered to erase yet. \"We must\nadmit that it is a different angle from the one we had so far.\"\nWe are stuck. Donovan's question is certainly in place. So I\ntry to gain some momentum by cleaning the board and writing in\nbig letters \"A process of on-going improvement.\"\nIt doesn't help much. We sit in silence for a while staring at\nthe board.\n\"Comments?\" I ask at last. And, as expected, it's Bob who\nvoices everybody's feeling.\n\"I'm sick and tired of these big words. Everywhere I go, I\nhear the same thing.\" He stands up, goes to the board, and mim-\nicking a first grade teacher he intones \"A process ... of ...\non-going . . . improvement.\"\nSitting back down he adds, \"Even if I wanted to forget it I\ncan't. Hilton Smyth's memos are all spotted with this phrase. By\nthe way Alex, these memos keep on coming, and more often than"}
{"306": "\"So, what are you suggesting should be done?\" I pour some\nmore fuel on his flames.\n\"To do what we have done here,\" he roars back. \"We, here,\nhave not done any of these. We have not launched even one\nformal improvement project. But look at what we have achieved.\nNo talks, no big words, but if you ask me, what we've achieved\nhere is the real thing.\"\n\"You're right,\" I try to calm the volcano that I have awak-\nened. \"But Bob, if we want to do the same in the entire division\nwe must pinpoint what exactly the difference is between what we\nhave done and what everyone else has tried to do.\"\n\"We haven't launched so many improvement projects,\" he\nsays.\n\"That is not accurate,\" Stacey responds. \"We have taken\nmany initiatives: in shop floor procedures, in measurements, in\nquality, in local processes, not to mention the changes that we\nhave made in the way we release material to production.\" Raising\nher hand to stop Bob from interrupting, she concludes: \"True,\nwe didn't call them improvement projects, but I don't believe the\ncrucial difference is that we didn't bother to title them.\"\n\"So why do you think we have succeeded where so many\nhave failed?\" I ask her.\n\"Simple,\" Bob jumps in. \"They talked, we did.\"\n\"Who is playing with words now,\" I shut him off.\n\"I think that the key,\" Stacey says in a thoughtful tone, \"is in\nthe different way we interpreted the word 'improvement'.\"\n\"What do you mean?\" I ask her.\n\"She is absolutely right!\" Lou beams. \"It's all a matter of"}
{"307": "pie are concentrating on reducing operating expenses as if it's\nthe most important measurement.\"\n\"Not even that,\" Bob interrupts. \"We were busy reducing\ncosts that didn't have any impact on reducing operating ex-\npenses.\"\n\"Correct,\" Lou continues. \"But the important thing is that\nwe, in our plant, have switched to regard throughput as the most\nimportant measurement. Improvement for us is not so much to\nreduce costs but to increase throughput.\"\n\"You are right,\" Stacey agrees. \"The entire bottleneck con-\ncept is not geared to decrease operating expense, it's focused on\nincreasing throughput.\"\n\"What you are telling us,\" I say slowly, trying to digest it, \"is\nthat we have switched the scale of importance.\"\n\"That's precisely what it is,\" Lou says. \"In the past, cost was\nthe most important, throughput was second, and inventory was a\nremote third.\" Smiling at me he adds, \"To the extent that we\nregarded it as assets. Our new scale is different. Throughput is\nmost important, then inventory\u2014due to its impact on\nthroughput and only then, at the tail, comes operating expenses.\nAnd our numbers certainly confirm it,\" Lou provides the evi-\ndence. \"Throughput and inventory had changed by several tens\nof percent while operating expenses went down by less than two\npercent.\"\n\"This is a very important lesson,\" I say. \"What you claim is\nthat we have moved from the 'cost world' into the 'throughput\nworld'.\"\nAfter a minute of silence I continue, \"You know what, it re-"}
{"308": "We spend good time. We bring up the actions we took and\nverify that each one actually has been guided by our new scale.\nBob is very quiet until he jumps to his feet.\n\"I nailed the bastard!\" he shouts, \"I have it!\"\nHe goes to the board, grabs a marker and put a heavy circle\naround the word 'improvement.' \"Process of on-going improve-\nment,\" he booms. \"Lou and his fixation on measurements forced\nus to concentrate on the last word. Don't you realize that the real\nsneaky SOB is the first one?\" and he draws several circles around\nthe word 'process.'\n\"If Lou has a fixation about measurements,\" I say somewhat\nirritated, \"then you certainly have a fixation about processes.\nLet's hope your fixation will turn up to be as useful as his.\"\n\"Sure thing, boss. I knew that the way we handled it was\ndifferent. That it wasn't just a matter of scales.\"\nHe returned to his seat still beaming.\n\"Do you care to elaborate?\" Stacey inquires in a soft voice.\n\"You haven't got it?\" Bob is surprised.\n\"Neither did we.\" We all looked perplexed.\nHe looks around and when he realizes that we are serious he\nasks, \"What is a process? We all know. It's a sequence of steps to\nbe followed. Correct?\"\n\"Yes . . .\"\n\"So, will anybody tell me what the process is that we should\nfollow? What is the process mentioned in our 'process of on-go-\ning improvement'? Do you think that launching several improve-\nment projects is a process? We haven't done that, we have fol-\nlowed a process. That's what we have done.\""}
{"309": "\"In the 'cost world' as Alex called it, we are concerned pri-\nmarily with cost. Cost is drained everywhere, everything cost us\nmoney. We had viewed our complex organization as if it were\ncomposed out of many links and each link is important to con-\ntrol.\"\n\"Will you please get to the point?\" Bob asks impatiently.\n\"Let him talk,\" Stacey is no less impatient.\nRalph ignores them both and calmly continues, \"It's like\nmeasuring a chain according to its weight. Every link is impor-\ntant. Of course, if the links are very different from each other\nthen we use the principle of the twenty-eighty rule. Twenty per-\ncent of the variables are responsible for eighty percent of the\nresult. The mere fact that we all know the Pareto principle shows\nus to what extent Lou is right, the extent to which we all were in\nthe cost world.\"\nStacey puts her hand on Bob's to prevent him from interfer-\ning. ^\n\"We recognize that the scale has to be changed,\" Ralph con-\ntinues. \"We choose throughput as the most important measure-\nment. Where do we achieve throughput? At each link? No. Only\nat the end of all operations. You see, Bob, deciding that\nthroughput is number one is like changing from considering\nweight to considering strength.\"\n\"I don't see a thing,\" is Bob's response.\nRalph doesn't let go, \"What determines the strength of a\nchain?\" he asks Bob.\n\"The weakest link, wise guy.\"\n\"So if you want to improve the strength of the chain, what"}
{"310": "STEP 1. Identify the system's bottlenecks.\n(After all it wasn't too difficult to identify the oven and\nthe NCX10 as the bottlenecks of the plant.)\nSTEP 2. Decide how to exploit the bottlenecks.\n(That was fun. Realizing that those machines should not\ntake a lunch break, etc.)\nSTEP 3. Subordinate everything else to the above decision.\n(Making sure that everything marches to the tune of the\nconstraints. The red and green tags.)\nSTEP 4. Elevate the system's bottlenecks.\n(Bringing back the old Zmegma, switching back to old,\nless \"effective\" routings. . . .)\nSTEP 5. If, in a previous step, a bottleneck has been broken go\nback to step 1.\nI look at the board. It's so simple. Plain common sense. I'm\nwondering, and not for the first time, how come we didn't see it\nbefore, when Stacey speaks up.\n\"Bob is right, we certainly followed this process, and we cy-\ncled through it more than once\u2014even the nature of the bottle-\nnecks we had to deal with changed.\"\n\"What do you mean by the 'nature of the bottlenecks?' \" I\nask.\n\"I mean a major change,\" she says. \"You know, something\nserious like the bottleneck changing from being a machine to\nbeing something totally different, like insufficient market de-\nmand. Each time that we've gone through this five-step cycle the\nnature of the bottleneck has changed. First the bottlenecks were\nthe oven and the NCX10, then it was the material release system"}
{"311": "\"Alex, why do you drive yourself so hard? Aren't the five steps\nthat you developed enough of an achievement for one day?\"\n\"Of course it's enough. It's more than enough. Finding the\nprocess that everybody is looking for, the way to proceed system-\natically on the line of on-going improvement, is quite an achieve-\nment. But Julie, I'm talking about something else. How can we\ncontinue to improve the plant rapidly?\"\n\"What's the problem? It seems that everything is sailing for-\nward quite smoothly.\"\nI sigh, \"Not exactly, Julie. I can't push aggressively for more\norders because we're afraid that any additional sales will create\nmore bottlenecks and throw us back into the nightmare of expe-\nditing. On the other hand, I can't ask for a major expansion in\nhiring or machines; the existing bottom line results don't justify it\nyet.\"\n\"My impatient husband,\" she laughs. \"It looks like you sim-\nply have to sit tight and wait until the plant generates enough\nmoney to justify more investments. In any event darling, very\nshortly it will be Donovan's headache. It's about time you allowed\nothers to worry.\"\n\"Maybe you're right,\" I say, not totally convinced."}
{"312": "37\n\"Something is wrong,\" Ralph says after we've made our-\nselves comfortable. \"Something is still missing.\"\n\"What?\" Bob says aggressively, all geared up to protect our\nnew creation.\n\"If step 3 is right . . .\" Ralph is speaking very slowly, \"if we\nhave to subordinate everything to the decision that we made on\nthe constraint, then . . .\"\n\"Come on Ralph,\" Bob says. \"What's all this 'if we have to\nsubordinate'? Is there any doubt that we must subordinate the\nnon-constraints to the constraints? What are the schedules that\nyou generate on your computers if not the act of subordinating\neverything to our decision about the bottlenecks' work?\"\n\"I don't doubt that,\" Ralph says apologetically. \"But when\nthe nature of the constraint has changed, one would expect to see\na major change in the way we operate all non-constraints.\"\n\"That makes sense,\" Stacey says encouragingly. \"So what is\nbothering you?\"\n\"I don't recall that we did such changes.\"\n\"He's right,\" Bob says in a low voice. \"I don't recall it ei-\nther.\"\n\"We didn't,\" I confirm after a while.\n\"Maybe we should have?\" Bob says in a thoughtful voice.\n\"Let's examine it,\" I say. And then, \"When was the first time\nthe constraint changed?\"\n\"It happened when some green-tag parts started arriving at\nassembly too late,\" Stacey says without hesitation. \"Remember"}
{"313": "\"We did it because it made perfect sense,\" I say. \"Reality\ncertainly proved us right. So Ralph, in that case at least, we did\naffect all the non-constraints. Should we move on?\"\nRalph doesn't answer.\n\"Something's still troubling you?\" I inquire.\n\"Yes,\" he says, \"but I can't put my finger on it.\"\nI wait for him.\nFinally Stacey says, \"What's the problem, Ralph? You, Bob,\nand I generated the work list for the constraints. Then you had\nthe computer generate release dates for all material, based on\nthat list. We definitely changed the way we operated a non-con-\nstraint, that is, if we consider the computer as a non-constraint.\"\nRalph laughs nervously.\n\"Then,\" Stacey continues, \"I made my people obey those\ncomputer lists. That was a major change in the way they operate\n\u2014especially when you consider how much pressure the foremen\nput on them to supply them with work.\"\n\"But you must admit the biggest change was on the shop\nfloor,\" Bob contributes. \"It was very difficult for most people to\nswallow that we really meant they shouldn't work all the time.\nDon't forget that the fear of layoffs was hanging heavily above\nus.\"\n\"I guess it's all right,\" Ralph gives up.\n\"What did we do with the method we were using?\" Lou asks.\n\"You know, the green and red tags.\"\n\"Nothing,\" Stacey replies. \"Why should we do anything\nabout it?\"\n\"Thank you, Lou,\" Ralph says. \"That is exactly what was"}
{"314": "\"I just realized the impact that those darn tags have on our\noperation.\"\n\"Well?\" Bob presses her.\n\"I'm embarrassed,\" she says. \"I've been complaining about\nour problems with the six or seven capacity constraint resources,\nI raised all the red flags, I've gone as far as to demand that in-\ncoming orders be restricted. And now I see that I've created the\nproblem with my own hands.\"\n\"Fill us in, Stacey,\" I request. \"You're way ahead of us.\"\n\"Of course. You see, when do the green and red tags have an\nimpact? Only when a work center has a queue, when the worker\nhas to choose between two different jobs that are waiting; then he\nalways works on the red tag first.\"\n\"So?\"\n\"The largest queues,\" Stacey goes on, \"are in front of the\nbottlenecks, but there the tags are irrelevant. The other place\nwhere we have relatively high queues is in front of the capacity\nconstraint resources. These resources supply some parts to the\nbottlenecks, red-tag parts, but they work on many more green-\ntag parts, parts that go to assembly not through the bottlenecks.\nToday they do the red-tag parts first. This naturally delays the\narrival of the green parts to assembly. We catch it when it is pretty\nlate, when holes are already evident in the assembly buffer. Then,\nand only then, we go and change the priorities at those work\ncenters. Basically, we restore the importance of the green parts.\"\n\"So what you're telling us,\" Bob cannot contain his surprise,\n\"is that if you just eliminate the tags, it will be much better?\"\n\"Yes, that's what I'm saying. If we eliminate the tags and we"}
{"315": "fellows, the fact that there are more and more holes indicates that\neventually we will run into the problem of insufficient capacity,\nbut not right now. I'll take care of those tags immediately. You\nwon't see them tomorrow.\"\n\"Well, this discussion was very beneficial,\" I conclude. \"Let's\ncarry on. When was the second constraint broken?\"\n\"When we started shipping everything much ahead of time,\"\nBob answers. \"Shipping three weeks earlier is a clear indication\nthat the constraint is no longer in production but in the market.\nLack of sufficient orders limited the plant from making more\nmoney.\"\n\"Correct,\" Lou confirms. \"What do you think: did we do\nanything different on the non-constraints?\"\n\"Not me,\" says Bob.\n\"Me neither,\" echoes Ralph. \"Hey, wait a minute. How come\nwe continue to release material according to the oven and the\nNCX10 if they are no longer the constraints?\"\nWe look at each other. Really, how come?\n\"Something even funnier is going on. How come my com-\nputer shows that these two work centers are still a constraint, that\nthey are constantly loaded to one hundred percent?\"\nI turn my eyes to Stacey, \"Do you know what's going on?\"\n\"I'm afraid I do,\" she admits. \"It's definitely not my day.\"\n\"And all this time I wondered why our finished goods were\nnot depleting at a faster rate,\" I say.\n\"Will one of you tell us what's going on?\" Bob says impa-\ntiently.\n\"Go ahead, Stacey.\""}
{"316": "situation where we hold mountains of some products and not\neven one single unit of others.\"\n\"That's good,\" Lou says. \"It means we can easily deplete it.\nAlex be careful not to do it too fast, remember the bottom-line\nramifications.\"\nIt's Stacey's turn to be puzzled. \"Why shouldn't we get rid of\nthe finished products as fast as possible?\" she asks.\n\"Never mind,\" I impatiently say. \"Lou can, and should, ex-\nplain it to all of you later. Right now we should correct our five-\nstep process. Now we all know to what extent Ralph was right,\nsomething is definitely missing.\"\n\"Can I correct it?\" Stacey says sheepishly, and goes to the\nboard.\nWhen she returns to her seat the board has the following:\n1. IDENTIFY the system's constraint(s).\n2. Decide how to EXPLOIT the system's constraint(s).\n3. SUBORDINATE everything else to the above decision.\n4. ELEVATE the system's constraint(s).\n5. WARNING!!!! If in the previous steps a constraint has been\nbroken, go back to step 1, but do not allow INERTIA to cause\na system's constraint.\nExamining the board, Lou moans, \"It's much worse than I\nthought.\"\n\"On the contrary,\" I'm surprised. \"It's much better than I\nthought.\"\nWe look at each other. \"You first,\" I say. \"Why do you claim\nthat it's much worse?\"\n\"Because I've lost my only guideline.\""}
{"317": "\"Very good guideline,\" I smile. \"But what is your problem?\"\n\"Don't you see, the problem is much bigger; it's not only cost\naccounting. We put on the green and red tags not because of cost\naccounting, but because we realized the importance of the bottle-\nnecks. Stacey created orders for finished goods because of our\nnew understanding, because she wanted to make sure that the\nbottlenecks' capacity will not be wasted. I thought that it takes a\nlot of time to develop inertia. What I now see is that it takes less\nthan one month.\"\n\"Yes, you are right,\" I say gloomily. \"Whenever the con-\nstraint is broken it changes conditions to the extent that it is very\ndangerous to extrapolate from the past.\"\n\"As a matter of fact,\" Stacey adds, \"even the things that we\nput in place in order to elevate the constraint must be reexam-\nined.\"\n\"How can we do it?\" Bob asks. \"It's impossible to question\neverything every time.\"\n\"Something is still missing,\" Ralph summarizes.\nSomething definitely is still missing.\n\"Alex, it's your turn to explain,\" Lou says.\n\"Explain what?\"\n\"Why did you claim that it's much better?\"\nI smile. It's about time for some good news.\n\"Fellows, what stopped us from once again taking another\njump on the bottom line? Nothing, except for the conviction that\nwe don't have enough capacity. Well, now we know differently.\nNow we know that we have a lot of spare capacity.\"\nHow much spare capacity do we actually have?"}
{"318": "38\nIt is six o'clock in the morning when I pick up Lou and\nRalph at the plant. We (I) decided that it will be best, since pick-\ning them up at their houses would mean I would have had to\nleave home close to five. In any event, we're probably not going\nto spend more than a few hours at headquarters so it's reasonable\nto assume that we'll be back to work in the afternoon.\nWe hardly talk. Ralph, in the back seat, is busy with his lap-\ntop computer. Lou probably thinks that he's still in bed. I drive\non automatic pilot. That is, my mind is busy constructing imagi-\nnary conversations with Johnny Jons. I somehow have to con-\nvince him to get many more orders for our plant.\nYesterday, in the heat of discovering the amount of free ca-\npacity that we have, I looked only on the bright side. Now I\nwonder if I'm not just asking for miracles.\nI recheck the numbers in my head. In order to fill our capac-\nity Johnny will have to come up with over ten million dollars of\nadditional sales. It is totally unrealistic that he holds so much up\nhis sleeve.\nSo, squeezing, begging, and pleading techniques will not\nhelp. We'll have to come up with some innovative ideas. Well, the\ntruth is that so far I haven't been able to come up with any. Let's\nhope Johnny has some clever ideas; he's the one who is supposed\nto be the expert in sales.\n\"I want you to meet Dick Pashky,\" Johnny Jons says as we\nenter the small conference room. \"He's one of my best people.\nDedicated, professional, and above all he's full of innovative ap-"}
{"319": "of a pile of old junk for full price, but Alex, you're the best\u2014ten\nmillion dollars!\"\nHe continues to laugh, but I don't join in.\n\"Johnny, put on your thinking cap. You must find more or-\nders for my plant, ten million dollars more.\"\nHe stops laughing and looks at me, \"You are serious. Alex,\nwhat's happened to you? You know how tough it is to get more\nbusiness these days; it's dog eat dog out there. Everybody is cut-\nting each other's throats for the smallest order and you're talking\nabout ten million dollars more?\"\nI don't hurry to respond. I lean back in my seat and look at\nhim. Finally I say, \"Listen Johnny, you know that my plant has\nimproved. What you don't know is to what extent it's improved.\nWe're now capable of delivering everything within two weeks.\nWe've demonstrated that we never miss an order, not even by\none day. Our quality has improved to the extent that I'm sure\nwe're the best in the market. We are very responsive, very quick,\nand above all, very reliable. This is not a sales pitch, it's the\ntruth.\"\n\"Alex, I know all this. I hear it from the best source, from my\nclients. But that doesn't mean that I can immediately turn it into\ncash. Sales take time, credibility is not built overnight, it's a grad-\nual process. And by the way, you shouldn't complain; I'm bring-\ning you more and more sales. Be patient and don't press for\nmiracles.\"\n\"I have twenty percent spare capacity.\" I say, letting this\nsentence hang in the air.\nFrom the lack of response I understand that Johnny doesn't"}
{"320": "Lou says in his quiet voice, \"then it must be that clients are press-\ning for lower prices.\"\n\"Pressing is not the word. Squeezing is much more appropri-\nate. Can you imagine, and this is just between us, in some cases\nI'm forced to accept business for practically zero margin.\"\nI start to see the light at the end of the tunnel.\n\"Johnny, do they sometimes demand prices that are lower\nthan our cost?\"\n\"Sometimes? All the time.\"\n\"And what do you do?\" I continue.\n\"What can I do?\" he laughs. \"I try to explain the best I can.\nSometimes it even works.\"\nI swallow hard and say, \"I'm ready to accept orders for ten\npercent below cost.\"\nJohnny doesn't hurry to answer. His peoples' bonuses are\nbased on total sales dollars. Finally he says, \"Forget it.\"\n\"Why?\"\nHe doesn't answer. I persist, \"Why should I forget it?\"\n\"Because it's stupid, because it doesn't make any business\nsense,\" he says in a hard voice, and then softer, \"Alex, I don't\nknow what tricks you have in mind but let me tell you, all those\ntricks have a very short life span before they explode in your face.\nWhy do you want to ruin a promising career? You've done an\noutstanding job, why go and mess it up? Besides, if we lower\nprices for one client, it's just a matter of time until the others find\nout and demand the same. What then?\"\nHe has a point. The last argument shows that the light at the\nend of the tunnel was just a train."}
{"321": "prices. At the end, just two days ago, when everything is agreed,\nhe faxes me that our prices are not acceptable and sends his\ncounter offer. I was expecting the usual thing, asking for price\nreductions of ten percent, maybe fifteen percent considering the\nlarge quantities that he is willing to buy, but no, these Europeans\nprobably have a different perception. For example, Model\nTwelve, the one that you pulled such a miracle on. Our price is\nnine hundred and ninety-two dollars. We sell it to Burnside for\neight hundred and twenty-seven dollars; they're a big client and\nthey consume very large quantities of this particular product. The\nbastard had the nerve to offer seven hundred and one dollars.\nDid you hear that! Seven hundred and one dollars. Now you\nunderstand?\"\nI turn to Ralph, \"What's our material cost for Model\nTwelve?\"\n\"Three hundred thirty-four dollars and seven cents,\" Lou\nanswers without any hesitation.\n\"Johnny, are you sure that accepting this order will not have\nany impact on our domestic clients?\"\n\"Not unless we go out, and sing it from the rooftops. On this\npoint Dick is right, no impact. But the whole idea is ridiculous.\nWhy are we wasting our time?\"\nI look at Lou, he nods.\n\"We'll take it,\" I say.\nWhen Johnny doesn't respond, I repeat, \"We'll take it.\"\n\"Can you explain what is going on?\" he finally says, between\ngritted teeth.\n\"It's very simple,\" I answer. \"I told you that I have spare"}
{"322": "Johnny gives me a long look and then turns to Dick, \"Bring\nit.\" \"\nOnce Dick is on his way, Johnny says in a puzzled voice, \"I\ndon't get it. You want to sell in Europe for a price that is much\nless than what we get here, even less than the production cost,\nand you still claim that you'll make a lot of money? Lou, you're a\ncontroller, does it make sense to you?\"\n\"Yes,\" Lou says.\nSeeing the miserable expression on Johnny's face, I jump in\nbefore Lou has a chance to explain. Financial calculations, show-\ning the fallacy of the 'product cost' concept won't help, it will just\nconfuse Johnny even more than he's confused now. I decide to\napproach it from another angle.\n\"Johnny, where do you prefer to buy a Japanese camera, in\nTokyo or in Manhattan?\"\n\"In Manhattan, of course.\"\n\"Why?\"\n\"Because in Manhattan it's cheaper, everybody knows that,\"\nJohnny says confidently, here he's on solid ground. \"I know a\nplace on Forty-seventh Street where you can get a real bargain\u2014\nhalf price compared to what they asked me to pay in Tokyo.\"\n\"Why do you think it is cheaper in Manhattan?\" I ask, and\nthen answer my own question, \"Ah, we know, transportation\nprices must be negative.\"\nWe all laugh.\n\"O.K. Alex. You've convinced me. I still don't understand\nbut if it's good for the Japanese, it must be profitable.\"\nWe work on the numbers for almost three hours. It's a good"}
{"323": "\"Now you're really getting difficult. I knew that this was too\ngood to be true.\"\n\"That's not the point, Johnny. I want to use this deal as a\nbeachhead to penetrate Europe. We can't afford a price war. We\nmust come up with something else besides price, something that\nwill make it very difficult to compete with us. Tell me, what's the\naverage supply time in Europe?\"\n\"About the same as here, eight to twelve weeks,\" he answers.\n\"Good. Promise your Monsieur that if he commits to the\nquantities per year, we'll deliver any reasonable quantity within\nthree weeks of receiving his fax.\"\nIn astonishment he asks, \"Are you serious?\"\n\"Dead serious. And by the way, I can start to deliver immedi-\nately. I have whatever's needed for the first shipment in stock.\"\n\"I guess it's your neck,\" he sighs. \"What the heck, in any\nevent you will have full responsibility very shortly. If I don't hear\nfrom you, I'll fax him tomorrow. Consider it a done deal.\"\nOnly after we pull out of the parking lot do we let ourselves\ngo; it takes us more than fifteen minutes to settle down. That is,\nLou and Ralph dive into polishing the numbers. From time to\ntime they come up with a slight correction, usually not more than\na few hundred dollars. Compared to the total deal it's not signifi-\ncant at all. But Lou finds it relaxing.\nI don't let it bother me. I sing at the top of my voice.\nIt takes us more than half the way home until they are satis-\nfied. Lou announces the final number. The contribution to the\nnet profit of the plant is an impressive seven digits, a fact that\ndoesn't deter him from specifying it down to the last cent."}
{"324": "\"Hard to tell,\" he comments. \"Especially when Johnny is\nholding him so tightly under his thumb. Alex, how are you going\nto do it?\"\n\"Do what?\"\n\"Change the entire division?\"\nThat puts an end to my euphoria. Damn you Lou, why did\nyou have to bring it up?\n\"God have mercy on me,\" I say. \"Yesterday we were talking\nabout inertia. We were complaining about the inertia that we\nhave. Compare it to the inertia that we are going to face in the\ndivision.\"\nRalph laughs, Lou groans, and I feel pity for myself.\nThis week, even though we made such impressive progress,\none thing was definitely proven\u2014I'm still managing by the seat of\nmy pants.\nTake yesterday, for example. If it weren't for Ralph's instinct\nthat something was missing, we wouldn't even have noticed the\nhuge, open opportunities. Or today. How close was I to giving\nup? If it hadn't been for Lou putting us on the right track ....\nI must find out just what are the management techniques I\nshould master. It's simply too risky not to. I must concentrate on\nit. I even know where to begin. . . .\nMaybe I was holding the key all along. What did I say to Julie\nin the restaurant? My own words echo in my head: \"When did\nJonah have the time to learn so much? As far as I know he never\nworked one day of his life in industry. He's a physicist. I can't\nbelieve that a scientist, sitting in his ivory tower, can know so\nmuch about the detailed realities of the shop floor.\""}
{"325": "asking me to develop the methods, just to determine clearly what\nthey should be. Maybe popular science books would be sufficient?\nAt least I should give it a try.\nI should go to the library and start digging. The first modern\nphysicist was Newton, that's probably the place to start.\nI'm sitting in my office, my feet up on the desk and staring\nblankly into the room.\nThe entire morning, I got only two calls\u2014both from Johnny\nJons. First he called to inform me that the deal with the French is\nsigned. He was very proud of the fact that he negotiated a better\ndeal than expected; in return for the flexibility and immediacy of\nour response to their future requests, he succeeded in squeezing\nslightly higher prices.\nThe second time he wanted to know if he could approach\nour domestic clients with the same concept. That is, to shoot for a\nlong-term contract where only the overall yearly quantities are\nfixed, and we promise three weeks' delivery for any specific re-\nquest.\nI assured him that we don't have any problem responding,\nand encouraged him to go ahead.\nHe's excited. I'm far from it.\nEverybody is busy. Launching this huge new deal has made\nthem really busy. I'm the only one who has nothing to do. I feel\nredundant. Where are the days of the telephone ringing off the\nhook, when I had to run from one important issue to the other,\nwhen there were not enough hours in the day?\nAll those calls and meetings were fire fighting. I remind my-\nself. No fires, no fighting. Now, everything is running smoothly\u2014"}
{"326": "\"You're really into it,\" I comment as she joins me at the\nkitchen table.\n\"Yeah, it's fascinating.\"\nI hand her a steaming cup. \"What can be so fascinating\nabout ancient Greek philosophy?\" I wonder aloud.\n\"It's not what you think,\" she laughs. \"These dialogues of\nSocrates are really interesting.\"\n\"If you say so,\" I don't try to disguise my skepticism.\n\"Alex, your perceptions are all wrong, it's not at all like what\nyou think.\"\n\"So what is it?\" I ask.\n\"Well, It's hard to explain,\" she hedges. \"Why don't you try\nto read them yourself?\"\n\"Maybe one day I will,\" I say, \"but for the moment I've\nenough reading to do.\"\nShe takes a sip from her cup. \"Did you find what you're\nlooking for?\"\n\"Not exactly,\" I admit. \"Reading popular science books\ndoesn't lead you directly to management techniques. But I've\nstarted to see something interesting.\"\n\"Yes?\" she says encouragingly.\n\"It's how physicists approach a subject; it's so vastly different\nfrom what we do in business. They don't start by collecting as\nmuch data as possible. On the contrary, they start with one phe-\nnomenon, some fact of life, almost randomly chosen, and then\nthey raise a hypothesis: a speculation of a plausible cause for the\nexistence of that fact. And here's the interesting part. It all seems\nto be based on one key relationship: IF ... THEN.\""}
{"327": "\"Things start to be connected to each other. Things that we\nnever thought were related start to be strongly connected to each\nother. One single common cause is the reason for a very large\nspectrum of different effects. You know Julie, it's like order is\nbuilt out of chaos. What can be more beautiful than that?\"\nWith glittering eyes she asks, \"Do you know what you have\njust described? The Socratic dialogues. They're done in exactly\nthe same way, through exactly the same relationship, IF ...\nTHEN. Maybe the only difference is that the facts do not concern\nmaterial but human behavior.\"\n\"Interesting, very interesting. Come to think about it,\" I say,\n\"my field, management, involves both material and people be-\nhavior. If the same method can be used for each then it's proba-\nbly the basis for Jonah's techniques.\"\nShe thinks about it for a while. \"You're probably right. But if\nyou are then I'm willing to bet that when Jonah starts to teach\nyou those techniques you'll find that they are much more than\ntechniques. They must be thinking processes.\"\nWe each dive into our thoughts.\n\"Where do we take it from here?\"\n\"I don't know,\" I answer. \"Frankly, I don't think that all this\nreading really gets me closer to answering Jonah's question. Re-\nmember what he said? 'I'm not asking you to develop the man-\nagement techniques, only to determine what they should be.' I'm\nafraid I'm trying to jump to the next step, to develop them. De-\ntermining the management techniques must come from the need\nitself, from examining how I currently operate and then trying to\nfind out how I should operate.\""}
{"328": "39\n\"Any messages?\" I ask Fran.\n\"Yes,\" she answers, to my surprise. \"From Bill Peach. He\nwants to talk to you.\"\nI get him on the phone. \"Hey Bill, what's up?\"\n\"I just received your numbers for last month,\" he says.\n\"Congratulations hotshot, you definitely made your point. I've\nnever seen anything even remotely close to this.\"\n\"Thank you,\" I say pleased. \"By the way, what are the results\nat Hilton Smyth's plant?\"\n\"You must turn the dagger, huh?\" he laughs. \"As you pre-\ndicted, Hilton is not doing too well. His indicators continue to\nimprove, but his bottom line continues to sink into the red.\"\nI cannot contain myself, \"I told you that those indicators are\nbased on local optimum and that they have nothing to do with\nthe global picture.\"\n\"I know, I know,\" he sighs. \"As a matter of fact, I think that I\nknew it all along, but I guess an old mule like me needs to see the\nproof in black and red. Well, I think that I've finally seen it.\"\n\"It's about time,\" I think to myself but to the phone I say,\n\"So what's next?\"\n\"This is actually why I called you, Alex. I spent the entire day\nyesterday with Ethan Frost. It seems that he's in agreement with\nyou, but I can't understand what he is talking about.\" Bill sounds\nquite desperate. \"There was a time that I thought I understood\nall this mumbo jumbo of 'cost of goods sold' and variances, but\nafter yesterday, it's obvious that I don't. I need someone who can"}
{"329": "ing that we can make a lot of money if we sell below what it costs\nus to produce. That is pure baloney.\"\nI laugh, \"See you tomorrow.\"\nBill Peach abandoning his precious indicators? This is some-\nthing I have to tell everyone; they'll never believe it. I go to Don-\novan's office, but he's not there, nor is Stacey. They must be on\nthe floor. I ask Fran to locate them. In the meantime I'm going to\nLou to tell him the news.\nStacey reaches me there. \"Hey boss, we have some problems\nhere. Can we come in half an hour?\"\n\"No rush,\" I say. \"It's not so important, take your time.\"\n\"I don't agree,\" she says. \"I'm afraid that it is important.\"\n\"What are you talking about?\"\n\"It probably has started,\" she answers. \"Bob and I will be in\nyour office in half an hour. Okay?\"\n\"Okay,\" I say, quite puzzled.\n\"Lou, do you know what's going on?\" I ask.\n\"No.\" he says. \"Unless of course, you're referring to the fact\nthat Stacey and Bob have been busy for the last week, playing\nexpeditors.\"\n\"They are?\"\n\"To make a long story short,\" Bob concludes the briefing of\nthe last hour, \"already twelve work centers are on unplanned\novertime.\"\n\"The situation is out of control,\" Stacey continues. \"Yester-\nday one order was not shipped on time, today three more will be\ndelayed for sure. According to Ralph, we're going downhill from\nthere. He claims that before the end of the month we'll miss the"}
{"330": "\"How come?\" I ask them.\n\"I told you,\" Bob says. \"Order no. 49318 is stuck because\nof . . .\"\n\"No Bob,\" Stacey stops him. \"It's not the details that are\nimportant. We should look for the core problem. Alex, I think\nthat we simply accepted more orders than we can process.\"\n\"That's obvious,\" I say. \"But how come? I thought we\nchecked that the bottlenecks have enough capacity. We also\nchecked your seven other problematic work centers. Did we make\na mistake in the calculations?\"\n\"Probably,\" Bob answers.\n\"Not likely,\" is Stacey's response. \"We checked and double\nchecked it.\"\n\"So?\"\n\"So, I don't know,\" Bob says. \"But it doesn't matter. We have\nto do something now, and fast.\"\n\"Yes, but what?\" I'm a little impatient. \"As long as we don't\nknow what caused the situation, the best we can do is to throw\npunches in all directions. That was our old mode of operation. I\nhad hoped that we learned better.\"\nI accept their lack of response as agreement and continue,\n\"Let's call Lou and Ralph and move into the conference room.\nWe must put our heads together to figure out what is really going\non.\"\n\"Let's get the facts straight,\" Lou says after less than fifteen\nminutes. \"Bob, are you convinced that you need to keep using so\nmuch overtime?\"\n\"The efforts of the last few days have convinced me that even"}
{"331": "\"Can we address the real issue?\" I say in a freezing voice.\nThey all turn to me waiting.\n\"Listening again to what you're saying, I don't see a major\nproblem,\" I say. \"It is obvious that we tried to swallow more than\nwe can chew. What we have to do is to determine by how much\nand then compensate. It is as simple as that.\"\nLou nods his head in approval. Bob, Ralph, and Stacey con-\ntinue to look at me with poker faces. They even look offended.\nThere must be something wrong in what I've said, but I can't see\nwhat.\n\"Ralph, by how much are our bottlenecks overloaded?\" I\nask.\n\"They're not overloaded,\" he says flatly.\n\"No problem there,\" I conclude. \"So let . . .\"\n\"He didn't say that,\" Stacey cuts me off.\n\"I don't understand,\" I say. \"If the bottlenecks are not over-\nloaded then . . .\"\nMaintaining an expressionless face she says, \"From time to\ntime the bottlenecks are starved. Then the work comes to them in\na big wave.\"\n\"And then,\" Bob continues, \"we don't have a choice but to\ngo into overtime. That's the case all over the plant. It looks like\nthe bottlenecks are moving all the time.\"\nI sit quietly. What can we do now?\n\"If it were as easy as determining some overloads,\" Stacey\nsays, \"don't you think we would easily solve it?\"\nShe is right. I should have more confidence in them.\n\"My apologies,\" I mutter."}
{"332": "We are facing many, traveling bottlenecks.\" It's apparent that\nthey've had that discussion before.\nI don't have any other suggestion, nor does anybody else. I\ndecide to gamble on Ralph's hunch. It worked in the past.\n\"Please proceed,\" I say to Ralph.\nHe goes to the board and takes the eraser.\n\"At least don't erase the five steps,\" Bob protests.\n\"They don't seem to help us much,\" Ralph laughs nervously.\n\"Identify the system's constraints,\" he reads. \"That is not the\nproblem now. The problem is that the bottlenecks are moving all\nover the place.\"\nNevertheless, he puts the eraser down and turns to the flip\nchart. He draws a row of circles.\n\"Suppose that each circle represents a work center,\" he starts\nto explain. \"The tasks are flowing from the left to the right. Now,\nlet's suppose that this one is a bottleneck,\" and he marks one of\nthe middle circles with a big X.\n\"Very nice,\" says Bob sarcastically. \"Now what?\"\n\"Now let's introduce Murphy into the picture,\" Ralph re-\nsponds calmly. \"Suppose that Murphy hits directly on the bottle-\nneck.\"\n\"Then the only thing left to do is to curse wholeheartedly,\"\nBob spits. \"Throughput is lost.\"\n\"Correct,\" Ralph says. \"But what happens when Murphy\nhits anywhere before the bottleneck? In such a case, the stream of\ntasks to the bottleneck is temporarily stopped and the bottleneck\nis starved. Isn't this our case?\"\n\"Not at all,\" Bob brushes it away. \"We never operated that"}
{"333": "releasing material two weeks before it was due at the bottleneck.\nThen it turned out that that's too much, so I cut it to one week\nand everything was okay. Now it's not okay.\"\n\"So increase it back,\" Bob says.\n\"I can't,\" Ralph sounds desperate. \"It will increase our lead\ntime beyond what we currently promise.\"\n\"What's the difference?\" Bob roars. \"In any event we're slid-\ning on our promises.\"\n\"Wait, wait,\" I cut into their quarrel. \"Before we do anything\ndrastic, I want to understand better. Ralph, let's go back to your\npicture. As Bob pointed out, we do hold some stock in front of\nthe bottleneck. Now let's suppose that Murphy hits somewhere\nbefore the bottleneck, then what?\"\n\"Then,\" Ralph says patiently, \"the flow of parts to the bottle-\nneck stops, but the bottleneck, using the stock that accumulated\nright in front of it, continues to work. Of course that eats into the\nstock and so, if we don't build enough stock to start with, the\nbottleneck might go down.\"\n\"Something doesn't match.\" Stacey says. \"According to what\nyou just said, we have to guarantee the uninterrupted work of the\nbottleneck by building stock that will last more than the time to\novercome Murphy on the upstream resource.\"\n\"Correct,\" says Ralph.\n\"Don't you see that it can't be the explanation?\" Stacey says.\n\"Why?\" Ralph doesn't get it, and neither do I.\n\"Because the time to overcome a problem upstream did not\nchange, we haven't faced any major catastrophies lately. So if the\nstock was sufficient to protect the bottlenecks before, it must be"}
{"334": "again. But what's the problem? We released enough material for\nthem.\"\n\"It's not the material that concerns me,\" I say. \"It's the ca-\npacity. You see, when the problem that caused the stoppage is\novercome, the upstream resources not only have to supply the\ncurrent consumption of the bottleneck, at the same time they\nhave to rebuild the inventory.\"\n\"That's right,\" Bob beams. \"That means that there are times\nwhen the non-bottlenecks must have more capacity than the bot-\ntlenecks. ./Vow I understand. The fact that we have bottlenecks\nand non-bottlenecks is not because we designed the plant very\npoorly. It's a must. If the upstream resources don't have spare\ncapacity, we won't be able to utilize even one single resource to\nthe maximum; starvation will preclude it.\"\n\"Yes,\" Ralph says. \"But now the question is, how much spare\ncapacity do we need?\"\n\"No, that is not the question,\" I gently correct him. \"Just as\nyour previous question, 'how much inventory do we need?' is not\nthe real question either.\"\n\"I see,\" Stacey says thoughtfully. \"It's a trade-off. The more\ninventory we allow before the bottleneck, the more time is avail-\nable for upstream resources to catch up, and so, on average, they\nneed less spare capacity. The more inventory the less spare ca-\npacity and vice versa.\"\n\"Now it's clear what's happening,\" Bob continues. \"The new\norders have changed the balance. We took more orders, which by\nthemselves didn't turn any resource into a new bottleneck, but\nthey did drastically reduce the amount of spare capacity on the"}
{"335": "two weeks. Let's hope that that will be enough. Now, we have to\nrebuild the inventory in front of the bottlenecks and in front of\nassembly. Stacey, take all the necessary steps to put the plant, and\nI mean all the non-bottlenecks, to work throughout the weekend.\nDon't accept any excuses, it's an emergency. I'll notify sales that\nuntil further notice they should not promise any delivery in less\nthan four weeks from receipt of the order. It will jeopardize their\nnew campaign, but that's life.\"\nRight in front of our eyes the baton has been passed. It's\nobvious who is the boss now. I feel proud and jealous at the same\ntime.\n\"Bob has taken over very nicely,\" Lou says as we enter my\noffice. At least this front is covered.\"\n\"Yes,\" I agree. \"But I hate to put him in a position where his\nfirst independent actions are so negative.\"\n\"Negative?\" Lou asks. \"What do you mean by negative?\"\n\"All the actions he is forced to take are leading in the wrong\ndirection.\" I answer. \"Of course, he doesn't have any choice, the\nalternative is much worse, but still. . . .\"\n\"Alex, I'm probably thicker than usual today, but I really\ndon't understand. What do you mean by 'leading in the wrong\ndirection?' '\n\"Don't you see?\" I'm irritated by the whole situation. \"What\nis the unavoidable result of telling sales that they should quote\nfour weeks' delivery? Remember, just two weeks ago we went out\nof our way to persuade them to quote two weeks. They didn't\nhave much confidence then. Now, it will cause them to drop the\nentire sales campaign.\""}
{"336": "\"Yup,\" he agrees.\n\"Somewhere, I've made a mistake,\" I say. \"A mistake that\nnow is causing us to pull back. You know Lou, we still don't know\nwhat we're doing. Our ability to see what's in front of us resem-\nbles that of moles. We're reacting rather than planning.\"\n\"But you've got to agree that we are reacting much better\nthan before.\"\n\"That's not a real comfort Lou, we're also moving much\nfaster than before. I feel as if I'm driving looking only in the rear\nview mirror, and then, when it's almost too late, we make last\nminute course corrections. It's not good enough. It is definitely\nnot good enough.\""}
{"337": "40\nI'm driving back from headquarters with Lou. We've been\ndoing this every day for the last two weeks. We are not in what\none might call a cheerful mood. Now we know every little detail\nof what's going on in the division, and the picture doesn't look\ngood at all. The only bright spot is my plant. No, I should get\nused to the fact that it's Donovan's plant. And it's not a bright\nspot, that's a gross understatement. It's the real savior.\nDonovan succeeded getting everything under control before\nthe clients had any reason to complain. It will take him some time\nto regain the confidence of our sales people, but with me pressing\nfrom the other side it will not take long before it will be okay.\nThis plant is so good that Lou and I were led astray for some\ntime. The reports on the division gave us the impression that the\nsituation is quite good. Only when we went through the elaborate\nwork of separating out Donovan's plant was the real picture ex-\nposed. And it's not pretty. It's actually quite disastrous.\n\"Lou, I think we did the exact thing that we knew we\nshouldn't do.\"\n\"What are you talking about?\" he says. \"We haven't done\nanything yet.\"\n\"We have gathered data, tons of data.\"\n\"Yes, and there's a problem with the data,\" he says. \"Frankly,\nI've never seen such a sloppy place. Every report is missing at\nleast back-up details. You know what I found today? They don't\neven have a report on late receivables. The information is there\nbut\u2014can you believe\u2014it's scattered in at least three different"}
{"338": "Irritated he says, \"I don't know what you're talking about.\nDon't you want me to correct things which are obviously wrong?\"\nHow am I going to explain it to him? I try again.\n\"Lou, suppose that you do succeed in collecting four days\nout of the open receivables. By how much will throughput, inven-\ntory, and operating expense be improved?\"\n\"They'll all be slightly improved,\" he says. \"But the major\nimpact will be on cash. You shouldn't sneeze at four days' cash.\nBesides, improving the division requires many small steps. If ev-\neryone does his share, together we can lift it.\"\nI drive silently. What Lou said makes sense, but somehow I\nknow that he is wrong. Deadly wrong.\n\"Lou, help me here. I know that improving the division will\nrequire many small improvements, but . . .\"\n\"But what?\" he says. \"Alex, you are too impatient. You know\nwhat they say, Rome was not built in a day.\"\n\"We don't have hundreds of years.\"\nLou is right, I am impatient. But shouldn't I be? Did we save\nour plant by being patient? And then I see it. Yes, many small\nactions are needed, but that doesn't mean that we can afford to\nbe satisfied with actions that improve the situation. We must care-\nfully choose which ones to concentrate on, otherwise. . . .\n\"Lou, let me ask you. How much time will it take you to\nchange, for internal purposes only, the way that we evaluate in-\nventory?\"\n\"The mechanical work is not a real problem, that won't take\nmore than a few days. But if you're referring to the work it'll take\nto explain the ramifications, to explain to managers how this af-"}
{"339": "sure to show profits is up, so they build finished goods inventory\nto generate fictitious inventory profits. I see what you mean. We\ncan take the increase in finished goods as an indicator of the\nimpact of the way we value inventory. Wow, it's about seventy\ndays!\"\n\"Lovely,\" I say. \"Compare it to your four days of receivables.\nOn what should you work? Moreover,\" I keep on hammering,\n\"what is the impact on throughput?\"\n\"I don't see any,\" he answers. \"I see very clearly the impact\non cash, on inventory, and on operating expense, but not on\nthroughput.\"\n\"Don't you?\" I say mercilessly. \"What was the reason that\nthey gave us for not introducing the new models? Can you re-\ncall?\"\n\"Yes,\" he says slowly. \"They are convinced that introducing\nthe new models will force them to declare all the old ones they're\nholding in stock as obsolete. That would cause a major blow to\nthe bottom line.\"\n\"So, we continue to offer the old stuff rather than the new.\nWe continue to lose market share, but it's better than to bite the\nbullet of write-offs. Do you understand now the impact it has on\nthroughput?\"\n\"Yes, I do. You are right. But Alex, you know what? With\nsome extra effort I think that I can handle them both. I can work\non the problem of the way we value inventory and at the same\ntime arrange for more attention to the receivables.\"\nHe still doesn't get it but now I think I know how to handle\nit."}
{"340": "\"Frankly, yes.\"\n\"So am I, so am I.\" I mutter. \"Where do we start? Where do\nwe continue? On what should we concentrate first, on what sec-\nond? It's overwhelming.\"\n\"We need a process,\" he says. \"That's obvious. It's too bad\nthat the five-step process that we developed turned out to be\nfalse. No . . . Wait a minute Alex, that's not the case. At the end,\nthe problem was not wandering bottlenecks. It was insufficient\nprotection for the existing bottlenecks. Maybe we can use that\nfive-step process?\"\n\"I don't see how, but it's worthwhile to check it. Should we\nhead to the plant and give it a try?\"\n\"Certainly. I'll have to make some phone calls, but it's no\nproblem.\"\n\"No,\" I say. \"I have some commitments for tonight.\"\n\"You're right,\" he says. \"It's very important but not urgent.\nIt can wait for tomorrow.\"\n\"Identify the system's constraint(s),\" Lou reads from the\nboard. \"Do we accept it as the first step?\"\n\"I don't know,\" I say. \"Let's examine the logic that brought\nus to write it. Do you remember what it was?\"\n\"Roughly,\" he says. \"It was something about the fact that we\nadopted throughput as the number-one measurement.\"\n\"I'm afraid that roughly is not good enough,\" I say. \"At least\nnot at such an early stage in our analysis. Let's try again, from\nfirst principles.\"\n\"I'm all for it,\" he groans, \"But what do you call first princi-\nples?\""}
{"341": "\"Correct,\" says Lou. \"But I don't see the point in all this. I\ncan give you many more correct statements about organizations\nin general.\"\n\"Yes, you probably can, but look at the conclusion that we\ncan derive already. If any organization was built for a purpose\nand any organization is composed of more than one person, then\nwe must conclude that the purpose of the organization requires\nthe synchronized efforts of more than one person.\"\n\"That makes sense,\" he says. \"Otherwise we wouldn't need\nto create an organization; the efforts of individuals would suffice.\nSo?\"\n\"If we need synchronized efforts,\" I continue, \"Then the\ncontribution of any single person to the organization's purpose is\nstrongly dependent upon the performance of others.\"\n\"Yes, that's obvious.\" With a bitter smile he adds, \"Obvious\nto everybody except for our measurement system.\"\nEven though I wholeheartedly agree, I ignore his last com-\nment. \"If synchronized efforts are required and the contribution\nof one link is strongly dependent on the performance of the\nother links, we cannot ignore the fact that organizations are not\njust a pile of different links, they should be regarded as chains.\"\n\"Or at least a grid,\" he corrects me.\n\"Yes, but you see, every grid can be viewed as composed of\nseveral independent chains. The more complex the organization\n\u2014the more interdependencies between the various links\u2014the\nsmaller number of independent chains it's composed of.\"\nLou doesn't want to spend too much time on that point. \"If\nyou say so. But that's not so important. The important thing is"}
{"342": "\"Didn't we agree yesterday that the distorted measurements\nare the biggest constraint of the division?\"\nBob Donovan is right. Lou certainly has a fixation on mea-\nsurements. \"They are definitely a big problem,\" I say carefully.\n\"But I'm not convinced that they are the constraint.\"\n\"You're not?\" Lou is astonished.\n\"No I'm not,\" I say firmly. \"Do you think that the fact that\nmost of our products are already outdated in comparison to what\nthe competition is offering is not a major problem? Don't you\nrealize that the attitude in engineering, claiming that the basic\nrule of nature is that a project never finishes on time, is an even\nbigger problem. And what about marketing, have you seen any\nmarketing plan that has any chance of turning the situation\naround?\"\n\"No,\" he grins. \"As a matter of fact everything that I've seen\nof long term planning should be more appropriately categorized\nunder 'long term bullshitting.' '\nI'm on a roll. Today asking me about problems is like open-\ning a dam. \"Wait Lou, I haven't finished. What about the mental-\nity that is so prevalent in headquarters, the mentality of covering\nyour ass. Haven't you noticed that whenever we asked about\nsomething that doesn't go so well, everyone almost automatically\nstarted to blame everybody else?\"\n\"How could I not notice. Okay, Alex, I get your point. There\nare major problems all over. It seems that in our division there is\na whole herd of constraints, not just a few.\"\n\"I still claim that there are only few constraints. Our division\nis too complex to have more than a very few independent chains."}
{"343": "so. Here we were lucky. We were dealing with physical con-\nstraints, with bottlenecks, that's easy. But at the divisional level\nwe'll have to deal with measurements, with policies, with proce-\ndures. Many of them are cast already into behavioral patterns.\"\n\"I don't see the difference,\" I disagree. \"Here we had to deal\nwith all of the above. Come to think about it, even here the con-\nstraints were never the machines. Yes, we called and still call the\noven and the NCX10 bottlenecks, but if they were true bottle-\nnecks how come we succeeded to squeeze almost twice as much\nout of them as before? How come we increased throughput so\nmuch without buying more capacity?\"\n\"But we changed almost every aspect of how we operate\nthem, and how we operate everything around them.\"\n\"That is exactly my point,\" I say. \"What aspect of operation\ndid we change?\" Mimicking his voice I answer, \"The measure-\nments, the policies, the procedures. Many of them were cast into\nbehavioral patterns. Lou, don't you see? The real constraints,\neven in our plant, were not the machines, they were the policies.\"\n\"Yes, I do see. But still there are differences,\" he says stub-\nbornly.\n\"What differences? Name one.\"\n\"Alex, what's the use of pushing me to the corner? Don't you\nsee that there must be major differences? If there weren't, how\ncome we don't even have a clue of what the nature of the divi-\nsional constraint is?\"\nThat stops me dead.\n\"Sorry. You're right. You know, Lou, maybe we were lucky\nhere. We had physical constraints that helped us to focus our"}
{"344": "Looking at the board I add, \"What's written here is still\nvalid. Identifying the system's constraint is the first step. What we\nnow understand is that it also translates into a mandatory de-\nmand for a technique by which to do it. Lou, that's it. We found\nit.\"\nThe excitement causes me to stand up. \"Here it is,\" I an-\nnounce, \"here is the answer to Jonah's question. I'm going to call\nhim right now. You can imagine my first sentence: Jonah, I want\nyou to teach me how to identify the core problem.\"\nAs I turn to leave I hear Lou, \"Alex, I think that it might be a\nlittle premature.\"\n\"Why?\" I ask, my hand on the doorknob. \"Do you have any\ndoubt that that is what I must learn first?\"\n\"No,\" he says. \"On that I'm quite convinced. I just think that\nmaybe you should ask for more. Knowing the core problem ex-\nactly might be far from sufficient.\"\n\"You are right again,\" I calm down. \"It's just that I was look-\ning for the answer for so long.\"\n\"I understand, believe me, I understand,\" he smiles.\n\"Okay Lou.\" I sit down. \"What else do you think I should\nask Jonah to teach me?\"\n\"I don't know,\" he answers. \"But if the five steps are valid,\nmaybe what you should ask for are the techniques required to\nenable us to carry those steps out. We already found the need for\none technique, why don't we continue to examine the other four\nsteps?\"\n\"Good idea,\" I say enthusiastically. \"Let's proceed. The sec-\nond step is,\" I read from the board, \"decide how to exploit the"}
{"345": "cally. \"Change the policy! To what? Is it so simple to find a suit-\nable replacement? Maybe for you, Lou, not for me.\"\n\"For me neither,\" he grins. \"I know that cost accounting is\nerroneous, but that doesn't mean I've completely figured out\nwhat to replace it with. Alex, how does one go about correcting\nan erroneous measurement or any other policy?\"\n\"First, I think that you need the light-bulb idea, the break-\nthrough. The management techniques that Jonah talks about\nmust include the ability to trigger such ideas, otherwise those\ntechniques can't be used by mere mortals. You know, Lou, Julie\npredicted that as I come to it I'll recognize that we are not dealing\njust with techniques but actually with thinking processes.\"\n\"It started to look like it,\" Lou agrees. \"But triggering break-\nthrough ideas by itself is not enough. An even bigger obstacle is to\nverify that this idea really solves all the resulting bad effects.\"\n\"Without creating new ones,\" I add.\n\"Is it possible at all?\" Lou sounds very skeptical.\n\"It must be, if we want to plan rather than just react.\" As I\ntalk I find a much better answer. \"Yes, Lou, it must be possible.\nLook what happened to us with our solution of getting more\nsales. As a direct result of the French order we threw the plant\ninto a very unpleasant two weeks and we killed or at least delayed\na good marketing campaign. If we just thought systematically be-\nfore we implemented it, rather than after the fact, we could have\nprevented many problems. Don't tell me that it was impossible.\nAll the facts were known to us, we simply didn't have a thinking\nprocess that would force and guide us to examine it early in the\ngame.\""}
{"346": "the most fundamental things and at the same time we are asking\nfor the world.\"\n\"I've lost you,\" Lou says quietly.\nI stop and look at him. \"What are we asking for? For the\nability to answer three simple questions: 'what to change?', 'what\nto change to?', and 'how to cause the change?' Basically what we\nare asking for is the most fundamental abilities one would expect\nfrom a manager. Think about it. If a manager doesn't know how\nto answer those three questions, is he or she entitled to be called\nmanager?\"\nThroughout Lou signals that he is following me.\n\"At the same time,\" I continue, \"can you imagine what the\nmeaning is to being able to hone in on the core problem even in a\nvery complex environment? To be able to construct and check\nsolutions that really solve all negative effects without creating new\nones? And above all to cause such a major change smoothly, with-\nout creating resistance but the opposite, enthusiasm? Can you\nimagine having such abilities?\"\n\"Alex, that is what you have done. That's exactly what you\nhave done in our plant.\"\n\"Yes and no,\" I answer. \"Yes, that's what we have done. No\nLou, without Jonah's guidance all of us would be looking for new\njobs today. Now I understand why he refused to continue advis-\ning us. Jonah said it to me in the clearest way. We should learn to\nbe able to do it without any external help. I must learn these\nthinking processes, only then will I know that I'm doing my job.\"\n\"We should and can be our own Jonahs,\" Lou says and\nstands up. Then this reserved person surprises me. He puts his"}
{"347": "AN INTERVIEW WITH\nELI GOLDRATT AND OTHERS\nby David Whitford,\nEditor at Large, Fortune Small Business.\nDW:The Goal was published 20 years ago. Since then a lot has\nchanged in operations. New, powerful methodologies to im-\nprove operations, such as LEAN and Six Sigma, are widespread.\nThe emphasis on reducing lead time and improving due-date\nperformance has become the norm. Even The Goal's subtitle - a\nprocess of ongoing improvement - is a statement that is now\ntaken for granted by every organization.\nSo, my first question: Is The Goal still relevant?\nEG: How does a scientist go about judging the relevancy of a particu-\nlar body of knowledge? I believe that the decisive way is to choose\nan organization where all the competing knowledge is implemented.\nWe should choose a large company that is already using all the new\nmethodologies you mentioned; an organization that is using these\nmethodologies so extensively that there is an institutionalized orga-\nnizational structure - like a formal \"black-belt\" central office. The\nnext step is to choose a significant section of that organization, and\nproperly implement in it the body of knowledge in question. In our\ncase it will mean implementing TOC in one of the plants of that large\ncompany. Then, compare the performance of the chosen plant with\nthe\nperformance of the rest of the organization. Now we are able to reach\na conclusion: if no real difference is detected then the conclusion will"}
{"348": "DW: Did you conduct such an experiment? And if so can you\ntell us about the results?\nEG: Fortunately, I don't have to initiate such experiments, since many\nreaders of The Goal are kind enough to write to me and share their\nexperiences. From the letters that I received over the years let's pick\none that fits our conditions. Since we are discussing relevancy, it must\nbe a recent letter. It should be from a person who implemented TOC\nin a plant that is part of a large enough organization, an organization\nthat is using black-belts. And it should contain comparisons between\nthat plant and all other plants of that company.\nJudge for yourself if this letter fits our bill perfectly.\nDow Corning Corporation\nHealthcare Industries Materials Site\n635 N. Gleaner Road\nHemlock, MI 48626\nMay 20, 2004\nDear Dr. Goldratt:\nI wanted to share with you what we have accomplished within\nour organization by using the tools presented in your books,\n\"The Goal\" and \"It's Not Luck.\"\nWhen a colleague gave me a copy of \"The Goal,\" the plant\nat which I work was in a similar situation as Alex's plant in\nthe book. At that time, in 1998, our plant's on-time delivery\nwas approximately 50%. We were carrying over 100 days of\ninventory and we had customers on allocation because we\ncould not meet the demand for orders. In addition, our man-"}
{"349": "not solve the problems within my unit, or within our plant. I\nordered several copies of \"The Goal,\" and my colleague and\nI distributed them to our production manager, plant manager\nand manufacturing and quality engineers. Everyone was eager\nfor a solution to our problems.\nWithin my unit we identified the bottleneck and began to focus\nour resources there. Our plant is a non-union facility and many\nof the workers were also interested in what we were doing. I\nordered copies of \"The Goal\" for everyone who worked for\nme. By the time the six-month ultimatum came, my unit and\nanother had started to make significant changes, and the plant\nwas spared any ill recourse. However, the expectation was\nthat we would continue to improve. For the five years that\nfollowed, we continued to work on breaking our bottlenecks.\nWhen one moved, we attacked it again. We got pretty good,\nand could determine where the bottleneck would occur next.\nEventually, the bottleneck moved outside our plant as depicted\nin \"The Goal.\" However, we knew this would happen ahead\nof time and had already begun the indoctrination of our sales\nand marketing group.\nI recently moved out of production, but before I left, the results\nwithin my unit were: cycle time reduction of ~85\u00b0/o. Operator\nheadcount reductions of 35% through attrition; no layoffs were\nneeded. Work in process and finished goods inventory down\n~70%. On-time delivery went from ~50% to ~90% and the\nnumber of material handling steps were cut by over half. Our\nplant, and business unit have done very well too. And me, I\nreceived a promotion while in that position, and a compensa-"}
{"350": "Thank you for signing the book Dr. Sirias has forwarded to\nyou on my behalf. I am honored.\nSincerely,\nRobert (Rob) Kain P.E.\nSix Sigma Black Belt\nDow Corning Corporation\nLife Sciences/Specialty Chemical Business\nDW: Impressive, but why is only one business unit of Dow Corn-\ning using TOC? What bothers me is that this person is talking\nabout a span of over five years. If it worked so well, why didn't\nit spread to the other business units? Is it the Not-Invented-Here\n(NIH) syndrome?\nEG: Before we dive into speculation about psychology of organiza-\ntions, let's examine the facts. We are talking about a middle manager\nwho works in one corner of a large company. Why should we be\nsurprised that, in five years, this person was not yet able to take his\nwhole company through a major paradigm shift? And, by the way,\nas you read in his letter, he is making nice progress; he has already\nmoved into a much more influential position.\nDW: Still, even with enough time, is it possible for a middle\nmanager to influence his whole company?\nEG: Yes. But of course, such a person will need a lot of stamina and\npatience.\nDW: What makes you so sure that it is possible at all?\nEG: What evidence will convince you that it is possible?\nDW: Give me an example of a middle level manager working\nfor a large company who has succeeded in institutionalizing\nthe usage of the know-how written in The Goal. I mean institu-"}
{"351": "Kevin Kohls. (Eli Goldratt interview to be continued.)\nInterview with Kevin Kohls General Motors\nDirector of Throughput Analysis and Simulation for North\nAmerican Assembly Plants.\nDW: What drove you to seek help from The Goal?\nKK: It goes back almost 15 years, when I was starting off as a controls\nengineer at the Cadillac Detroit-Hamtramck assembly plant, just re-\nturning from Purdue University after completing a masters degree in\nelectrical engineering. When I left a year and half earlier, the plant\nwas just starting production. When I returned, they had yet to hit\ntheir production targets; in fact they were far short. As you might\nimagine, everyone was frustrated about not hitting these targets, and\nthere was a lot of effort being expended to improve the system, with\nminimal results.\nI was frustrated as well. The solutions I was putting in place rarely had\na significant impact on the production of the plant, and it wasn't clear\nwhy. About that same time, Dave VanderVeen from GM Research\nmade a presentation to Larry Tibbetts, who was then plant manager.\nDave was promoting a research tool that he said would help improve\nthroughput in the plant. Larry was very impressed, and asked me\nto go see Dave to find out if we could use this tool at Hamtramck.\nWhen I went down to the Research Building at the GM Tech Center\nin Warren, Dave explained what a bottleneck was and how his tool\nidentified it. He handed me a copy of The Goal and said if you want\nto understand bottlenecks and how to improve throughput, this is the\nbook to read.\nI took the book home and started to read it right away. The first thing"}
{"352": "DW: What was the problem?\nKK: It was an operation where they were installing the fuzzy, felt-like\nmaterial that goes in the ceiling of the car\u2014very big and very clunky.\nOur data said that the mean cycles between failures was about five\nminutes, and the mean time to repair was about a minute. I was\namazed that the line was stopping that often, and thought maybe the\ndata was wrong, so we went and looked for ourselves. Sure enough,\nwe watched the operator run for five cycles, stop the line, walk away,\npick up five more of these big, bulky items\u2014they weren't heavy but\nthey were big\u2014drag them back, restart the line, and continue to install\nthem. Every five cycles she would stop the line. Was it considered\na major problem before we looked at it? No. It's not like we were\nlosing an hour straight of production because something had broken\ndown. We were only losing one minute. But it was happening every\nfive cycles.\nWe could see immediately why the material wasn't closer to the\nline. There was a supervisor's office in the way. We found out there\nhad been a request made some time ago to move the office, but it\nwas considered very low priority and it wasn't getting done. So I got\nthe office moved, and lo and behold, throughput of the entire plant\nwent up, which was a surprise, because my experience told me that\nI couldn't expect that. Then we used the software to find the next\nbottleneck and continued on with that process until we were making\nour throughput goals very steadily, every day. That was a real change\nin the way that plant operated.\nDW: Did you take your insights to other GM plants?\nKK: Yes. We demonstrated the process when central office manage-\nment visited the plant, and it became apparent a lot of plants in GM"}
{"353": "KK: Yes, but there are other disciplines involved. You have to un-\nderstand simulation, and how it predicts throughput, and why it's\nimportant to understand where the bottleneck will be for a future\ndesign. But TOC is the basis for what we do. I still teach a two-day\ncourse. We might go to a plant and train the whole staff in how to\nuse TOC concepts. I always give out copies of The Goal ahead of time\nand ask them to read it before the training. It's gotten to the point in\nmanufacturing, however, where there are not that many people left to\ngo through the training. My internal customers are usually very savvy\nnow about TOC, bottlenecks, data collection and analysis. So I rarely\nhave to sell the concept anymore. Demand for data collection imple-\nmentation to drive the bottleneck software, for example, exceeds our\nability to install. And while I'm responsible for GM North America,\nthis week alone I have people in China and in Europe working on\nthese kinds of issues.\nDW: How has your use of TOC concepts changed over the\nyears?\nKK: What we found when we first started out is that we were dealing\nwith the low-hanging fruit. You look at that first example I told you\nabout, and it was very obvious that the office was in the way, and the\nsolution was just to move it. Over time, the solutions to the problems\nhave become a lot more difficult to find. This doesn't mean you can't\nsolve them, it just means you might have to use more scientific tech-\nniques. Now I might have to apply statistical methods as opposed to\nsimple observation to understand what's driving the problem at a\nwork station.\nAnother thing we're doing lately is applying what we've learned from\nThe Goal to the design of new plants and production lines. In -effect,"}
{"354": "KK: Yes and no. The Theory of Constraints is a very scientific, logical\nprocess. And because of that, when the game changes you can always\ngo back to the logic. Originally we just had to find the bottleneck,\nwalk out there, ask three or four questions, and we knew what to go\nand do. Now we can change the way we design whole manufactur-\ning processes to make sure they're better from the start. But the logic\nbehind TO C\u2014the conflict clouds, the current reality trees, the way we\nask questions to uncover the constraint\u2014all that still applies.\nI think the problem with too many other approaches is that once the\nfirst layer of problems goes away, and the crisis no longer exists, then\nit's, \"Phew! We're done!\" In the TOC world, you find yourself asking,\n\"Where has the constraint gone, and what can I do to help break it?\"\nSo you're never done.\nI'd like to be able to tell you that as soon as I started telling people\nabout these concepts, the whole organization immediately changed to\nthe new paradigm. The fact is that it has taken years to get the process\ngoing, and the leverage to make improvements is still significant, es-\npecially in a company as large as General Motors. It's much like the\nflywheel concept discussed in Good to Great, by Jim Collins. It's taken\na while to get the flywheel turning, but it's starting to go at a pretty\ngood clip right now!\nInterview with Eli Goldratt continued...\nDW: At Dow Corning it took about 5 years for TOC to spread\nfrom one section to a whole business unit In General Motors\nit took over ten years to be institutionalized throughout North\nAmerica. Does it always take years to spread from the origin\nto the whole company?"}
{"355": "DW: Can you give an example?\nEG: In order to prove my point let's take an extreme example. An\nexample of an operation that is not only large and complex but also\ndominated by large uncertainties - a repair depot of the United States\nMarine Corps. This depot is overhauling helicopters. It's very large\n- several thousand people. It is very complex - the helicopters are\ndisassembled to the smallest pieces. Even the paint is sandblasted\noff. Whatever has to be repaired is repaired. Whatever has to be\nreplaced is replaced. And then you reassemble the whole airplane.\nOne has to make sure that certain parts which were taken from the\noriginal airplane go back on the same airplane. What makes it even\nmore complex is the fact that two intrinsically different modes of\noperation have to be synchronized. The disassembly/assembly lines\nare a multi-project environment. The repair shops that feed the lines\nare a production environment, and the two must work in tandem.\nThe real challenge is the fact that the whole operation is dominated\nby high uncertainty - one doesn't know the content of the work until\nthe helicopter is disassembled and inspected. Surprises all over the\nplace. A real nightmare. Still, it took the commander less than a year\nto implement TOC. An implementation that was so solid that the\nprocess of on-going improvement continues with his successors.\nInterview with Robert Leavitt, Colonel,\nUnited States Marine Corps retired.\nManager, Sierra Management Technologies\nDW: You were responsible for implementing a TOC-based\nprogram in the Marine Corps?"}
{"356": "RL: We had problems delivering H-46s on time. The H-46 is a 25-to\n30-year-old Boeing helicopter used extensively in the Marine Corps\nas part of their assault support role. Because the airplane is so old and\nin frequent need of maintenance, anything over a single-digit number\nof airplanes on our hangar deck meant that you took a shadow off\nthe flightline. If you took a shadow off the flightline, that meant they\ndidn't have an airplane to do their mission. Our negotiated norm for\nturnaround time was 130 days, and on average we were somewhere\nbetween 190 and 205 days.\nDW: Sounds like you had a problem.\nRL: A problem, yes. So we implemented critical chain, and ultimately\ncut the number of airplanes in flow from 28 to 14. We were able to\nsell that to our customers. And the turnaround time went from 200\ndays to about 135. Now that in and of itself is probably a significant\nimprovement. But at the same time we were starting the process, they\nadded 30 days more worth of corrosion work to be done to the cabin.\nWe accommodated the 30 days within that 135-day delivery. So we\nwent from what would have been about 230 or 240 days to 135.\nDW: Why did this approach work where others had failed?\nRL: We had looked at a lot of the project management solutions,\nincluding material resource planning (MRP). TOC was the one that\nworked from all dimensions; building teamwork, understanding vari-\nability, and with a grounding in scientific thought. It was a holistic\napproach to solving the problems. It looked at the entire system and\nsaid, hey, once you find the key leverage point you'll get some sig-\nnificant returns. And then you can go back and find the next leverage\npoint, or constraint.\nDW: Did it take you a long time to find the constraint?"}
{"357": "biggest thing was the way we applied available resources; it didn't\nmake any sense. The estimators and evaluators really had about two\ndays worth of work and they were taking about 14. We figured out\nwhat was going on\u2014why that was a problem, why the scheduler set\nthat up\u2014and then reorganized.\nDW: Bottom line?\nRL: Well, the way it worked with the government, we were funded\nfor a certain number of airplanes each year. We started burning\nthrough the backlog and we actually produced a few extra airplanes.\nI know from talking to the new commanding officer down there that\nthey've increased the amount of product every year as they've gone\nforward.\nDW: And you had another example?\nRL: I also implemented TOC in the tail rotor blade cell at Sikorsky\nAircraft, the overhaul and repair division. We were averaging some-\nwhere between 15 and 19 tail rotor blades a month. It took us about\n73 days to finish a tail rotor blade and we had as many as 75 or 80 tail\nrotor blades in flow. Well, we changed the flow to more than 30 tail\nrotor blades in process, which means our turnaround time actually\nwas about 28 days.\nDW: How quickly did this improvement occur?\nRL: Three months. Now you can understand why I'm trying to build\na consulting practice around TOC.\nInterview with Eli Goldratt continued...\nDW: I'd say almost everybody I've talked to who has read The\nGoal agrees with its messages. It also seems clear that many"}
{"358": "accounting is completely false. As a matter of fact, financial managers\nare the only type of managers that knew, much before TOC, the fal-\nlacies of cost accounting. Moreover, in almost any company, the VP\nof finance is one of the few managers who sees the overall picture and\nis extremely frustrated to witness so many devastating local optima\ndecisions which do not view the organization as a whole. What we see\nin reality is the exact opposite; the financial managers rarely oppose\nTOC. On the contrary, in many (if not most) implementations, they\nare the driving force.\nDW: That's hard to believe. Can I interview such an enlightened\nfinancial manager?\nEG: As many as you want. As I said, such financial managers are the\nnorm rather than the exception.\nInterview with Craig Mead, Book Manufacturing\nVice President Finance, Thomson-Shore, Dexter, Michigan.\nDW: Tell me about Thomson-Shore.\nCM: We're in Dexter, Michigan, just outside Ann Arbor. Approxi-\nmately 40% of our customers are university presses. We would be\nconsidered a short-run printer, meaning we print runs of between 200\nand 10,000 copies. We're also an ESOP company-98% of the stock\nis owned by the employees. We've had as many as 300 employees.\nRight now we're at 280.\nDW: I understand that everybody in your company has read\nThe Goal.\nCM: We made it mandatory reading for all our employees.\nDW: Top to bottom?"}
{"359": "CM: Our main problem was with on-time delivery. We also had\nproblems with a department-type mentality at the company. People\nhad a hard time looking beyond their departmental responsibilities.\nEverybody was functional in thought.\nDW: Were you able to turn things around?\nCM: Yes. Before we started, we were at around a 70% on-time deliv-\nery. After implementing the TOC policies and practices, we got up\nto around 95%.\nDW: Your first step was to have everyone read The Goal?\nCM: Yes, that was the first step. The next step was to bring in a TOC\nconsultant. We put 30 people through a three-day training course on\nTheory of Constraints. From there the leadership group identified what\nwe thought was the constraint and began to follow the Five Steps.\nDW: What was the constraint you identified?\nCM: In our business we have two areas of major investment One is in\nthe press room and one is in the bindery. We basically settled on the\npress room as the constraint and began to manage the business with\nthat in mind. As we focused on the constraint and began to subordi-\nnate everything else to that, we began to break down departmental\nbarriers. It took a lot of education and training. We developed our\nown internal course for employees. Basically we took the three-day\ncourse, pared it down to about an hour, and had every employee go\nthrough that. The course dealt with the major concepts of constraint\nmanagement, subordination, flowing work, and removing localized\nthought processes.\nDW: What changes did you make in the press room?\nCM: We chartered some teams to look at the various products that we\nmade and began to challenge assumptions on how we use the presses."}
{"360": "customers' needs. By creating new standards we eliminated an incred-\nible amount of waste. Before, we were constantly reworking jobs to\nmeet what we thought were customer needs. In reality it was forever\nputting us farther and farther behind. Rethinking all our assumptions\nforced us to discipline ourselves and to maximize each component in\nthe press room. That allowed us to flow the work more consistently.\nDW: How did you involve the employees?\nCM: Employees at Thomson-Shore have the ability to influence the\nstandards and the way work moves within their area of expertise. When\nyou're strictly localized in your thinking, every person wants the job\ndesigned to benefit themselves. And that creates chaos. Before we did\nour TOC implementation, we could never agree on anything without\na long, involved discussion. If we wanted to make a change we had\nto get 12 people in a room and then try to reach a compromise on\neverything. We could never please everybody. Having everyone read\nThe Goal helped everyone understand that the basis for everything we\ndo wasn't localized thinking anymore. So, for example, if a job had to\nspend a little more time in the bindery, that's okay, as long as that's\nwhat's most effective for the press, which we had identified as the\nmajor\nconstraint. In the end we got the throughput that we needed.\nDW: As a finance guy, what was your specific contribution?\nCM: The Theory of Constraints is built on the premise of breaking\nthe barriers of the cost model of accounting, and we were a heavily\ncost-driven organization, as a lot of manufacturing companies are.\nEverything in the company was designed as the cost-system would\ndictate. That's where I began to add value\u2014by helping to develop dif-\nferent measurement tools that we could use instead of the traditional"}
{"361": "big happy family, you have fixed expenses and you have variable\nexpenses. Your variables are your materials and your fixed is every-\nthing else. And sitting around spending all your time trying to figure\nout how much electricity and square footage of air conditioning and\ncooling goes to the press room, how much to the bindery and the\nprepress and how much to the office doesn't help you manage your\nbusiness.\nDW: Because it distracts you from the goal.\nCM: Yes! Of meeting the needs of the customer. And flowing the\nwork in a timely fashion. When we began to concentrate on making\nthe work flow, that is, maximizing the capacity of the press room, and\nsubordinating everything else to that, we began to improve our on-\ntime delivery. The critical issue is how you measure the performance\nof the organization. We use two methods.\nDW: And they are?\nCM: Eli Goldratt talks about developing a constraint management\ntool. Ours is called TCP, for throughput contribution per press hour.\nWhen the market isn't a constraint, you choose which products and\nwhich customers to bring in based on that number. That's how you\nbuild profitability. Assuming, of course, that the constraint is not in\nthe market.\nDW: And when the constraint is in the market?\nCM: For that we came up with another internal measure. We call\nit CRH, for contribution margin per resource hour. We try only to\ncapture hours that represent value that customers pay for. We take\nthe contribution\u2014which is sales less materials\u2014and we divide by the\nhours consumed and come up with a relative measure that has validitv\nacross the whole organization. It has taught us an immense amount"}
{"362": "work, are difficult and cost us more to manufacture-it clearly pointed\nthat out. And then it also began to show us how technology affects\nour margins. I mean, we get most of our books on PDF files now,\nand the cost difference between working with a PDF file and working\nwith what I'll call the old conventional way is incredible. What was\nhappening was that we were being forced by the market to reduce\nour prices across the board, but then any job done the old way was\nnot very profitable. Hah! Not profitable at all! People were expecting\nPDF pricing for conventional work, and that just doesn't work. Bot-\ntom line: In a harsh business climate, in which the market is the new\nconstraint, and sales are declining, we've actually built profitability.\nSignificantly.\nDW: Does it help that you're an ESOP company? Does that\nmake\nit easier for employees to align their interests with the goal?\nCM: It depends on the individual. Someone who is ten years from\nretirement is more interested in the value of the stock. Someone who's\nbeen here three or four years, they're looking at the individual-based\nbonus. So we actually began to implement team bonuses instead of\nindividual-based bonuses. Today we're working on disconnecting the\nlink between compensation and performance feedback. Feedback is\ngoing to be all team-based.\nDW: You said you had 300 employees before and now you're\nat 280. Is that the fault of a bad business climate or a benefit\nof being more efficient?\nCM: It's both. The business climate has not been healthy. But at\nthe same time, some of the changes we made freed up capacity, and\nas people quit we didn't replace them, which built profitability. No"}
{"363": "CM: Yeah, we have more capacity than the market's willing to give.\nThat's an issue. I think we're prepared to meet the market when and\nif it comes back. And in order to do that we have to do three things.\nWe have to fulfill the requirements of speed and delivery. We have\nto stay profitable to maintain our equipment and provide the quality\nthat customers expect from us. And then, three, we have to have em-\nployees who are participating fully, who want to come to work every\nday, and who understand why they're here and why they're doing\nwhat they're doing. TOC has allowed us to do all three.\nInterview with Eli Goldratt continued...\nDW: I'm back to my previous question. How come most readers\nof The Goal do not rush to implement TOC?\nEG: TOC is built on the realization that every complex environment/\nsystem is based on inherent simplicity and the best way to manage,\ncontrol and improve the system is by capitalizing on this inherent sim-\nplicity. That's why the constraints are the leverage points. That's why\nthe five focusing steps are so powerful. But, what we have to bear in\nmind is that such an approach is a major paradigm shift. And people\nwill do almost anything before they will shift their paradigm.\nFrom observation, I can tell you that readers of The Goal proceed to\nimplement it mainly when three conditions are met. First, there is a real\npressure to improve. But that by itself is far from being enough. The\nsecond condition is that it is obvious to them that there is no remedy\nwithin their existing paradigm. In other words, they had already tried\neverything else. And the third condition is that something helped them\nto do the first step. This something might be a \"how to\" book, like"}
{"364": "Interview with Stewart Witt, Ongoing Improvement\nA consultant\nDW: I understand that your introduction to The Goal came be-\nfore you became a consultant\nSW: Right. I was VP of operations at the time for a small manufactur-\ning company, Ohmart/Vega Company, in Cincinnati, Ohio. Someone\ngave me the book with the recommendation to read it. And I read it,\nand it was very entertaining and made a lot of sense, and I promptly\nput it right back on the shelf.\nDW: I've heard stories like that before.\nSW: Right. I just wasn't ready yet. This company had hired me specifi-\ncally to improve their operations and prepare them for growth and\nmake them more efficient, all that stuff. I had talked the president\ninto hiring a consulting firm, saying, \"I can do these things but we\ncan get it done that much quicker with some help,\" and he was fine\nwith that. So we hired Grant Thornton, and they came in. We rear-\nranged everything, streamlined everything. They took a look at the\nsoftware we were using and made some other recommendations. We\npaid them about $120,000 and in about 6-8 months we started to see\nsome results. Everyone was very happy because we took lead times\ndown from, like, two weeks to one week. It was, wow, that's pretty\ngood! The problem was that the same improvements were happening\nin sales and marketing. So here comes 40% more orders in the same\ntime frame, and as it trickled out into the shop, so trickled away my\nimprovements. The capacity I had freed up was now being doubled\nup by all these extra orders and I was back in the same boat that I"}
{"365": "SW: Yeah, I spent all this money, all this time. All the things I knew\nhow to do I had done. I couldn't rearrange everything again. I couldn't\nlook at the software and come up with any new ideas. I had already\nemployed the best consultants that I knew.\nDW: Right So what did you do?\nSW: I signed up for Porsche mechanic school in California. It must\nhave been a weak moment in my life. I do amateur racing and there's\na saying that goes: you didn't make any mistake when you spun the\ncar and flew off the track; what you did was you went into the corner\nand ran out of talent. That's how I looked at it-I must not be cut out\nfor this job, there must be something I'm missing. I couldn't figure\nit out\nDW: How old were you?\nSW: That was ten years ago; so, early 30s. Mechanic school wasn't\na waste of time. I still use what I learned. I save 600 bucks doing my\nown tune-ups. But right before I left to go out there, someone said:\n\"You know, in San Jose there's a software company that has been cre-\nated to support the rules that are stated in The Goal, and by the way,\nthe Goldratt Institute has just issued a self-learning kit that you might\nbe interested in.\" So I went to my mechanic class, that was very fun.\nThen afterwards I stopped in San Jose, took a look at the software,\nand completed the workbook on the way home. I was so excited that\non Monday morning I got my staff together and I said: \"This is what\nwe're going to do. We've got nothing to lose. It looks like it's possible.\nIt almost looks too simple. Let's give it a try.\" They weren't very con-\nvinced. In fact they were pretty skeptical. I'd put them through a lot\nalready. One more thing, huh?\nDW: This was their first exposure to TOC?"}
{"366": "down, our on-time deliveries were starting to go up. At first I thought\nit was just a fluke.\nDW: What changed your mind?\nSW: Well, a month later here comes one of my welders and he says:\n\"Boss, I think my numbers are wrong. The lead time I've been mea-\nsuring is now about a day and a half.\" I said: \"How can that be?\" We\nwere still running more orders. I had even had to fire a guy in the\nmeantime, so we were down resources. And we hadn't bought any\nnew equipment. So I said, \"Okay, fine, let me check and I'll let you\nknow what I find out.\"\nDW: What did you find when you examined the numbers?\nSW: I told my welder: \"You know what? You're right, the numbers are\nwrong. The lead time is less than a day.\" Same resources, 40% more\norders, a fraction of the lead time. Took us two months to do that.\nCost us $500. The company was a hundred years old and they had the\nbest two quarters that they've ever had. One division that was losing a\nmillion dollars a month was now making a million dollars a month. If\nI hadn't seen it with my own eyes, I would never have believed it.\nDW: What was the constraint you exploited to make such\na huge difference?\nSW: We actually worked through about three of them. One of them\nhad to do with the fact that we were sending everything out to put a\nprotective coating on the pipes that held the measuring equipment.\nIt was a step that had been added at some point by the marketing\ndepartment, and it had developed into a constraint. So we had to go\nand find one or two more suppliers to handle the load.\nDW: And there were others?\nSW: One was the saws that cut the pipes. We offloaded some of the"}
{"367": "paint department was next, we did a couple of things there. At which\npoint the constraint shifted to engineering. We were waiting for some\nnew products to come out, and that's kind of where it ended up.\nDW: Do you believe that TOC is an infinite process? In other\nwords, is there always going to be another constraint you can\nfind and exploit?\nSW: Theoretically, it can go on forever. But from what I've seen, it\ngoes through one or two cycles within a facility, and then you've kind\nof broken the constraint in the production operation. Then it may\nmove to, say, engineering. Then you can apply Critical Chain to the\nengineering group and eliminate that as a constraint, and then the next\nconstraint usually is the market, and typically it's the existing market.\nUnless you're Coke or GE or whoever, you probably don't have a\ndominant position in your market. So you can still find room to grow.\nFinally, there are plenty of cases where, using the same capabilities\nthat you generated using TOC, you can attack new markets that you\nnever thought you could compete in. At that point, you're probably\ndoing all you can handle anyway.\nOr maybe it goes back to manufacturing again. Could be, yeah, and\nyou definitely know how to deal with that by then.\nDW: Alright So then you moved on?\nSW: I actually went to Grant Thornton for two years and worked on\ndeveloping other TOC skills and applying what I knew to an ERP\n[enterprise resource planning] implementation at a plant in Mexico,\nworking with Navistar International. I did that for about two years.\nTraveled to Mexico a lot, gained about 40 pounds, got no exercise.\nBut it was kind of fun. Then I went to work for a consulting firm.\nWithin about a month I was put on my first project, involving TOC,"}
{"368": "SW: Huge. The plant covered half of Tennessee, it seemed like, way\nout in the middle of nowhere. So we put a small team together. It\nwas me and another guy and about half a dozen folks at the site, and\nwe went through the exact same training I had done the first time at\nOhmart/Vega. Was exactly the same concept, exactly the same ideas.\nThe only thing different was the context. We had software systems we\nhad to integrate\u2014five different software systems that had the data in it\nwe needed. We identified the constraint, and did all the usual things,\nlike making sure there was a buffer in front of it, making sure the\nmaintenance guys were giving it top priority so if there's any trouble\nthey could fix things right away. We put a quality check in front of it\nso that we weren't wasting time processing any bad electrodes at that\npoint in the process.\nDW: What was the upshot?\nSW: No change whatsoever in on-time delivery. The company already\nhad an excellent record in that regard and by the time we had fin-\nished, it still had an excellent record. But the only reason they could\ndeliver on time before was because they had more inventory than\nthey really needed. They just stuffed the shelves full of electrodes,\nhad them sitting all over the place. So you see, we didn't disrupt their\ndelivery performance at all, they continued to deliver 100% on-time.\nBut in the end they did it with about 40% less inventory. And they\nwere very satisfied with that because that essentially freed up almost\n$20 million that they could now use elsewhere to run their business.\nBased on those results, the CEO stood up at a big meeting one day\nand said that this is what we're going to do worldwide. We brought\nrepresentatives from Spain, Brazil, Italy and South Africa to Clarksville\nas part of a worldwide implementation team. It's become a classic case"}
{"369": "the initial meetings with the client, I may approach it differently than\nsome of my colleagues. They'll come in and say: \"We have this line\nof services, which one do you want?\" What I do is ask questions, like\nJonah does in the book. That helps me decide if there is a fit for what\nI do. Basically, I try to help clients understand that if you address the\ncore problems rather than the symptoms so many people focus on,\nyou can almost promise good results.\nInterview with Eli Goldratt continued...\nDW: What are the limits of TOC? Can it be applied also to\nservice-based organizations?\nEG: Yes, but... And in our case the \"but\" is quite big.\nLet me start with the \"Yes.\" Yes, any system is based on inherent\nsimplicity, in this sense there is no difference between a manufactur-\ning organization and any other organization, including service orga-\nnizations. Yes, the way to capitalize on the inherent simplicity is by\nfollowing the five focusing steps; identify the constraint, decide how\nto exploit it, etcetera.\nThe \"but\" revolves around the fact that it might not be a triviality to\nfigure out how to actually perform each of the five steps; to figure out\nthe detailed procedures. In The Goal, I introduced the overall concept\nand, through the detailed procedures for production, proved its valid-\nity. In It's Not Luck, I've explained the thinking processes needed to\ndevelop the detailed procedures to perform each of the five steps. As\nteaching examples, I showed how the thinking processes are used to\ndevelop the detailed procedures for sales of several different cases of\nmanufacturing organizations. So, as a result, manufacturing organi-"}
{"370": "EG: As you know, we use the term service organization for a very\nbroad spectrum of totally different types of organizations. Organiza-\ntions that are different from each other no less than they are different\nfrom manufacturing. You are not talking about another book, you are\ntalking more of a library.\nDW: Can you give me an example of a TOC implementation\nin a service industry? Any type of service industry?\nEG: Let's start with a company that does not design or manufacture\nanything, and therefore is called a service organization. Still they\ndeal with physical products; something that you can touch. An office\nsupply company.\nDW: A distributor of office supply products?\nEG: Correct. But before you go and interview them, let me stress one\npoint. All the TOC detailed procedures for the logistical aspects of\ndistribution had long been developed and tested in many companies.\nBut this particular company still had to use heavily the thinking pro-\ncesses to properly develop the detailed procedures needed to properly\nposition itself in the market.\nInterview with Patrick Hoefsmit, Office Supply\nFormer managing director, TIM Voor Kantoor, 100-year-old\noffice supply company in the Netherlands.\nDW: What was your first exposure to The Goal?\nPH: I was one of the owners of a printing company. Pretty big com-\npany. Couple of hundred people, 40 presses. I was taking a course\nfrom someone who was explaining to me the difference between debit\nand credit-I'm a technical engineer, so I needed some explanation."}
{"371": "DW: That seems to be a large part of the appeal of The Goal,\nit's accessibility.\nPH: Yes, The Goal doesn't go really deep into the financial difficulties\nof running a company. As a matter of fact it completely makes it ir-\nrelevant. So for me it was also a great message that I could just ignore\nall these economist Ph.D. people-if they couldn't explain to me what\nwas going on, then forget about it! So that was my first experience\nwith the Theory of Constraints. Then somebody gave me an article\nthat said Eli Goldratt was in Holland to give a seminar. So I went\nthere. At the seminar Eli told us that he just increased the price for\nhis Jonah courses from $10,000 to $20,000 because otherwise top\nmanagement wouldn't come; something like that. So I said to him, \"I\npromise I will come, even at the old price!\" He said he had a better\ndeal for me. If I was to do the course, I could do so and I only had\nto pay him after the results were of such magnitude that the price of\nthe course was irrelevant.\nDW: Good deal.\nPH: Yeah, it was a perfect deal. So I went to New Haven, to America.\nHe had an institute there. Did the course, couldn't do anything with it.\nSo a year later I went to ajonah upgrade workshop; it was in Spain. Eli\nhas a very good memory, so when he ran into me he said, \"Hey, did\nyou pay for your course yet?\" I said, \"No, no, I didn't see any reason\nwhy I should.\" So he invited me for a private session. Some people\nwarned me about that! On Monday morning I had a private session\nhere in Rotterdam. That was a hefty morning. All my homework\nand all the things I did were to him completely irrelevant. The point\nwas, I was looking at my own company and looking for a production\nbottleneck when there was so much excess capacity and the constraint"}
{"372": "application of it. He slowly forced me to think-sometimes by yelling\nat me, \"Think!\" It was a hefty morning. And this story is described by\nhim in It's Not Luck\u2014the candy wrappers case. We finally made some\nmoney over there. Actually, a lot of money. Later I discovered that\nmy nephew, who was the other 50% owner of the company, wasn't\ndoing much and was taking out more money than we had agreed\nupon, so we decided to split the company in two. I did the split and\nhe chose which part he wanted. I never imagined that he would keep\nthe printing business, which I had been running, and leave me with\nthe office supply business, which had been his responsibility.\nDW: Did you know anything about the office supply?\nPH: No, nothing at all. The company was pretty big, it was number\nfour or five in the Netherlands. It was making an awful loss. Com-\npetition was suddenly fierce and only concentrated on price. Other\ncompanies were very subtly sending brochures to every small business\nin the Netherlands with prices on the front cover that I couldn't get\nfor myself as a wholesaler. This was really awful. All our good custom-\ners became suddenly more and more interested in price. They said,\n\"How is it possible that we pay twice as much as what's on the front\ncover of this brochure?\"\nDW: It sounds like an impossible situation.\nPH: Well, it was, it was really awful. We had something like four or\nfive thousand customers, 20 sales people. The only thing we could\nthink of was to also lower prices, and do it only on items where we\nhad to. That was not a long-term solution but that was what every-\nbody else was doing. So the conventional way of doing business in\noffice supplies was pretty soon completely gone. We got tenders for\noffice supplies-which was unheard of\u2014where you had to fight with"}
{"373": "company but I made it about the customers' situation: Why is this\ncustomer complaining so heavily about price? After long thought and\na lot of discussions with my sales people, the only thing we could come\nup with is that he's thinking this is the only way that he can decrease\nthe total cost of office supplies; that he can't do anything about the\ntremendous cost of having to stock supplies, and store them, and the\ncost of bringing the stuff to the right people in the building. Well, I\nknow what kind of a mess customers can make out of it. In most of-\nfices where you open drawers, there's more stock in the office than\nanybody can imagine. While at the same time they are screaming for\na specific item which has to be brought to them by taxi in crazy short\ndelivery times. In Rotterdam we are down to four-hour delivery times!\nNot even 24, just four-hour delivery times, which is completely crazy\nfor office supplies. I mean, we're not saving lives here.\nSo this is what we offered our customers: That we would take over all\nthis hassle of supplying everybody in the office with the right equip-\nment, the right articles, at the right time. We offered them cabinets\nwith office supplies in them. We owned both the cabinets and the\ncontents. The supplies were for a specific working group. Whatever\nthey took out was considered sold, whatever was left was still ours. We\nreplenished these cabinets every week. We made it very easy for them\nto check on us. And more importantly, we could give specific data\nabout each department, explaining that certain items were consumed\nfast. For instance you might need a new pair of scissors once in three\nmonths, but not every week.\nDW: So you could discover theft?\nPH: Well, we didn't call it theft, we called it overconsumption. But\nof course it was theft, yes. So suddenly this guy who was responsible"}
{"374": "the actual articles, the total cost of providing office supplies for their\nworkers dropped by 50% because they didn't have the internal hassle\nof misplacements, overstocking, and things like that. So they didn't\ncare that much anymore about the original price we charged. When\nI sold my company a couple of years ago, the due diligence took a\nlong time because they couldn't believe our added value.\nDW: What were the numbers?\nPH: Normal gross margins in the industry were very much below\n20%. Above 20% was suspicious. We were above 30%, which makes\na lot of difference. And we were not ripping people off. They were\nextremely satisfied with our service.\nDW: How did you go about selling the concept to your\ncustomers?\nPH: We had a department which was making appointments with\nfinancial directors, not the guy normally responsible for purchasing\noffice supplies. That other guy was scared for his job when you came\nwith this solution. And we made a short movie to show the current\nsituation in their office and how people were screaming for office\nsupplies and things like that, and how great it would be if we could\ntake over their stock and their responsibility and solve this problem.\nAnd this worked really great. Something like 30% of the sales visits\nwere successful sales. Again, the prices we were charging for supplies\nwas no longer an issue\nDW: For anyone?\nPH: Not exactly. We still had some customers who were focused on\nprice. We didn't chase them away. We just gave them completely\ndifferent conditions. We told them that if price is what matters most,\nyou have to buy big quantities and you shouldn't care about delivery"}
{"375": "start buying on price. And I could combine my orders with those of\nthe bigger customers who still wanted to do business just on price.\nDW: Those must have been a very satisfying couple of years for\nyou as you explored this new way of doing business.\nPH: Well, yes, for a couple of years it's really fun. Because you're\nwinning a race. Of course at the beginning I was relatively small; I\nwas number four or five in the country. I was really afraid the bigger\ncompanies would copy my cabinet system.\nDW: Did they?\nPH: Yes, a little bit. But they didn't get the message. It was actually\nreally funny. They were prepared to deliver cabinets but the customer\nhad to buy the cabinet and the content as well. They were never willing\nto do it on consignment terms, which is what made it work. So that\nwas a big difference to start with. Secondly, they didn't understand\nmy replenishing system of stuffing the cabinets full enough that you\ncould survive a couple of weeks. What they offered was so different\nthat we could immediately show the customer that with our competi-\ntors, you'll still have to do it yourself, you'll have to take responsibility.\nWhereas in my case, when you change a printer, for example, and\nyou don't tell me, I will find out you don't use this cartridge any more\nand I'll adjust. These cartridges are very expensive, do you want the\nresponsibility? That's the main difference of consignment.\nDW: Later were you able to discover new constraints that\nopened the way to new growth?\nPH: Ultimately the constraint moved back inside the company. The\nnew constraint became; how quickly can we measure or install a new\ncabinet? At first we could only do something like two or three cabi-\nnets a day. People were standing in line for cabinets. We had waiting"}
{"376": "come arrogant. And we had the same situation! It was great! And to\nthink that we had started with all those competitors, all the problems,\nand 20 sales guys who were really discouraged, they didn't know what\nto do. And here we came with this really simple solution. I'm amazed\nthat to this day nobody's really copying it.\nDW: Would you have discovered this breakthrough had you\nnot been exposed to Goldratt's theories?\nPH: First of all, I wouldn't have known how to attack the problem\nSince I was working at the printing company and my nephew was\nworking at the office supplies company, I never expected that we\nwould\nchange roles. Nevertheless, I knew how much loss they made. And by\nthen I was so convinced that just by applying Theory of Constraints,\nI would figure out a way to solve the problem. It took me something\nlike three or four weeks to see the light and understand what was go-\ning on and how to solve it. I survived that month by sitting back and\nsaying, \"Okay, no panic, no panic, let's not be hasty. As long as we\ndon't have a breakthrough idea I'm not going to make any changes \"\nI was just sitting back and thinking and discussing with people how\nwe could solve the problem, until we solved it. And that's one of the\ngood things about theory of constraints. You know in these cases that\neventually you will come up with a breakthrough idea.\nDW: You have only to find it\nPH: Yes, and I became better and better at it. It takes Eli about five\nminutes to find the constraint and how to brake it. In most cases, I can\nfind the same within a week. Compare it to just doing more of\nthe'same.\nI very often use this funny story about two guys on a safari. And after"}
{"377": "Interview with Eli Goldratt continued...\nDW: Can you give me another example? Of a service company\nthat does not deal with physical products?\nEG: To demonstrate how different one type of service company is\nfrom another, I suggest you interview both a bank and a financial\nadvisors company. Then interview another, obviously different, type\nof service industry, a hospital\nInterview with Richard Putz, A Midwest Bank\nFormer CEO of Security Federal Bank.\nDW: How did you conceive of applying the principles outlined\nin The Goal to the banking industry?\nRP: I was flying back from Los Angeles one night. And I was re-\nmembering my days as a consultant at Coopers & Lybrand, working\nwith the folks who were handling the manufacturing engagements.\nThat's where I was first exposed to The Goal. And I began to think that\nwhen you look at how a bank operates\u2014for example, how it moves\nthrough the process of putting loans together\u2014it's really no different\nthan manufacturing. Why couldn't I use something that worked in\nmanufacturing and apply it to a bank? The process is the same, we\njust give it different labels. So I started testing that out.\nDW: How did that go over with the staff?\nRP: In the beginning they were skeptical. I got all of the people who\nreport directly to me into the board room, we sat down, I passed out"}
{"378": "DW: So how did you approach the problem?\nRP: Traditionally the tough issue within banking is how you manage\nall the regulatory constraints that you're faced with. Banks are just\nimmersed in regulations. And if you actually tried to manage accord-\ning to the regulatory measurements, your bank would fail. You bring\nthat up to the regulators and they laugh. There's just this whole slew\nof things, some of which contradict themselves. Some of them were\ncreated when lawmakers added them onto banking legislation because\nthey looked good, or else to fit a particular situation at the time.\nDW: You're talking about regulations that keep banks out of\ncertain businesses?\nRP: Right, as well as those that mandate certain loan mixes, how you\napproach a market, that type of thing.\nDW: Preservation of asset ratios and so forth?\nRP: You got it. We took a slightly different approach. We decided we\nhad to figure out what our real market constraint was. Using TOC,\nwe found it had to do with service levels and how we were solving\nproblems for our customers, not with the specific products we were\noffering. So we ended up gearing the whole bank toward solving prob-\nlems for our customers. Part of the solution\u2014the injection that broke\nthe conflict\u2014was the creation of personal banking for everybody, not\njust for wealthy people. Banks normally assume it's not worth spend-\ning time with you if you have only $100,000 when they can spend\nthat time with a guy who's got $10 million. We discovered that a guy\nwho only has $ 100,000 isn't really going to spend a lot of time with\nyou anyway; he's just not there very often. So we stopped worrying\nabout that and began focusing on how to better manage our customer\nrelationships across the board. People ended up coming to our bank-"}
{"379": "RP: Right. We had more than 300 correspondent banks, all over the\ncountry. National City and Bank of America would sell us mortgages.\nWhat we discovered-also using TOC, and this is how we expanded\nthis business\u2014is that most people with a loan viewed the bank that\nserviced the loan as their bank. So, whether Freddie Mac or Fannie\nMae or PNC or any other investor actually owned the loan, we wanted\nto own the servicing asset. It was more valuable in terms of building\ncustomer relationships than the loan itself.\nAlso, these days it's a lot easier, but it used to take forever to get a\nmortgage approved. That's because there are all these things you\nhave to have in place\u2014again, to satisfy the regulators. We looked at\nthat and said, \"Okay, what's the conflict here?\" We built our conflict\nclouds, and we built a current reality tree, and we discovered there\nare only three things that end up deciding whether a loan is a go or\na no-go. If we just focus on doing those three items, and worry about\nplugging everything else into the file later, we can speed things up. In\nfact we were able to cut the approval time almost in half. That made\nus really popular with realtors and mortgage brokers, which brought\nus more business.\nDW: What effect did TOC have on customers' ordinary day-to-\nday interactions with tellers?\nRP: Most of the tellers said they wanted to do this TOC thing, too.\nWell, what do they really need to do? They really don't need to know\nhow to do future reality trees because their everyday life is not involved\nin future reality trees. But a teller is often dealing with conflict resolu-\ntion. Tellers represent the frontline defense, especially at savings and\nloans. People come up to them and say: \"This doesn't work, this is\nout of balance, they screwed this up,\" and it's the tellers who have to"}
{"380": "was that the perceived constraint-the regulatory climate-was\nnot the actual constraint\nRP: Correct. I would walk into the office of my compliance officer\nand I'd say, 'Jeff, I got this idea.\" And he would just automatically\npoint to this poster on his wall that basically said: If you can dream\nit, there's a regulation for it.\nDW: And yet even in that environment, you found ways to\ngrow.\nRP: We did things in the banking industry that were totally unheard\nof. We actually had regulators visit us more often than other banks\nbecause those other banks kept calling them and saying: \"They've got\nto be doing something illegal, you need to check them out.\"\nInterview with David Harrison, Administrative Ser-\nvices, Founder, Positive Solutions, Newcastle, U.K.\nDW: Tell me about Positive Solutions.\nDH: We provide management and administrative services to inde-\npendent financial advisors. At present we have 755 of those people\nwho rely upon us to help them with such things as compliance with\nfinancial services regulations, collection of commissions, and so forth.\nThat's the company we built, 60% of which we sold recently to the\nAegon group, one of the world's largest insurers.\nDW: How have you made use of The Goal?\nDH: In a couple of ways. First and foremost we use the five focus-\ning steps almost instinctively now, in that we seek to identify the\nconstraint in any problem before we do anything else. That's sort of\nbeen my mantra, if you like\u2014before we go any farther, let's identify"}
{"381": "RP: In a couple of ways. First and foremost we use the five focussing\nsteps almost instinctively now, in that we seek to identify the con-\nstraint in any problem before we do anything else. That's sort of been\nmy mantra, if you like\u2014before we go any farther, let's identify the\nconstraint.\nBeyond that, a big part of what we do is acquire new independent\nfinancial advisors\u2014we want people to join our organization, and the\npeople we use to recruit them we call our business consultants. Oded\nCohen, of Goldratt UK, helped us build a process for that. He broke\nit down into very discrete steps and helped us program software which\nhelps us track how each of our business consultants is succeeding, or\nnot. At any point in time they may have 150-200 people they're hav-\ning conversations with about joining Positive Solutions. We've got\nthem to think of each of those people as a project. That streamlined\nthe process and also got our business consultants to think in a more\nlogical fashion.\nDW: What distinguishes Theory of Constraints from other man-\nagement techniques you've looked at?\nRP: I think it can be very easily applied in a simple process. As I have\nsaid, the one I use more than anything else is the five focussing steps.\nA lot of the problems which arise in business are about lacking focus.\nI guess if people were to describe Positive Solutions, it would be as a\nvery focussed organization. We don't seek to be all things to all people.\nWe stick to what we know will be the most profitable areas to us at\nany point in time. We've been working on the same constraint for\nfive years.\nDW: And that is?\nRP: Our ability to recruit the right people at a pace which fits our"}
{"382": "are profitable, then why stop recruiting them? Just because it's not\ngetting any easier? Well, it's not actually getting any harder, either.\nIt's just another day at the office. But we can work all of our financials\nback to simply the number of advisors that we have. Therefore, we\ndon't go any farther.\nDW: That's your focus?\nRP: That's our focus. We've identified the constraint, now let's ex-\nploit it, make the most of it. Therefore we have easily one of the best\nrecruiting machines in the UK in this sector. We approach recruit-\nment very differently from all our competitors. Our competitors will\nadvertise, they'll try to acquire businesses, for example, rather than\nthe approach that we have, which is to recruit people one by one.\nOur rate of growth might at first appear to be slow. But because our\nadvisors have been recruited in the right way, we don't lose many of\nthem. That's the beauty of TOC: As you really dig in to identify the\nconstraints, you begin to understand these things.\nDW: Have you thought about what the next constraint will be?\nRP: Of course, at present there is still a market for further indepen-\ndent financial advisors to join us. There are about 25,000 of these\npeople in the UK and we have less than 1000 of them. Now the qual-\nity of some of those 25,000, and the fact that not everybody will join\nus in any case, means at some point the effort needed to increase the\ncapacity just won't be worth it versus the energy we could put into\nsomething else. At that point, you say, \"We've now changed our plan.\nWhat is the constraint in our new plan?\" Frankly, it's about retaining\nthe clients' money. At present what we do is introduce clients to a\nvariety of manufacturers of financial services. The money goes to the\nmanufacturers and they give some of it back to us in the form of"}
{"383": "Interview with Dr. Antoine Van Gelder\nA South African Hospital\nUniversity of Pretoria\nDW: You're not a typical Eli Goldratt disciple, are you?\nAV: I'm a university professor with a dual appointment, head of the\ndepartment of internal medicine at the University of Pretoria and\nhead of the department of internal medicine at Pretoria Academic\nHospital. In 1992 I got an invitation to attend one of Eli Goldratt's\ncourses in Pretoria. Not one run by him himself but by a subsidiary of\nthe Goldratt Institute. At that time I knew nothing about theory of\nconstraints and I had not read The Goal. I got myself into this out of\ncuriosity more than anything else.\nDW: Why? What kind of help were you looking for?\nAV: Let me put it this way. I was literally sitting in my office, with mv\nhead in my hands, highly frustrated, with piles of paper all around\nme, going through correspondence. I opened a letter, saw that it was\nanother invitation to a course, threw it away, and as I threw it in mv\nwastepaper basket my eye caught the price of this particular course.\nIt was the South African equivalent of about $18,000. That caught mv\nattention. I thought if any course was worth that amount it was worth\nlooking at. This was a two week course in production management,\nthe invitation was addressed to the engineering faculty. It had gotten\nto the medical faculty by mistake. The course was actually offered\nfree to university professors. So because of my deep frustration with\nsome of the management issues I had in my department, and because\nI had some time off the next week, I phoned. I planned to only go for"}
{"384": "Quite a lot of that is in It's Not Luck, which was published later. But\nthe logic grabbed me because I was this frustrated man who was run-\nning a department of medicine and I had not been trained to do that.\nI had no insight into management issues. Suddenly I saw that here\nwas a potential way of analyzing my department.\nDW: What were the parallels?\nAV: My department was in chaos, total chaos. Everything coming\nand going, not knowing what was what\u2014much as things were in the\nfactory that is the setting of The Goal. During the course, The Goalwas\nmentioned. I bought it, read it through in one night, and I thought to\nmyself, that's my environment. A chaotic system is not necessarily a\nfactory. It could be a hospital with people coming and going. It could\nbe a department with a whole lot of prima donnas-the doctors\u2014that\nneed to be managed. That parallel struck me.\nNow if I can answer your question a bit more precisely. When one is\nintroduced to theory of constraints, the first thing you see is a system\nwhere the causality is hidden. In other words, it's chaotic. Things\nhappen, you have no control. Suddenly, though, it becomes a system\nthat can be analyzed in terms of certain key points\u2014leverage points.\nAnd one learns that addressing these key points\u2014rather than launch-\ning a symptomatic firefight\u2014is the way to exert control over these\nsystems. Remember, this was in the early 1990s, before frameworks\nlike systems theory had moved to the forefront and become part of\nthe main buzz. Though the theory of constraints doesn't talk about\nsystems theory, already it was offering an approach by which a com-\nplex system could be managed in terms of a few key leverage points.\nDW: Did you wind up attending both weeks of the course?\nAV: Correct. Then I came back to the hospital. There are two points"}
{"385": "Second, our outpatient clinic, like most hospital outpatient clinics at\nthat time, and even now in many parts of the world, was plagued by\ninefficiencies and long waiting lists. The more we fought the ineffi-\nciencies, the more money we poured into the system, the longer the\nwaiting lists seemed to become. This is the problem with the national\nhealth system in Britain as we speak. Now in my department, it seemed\nto me as though the processing of patients by doctors could really be\nviewed as a production line, just as in The Goal. The times are differ-\nent, and obviously people aren't machines. All of those issues I ac-\nknowledged. But I saw that parallel.\nDW: How did you attack the problem?\nAV: The manager in charge of that clinic and I sat down and I told\nher about the principles used in The Goal Between the two of us-with\nher doing most of the work\u2014we identified our constraint. We realized\nthat we lost a tremendous amount of capacity whenever patients or\ndoctors wouldn't show up for scheduled appointments. That time lost\nwas not recoverable. So we developed a call-in list, which we called\nthe patient buffer. A day or two before a scheduled appointment we\nwould phone patients and make sure that they would be coming into\nthe clinic. If not, we would find substitute patients. The result was less\nloss of capacity. Our waiting list at that time was about eight or nine\nmonths long, which is common for this type of waiting list. As a mat-\nter of fact in the UK now some of these waiting lists are over one year.\nIn about a six month period we got our waiting list below four months,\nwhich was roughly half of what most other hospitals were doing in\nSouth Africa at that time.\nDW: Yours is a public hospital?\nAV: Yes, we're part of the state health system. In other words, not for"}
{"386": "question people should be asking is, what can I do to achieve the\nlarger goal of the hospital, which is to throughput new patients? It's a\nsimple concept but implementing it took about two months of meet-\ning with staff. Each person then developed an action plan aimed at\nmaking sure more patients moved through the system more efficiently.\nIn a period of a year, this hospital moved from a 20% shortfall on its\nbudget to where it began showing a profit.\nDW: So you've become a Goldratt consultant yourself?\nAV: Yes. I presented the results from our hospital's outpatient clinic\nat one of the Goldratt symposia in the early 1990s. This was the first\nreport of a medical implementation of the theory of constraints. Eli\nGoldratt was there to hear my presentation, and afterwards he in-\nvited me to join the Goldratt Institute as an academic associate. I was\nbased at the university but involved in the implementations of his\nconsulting company. I did quite a bit of work in the mining industry-\nnothing to do with medicine! It was pure theory of constraints, straight\nout of the book. It allowed me to develop my own skills.\nDW: What's a doctor doing advising mining companies?\nAV: It's interesting that you say that. I'm a physician, not a surgeon,\nIn other words I'm a thinker, not a doer. I say that facetiously but as\na physician, it's all about diagnosis. And the whole process of diagno-\nsis, whether it's a patient or an organization, is the application of the\nscientific method. Eli Goldratt says that his theory of constraints is\nsimply the application of the scientific method. So it's almost natural\nthat an advisor to a mining company\u2014in terms of diagnosing what's\nwrong and what to do about it-could be a physician. In fact some of\nthe teaching materials that the Goldratt Institute uses refer to the\nmedical model. It asks trainee consultants, How does a doctor ap-"}
{"387": "theory of constraints is about thinking processes, it's a subset of logic.\nIn other words, the scientific method.\nDW: Has any of this made you a better teacher of physicians?\nAV: Absolutely. Absolutely. I've told you that diagnosing a patient\nand diagnosing a business is the same thing. But a doctor learns to\ndiagnose by watching other doctors. It's not taught as a science. The\nprocesses of diagnosis are taught but what might be called the phi-\nlosophy of diagnosis is not taught as it is in the theory of constraints.\nThe traditional approach is, watch what I do. The approach that I've\nsince followed is, let's look at how the scientific method works, then\nlet's see if we can apply this to a patient. Most students take to this\nvery well.\nInterview with Eli Goldratt continued...\nDW: That will do it\nEG: Please, one more. The jewel in the crown, at least in my eyes, is\nthe usage of TOC in education. Yes, in kindergartens and elementary\nschools. Don't you agree that there is no need to wait until we are\nadults to learn how to effectively insert some common sense into our\nsurrounding?\nInterview with Kathy Suerken, CEO\nTOC For Education,\nAn international nonprofit dedicated to teaching TOC think-\ning processes to schoolchildren."}
{"388": "now? Go to a different school?\" And he said, \"Kathy, you'll have to\nfind another goal.\" Six months later he said, \"There's a book you\nhave to read, we're passing it around at our office and everyone's\nsigning the back if they recommend it.\" That was my introduction to\nThe Goal. Within six months, I wrote a letter to Eli Goldratt that be-\ngan, \"Dear Dr. Goldratt, if you were to walk into the office of Frank\nFuller, Ruckle Middle School's principal, on his desk you would find\na copy of The Goal.. . and thereby hangs a tale.\" I went on to say how\nI was using the ideas and concepts to run this project.\nDW: Did you hear back from Eli?\nKS: Within four days, with a copy of his newly revised book. And\nthen within about a week or so I heard from Bob Fox, who was presi-\ndent of the Goldratt Institute at that time, and they offered to send me\nto Jonah school on scholarship. So I went through the course. Later I\nwent through a facilitator program on how to become a trainer of\nJonah processes. And then I went back and taught a pilot course to\nkids. By the end of the year my kids were using the thinking pro-\ncesses, which they learned brilliantly. They were the most Socratic\nlearners and teachers of other kids that you ever saw. It was pretty\nconvincing evidence to me that this stuff works with kids, and it\nlaunched me into the role I have now.\nDW: Was it a course about TOC or a course that used TOC\nmethods to teach other content?\nKS: It was a class on world cultures\u2014basically a class on perspectives,\nwhich of course this is so aligned with. We used methods derived\nfrom TOC to advance the curriculum. Later I taught a critical think-\ning skills course that was pure TOC. In that course I was teaching\ncause and effect as a skill. We used concepts like the conflict cloud to"}
{"389": "weakest link?\" Stuff like that. It wasn't a test. I just wanted to know if\nthey were getting it. That night I looked at their answers and I real-\nized maybe half of them got it and half of them didn't. So I went back\nthe next day and I asked them again, \"What determines the strength\nof the chain?\" I called on one boy\u2014let's say his name was Mike-who\nI knew was struggling. He was rambling on and on. He did not get it.\nAnd I did not know what to ask Mike to get the answer out of him. So\nthen I looked at my other students. And I knew if I called on John, for\nexample, who did get it, he would just tell Mike the answer, and that's\nnot what I wanted. So I said, \"No one can give Mike the answer. You\ncan ask Mike a question to help him think of the answer.\" And that is\nwhen one of my other students raised her hand. She said, \"Remem-\nber when we were doing the cloud on teach fast, teach slow? The\nproblem of making sure everyone understands but the fast ones don't\nget bored?\" That's when I saw what was happening. As the other\nstudents began asking Mike questions designed to draw the answer\nout of him, I could see that everyone was engaged. It was a wonderful\nexample of cooperative learning. Because everyone had to think. Even\nif they already knew the answer, they were thinking hard about how\nto guide others to the answer.\nDW: How do you introduce TOC to schools where it has never\nbeen taught before?\nKS: We usually start with teaching TOC as a generic process, then\nfigure out how to apply it to a specific curriculum. Initially it was\neasier to get it in through the counseling element of the school-the\nbehavior application. That seemed to be the most obvious way in.\nDW: How do counselors use TOC?\nKS: Let's say the child is sent in to the guidance office with a behav-"}
{"390": "other branch, the positive one. Then the counselor asks, \"Okay, which\nwould you prefer? It's up to you.\"\nOne of the first teachers that was using this in a classroom in Califor-\nnia was working with at-risk students. They were at risk of failing\nacademically and behaviorally. She was teaching the process outright,\nas a skill. And she had her students do cause and effect branches.\nOne boy did it on, \"I'm going to steal a car, go on a joy ride.\" She\nwent to help him, because he couldn't get the branch started. \"She\nsaid, \"What's the problem?\" He said, \"This is the first time I've ever\nthought of something ahead of time.\" In the end he had to go to the\ndriver education teacher and get some information to finish the branch,\nwhich is great. He found out what would happen to him if he got\ncaught, because he didn't really know. How do you quantify the re-\nsults of something like that?\nDW: You've since developed other applications?\nKS: Yes, and they're interconnected. Because behavior changes atti-\ntudes. Or maybe I should say that attitudes impact behavior. If a stu-\ndent can make a more responsible decision, and he gets a favorable\nimpact, his attitude toward the teacher and what he's doing in school\nchanges. That's bound to have some impact on his learning. But ad-\nditionally, we have, in the past two years, really worked on how to\ndeliver the TOC learning process through curriculum content. Or,\nagain, maybe it's the other way around: How to teach content using\nthe TOC processes. Because teachers do not want to interrupt class to\nteach a life skill. They have to teach the curriculum.\nDW: I understand you've introduced TOC to young people in\nprison settings.\nKS: I went into a juvenile jail in California about five years ago. I"}
{"391": "them to tell me what they wanted out of life. They said things like.\n\"We just want to get out of here, lady.\" I said, \"Do you think that's\nenough to keep you out of here?\"\nFinally, one boy said to me, \"I just want a better life for my kids.\"\nThese were 16 to 19-year-old old black and Hispanic males. I looked\nat this guy and I said, \"I'm sorry I don't understand, what do you\nmean? You have kids?\" He said, \"Yes, I have a two-year-old and a\nbaby.\"\nAnyway we had this goal on this rickety old chalkboard, \"A better\nlife.\" I said, \"Okay, what is preventing you from having a better life?\"\nThey said, 'Jealous people.\" I turned around and I said again, \"I'm\nsorry, I don't understand what you mean by jealous.\" Because I'm\nthinking to myself, and not facetiously, \"who could be jealous of them,\nthey're in jail?\" And that's when they said, \"Oh, but if you go back\nand try to get out of the gang they'll be jealous, they don't want you\nto leave the gang, you can't leave,\" and all this.\nThey also mentioned prejudice as an obstacle. And as I'm making\nthis list I am thinking, \"I am in over my head.\" There was nothing I\ncould think of that would overcome the obstacles these kids were\nfacing. But I didn't need to worry about it. Because they had the\nanswer. They went down the list and they added more obstacles\nlike, \"my past,\" and \"criticism,\" and about halfway through they\ngave me something brilliant: \"Me. Myself. I have to change my-\nself. Right away.\"\nI later received letters from some of those kids. One of them said,\n\"Before we had that talk, even making it to 21 was hard to see in my\nfuture. But you gave me hope.\" Now I ask you, did I give him the\nhope? No! It came from him! But he wrote, \"You gave me hope that"}
{"392": "Absolutely. What it helps people do is to make sense of things. Many\ntimes, even in affluent communities, students are motivated only be-\ncause their parents want them to achieve. But learning does not make\nsense to them. It doesn't seem relevant. They're doing it only be-\ncause they have all the right environmental factors. What could be\nunleashed from those children if we could present information to them\nin such a way that they could derive their own answers instead of\nproviding answers that were simply memorized? It's all about un-\nleashing people's potential. I have felt many times as a teacher that\ndisruptive behavior comes from the high achievers as well as the low\nachievers-because the high achievers are bored! In TOC we have a\nway to differentiate instruction with one learning process. To bring\nthem all with you.\nDW: What is your goal for TOC For Education?\nKS: I see empowered learners, enabled learners, and the real joy of\nlifelong discovery. All those platitudes that we aspire to, I see them\nbeing practically achieved. As well as people being kinder to each\nother. I see this as the real language of civility. Once I had to give a\npresentation about TOC to a group of teachers. We put on a play\nwith some of my students. And afterwards the students were saying,\n\"Mrs. Suerken, what's going to happen? This is so effective, there\nwon't be any problems left.\" I thought, that will probably never hap-\npen! But that's the way they saw it. I wish you could come to our\nconference in Serbia in May! We're going into Thailand this month\nthrough an organization called the Girl's Brigade, like the Girl Scouts.\nWe have somebody in Singapore that's taking it into the sports coun-\ncil, into sports applications. We're in Malaysia. My new director in\nthe United States, he's going to start a private school next fall and"}
{"393": "For information about other books on the\nTheory of Constraints (TOC)\nplease visit our web site at:\nwww.northriverpress.com"}
{"45": "SOME PROBLEMS YOU JUST CAN\u2019T SOLVE . . .\nSOLVE THEM ANYWAY\nEventually, you will run into a brick wall that is tougher than\nyour head. Don\u2019t keep pounding; it has no effect on the wall\nand does your head no good.\nMy unofficial mentor at McKinsey asked me to work with him on\nwhat promised to be a fun and exciting study. The client, a major\nfinancial institution in the midst of reorganizing its investment\nmanagement business, faced severe challenges of heroic dimen-\nsions: thousands of employees, billions of dollars. The McKinsey\nteam included my mentor and my favorite EM. It seemed the per-\nfect recipe for an enjoyable, challenging McKinsey engagement.\nThe recipe might have been right, but the result left a bad taste\nin our mouths. Factions within the client\u2019s senior management pre-\nvented us from doing our job. Data we asked for arrived late, or\nin an unusable form or not at all. People we needed to interview\nrefused to speak with us. The members of the client team vigor-\nously pursued their own agendas at the expense of reaching a solu-\ntion. We spent several uncomfortable months on this study and,\nin the end, had to make what recommendations we could, \u201cdeclare\nvictory,\u201d and get out.\nMy team\u2019s experience was hardly unique in the history of the\nFirm. The road of problem solving is often strewn with obstacles.\nData to prove your hypothesis may be missing or bad. Sometimes\nbusinesses realize too late that they have a problem; by the time\nMcKinsey, or anyone, addresses the problem, the business is"}
{"46": "The first thing to understand about politics\u2014and how it can help\nor hinder you from doing your job\u2014is that businesses are full of\nreal people. When you look at the boxes on an organization chart,\nyou are really looking at people. When you move those boxes\naround, you change someone\u2019s life. As one former McKinsey EM\nremarked, \u201cSometimes change management means just that\u2014\nchanging management.\u201d\nWhen members of a McKinsey team go into a client, they\ncarry change with them. Some at the client will welcome the\nbringers of change as white knights riding to the rescue; others\nwill see McKinsey as an invading army to either flee or drive out,\ndepending on their power in the organization. As one former\nMcKinsey-ite put it, \u201cIt was a rare engagement when there\nwasn\u2019t at least one sector of the client organization that did not\nwant us there and did not want us to come up with a real answer.\u201d\nIn most cases, when senior management brings in McKinsey,\nenough players in the organization will cooperate willingly and\nMcKinsey can be effective. A few malcontents may grumble or\neven cause trouble, but in the end they will be either converted to\nMcKinsey\u2019s cause or bypassed. Sometimes, however, one power-\nful faction in an organization calls in McKinsey against the wishes\nof another powerful faction. That\u2019s when trouble arises, as we\nfound out.\nYou have several options to pick from when confronted by a\nproblem that seems too difficult to solve.\nRedefine the problem. You can tell your client that the prob-\nlem is not X, it\u2019s Y. This is especially useful when you know that\nsolving Y will add a lot of value, where as trying to wrestle with\nX would cost a lot of time and resources for little result. If you\nmake this switch very early on, you show great business judgment;"}
{"47": "Tweak your way to a solution. Sometimes you will come up\nwith a great solution that you know the client organization can-\nnot implement. This is especially true with organizational\nchange\u2014it is easy to devise an optimal organization, but you usu-\nally have to deal with the personnel resources that the client\nalready has. When that happens to you, expand your time horizon.\nDon\u2019t worry about implementing your solution immediately. As\npeople leave the organization, you can \u201ctweak\u201d your way to your\noptimum over time.\nWork through the politics. Even political problems are solu-\nble. Most people in business are rational, at least in their business\nconduct. They react to incentives. Therefore, when you face polit-\nical opposition, it usually means that your solution has negative\nimplications for someone in the organization. So politics is just\npeople acting in their own interests.\nTo work through the politics, you must think about how your\nsolution affects the players in an organization. You must then build\na consensus for change that takes account of the different incen-\ntives and organizational factors driving the politics. Consensus\nbuilding may require you to change your solution to make it\nacceptable. Do it. Remember that politics is the art of the possi-\nble, and it\u2019s no good devising the ideal solution if the client refuses\nto accept it."}
{"48": "3\n80/20 AND OTHER RULES\nTO LIVE BY\nThis chapter contains a number of rules that McKinsey con-\nsultants have found useful when trying to solve problems.\nThey are difficult to classify. Call them my \u201cOther Issues.\u201d"}
{"49": "80/20\nThe 80/20 rule is one of the great truths of management con-\nsulting and, by extension, of business. You will see it wher-\never you look: 80 percent of your sales will come from 20\npercent of your sales force; 20 percent of a secretary\u2019s job will\ntake up 80 percent of her time; 20 percent of the population\ncontrols 80 percent of the wealth. It doesn\u2019t always work\n(sometimes the bread falls butter-side up), but if you keep\nyour eyes peeled for examples of 80/20 in your business, you\nwill come up with ways to improve it.\nI saw the 80/20 rule at work all the time at McKinsey, and I\u2019ve\nalways been impressed by its power as a problem-solving rule\nof thumb.\nIn my first-ever McKinsey study, when I was between years at\nbusiness school, I joined a team working with a large New York\nbrokerage house. The board of directors wanted McKinsey to\nshow them how to improve the profitability of their institutional\nequity brokerage business\u2014the selling of stocks to large pension\nfunds and mutual funds like Fidelity and T. Rowe Price.\nWhen a client asks the question \u201cHow do I boost my profits?\u201d\nthe first thing McKinsey does is take a step back and ask the ques-\ntion \u201cWhere do your profits come from?\u201d The answer to this is not\nalways obvious, even to people who have been in their particular\nbusiness for years. To answer this question for our client, our team\nwent through every account of every broker and every trader by\ncustomer. We spent several weeks analyzing this mountain of data"}
{"50": "\u2022 80 percent of the sales came from 20 percent of the\nbrokers.\n\u2022 80 percent of the orders came from 20 percent of the\ncustomers.\n\u2022 80 percent of the trading profit came from 20 percent\nof the traders.\nThese results pointed to some serious problems in the way the\nclient allocated its staff resources, and we focused on those like a\nlaser. Once we started digging, we found that the situation was\nmore complex than simply \u201c80 percent of the sales staff is lazy or\nincompetent\u201d (not that we ever thought that was the case to begin\nwith). We discovered, to give one example, that our client\u2019s three\ntop brokers handled the 10 biggest accounts. By sharing these big\naccounts out among more brokers, and by dedicating one senior\nand one junior broker to each of the three largest customers, we\nactually increased total sales from these accounts. Rather than\ndivide up the pie more fairly, we increased the size of the pie. Thus,\n80/20 gave us a jump-start in solving the client\u2019s problem.\n80/20 is all about data. What are your sales figures by product?\nWhat is your margin by product? How does each member of your\nsales team perform in terms of sales? In terms of profits? What is\nthe success rate of your research teams? What is the geographical\ndistribution of your customers? If you know your business well\n(and you\u2019d better if you want to survive), then you know the right\nquestions to ask. When you have your data, put it on a spreadsheet\nor in a database. Sort it in various ways. Play with the numbers.\nYou will begin to see patterns, clumps that stand out. Those pat-\nterns will highlight aspects of your business that you probably did\nnot realize. They may mean problems (a big problem if 80 percent\nof your profits come from 20 percent of your product lines), but"}
{"51": "DON\u2019T BOIL THE OCEAN\nWork smarter, not harder. There\u2019s a lot of data out there\nrelating to your problem, and a lot of analyses you could do.\nIgnore most of them.\nMcKinsey gathers enough facts to prove or disprove a hypothe-\nsis or support or refute an analysis\u2014and only enough facts.\nThis is the flip side of fact-based analysis in a business situa-\ntion. Anything more is a waste of time and effort when both are\nprecious commodities.\nI had this lesson brought home to me late one night while I was\ndrafting a \u201cfact pack\u201d on a client\u2019s competitor. I had gathered a\nmountain of data and was trying to wring out a few new insights\nfrom it. My EM, Vik, walked into my office, briefcase and coat in\nhand, and asked how my work was going. I told him it was going\nwell, but I thought I could pull together a few more charts. He\npicked up my draft, leafed through it, and said, \u201cEthan, it\u2019s eleven\no\u2019clock. The client will love this. No one will be able to absorb\nmore than you have here. Call it a day. Don\u2019t boil the ocean.\u201d We\nshared a cab home.\n\u201cDon\u2019t boil the ocean\u201d means don\u2019t try to analyze everything.\nBe selective; figure out the priorities of what you are doing. Know\nwhen you have done enough, then stop. Otherwise, you will spend\na lot of time and effort for very little return, like boiling the ocean\nto get a handful of salt."}
{"52": "FIND THE KEY DRIVERS\nMany factors affect your business. Focus on the most impor-\ntant ones\u2014the key drivers.\nIn any McKinsey team meeting where problem solving is on the\nagenda, someone will use the inelegant phrase \u201ckey drivers,\u201d as\nin, \u201cVik, I think these are the key drivers of this issue.\u201d In other\nwords, there may be a 100 different factors affecting the sales of\nour widgets\u2014weather, consumer confidence, raw material prices\u2014\nbut the three most important ones are X, Y, and Z. We\u2019ll ignore\nthe rest.\nEngineers learn something called the Square Law of Compu-\ntation. It states that for every component of a system\u2014for every\nadditional equation in a problem\u2014the amount of computation\nrequired to solve the system increases at least as fast as the square\nof the number of equations. In other words, if the complexity of\nyour problem doubles, the time it takes to solve it quadruples\u2014\nunless you make some simplifications. For example, our solar sys-\ntem contains millions of objects, all having gravitational effects\non one another. When analyzing planetary motion, astronomers\nstart by ignoring most of these objects.*\nFocusing on the key drivers means drilling down to the core\nof the problem, rather than picking the whole problem apart piece\nby piece, layer by layer. Then, you can apply thorough, fact-based\nanalysis where it will do the most good and avoid going down\nblind alleys."}
{"53": "Syntactical foibles aside, \u201ckey drivers\u201d is a very powerful con-\ncept. It saves you time. It saves you effort. It keeps you from boil-\ning the ocean.\nTHE ELEVATOR TEST\nKnow your solution (or your product or business) so thor-\noughly that you can explain it clearly and precisely to your\nclient (or customer or investor) in 30 seconds. If you can do\nthat, then you understand what you\u2019re doing well enough to\nsell your solution.\nImagine it\u2019s time for that big, end-of-engagement presentation. You\nand your team have been up until 2 a.m. putting together your blue\nbooks,* making sure that every i has been dotted and every t\ncrossed. You\u2019re all wearing your best suits and trying to look on the\nball. The senior executives of your Fortune 50 client, anxious to\nhear McKinsey\u2019s words of wisdom, are taking their places around\nthe boardroom table on the top floor of the corporate skyscraper.\nThe CEO strides into the room and says, \u201cSorry, folks. I can\u2019t stay.\nWe have a crisis and I have to go meet with our lawyers.\u201d Then he\nturns to you and says, \u201cWhy don\u2019t you ride down in the elevator\nwith me and tell me what you\u2019ve found out?\u201d The ride will take\nabout 30 seconds. In that time, can you tell the CEO your solution?\nCan you sellhim your solution? That\u2019s the elevator test.\nMany companies use the elevator test (or something similar)\nbecause it\u2019s an excellent way of making sure that their executives\u2019"}
{"54": "time gets used efficiently. Procter & Gamble tells its managers to\nwrite one-page memos. A Hollywood producer will tell a screen-\nwriter to \u201cgive me the bullet\u201d on a new script; if, after 30 seconds,\nthe producer likes what she\u2019s heard, the writer will get a chance\nto talk further, and maybe make a sale. Jason Klein instituted the\nelevator test when he took over as president of Field & Stream:\nMy sales force could not explain the magazine to customers.\nOur advertisement space was shrinking. Then I trained my\nentire sales force on the elevator test. I challenged them to\nexplain the magazine to me in 30 seconds. It became a valu-\nable tool for them, and our ad base has grown every year.\nHow do you encapsulate six months\u2019 work in 30 seconds?\nStart with the issues that your team addressed. The client wants\nto know the recommendations for each issue and the payoff.\nIf you have a lot of recommendations, stick to the three most\nimportant\u2014the ones with the biggest payoffs. Don\u2019t worry\nabout the supporting data; you can talk about that when you\nhave more time.\nFor example, your analysis shows that a manufacturing client\ncan\u2019t sell enough widgets because its sales force is organized by ter-\nritory when it should be organized by buyer category. You have\nlots of data illustrating this: analyses of salespeople by buyer type,\nbuyer interviews, field visits to retail and wholesale outlets, and\nso forth. When you\u2019re on that elevator ride, just tell the CEO, \u201cWe\nthink you can boost sales of widgets by 50 percent in three years\nif you reorganize your sales force by buyer category. We can talk\nabout the details later. Good luck with the lawyers.\u201d"}
{"55": "PLUCK THE LOW-HANGING FRUIT\nSometimes in the middle of the problem-solving process,\nopportunities arise to get an easy win, to make immediate\nimprovements, even before the overall problem has been\nsolved. Seize those opportunities! They create little victories\nfor you and your team. They boost morale and give you\nadded credibility by showing anybody who may be watch-\ning that you\u2019re on the ball and mean business.\nWhenever possible, McKinsey consultants put this doctrine into\npractice. Clients can get very impatient for a result during the six\nmonths (or more) that a big McKinsey engagement can last. Giving\nthe client something practical before the end helps reduce the pres-\nsure on the team.\nAt my stockbroker client, after we had derived a number of\ninsights (thanks to 80/20) from our analysis of sales and trading\ndata, we wanted to communicate our findings to the senior man-\nagers of the institutional equities department. We set up a meeting\nwith the department head and the heads of all the business units\nin the division: sales, trading, research, and so on.\nSince I had taken the lead in the actual analysis of the data, I\ngot to present our findings. They hit this group of very experienced\nWall Street executives like a hammer. The client had no idea just\nhow inefficient its operation was.\nThe presentation had two important effects. First, it convinced\nthose executives who had not been particularly keen on McKin-\nsey\u2019s presence in the first place that they had a problem and we"}
{"56": "ier. Before the meeting, I was some smart-ass MBA poking around\ntheir business. After the meeting, I was someone who was work-\ning for them to solve their problems.\nBy plucking the low-hanging fruit, by resisting the temptation\nto hoard our information until some big end-of-study presentation,\nwe made our client more enthusiastic, our jobs easier, and our-\nselves happier.\nThis rule is really about satisfying your customer in a long-\nterm relationship. Your customer could be the purchaser of your\nproducts, or it could be a client for your services, or it could be\nyour boss. Whoever it is, it pays to keep him happy and let him\nknow that he is your top priority. If you are on, say, a software\ndesign project with a three-month lead time and you\u2019ve put\ntogether a usable program that solves part of the problem in two\nweeks, show it to your boss. Don\u2019t wait! Solving only part of a\nproblem can still mean increased profits. Just don\u2019t let anybody\nthink you\u2019ve given up on a complete solution. Those little wins\nhelp you and your customers.\nMAKE A CHART EVERY DAY\nDuring the problem-solving process, you learn something\nnew every day. Put it down on paper. It will help you push\nyour thinking. You may use it, or you may not, but once you\nhave crystalized it on the page, you won\u2019t forget it.\nMaking a chart every day may strike you as somewhat anal-reten-"}
{"57": "In the course of a typical day at McKinsey, you could start with\na quick brainstorming session at 9 a.m., move on to a client inter-\nview at 10, a factory tour at 11, and then a sandwich lunch with\nyour director. You might follow this with more client interviews,\nan end-of-day team meeting, and then a quick trip down to Whar-\nton to participate in a recruiting seminar. In the midst of all this,\nit is very easy for the facts to blend into one another like pools of\ndifferent-colored inks on a sheet of blotting paper. Even if you take\ngood notes at your interviews and have the minutes of your team\nmeetings, important points could get lost.\nYou can avoid this by sitting down for half an hour at the end\nof the day and asking yourself, \u201cWhat are the three most impor-\ntant things I learned today?\u201d Put them down in a chart or two\u2014\nnothing fancy; neatness doesn\u2019t count. If the facts don\u2019t lend\nthemselves to charting (although McKinsey-ites try to put every-\nthing in charts), just write them down as bullet points. Put your\nresults someplace where they won\u2019t get lost\u2014don\u2019t just toss them\ninto your in-tray. Later, when you are in analysis mode, you can\ncome back to your charts and notes and think about what they\nmean and where they fit in terms of your solution.\nOf course, this little tip can be taken too far. One EM from\nGermany, while working out of the New York office, would write\na whole presentation every night. I wouldn\u2019t recommend this for\nmost people\u2014at least those with a life. Then again, the EM was far\nfrom home, didn\u2019t know anyone in town, and had nothing better\nto do. He should have followed some of the suggestions presented\nin Part Four."}
{"58": "HIT SINGLES\nYou can\u2019t do everything, so don\u2019t try. Just do what you\u2019re\nsupposed to do and get it right. It\u2019s much better to get to first\nbase consistently than to try to hit a home run\u2014and strike\nout 9 times out of 10.\nShortly after I joined McKinsey, the New York office held a retreat\nat a resort upstate. One day we associates had to interrupt our\nstrenuous regimen of golf, paintball, and wine tasting to hear a lec-\nture (hey, you have to do some work at these things). The speaker\nwas the CEO of a major electronics company, a client of the Firm,\nand a McKinsey alumnus himself. His main message was \u201cDon\u2019t\ntry to knock the ball out of the park. Hit singles. Get your job\ndone\u2014don\u2019t try to do the work of the whole team.\u201d\nHis speech took me by surprise. McKinsey associates have\nspent their whole lives \u201cknocking it out of the park.\u201d They all have\nfirst-class academic backgrounds combined with records of\nachievement in other fields. They had to impress a group of sharp-\neyed and skeptical McKinsey consultants just to make it past the\nfirst job interview at the Firm. To gear down upon joining would\nstrike most of them as strange, if not distasteful.\nIt took several years of gaining perspective before I understood\nthe wisdom of the CEO\u2019s words. There are three reasons he\nwas right:\n\u2022 It\u2019s impossible to do everything yourself all the time.\n\u2022 If you manage it once, you raise unrealistic expectations\nfrom those around you."}
{"59": "It\u2019s impossible to do everything yourself all the time. Business\nproblems are complicated\u2014the problems McKinsey deals with espe-\ncially so. If you don\u2019t leverage the other members of your team to\nsolve these problems, you are wasting valuable resources. The prin-\nciple applies as much to senior managers as to junior executives\nwhose MBA diplomas are still wet with ink. Very few people have\nthe brainpower and energy to be a one-man show all the time.\nIf you manage it once, you raise unrealistic expectations from\nthose around you. Suppose, for a moment, that you manage,\nthrough superhuman effort, to perform well beyond what is nor-\nmally expected of you. You hit that ball out of the park and (what\nthe heck) the bases were loaded. Congratulations. Of course, now\nyour boss or your shareholders will expect you to do the same\nthing every time you step up to the plate.\nOnce you fail to meet expectations, it is very difficult to regain\ncredibility. At McKinsey, it is said that you are only as good as\nyour last study. If you have one \u201cbad\u201d engagement, all your hard\nwork before that doesn\u2019t matter. EMs won\u2019t want you on their\nteams. You won\u2019t be staffed on the interesting projects. You won\u2019t\nbe put in a position to excel. Your career at the Firm will suffer.\nPrepare your r\u00e9sum\u00e9.\nThe same thing happens in the stock market. A high-flying\ncompany that posts 20 percent profit increases every year sees its\nstock price soar. When it misses one quarter, even by as little as a\ncent, its momentum reverses. Wall Street drops the stock like a hot\npotato and its share price plummets. After that, even when the\ncompany gets back on the growth track, several years can go by\nbefore investors trust it enough to pile back in.\nWhen I was a kid, I had a fantasy baseball board game. You\npicked your team from a combination of then current players (Carl"}
{"60": "paper marked out in sections printed with a result: single, double,\nhome run, strikeout, and so forth. The size of each section\ndepended on the player\u2019s career record. To play the game, you\u2019d\nput the circle around a little pointer and spin the pointer; wherever\nit landed was the result for that player\u2019s turn at bat. The one thing\nI remember from that game was that the home run kings like Ruth,\nDiMaggio, and Aaron had the biggest strikeout zones too.\nIt\u2019s all very well to talk of the necessity to strive purposefully\nand, if you fail, to fail gloriously. It\u2019s OK for Mark McGwire to\nstrike out a lot, as long as he keeps hitting those home runs. In the\nbusiness world, though, you\u2019re much better off hitting singles.\nLOOK AT THE BIG PICTURE\nEvery now and then, take a mental step back from whatever\nyou\u2019re doing. Ask yourself some basic questions: How does\nwhat you\u2019re doing solve the problem? How does it advance\nyour thinking? Is it the most important thing you could be\ndoing right now? If it\u2019s not helping, why are you doing it?\nWhen you are trying to solve a difficult problem for your client or\ncompany, you can easily lose sight of your goal amid the million\nand one demands on your time. It\u2019s like you are hip-deep in a bog,\nfollowing a muddy channel that you can\u2019t see. Analysis B follows\nanalysis A and seems in turn to be followed seamlessly by analysis\nC. New data comes in and points to yet more analyses with which\nto fill your days (and nights)."}
{"61": "this by looking at \u201cthe big picture\u201d: the set of issues that make up\nyour operating hypothesis. How does what you\u2019re doing fit into\nthe big picture? A particular analysis may be intellectually correct,\neven interesting, but if it doesn\u2019t take you closer to a solution, it\u2019s\na waste of time. Figure out your priorities; you can do only so\nmuch in a day. There is nothing quite so frustrating as looking\nback over the course of a day or week and realizing, not that you\nhaven\u2019t come up with any end products, but that what you have\ncome up with is worthless in terms of the problem at hand.\nAs one former McKinsey EM told me, \u201cPerhaps the most valu-\nable thing I learned during my time at the Firm was to think about\nthe big picture\u2014to take a step back, figure out what I\u2019m trying to\nachieve, and then look at whatever I\u2019m doing and ask myself,\n\u2018Does this really matter?\u2019\u201d\nJUST SAY, \u201cI DON\u2019T KNOW\u201d\nThe Firm pounds the concept of professional integrity into its\nassociates from their first day on the job, and rightly so. One\nimportant aspect of professional integrity is honesty\u2014with\nyour clients, your team members, and yourself. Honesty\nincludes recognizing when you haven\u2019t got a clue. Admitting\nthat is a lot less costly than bluffing.\nIt was the morning of an important progress meeting at our client,\na Fortune 50 manufacturing company. The team and John, our\nED,* were going over the various sections of the presentation. I"}
{"62": "had already been through my piece of it; I had been up until 4 a.m.\ngetting it ready and I was exhausted. As the discussion moved to\nanother section, one that I had nothing to do with and knew little\nabout, my brain started slipping into that blissful place known as\nsleep. I could hear the other members of the team discussing vari-\nous points, but their words slipped away from my mind like water\nthrough a child\u2019s cupped fingers.\nSuddenly, my reverie evaporated as John asked me, \u201cSo, Ethan,\nwhat do you think about Suzie\u2019s point?\u201d Momentary shock and\nfear yielded to concentration as I tried to recall what had just been\nsaid. Years of Ivy League and business school reflexes took over\nand I came out with a few lines of general agreement. Of course,\nwhat I said might just as well have come out of a horse\u2019s backside.\nIf I had told John, \u201cI\u2019m not really sure\u2014I haven\u2019t looked at this\nissue before,\u201d I would have been fine. Even if I had said, \u201cSorry, I\njust lost it for a minute,\u201d he would have understood; after all, he\nhad been through exactly the same experience, like every other\nMcKinsey-ite. Instead, I tried to fake it, and ended up slipping in\nmy own horsefeathers.\nAt the end of the engagement, several weeks later, the team had\nits final party. We went out to TGI Friday\u2019s, ate a lot of nachos,\nand drank a lot of beer. Then the EM began presenting each of the\nteam members with presents of a rude and/or humorous nature.\nFor my gift, he handed me a little desktop picture frame around the\nfollowing words, neatly printed in the McKinsey official font: \u201cJust\nsay, \u2018I don\u2019t know.\u2019\u201d\nThis is sage advice, and that picture frame remains on my desk\nto this day."}
{"63": "DON\u2019T ACCEPT \u201cI HAVE NO IDEA\u201d\nPeople always have an idea if you probe just a bit. Ask a few\npointed questions\u2014you\u2019ll be amazed at what they know.\nCombine that with some educated guessing, and you can be\nwell along the road to the solution.\nIf you ask people a question about their business and they tell you, \u201cI\nhave no idea,\u201d don\u2019t just walk away in defeat. \u201cI have no idea\u201d is a\ncode; it really means, \u201cI\u2019m too busy to take the time to think about\nthis,\u201d or \u201cI don\u2019t think I\u2019m smart enough to know about these things,\u201d\nor worst of all \u201cI\u2019m too lazy to come up with anything useful.\u201d\nDon\u2019t accept \u201cI have no idea\u201d\u2014treat it as a challenge. Like the\nsculptor who turned a block of marble into an elephant by chisel-\ning away everything that didn\u2019t look like an elephant, you must\nchip away at \u201cI have no idea\u201d with pointed questions.\nWhen Jason Klein wanted to put together a new business unit,\nhe was sure that his top competitor was outspending him by a factor\nof 10. How could he prove this to his board of directors so they\nwould give him more funding? He told his team to put together a\nP&L (profit and loss statement) for the competitor that showed what\nit was spending. As he recalls it:\nWhen I first suggested that we do this analysis, my people\nsaid, \u201cWe have no idea.\u201d So I challenged them. Did they\nknow how much our competitor was spending on advertis-\ning? No, but they could make an educated guess. Did they\nknow how much our competitor was spending on produc-\ntion costs? No, but they could make an estimate of the com-"}
{"64": "In the end, we put together a pretty comprehensive P&L\nbacked up by solid assumptions. It may have been off by a\nfactor of 2, but who cares? What mattered was that it was\naccurate enough to make the business decision that was on\nthe table.\nJust as you shouldn\u2019t accept \u201cI have no idea\u201d from others, so\nyou shouldn\u2019t accept it from yourself, or expect others to accept it\nfrom you. This is the flip side of \u201cI don\u2019t know.\u201d With a bit of\nthinking and searching, you will usually find that you do know or\ncan find out something about a question or issue (unless, of course,\nyou have fallen asleep in the middle of a team meeting)."}
{"65": "This page intentionally left blank."}
{"66": "(cid:2)\n(cid:2)\nPART TWO\nTHE\nM KINSEY WAY\nC\nOF WORKING\nTO SOLVE BUSINESS\nPROBLEMS\n(cid:2) (cid:2)"}
{"67": "(cid:2)\n(cid:2)\nIn Part One, we looked at the way McKinsey thinks\nabout business problems and uses fact-based, hypothe-\nsis-driven, structured analysis to arrive at solutions for\nits clients. In Part Two, we will see how the Firm imple-\nments its problem-solving model on a day-to-day basis.\nWe will go through a McKinsey engagement in\nchronological order, starting with the selling (or, in\nMcKinsey\u2019s case, nonselling) process, progressing to\norganizing a team, conducting research, and brain-\nstorming.\nThe goal in Part Two is for you to experience what\nit\u2019s like to participate in a typical McKinsey study.\nI hope, however, that, unlike a typical McKinsey\nengagement, it won\u2019t take you six months of working\nuntil 1 a.m. to finish it.\n(cid:2) (cid:2)"}
{"68": "4\nSELLING A STUDY\nABOUT THE SELLING PROCESS\nAT M KINSEY\nC\nThe selling process at McKinsey differs from that of most\norganizations because, as any McKinsey-ite will tell you, the\nFirm doesn\u2019t sell. The Firm may not sell, but it certainly\nbrings in a continuing and growing volume of business, so\nthere\u2019s something to be learned from the way McKinsey gets\nitself through its clients\u2019 doors.\nGetting your foot through the door is only half the battle\nwhen marketing your skills as a problem solver, however.\nYou also have to put together your problem-solving package\nin way that ensures your success. McKinsey has learned a\nthing or two about that as well. In this chapter, we will look\nat the Zen of McKinsey salesmanship and learn how to trim\na problem-solving project to a manageable size and scope."}
{"69": "HOW TO SELL WITHOUT SELLING\nBusiness problems are like mice. They go unnoticed until\nthey start nibbling your cheese. Just building a better mouse-\ntrap will not make the world beat a path to your door. Peo-\nple who don\u2019t have mice won\u2019t be interested\u2014until the mice\nshow up; then they need to know you have the mousetrap.\nThis might sound like the musings of a Zen monk (or per-\nhaps a management consultant from California). But some-\ntimes the right way to sell your product or service is not to\nbarge into your customer\u2019s home with a bunch of free sam-\nples. Just be there, at the right time, and make sure the right\npeople know who you are.\nAt around 10 one evening, I went up to the office of Dominic, the\npartner on my team, to drop off some documents that I knew he\nwanted to see in the morning. To my surprise, he was still at his\ndesk. When I asked what kept him so late, he told me he had a\n\u201cbeauty parade\u201d for a prospective client the next morning.\n\u201cGood luck,\u201d I told him as I left. \u201cI hope you sell them on\nthis one.\u201d\n\u201cNo, no,\u201d he replied. \u201cRemember, McKinsey doesn\u2019t sell.\u201d\nThis dictum may sound strange. How could a company grow\nto the size of McKinsey without selling? But it is true. This curi-\nous aspect of McKinsey\u2019s culture stems from the roots of the Firm\u2019s\nfounders in \u201cwhite shoe\u201d law and accounting firms before World\nWar II. In those days, it was considered beneath the dignity of pro-\nfessional service firms to advertise or solicit business."}
{"70": "No senior McKinsey directors make cold calls at the offices of Bill\nGates and Ted Turner asking if they have problems they want\nsolved. The Firm does not run ads in Forbes or Barron\u2019s advertis-\ning 50 percent off telecommunications consulting. Although a\npartner\u2019s compensation depends in large part on the amount of\nbusiness he brings to the Firm, no one goes out to knock on doors.\nThe Firm waits for the phone to ring.\nAnd ring it does, not because McKinsey sells, but because\nMcKinsey markets. It does this in several different ways, all of\nthem designed to make sure that on the day a senior executive\ndecides she has a business problem, one of the first calls she makes\nis to the local office of McKinsey. The Firm produces a steady\nstream of books and articles, some of them extremely influential,\nsuch as the famous In Search of Excellence by Peters and Water-\nman.* McKinsey also publishes its own scholarly journal, The\nMcKinsey Quarterly, which it sends gratis to its clients, as well as\nto its former consultants, many of whom now occupy senior posi-\ntions at potential clients. The Firm invites (and gets) a lot of cov-\nerage by journalists. Many McKinsey partners and directors are\ninternationally known as experts in their fields. Examples are Low-\nell Bryan, who has advised congressional banking committees, and\nKenichi Ohmae (who recently left the Firm), the business guru\nwhose nickname in Japanese is \u201cKeiei no Kamisama\u201d\u2014the God\nof Management.\nMcKinsey maintains a vast network of informal contacts with\npotential clients as well. The Firm encourages its partners to par-\nticipate in \u201cextracurricular activities\u201d such as sitting on the\nboards of charities, museums, and cultural organizations; many\nmembers of these boards are executives at current or potential\nclients. McKinsey consultants also address industry conferences."}
{"71": "Occasional meetings with former clients allow partners not only\nto check up on the effects of past McKinsey projects, but to make\nsure that the Firm maintains some \u201cshare of mind\u201d should new\nproblems arise at the client.\nThese efforts could not be construed as selling, but they make\nsure that the right people know the Firm is there. That keeps the\nphones ringing.\nIf you\u2019re in sales, then you probably do have to make the cold\ncalls. For some people that is the fun of selling. But even the best\nfoot-in-the-door saleswoman needs to market.\nYou may not be on the same charitable board as billionaire\ninvestor Warren Buffett, but you can still find ways to network\nwith existing and potential clients and customers. Trade shows,\nand conferences, even the right bars, will give you the chance to\nmake sure they know who you are. Does your particular field have\na trade journal? These magazines are always looking for copy from\nindustry insiders: Write a good article and you will get your name\nin front of people who would otherwise never have heard of you.\nMeet your competitors too. Today\u2019s competitor could change jobs\nand become tomorrow\u2019s customer. Make sure he knows you! It all\nadds up to making sure your name is the one your customers think\nof when they have a need you can fill."}
{"72": "BE CAREFUL WHAT YOU PROMISE:\nSTRUCTURING AN ENGAGEMENT\nWhen structuring your project, whether you are selling your\nservices as a consultant or have been picked by your organi-\nzation to solve an internal problem, don\u2019t bite off more than\nyou can chew. Set definite milestones that you can meet. That\nway, you\u2019ll have targets you can achieve and your client will\nbe satisfied.\nWhen clients come to McKinsey with a problem, they want it fixed\nyesterday and for nothing. Fortunately, most clients realize that\nthis desire is just slightly unrealistic. Still, when structuring an\nengagement, McKinsey (usually in the person of a DCS* or ED)\nfaces a lot of pressure to deliver the maximum results in the mini-\nmum time. McKinsey bills by the hour, and those hours do not\ncome cheap.\nThe ED (or whoever is structuring the engagement) stands\nbetween the client and its demands on one side, and the engage-\nment team on the other. The team can be pushed only so far for so\nlong before the quality of its work begins to decline. McKinsey\nconsultants, in general, work very hard over the course of a study,\nbut they do have limits; they also have lives, which they would\nlike, at least occasionally, to live. The challenge for the ED is to\nbalance the desires and budget constraints of the client with the\nlimits of the team. The ideal synthesis of these two opposing\nforces is a project that a team of four to six consultants can com-"}
{"73": "plete in three to six months and that will produce tangible results\nfor the client.\nAs the Firm spends time within a client organization, it almost\nalways uncovers new problems that could benefit from McKinsey\u2019s\nexpertise. These problems, however must be addressed at another\ntime and in another engagement. Consequently, McKinsey engage-\nments tend to generate new business of their own accord.\nThus, as long as the client is happy with the results that the\nFirm produces, McKinsey is likely to have a stream of new business\n(for which it often will not need to compete).\nAs an organization, McKinsey is extremely good at figuring\nout how much a team can do over the length of a typical study. The\nbest EDs can balance the competing demands of client and team to\na nicety; they tell the client, \u201cWe\u2019re going to do X and Y. We could\ndo Z, but it would kill the team,\u201d while telling the team, \u201cLook,\nwe\u2019ve already promised the client that we would do Z, so we\u2019ve\ngot to deliver.\u201d They then work the team to its limit while simul-\ntaneously making the client feel that he is getting value for money\nand exceeding his expectations.\nOf course, not every ED is that good. In my time at the Firm,\ncertain EDs had reputations for overpromising to the client and\nthen putting their teams through hell. They were to be avoided,\nalong with EDs who were vague about the exact nature of the end\nproduct of a study and left the team to figure out just what it was\nsupposed to do.\nWhat lessons does the McKinsey experience give for the way\nyou should structure your problem-solving project? If you are a\nconsultant putting together a proposal for an outside client, then\nthe answer is simple: Don\u2019t bite off more than you (and your team)\ncan chew and know what your end product is going to be."}
{"74": "lesson for you is a bit more complicated. Don\u2019t blithely accept the\nassignment and say, \u201cSure, boss.\u201d If you do, you could be setting\nyourself up for a fall.\nBefore you go hot footing it in search of a solution, get a feel\nfor the scope of the problem. Is it something you and your team\ncan solve in the time allotted? If not, either get more time or, even\nbetter, sit down with your boss and break the problem down into\nbite-size chunks. Figure out what the end product of each chunk\nwill be: a recommendation, an implementation plan, a new prod-\nuct design, and so forth. Figure out what resources you will need to\nreach your goal and get a commitment from your boss that you\nwill have them. Doing all this ahead of time can save you a lot of\ngrief a few months down the road.\nStructuring your project properly at the beginning may not\nguarantee your success, but it at least gets you off to the right start."}
{"75": "This page intentionally left blank."}
{"76": "5\nASSEMBLING A TEAM\nABOUT TEAMS AT M KINSEY\nC\nAt McKinsey, you never walk alone\u2014or, at least, you never\nwork alone. Everything at the Firm happens in teams, from\nfront-line work on client engagements all the way up to\nfirmwide decision making. The smallest team I ever\nworked on consisted of me and my EM on a pro bono\nengagement for a New York theater company. At the other\nend of the scale, the Firm\u2019s largest clients might have sev-\neral five- or six-person teams working on site at one time;\ntogether, these form a \u201cmetateam.\u201d In the early 1990s,\nmembers of the AT&T metateam decided to get together to\ndiscuss their work; the Firm\u2019s headquarters didn\u2019t have a\nroom large enough to hold them all, so they had to book\na New Jersey hotel.\nMcKinsey relies on teams because they are the best way\nto solve the problems that the Firm\u2019s clients face. The com-\nplexity of these problems makes it impossible for one per-\nson to solve them\u2014at least to the Firm\u2019s high standards.\nMore people mean more hands to gather and analyze data\nand, more important, more minds to figure out what the\ndata really mean. If you face complex problems in your\nbusiness, you should probably put together a team to help\nsolve them too. In the face of complexity, many hands don\u2019t"}
{"77": "The Firm has developed a number of strategies for putting\ntogether and maintaining high-performance teams. In this\nchapter, you will learn how to select the right people for your\nteam. You will also discover some tricks for keeping your\nteam happy and productive under pressure."}
{"78": "GETTING THE MIX RIGHT\nYou can\u2019t just throw four random people at a problem and\nexpect them to solve it. Think about what sorts of skills and\npersonalities will work best for your project. Then choose\nyour teammates carefully.\nTo succeed as a business problem solver, you must choose your\nteam carefully, getting the best mix of people from the resources\nyou have available. McKinsey benefits from a global pool of tal-\nented, intelligent individuals whose strengths and weaknesses the\nFirm tracks closely. Even with this advantage, EMs and EDs must\nlearn the art of team selection. Their experiences can help you,\neven if you can\u2019t call on the same level of resources.\nMcKinsey-ites subscribe to one of two theories of team selec-\ntion. The first theory states that intellectual horsepower is every-\nthing\u2014find the smartest people for your team regardless of their\nexperience or personal hygiene. The second theory says that what\nreally matters is specific experience and skills; intelligence is a\ngiven within the Firm\u2014every McKinsey consultant is smart or he\nwouldn\u2019t be there.\nNeither of these theories is completely correct, but neither of\nthem is completely wrong either. Proper team selection varies from\nproblem to problem and client to client. Some problems will yield\nonly to large amounts of analytical firepower. For instance, if you\nhave mountains of complex data that you need to decipher, then\nyou want the two or three best number crunchers that you can\nfind, regardless of whether they can simultaneously walk and chew"}
{"79": "would prefer to have someone on your team with good people\nskills and experience in implementing change.\nAnother important team selection lesson emerges from the\nMcKinsey team assignment process. When an engagement begins,\nthe EM and ED pick their associates from the pool of available\nresources at the time. The \u201cmanager of associate development\u201d or\nthe office manager will tell them who is available and give them a\nsheet listing each associate\u2019s experience and rating each one on\nanalytical ability, client management skills, and so forth. The\nbiggest mistake in team selection comes from taking those ratings\nat face value. A smart EM alwaystalks to potential team members\nbefore taking them on.\nBy extension, if you are in a position to pick your team mem-\nbers before embarking on a project, never just accept people who\nare supposed to be good. Meet them face to face. Talk to them;\nsee what\u2019s behind the recommendations. Maybe in her last assign-\nment Sally just got lucky. Or maybe Pete\u2019s the CEO\u2019s nephew and\nhis last boss was scared to tell the truth about him. (Of course, if\nhe is the CEO\u2019s nephew, you may be stuck with him.) Maybe\nCarol\u2019s brilliant, but after spending 15 minutes talking to her, you\nknow she would drive you crazy if she were on your team.\nJust remember, if you are lucky enough to be able to choose\nwhom you will work with, choose wisely."}
{"80": "A LITTLE TEAM BONDING GOES A LONG WAY\nIt\u2019s a truism that a team will perform better and its members\nwill have a better time if the team members get along well. As\na team leader, you should make an effort to promote team\nbonding; just make sure it doesn\u2019t become a chore.\nFor McKinsey-ites, team-bonding activities are a given. In the\ncourse of an engagement, you expect to go out at least a few times\nto the nicest restaurants in town, or to see a show or a game on\nMcKinsey\u2019s (and, eventually, the client\u2019s) nickel. One ED even took\nhis whole team to Florida for a long weekend.\nAs a team leader, the question for you is how much formal\nteam bonding is enough. After talking with a number of former\nMcKinsey-ites, and reflecting on my own experience, I\u2019m going to\ngo out on a limb and say that the answer is not much. A little team\nbonding goes a long way. As a team leader, you have the far more\nimportant job of looking after team morale (see the next section).\nFormer SEM Abe Bleiberg put it like this:\nI\u2019m not sure that team bonding is all that important. What\u2019s\nimportant is that a team works together well, and that will\ncome or not over the course of a project. It\u2019s also important\nthat individuals feel respected and that they feel that their\nideas are respected.\nTeam bonding is not, \u201cDid you take your team to\nenough dinners? Did you go out to the movies? Did you go\nto the circus?\u201d Most people, even very hard-working people,\nwant to have a life, to be with their families. I think that\u2019s"}
{"81": "If a team is going to bond, it will mostly bond through work.\nA typical McKinsey team works at the client for 10 to 14 hours a\nday, plus a day over the weekend at the office. That\u2019s plenty of\ntime for bonding. Also, on an out-of-town study, team members\nwill eat dinner together more often than not. Why, as a team\nleader, would you want to take up yet more of their time? If the\nteam isn\u2019t bonding, how is a fancy dinner going to help? Will it\nmake a bad work experience good?\nSo, when managing your team, be selective with team-bond-\ning activities. Try to get your team\u2019s \u201csignificant others\u201d\ninvolved; this will help them understand what their loved ones\u2014\nyour teammates\u2014are doing, and it will help you understand\nyour teammates. Above all, respect your teammates\u2019 time. One\nformer associate noted that, at McKinsey, the best team dinners\nwere at lunch \u2014they showed that the EM knew the associates\nhad lives.\nTAKE YOUR TEAM\u2019S TEMPERATURE TO\nMAINTAIN MORALE\nMaintaining your team\u2019s morale is an on-going responsibility.\nIf you don\u2019t do it, your team will not perform well. Make\nsure you know how your team feels.\nIn my time at McKinsey, I worked on two studies that didn\u2019t turn\nout well. Both studies were charged with client politics\u2014the\nMcKinsey team became a football kicked between rival factions"}
{"82": "After the other, I was ready to leave the Firm.* Why the different\nreactions? Team morale.\nMy \u201cbad\u201d EM (who shall remain nameless) managed by the\nMushroom Method: \u201cCover them with manure and keep them in\nthe dark.\u201d We associates never felt that we knew what was going\non; I never got a sense that what I was doing was valuable, either\nto the team or to the client. My \u201cgood\u201d EM, on the other hand,\nalways let us know what was going on, and if he didn\u2019t know, Vik\ntold us so. We knew about the client politics\u2014we understood it\u2014\nand that made it easier for us to work with it. Also, I knew that\nVik\u2019s door was always open and that he was pulling for us as much\nas for the client.**\nWhat\u2019s the secret to maintaining team morale? There isn\u2019t\none\u2014just a few simple rules to remember.\nTake your team\u2019s temperature. Talk to your teammates. Make\nsure they are happy with what they are doing. Find out if they have\nquestions about what they are doing or why they are doing it, and\nanswer them. If they are unhappy, take remedial action quickly.\nSteer a steady course. If you change your mind all the time\nabout the team\u2019s priorities or the analyses you\u2019re doing, your team\nwill quickly become confused and demoralized. Know where\nyou\u2019re going and stay your course. If you need an extra day to fig-\nure it out, take it. If you need to make a big change, let your team\nknow, explain why, and let people contribute to, or at least see,\nyour thought process.\nLet your teammates know why they are doing what they\u2019re\ndoing. People want to feel that what they are doing is adding value\nto the client. There are few things more demoralizing than doing\nsomething that you and your team leader know is valueless. No"}
{"83": "one on your team should ever feel, \u201cI\u2019ve just spent two weeks of\nmy life for nothing.\u201d\nTreat your teammates with respect. There is no excuse for\ntreating people with disrespect; it\u2019s completely unprofessional.\nRespect doesn\u2019t just mean politeness. It means remembering that\nyour teammates may have different priorities than you do, and that\nthey may have lives outside of work. You may like to work until\nmidnight six days a week, but your team may have better things\nto do. There will be times, of course, when the team must work\nall hours, but be sure it really is one of those times before you call\na team meeting at 10 p.m. Respect also means never asking some-\none to do something you wouldn\u2019t do or haven\u2019t done yourself. As\nan associate, I always felt a bit better knowing that if I was in the\noffice at midnight, my EM was too.\nGet to know your teammates as people. Are they married? Do\nthey have kids? What are their hobbies? It will help you to under-\nstand them. Share a bit about yourself as well; that makes it more\nlikely that your teammates will think of you as part of \u201cus,\u201d rather\nthan \u201cthem.\u201d This, incidentally, is a much better way of team\nbonding than taking your team out to the ball game.\nWhen the going gets tough, take the Bill Clinton approach.\nSometimes, as in my two bad experiences, you will be dealt a bad\nhand. The problem is difficult; the client is difficult. There\u2019s not a\nlot you can do beyond telling your team, \u201cI feel your pain.\u201d At\nsome point, you have to soldier on; that\u2019s life.\nSpending months solving complex business problems is no bed\nof roses. If you follow the rules on maintaining morale, however, at\nleast your team won\u2019t feel like resigning when it\u2019s all over."}
{"84": "6\nMANAGING HIERARCHY\nABOUT THE M KINSEY CHAIN\nC\nOF COMMAND\nMcKinsey has something of a split personality when it comes\nto hierarchy. On the one hand, the Firm claims that it has no\nreal hierarchy. On the other hand, any McKinsey-ite past or\npresent can tell you that two hierarchies (at least) exist within\nMcKinsey. Both statements are correct.\nI cannot imagine a flatter organization than McKinsey. I\ncould, as an associate, walk into my ED\u2019s office without an\nappointment and talk to him about our study. In meetings at\nthe Firm, every idea, whether it comes from the youngest\nbusiness analyst or the oldest director, carries the same\nweight and is debated and attacked accordingly (at least\nthat\u2019s the way it\u2019s supposed to be, and usually is).\nAt the same time, McKinsey has a definite chain of com-\nmand. The directors and, to a lesser extent, the partners\nmake decisions about the direction of the Firm, and the\nEMs, associates, analysts, and support staff live with them.\nIf I disagreed with my EM over an issue, at the end of\nthe day, his opinion won out. Likewise, my ED\u2019s opinion\ntrumped my EM\u2019s.\nThen, McKinsey has another, unofficial hierarchy: one\nbased on experience and credentials\u2014how good you are (or"}
{"85": "their assignments, while hot EMs had associates clamoring to\nwork for them and everyone sought out the best EDs and\nDCSs as mentors and career makers. On the other hand,\nassociates who performed poorly didn\u2019t last very long at the\nFirm\u2014after one bad engagement, no EM or ED would want\nthem on the team. Likewise, the associates generally knew\nwhich EMs to avoid and which EDs had missed the Firm\u2019s\nfast track.\nEvery organization has its own approach to hierarchy.\nYour own may look nothing at all like the Firm\u2019s. Still, every\nMcKinsey-ite has learned a few lessons about dealing with\nhierarchy that should work in any organization. They can\nhelp you stay out of trouble and get ahead."}
{"86": "MAKE YOUR BOSS LOOK GOOD\nIf you make your boss look good, your boss will make you\nlook good. That\u2019s the quid pro quo of hierarchy.\nI was a first-year associate and I\u2019d just spent several weeks putting\ntogether a comprehensive competitor analysis for my client. When\nit came time to share my findings with the senior management of\na very hierarchical manufacturing company, I was too \u201cgreen\u201d to\nmake the presentation. My EM got the job instead. I was disap-\npointed, but I understood the rationale behind the decision.\nIt then became my job, over the course of several hours, to\nmake my EM as conversant with my analysis as I was. The next\nday, he delivered the presentation very convincingly. When the\nclient asked questions, my EM answered them; all the while, I was\nwriting him notes, whispering facts in his ear, and pointing out\nimportant pages in the presentation document. The client was suit-\nably impressed with the presentation and with my EM. My EM\n(my boss) and my ED (my boss\u2019s boss) were impressed with me. I\nhad done my job and the Firm would know about it.\nIn any hierarchical organization, the most important person\nin your world, day in and day out, is your boss. When you work in\nteams, away from the main office, in a distant city or foreign coun-\ntry, that importance increases by an order of magnitude. Your boss\nmay be the only person in your organization who can see you.\nYou\u2019d better make her happy. The best way to do that is to make\nher look good.\nMaking your boss look good means two things. Firstly, it"}
{"87": "Second, make sure your boss knows everything you know when\nshe needs to know it. Keep the information flowing. Make sure\nyour boss knows where you are, what you are doing, and what\nproblems you may be having. At the same time, don\u2019t overload\nher with information. Think about what your boss needs or wants\nto know. Use a well-structured e-mail or voice mail to convey\nthe information.\nGetting these things right helps you as much as it helps your\nboss. To paraphrase a famous hairdresser, if your boss looks good,\nyou look good.\nAN AGGRESSIVE STRATEGY FOR\nMANAGING HIERARCHY\nIf you have the stomach for it, assert your equality in the\norganization. Keep on doing it until someone tells you\notherwise. Obviously, this is not a strategy for everyone.\nHamish McDermott, a newly hired associate fresh from a graduate\ndegree in philosophy at Cambridge, found himself assigned to an\ninternal research project\u2014what McKinsey calls practice develop-\nment, or PD\u2014for Lowell Bryan, a director of the Firm and a very\nimposing presence. Lowell had just finished the second chapter of\na book on bank failures and asked Hamish and the rest of the team\nfor their comments. Hamish took Lowell at his word and pro-\nceeded to write an account of all the logical flaws in the chapter. As\nHamish remembers it:"}
{"88": "inconsistent and where his arguments were failing. Of\ncourse, I had used a very dry, superior tone, as if it were\nan exam question at Cambridge: \u201cSo-and-so has made a\nvaliant effort to expound his thesis, but failed for the\nfollowing 16 reasons.\u201d\nLowell was working out of town at the time, and Hamish\nfaxed his comments directly to him, without showing them to his\nEM. In many firms, this would have been enough to get Hamish\nthe sack, but Lowell was fine with it. Hamish\u2019s EM later com-\nmented that maybe he should have been more careful about the\ntone of his comments. In fact, when the book came out, Lowell\ngave Hamish a copy inscribed with the message \u201cThanks for all\nyour help, especially with Chapter 2.\u201d Hamish went on to have a\nvery successful career at the Firm.\nThis story shows that, in a meritocratic organization at least,\nyou can assert your equality until shown otherwise\u2014until someone\ntells you, \u201cNo, you have to do what I tell you.\u201d You\u2019ll find it\u2019s rare\nfor that to happen. As Hamish said:\nIt sounds extreme, but in a way, to be a successful consul-\ntant, you have to assert yourself. Very often, you\u2019ll be in a sit-\nuation where you just have to assume that you can do\nsomething, or talk to someone, or get access to some bit of\ninformation, even though you may not have the explicit\nauthority to do so.\nThis is a risky strategy, and the more hierarchical an organiza-\ntion, the riskier it becomes. In a more rigid organization, be more\nsensitive of where the limits to others\u2019 authority lie. And be ready\nto back down quickly; otherwise, someone will stomp on you."}
{"89": "This page intentionally left blank."}
{"90": "7\nDOING RESEARCH\nABOUT RESEARCH AT M KINSEY\nC\nThe McKinsey problem-solving process begins with research.\nBefore a team can construct an initial hypothesis, before it\ncan disaggregate a problem into its components and uncover\nthe key drivers, it has to have information.\nAt the start of a McKinsey-ite\u2019s career, most of his time is\nspent gathering data, whether from one of the Firm\u2019s\nlibraries, from McKinsey\u2019s many databases, or from the\nInternet. Gathering, filtering, and analyzing data is the skill\nexercised most by new associates.\nAs a result, McKinsey-ites have learned a number of\ntricks for jump-starting their research. You can use these\ntricks to find the answers to your business problem too."}
{"91": "DON\u2019T REINVENT THE WHEEL (PART 2)\nWhatever the problem, chances are that someone, some-\nwhere, has worked on something similar. Maybe that person\nis in your organization and can answer all your questions in\nthe course of a phone call. Maybe other people in your field,\nin another division or another company, have seen the same\nproblem already\u2014find out who they are and get to know\nthem. Do your research and ask questions; you will save\nyourself a lot of time and effort. Your time is valuable, so\ndon\u2019t waste it by reinventing the wheel!\nMcKinsey keeps an electronic database called PDNet* containing\nreports from recent engagements and internal research. When I\nwas a first-year associate, one of my jobs at the start of an\nengagement was to search PDNet for anything that would shed\nlight on our current project: comparable industries, comparable\nproblems. Inevitably, any PDNet query produced a mountain of\ndocuments that I then had to wade through to find the few that\nmight be relevant. Still, this long day\u2019s (and, as often as not,\nnight\u2019s) work usually yielded something to point us in the\nright direction.\nMcKinsey has other resources that help its consultants work\nsmarter, not harder. These include an excellent business library\nthat holds every business book or magazine you care to mention;\nit also has access to all the major commercial databases such as\nLexis/Nexis, Dun & Bradstreet, Datastream, and the Internet.\nMost important, the library has a dedicated staff of information"}
{"92": "specialists who work extremely hard to supply the consultants\nwith information\u2014whether from PDNet, the library, or any\nother source. The Firm also has a cadre of senior information\nspecialists who are experts in particular industries; they were\nan especially valuable resource when we found ourselves work-\ning one month for a client in banking and the next for a jet\nengine manufacturer.\nIn my first McKinsey engagement after joining the Firm, our\nclient, the financial arm of a very large computer hardware and\nsoftware manufacturer, wanted advice on expanding internation-\nally. The client especially wanted to understand how major for-\neign conglomerates maintained financial and managerial control\nof their offshore subsidiaries and what the pros and cons of their\nvarious methods were. My engagement manager put me in\ncharge of that part of the project. I had three weeks to gain an\nintimate understanding of four of the world\u2019s largest conglomer-\nates and to figure out what, if anything, our client could learn\nfrom them.\nI went to PDNet first. Fortunately for me, another McKin-\nsey team had recently put together an organizational profile of\none of the most complex of my subjects, Daimler-Benz. That\nafternoon I had in my hands what would have taken me a week\u2019s\nconcentrated research; more important, I learned the names of\nthe real experts on Daimler-Benz, whom I called with follow-up\nquestions. I had that much more time to work on the other com-\npanies, and the team was able to produce a document that\nimpressed our client.\nYou may not have PDNet, but if you work in a large organiza-\ntion you probably have access to much of your company\u2019s \u201ccorpo-\nrate memory\u201d\u2014databases, files, training manuals, and coworkers."}
{"93": "pers, data feeds, and (most important these days) the Internet. What\nabout your local library? You will find loads of information and\nvaluable resources there with a few hours\u2019 digging.\nGet to know your competitors. Many businesspeople will share\nsome information on the principle that \u201cwhat goes around, comes\naround.\u201d If you\u2019re in, say, advertising, find the caf\u00e9 in your city\nwhere other ad execs hang out. Tap into your industry\u2019s \u201cbuzz.\u201d\nWhatever you\u2019re doing, chances are someone, somewhere has\ndone something similar. Learn from others\u2019 successes and mistakes.\nLeverage your valuable time and don\u2019t reinvent the wheel!\nSPECIFIC RESEARCH TIPS\nUse these tried-and-tested tips to jump-start your research.\nDuring my research for this book, I interviewed dozens of former\nMcKinsey-ites. They gave me, along with structured answers to my\nspecific questions, a number of tips and tricks to succeed in the var-\nious aspects of life at McKinsey. Here is a grab bag of tips to make\nyour research more efficient and effective.\nStart with the annual report. If you want to get up to speed\non a company as quickly as possible, the first place to turn is the\nannual report. It\u2019s easy to obtain (many companies now post their\nannuals on the World Wide Web) and contains a great deal of\ninformation beyond the financial data.\nWhen you get a company\u2019s annual, turn first to the \u201cMessage\nto Shareholders\u201d or \u201cChairman\u2019s Remarks\u201d at the front. If you"}
{"94": "management hopes to take the company in the future, and the\nstrategy for getting there. You\u2019ll usually also get a quick break-\ndown of key financial indicators such as stock price, revenue, and\nearnings per share. Go further into the annual, and you\u2019ll find out\nabout the company\u2019s business units and product lines, who its\nsenior managers are, and where the company has offices and pro-\nduction facilities. Then you can plough into the numbers.\nA company\u2019s annual report will get your research off to a\nrapid start.\nLook for outliers. When you\u2019ve collected a large amount of\ndata on a particular aspect of your problem, look for outliers\u2014\nthings that are especially good or bad. Use a computer to get a\nquick picture.\nFor example, suppose you are collecting data on your com-\npany\u2019s sales force. Enter the average sales of each salesperson and\ndivide it by the number of accounts served by that salesperson for,\nsay, the last three years; this gives you the average sales per\naccount. Type the data into your favorite spreadsheet software and\nsort the averages from lowest to highest. Then look at the two or\nthree best and worst figures. Congratulations, you\u2019ve just found a\nfruitful area for research. Figure out why the numbers are so good\nor bad and you\u2019ll be well on your way to fixing the problem.\nLook for best practice. There\u2019s an old saying that no matter\nhow good you are at something, there\u2019s always somebody better.\nThis is as true in business as it is anywhere else. Find out what the\nbest performers in the industry are doing and imitate them. Often,\nthis is the quickest antidote to poor performance.\nUsually, you can\u2019t find out about best practice in the library.\nYou have to think creatively. If some of your competitors have best\npractice, they probably won\u2019t tell you their secrets. Talk to other"}
{"95": "Sometimes you can find best practice within your company.\nSomeone, some team, or some division is outperforming the rest of\nthe company. Find out why. Figure out how to implement the top\nperformer\u2019s secrets throughout your organization. The result will\nbe a huge payoff to your business."}
{"96": "8\nCONDUCTING INTERVIEWS\nABOUT INTERVIEWING AT M KINSEY\nC\nIn every McKinsey engagement, someone on the team will\nconduct an interview. In most engagements, the team will\nconduct lots of them. There is always someone who has\ninformation that the team needs: an executive at the client,\na production-line supervisor, a supplier, a customer, an\nindustry expert, even a competitor. Interviewing is the way\nMcKinsey consultants fill the gaps in their knowledge base\nand tap into the experience and knowledge of their clients.\nInterviewing is such an important part of the McKinsey\nproblem-solving process that it merits its own chapter in this\nbook, separate from research. You can learn a lot from read-\ning magazine articles, books, and scholarly papers, but to\nget the nitty-gritty on an organization, you have to ask ques-\ntions of and get answers from the people on the front line.\nInterviewing is a skill in its own right, and most people have\nno idea how to go about it.\nYou might think that even though interviewing McKinsey-\nstyle is a good technique for consultants who need to get up\nto speed on unfamiliar industries, it is of little use to execu-\ntives in more settled positions. I disagree. In today\u2019s business\nworld, no matter who you are, from the most junior of\njunior managers to the most senior of senior vice presidents,"}
{"97": "a multifunctional team as part of a merger; you might be told\nto set up and run a new business. The possibilities are end-\nless, but they all require you to pick someone\u2019s brain, chew\nthe fat, or get up to speed. Call it what you like, it\u2019s an inter-\nview when you ask questions and get answers.\nIn this chapter, I will take you through the interviewing\nprocess, from preparing your interview guide to writing your\nthank-you note. If you read no other chapter of the book\nfrom start to finish, read this one. I think you\u2019ll learn some-\nthing very valuable that you won\u2019t find elsewhere."}
{"98": "BE PREPARED:WRITE AN INTERVIEW GUIDE\nWhen you go into an interview, be prepared. You may have\nonly 30 minutes with a person whom you may never see\nagain. Know what you\u2019re going to ask.\nWhen I asked McKinsey alumni for their best advice on inter-\nviews, every single one of them said, \u201cWrite an interview guide.\u201d\nMany people resent being interviewed, or at least begrudge you\nthe time that you are taking from their day. A guide is your best\ntool for getting what you want from interviewees and for mak-\ning the best use of your time\u2014and theirs.\nYou must think on two levels when constructing your guide.\nFirst, and obviously, what are the questions to which you need\nanswers? Write them all down in any order. Second, and more\nimportant, what do you really need from this interview? What\nare you trying to achieve? Why are you talking to this person?\nDefining your purpose will help you put your questions in the\nright order and phrase them correctly.\nIt helps to know as much as possible about the interviewee\nin advance. Is she a prickly CEO who might bite your head off\nif you ask a sensitive question? Or is she a middle-level manager\nwhose pleas for change in her organization have gone unheeded?\nBoth might know the same piece of information, but you\u2019d\napproach each one differently.\nAt McKinsey we were taught that, as a rule, an interview\nshould start with general questions and move on to specific ones.\nDon\u2019t dive right into a sensitive area like \u201cWhat are your respon-"}
{"99": "This will help the interviewee \u201cwarm up\u201d and allow you to\ndevelop rapport.\nWhen deciding on which questions to ask, you might want to\ninclude some to which you know the answer. This may sound\ncounterintuitive, but it\u2019s really very useful. On questions of fact,\nasking a \u201cringer\u201d will give you some insights into the interviewee\u2019s\nhonesty and/or knowledge. For complex issues, you may think you\n\u201cknow\u201d the answer, but there may be more than one; you should\nfind out as many as possible.\nOnce you\u2019ve written your guide, look at it and ask yourself,\n\u201cWhat are the three things I most want to know by the end of the\ninterview?\u201d These are the things you will focus on when you go\ninto the interviewee\u2019s office, the three things that you will try your\nhardest to obtain before you leave. Sometimes you won\u2019t even get\nthose answers (see \u201cDifficult Interviews\u201d later in the chapter), and\nsometimes they\u2019ll come easily. Anything more is gravy.\nFinally, every interview guide should conclude with what I call\nthe prototypical McKinsey question. When you\u2019ve asked all your\nquestions, or you\u2019re running out of time, put away your guide and\nask the interviewee if there\u2019s anything else he\u2019d like to tell you or\nany question you forgot to ask. As often as not, the interviewee\nwill say no, but every once in a while you\u2019ll strike paydirt. Remem-\nber that, chances are, the people you interview know their organi-\nzations, their business units, or their departments better than you\ndo. They may know which problems are eluding senior managers,\nwho\u2019s pushing which agenda, or where the bones are buried. And\nsometimes, if you\u2019re lucky, they\u2019ll tell you."}
{"100": "WHEN CONDUCTING INTERVIEWS, LISTEN\nAND GUIDE\nWhen you\u2019re picking people\u2019s brains, ask questions and then\nlet them do the talking. Most people like to talk, especially\nif you let them know you\u2019re interested in what they\u2019re saying.\nKeep the interview on track by breaking in when necessary.\nMcKinsey consultants receive a lot of training in interviewing tech-\nniques. The first thing we were taught was \u201calways let the inter-\nviewee know you are listening.\u201d We did this by filling the gaps in\nthe interviewee\u2019s conversation with verbal placeholders such as\n\u201cyes, or \u201cI see,\u201d and even just \u201cuh-huh\u201d (this particular choice I\nlike to call the McKinsey grunt*). Uh-huh may not seem like much,\nbut it shows that you\u2019re paying attention (even when you\u2019re not!),\nand it gives the other person a chance to gather his thoughts and\ncatch his breath.\nWe also learned to communicate our interest through body lan-\nguage. When the interviewee was speaking, we leaned toward her\nslightly. When she completed a sentence, we nodded. And we\nalways took notes. Even if the interviewee was babbling (and this\nhappened often enough), we had our notepads and pens out and\nwrote things down. Like the McKinsey grunt, note taking implied\nwe were paying attention and kept us prepared in case the inter-\nviewee did say something important.\nThis technique could be carried too far, of course. According\nto one piece of Firm lore, two consultants went to interview a"}
{"101": "high-level executive at a client. The engagement manager made\nhis introduction, then started asking questions. The executive\nanswered in detail. All through this, the associate nodded and\ninterjected with \u201cyes,\u201d \u201cuh-huh,\u201d and \u201cI see\u201d while taking notes\nat a furious pace, just as he had been taught, but he never asked\nany questions himself. The EM asked follow-up questions and the\nassociate just kept on nodding and saying \u201cuh-huh.\u201d When the\ninterview ended, the EM thanked the executive for his time and\nthe two consultants got up to leave. As they were shaking hands,\nthe executive pointed to the associate and asked the EM, \u201cDoes he\nspeak English?\u201d\nWhen McKinsey consultants conduct interviews, it\u2019s because\nthey want access to the information, experience, and anecdotes in\nother person\u2019s heads. Consultants are there to listen, not to talk.\nThey need to remember that the other person has a separate\nagenda, and needs to be kept on track. The process can sometimes\nprove difficult. I once had to interview a purchasing manager from\na client\u2019s plant in Idaho. He knew all about the plant\u2019s suppliers,\ncustomers, input requirements, and manufacturing processes, but\nall he cared about was fishing\u2014fly fishing. \u201cDid I fly fish?\u201d he\nwanted to know. I should try it. If I was ever in the Pocatello area,\nhe could set me up. You get the picture. I felt a bit bad getting him\noff his favorite subject, but I was there to get information, not\nswap fish stories.\nThe main thing to remember when trying to get information\nfrom other is that they need to feel you are listening and that\nyou\u2019re interested in what they have to say. Use positive body lan-\nguage and always take notes. One final trick: If you want people to\nsay more than they have, if you think they have left out something\nimportant but you\u2019re not sure what it is, say nothing. Let the"}
{"102": "been giving you a prepared \u201cscript,\u201d they will probably drop it,\nbecause the one thing they were not prepared for was silence. Try\nit and see. It\u2019s surprisingly effective.\nSEVEN TIPS FOR SUCCESSFUL INTERVIEWING\nAlways think strategically when conducting an interview.\nYou have a goal to reach and limited time to reach it. Here\nare seven tried-and-tested stratagems to help you get what\nyou want from an interviewee.\n1. Have the interviewee\u2019s boss set up the meeting. Going\nthrough the boss tells the interviewee that the interview is impor-\ntant. He\u2019ll be less likely to jerk you around if he knows his boss\nwants him to talk to you.\n2. Interview in pairs. It\u2019s very difficult to conduct an effective\ninterview on your own. You may be so busy taking notes that it\nbecomes difficult to ask the right questions. You may miss non-\nverbal clues that the interviewee is giving. Sometimes, it is useful\nfor a pair of interviewers to \u201ctag-team\u201d\u2014switch roles from ques-\ntion poser to note taker during the session. The approach is espe-\ncially effective when one of the interviewers has specific knowledge\non certain issues that will be covered. Furthermore, it is always\nuseful to have two different views of what actually happened in the\ninterview. Just make sure that whoever writes up the interview\nnotes corroborates them with the other interviewer.\n3. Listen; don\u2019t lead. In most interviews, you are not looking"}
{"103": "is to listen. Talk as little as possible, just enough to keep the inter-\nview on track. Remember that the interviewee probably knows a\nlot more about her business than you do, and most of the infor-\nmation she gives you will be useful one way or another.\nHere\u2019s another trick for keeping the information flowing. Ask\nopen-ended questions. If you ask yes-or-no or multiple-choice\nquestions, that\u2019s all you will get. For example, suppose you want to\nfind out when a store\u2019s busiest season is. You think it is either sum-\nmer or winter, but you\u2019re not sure. If you ask the store manager, \u201cIs\nyour busiest season summer or winter?\u201d she might say summer;\nshe might say winter; or she might say, \u201cActually, it\u2019s spring,\u201d in\nwhich case, you\u2019ve just highlighted your lack of knowledge about\nher business. If you ask her, \u201cWhat is your busiest season?\u201d she\nwill give you the answer, and probably in more detail than if you\ngave her multiple choice\u2014for example, \u201cWe\u2019re busiest in the\nspring, specifically at Easter.\u201d By asking the open-ended question,\nyou get a much better result.\n4. Paraphrase, paraphrase, paraphrase. Before going out on\ninterviews, every McKinsey consultant is trained to repeat back a sub-\nject\u2019s answers in slightly different form. I cannot overstress how\nimportant this is. Most people do not think or speak in a completely\nstructured way. They ramble, they digress, they jumble important\nfacts among irrelevancies. If you repeat their own words back to\nthem\u2014ideally with some structure applied\u2014then they can tell you\nwhether you understood them correctly. Paraphrasing also gives the\ninterviewee a chance to add information or amplify important points.\n5. Use the indirect approach. An EM had on his team a new\nassociate, fresh out of the Navy. The two had put together a very\nclear interview guide and had agreed on a specific set of goals for\nan interview with a middle-level manager at their client, so the EM"}
{"104": "if it were an interrogation rather than an interview. As you might\nimagine, the interviewee was rattled; he became defensive and\nessentially refused to cooperate.\nThe moral of this tale is \u201cBe sensitive to the interviewee\u2019s feel-\nings.\u201d Understand that the person may feel threatened. Don\u2019t dive\nright into the tough questions. If you have to dance around the\nimportant issues for a few minutes, that\u2019s OK. Take time to make\nthe interviewee comfortable with you and the interview process\n(for an in-depth discussion, see the next section).\n6. Don\u2019t ask for too much. There are two reasons not to ask\nfor everything the interviewee has. First, you might get it. When\nyou write your interview guide, you narrow down your goals to\nthe two or three most important questions. If you then ask the\ninterviewee for the sum total of his knowledge of the widget indus-\ntry, you may find yourself wading through a lot of information to\nget what you really need, if you even find it at all.\nSecond, you want to stop short of the straw that breaks the\ncamel\u2019s back. Remember, being interviewed, especially in the con-\ntext of a business problem, is an uncomfortable experience for\nmany. If you compound that discomfort by pressing too hard, you\nmay find that the interviewee becomes uncooperative or even hos-\ntile. You never know when you may want to come back to this per-\nson for more information, so don\u2019t shut the door.\n7. Adopt the Columbo tactic. If you watched TV in the 1970s,\nyou may remember Peter Falk\u2019s trenchcoat wearing detective, Lieu-\ntenant Columbo. After he finished quizzing a murder suspect\nabout her whereabouts on the night in question, he would pick up\nhis rumpled raincoat and head out the door. As he reached the\nthreshold and was about to leave, he would turn around, stick his\nfinger up to his temple, and say, \u201cExcuse me, ma\u2019am, but there\u2019s"}
{"105": "If there\u2019s a particular question you need the answer to, or a\npiece of data that you want, the Columbo tactic is often a good\nway to get it. Once the interview is over, everybody becomes more\nrelaxed. The interviewee\u2019s sense that you have some power over\nhim will have disappeared. He is far less likely to be defensive, and\nwill often tell you what you need or give you the information you\nseek on the spot. Try it; it works.\nYou might also want to try the \u201csuper-Columbo\u201d tactic.\nInstead of turning around at the door, wait until a day or two has\npassed, then drop by the interviewee\u2019s office. You were just passing\nby and remembered a question you forgot to ask. Again, this\nmakes you much less threatening, and makes it more likely that\nyou will get the information you need.\nDON\u2019T LEAVE THE INTERVIEWEE NAKED\nRemember that, for many people, being interviewed about\nproblems in their job or business can be unnerving. You have\na responsibility to be sensitive to their fears. It\u2019s not only the\nright thing to do; it makes good business sense too.\nA McKinsey associate and his ED went to interview a middle-level\nmanager at a large pharmaceuticals company that the Firm was\nhelping to restructure. The man had been with the company for 20\nyears; now he was terrified that McKinsey would get him fired.\nWhen the consultants came into his office, he was sweating bullets.\nAfter introductions were made, he asked the consultants if they"}
{"106": "shaking too much. He put the pot down and tried again\u2014still no\ngood. Finally, he had to place the rim of the cup hard against the\nbrim of the pot to get the coffee out.\nI offer this story to show just how unsettling an interview can\nbe. As the interviewer, someone who is investigating a business\nproblem, you carry power and authority. Not over a CEO or a top-\nlevel manager perhaps, but over many others. Imagine what you\nrepresent to, say, a store manager whose boss told her to talk to\nyou and who knows there are problems in her organization. I\nbelieve you have a professional responsibility to respect the inter-\nviewee\u2019s anxiety, to allay it, and not to take advantage of it.\nRespecting the interviewee\u2019s anxiety means not leaving him\nfeeling naked at the end of the interview, as if he\u2019s been the sub-\nject of a military interrogation. Remember that you\u2019re looking for\njust two or three things in any interview. You shouldn\u2019t need to\nsqueeze an interviewee dry to get them. Also, be circumspect about\nasking questions that, though perfectly appropriate in a business\ncontext, may touch on deeply personal matters from the inter-\nviewee\u2019s perspective. For example, your first question should\nprobably not be \u201cSo, what is it that you do, exactly?\u201d\nAllaying the fears of interviewees means demonstrating how\nthe process benefits them\u2014not just the interview, but the whole\nprocess of solving the organization\u2019s problems. If you make their\njob more efficient, that should benefit them. Likewise, if you\nimprove the profitability of their employer, that might work to\ntheir advantage too. Don\u2019t be afraid to offer a quid pro quo. Inter-\nviewees are giving you information; if you have information that\nyou can share with them, do so. Most people would rather know\nmore about what is going on in their organization.\nNot taking advantage of the interviewee\u2019s fears means resist-"}
{"107": "There is no need, in the first instance, to flash your authority like\na police badge. If you do that, you may find that, like a gangster\nin an old cops and robbers movie, the interviewee \u201cclams up.\u201d\nIf you run into real obstruction or hostility (see the next section),\nyou may have to bring your authority into play, but not until\nthen. Along with power comes the responsibility to use it wisely.\nDIFFICULT INTERVIEWS\nConduct enough interviews and you will encounter difficult\nones. Some of them are easy enough to handle, once you\nknow how. Others will test your strength and spirit.\nA major New York brokerage house, worried that it was lagging\nits rivals in profitability, called in McKinsey to do a complete\nreview of its many businesses. The stakes for the company and its\nexecutives were high; the prospect of mass firings loomed. The var-\nious players in the organization squared off into pro- and anti-\nMcKinsey camps and promoted their agendas vigorously.\nHamish McDermott, a newly promoted EM, had scheduled a\nmeeting with one of the broker\u2019s senior managers and his manage-\nment team. He walked into the man\u2019s office and introduced him-\nself. The man replied, \u201cHamish McDermott, eh? You\u2019re the \u2014\u2014\u2014\nthat\u2019s been telling the board I\u2019ve been refusing to meet my cost\nreduction target.\u201d\nSometime in your career, if you aggressively pursue solutions to\nbusiness problems, you will run into a situation like Hamish\u2019s."}
{"108": "His words were pretty shocking to me, especially since they\nweren\u2019t true. But I didn\u2019t get angry and I didn\u2019t back down.\nI simply explained that I thought he was mistaken and said\nthat we still had to have our meeting.\nHe did it partly because he was a difficult guy and partly\nto see if we would back down. If someone says something\nblatantly false like that, you have to challenge him; you can\u2019t\nback down.\nThis strategy worked very well for us. Afterwards several\nof his staff came and apologized that we had been insulted in\nthis way. They thought we had handled it with a great deal\nof dignity and strength; we established credibility with a\nnumber of important people in the organization, and that\nhelped us later on.\nThe limits to this strategy are the limits of your authority in\nthe organization. McKinsey consultants usually have the back-\ning of the top management at the client and can thus stand up to\nanyone. If you aren\u2019t so favored, just remember that if the per-\nson you are interviewing is more senior than the person who\nauthorized your project, you will probably have to back down\nwhen challenged.\nA less hostile, but equally difficult situation arises when inter-\nviewees refuse to give you information. They won\u2019t answer your\nquestion or give you access to relevant documents or data. When\nthis happens, it is time to \u201cpull rank.\u201d If you\u2019re there asking them\nquestions, then someone in your organization (or your client)\nwants you to be there. Let them know that. If they still refuse, play\nhardball. If necessary, call the boss on the phone there and then.\nYou probably won\u2019t have to; just the suggestion should open the"}
{"109": "You may also encounter difficulty when interviewing what psy-\nchiatrists might call the passive-aggressive type, but I like to call\n\u201cthe Sandbagger.\u201d Sandbaggers will talk all you want; they just\nwon\u2019t tell you anything, as one former McKinsey EM found out:\nI walked into this woman\u2019s office\u2014we had blocked out an\nhour for the interview\u2014and she told me that she could give\nme only half an hour. She then proceeded to talk for the next\n30 minutes about what shethought McKinsey did and why\nit was that McKinsey was there. When she ran out of steam\non that topic, she told me the story of her life. I couldn\u2019t get\na word in edgewise.\nHandling a Sandbagger requires an indirect approach. Often\nthe most useful strategy is finding someone else in the organization\nwho can tell you what you want to know. If the Sandbagger is the\nonly source for the information, then you may have to ask her boss\nto have a quiet word with her.\nThe last category of difficult interview is also the hardest to\nhandle. I can think of few more stressful situations than being face\nto face with someone who knows your work will likely get him\nfired\u2014and you know it too. Unfortunately, there is very little you\ncan do in such cases apart from playing the \u201cgood soldier.\u201d You\nhave to do your job, and you have to get the interviewee to help\nyou. It\u2019s for the good of the organization. You can\u2019t become angry\nor upset, despite the unfairness of it all. In a way, there is no effec-\ntive strategy. You just have to reach down inside yourself and push\non through. No one ever said life was fair."}
{"110": "ALWAYS WRITE A THANK-YOU NOTE\nWhen you get back to your office after interviewing some-\none, take the time to write a thank-you letter. It\u2019s polite and\nprofessional, and could pay you back in unexpected ways.\nIf your mother was like my mother, then when you were a kid you\nwere always told to write a thank-you note after you received a\ngift. I have a large extended family, so after holidays and birth-\ndays I spent what seemed like weeks writing letters to aunts and\nuncles and second cousins thanking them for whatever it was they\nhad sent me, whether I liked it or not. My mom would always be\nafter me, making sure I wrote those thank-you notes (as well as\nreading them and checking my penmanship). I didn\u2019t realize it\nthen, but this proved good training for my time at the Firm.\nAfter you\u2019ve taken half an hour or more out of someone\u2019s day\nto interview and get information from him, you should take the\ntime to thank him in writing. As my mom could tell you, it\u2019s\npolite. It shows that you value the interviewee\u2019s time as much as\nhe does. It\u2019s also professional. Sending someone a few choice\nwords below your corporate letterhead puts your company in\na favorable light.\nMy mom taught me to avoid boilerplate thank-you notes.\n\u201cDear \u2014\u2014\u2014, thank you for the \u2014\u2014\u2014. I will always cherish it\u201d\nwas unacceptable when I was a boy. It\u2019s still unacceptable today.\nThat doesn\u2019t mean each thank-you note you write needs to be a\ngem of immaculately crafted prose. Just make sure it doesn\u2019t read\nlike a computer-generated form letter. I keep the text of a basic"}
{"111": "it\u2019s worth it. It\u2019s also much easier than it was when I was 13, and\nhad to write all those thank-you notes by hand.\nSometimes a thank-you note can yield an unexpected payoff.\nEvery new McKinsey-ite hears the story of the associate who needs\nto interview a senior sales executive at an agricultural products\ncompany somewhere in America\u2019s heartland. When he calls the\nman saying he\u2019s from McKinsey and would like an hour of his\ntime, he receives an effusive welcome from the other end of the\nline. \u201cCome on down,\u201d he is told. When the associate arrives after\na long journey, the man shows him a letter on McKinsey stationery,\nfrom another McKinsey associate, thanking the executive for his\ntime on a day 15 years in the past. The letter has pride of place on\nthe man\u2019s office wall, along with his college diploma, hanging in\na frame.\nSometimes a little politeness goes a long way."}
{"112": "9\nBRAINSTORMING\nABOUT BRAINSTORMING AT M KINSEY\nC\nWhen the study has been sold, the team assembled, and the\npreliminary research done, the real work can begin. Brain-\nstorming is the sine qua non of strategic consulting. It\u2019s what\nthe clients really buy. Let\u2019s face it. Most large, modern cor-\nporations are chock full of intelligent, knowledgeable man-\nagers who are darned good at day-to-day problem solving.\nMcKinsey offers a new mindset, an outsider\u2019s view that is not\nlocked into \u201cthe company way\u201d of doing things. That\u2019s what\nclients need when problems cannot be solved within the orga-\nnization, and it starts in a meeting room with a table, some\nchairs, a bunch of pads, pens and pencils, some markers, and\na clean \u201cwhite board.\u201d\nBefore the first brainstorming session, McKinsey consul-\ntants do their homework. Everyone on the team reads the\nresults of the PDNet and library searches. The associates put\ntogether and distribute \u201cfact packs\u201d based on their prelimi-\nnary research. The ED, the EM, and possibly the more senior\nassociates on the team come up with initial hypotheses that\nthe team will then test to destruction.\nBrainstorming takes time. Typically, a McKinsey team\nblocks out two hours, if not more, for a brainstorming ses-\nsion. Some team leaders prefer weekends for their meetings,"}
{"113": "night, fueled by deliveries of pizza, Chinese food, or sushi\n(my personal favorite). I even recall some teams bringing in\na six-pack or two of beer for a weekend session (presumably\nto stimulate the flow of ideas). McKinsey\u2019s U.S. offices keep\n\u201cmenu books\u201d of the favored local food delivery services;\nthese books see a lot of use.\nThe most important ingredient for successful brainstorm-\ning is a clean slate. There\u2019s no point calling a meeting if\nyou\u2019re just going to look at the data in the same old way. You\nhave to leave your preconceptions and prejudices at the door\nof the meeting room. That way, you are free to manipulate\nthe facts in your mind.\nI like to think of brainstorming as playing with that old\npuzzle Rubik\u2019s Cube. Each fact is a face on one of the small\ncubes. Turn the faces this way and that, and you\u2019ll come up\nwith the answer, or at least an answer.\nAnother metaphor I like to use is shuffling a pack of\ncards. Each fact is a card. When you first open the pack, all\nthe cards are in order. How boring. Shuffle the cards, or\nthrow them into the air and see how they land. Now you\nmight find some interesting patterns: straights, flushes, full\nhouses. The same thing happens when you toss around facts\nand ideas.\nIn the next few lessons, we\u2019ll take a closer look at the var-\nious aspects of brainstorming \u00e0 la McKinsey, and learn a few\ntips to make your brainstorming more productive."}
{"114": "PROPER PRIOR PREPARATION\nAlthough brainstorming has an airy-fairy, college bull ses-\nsion connotation to some, in reality effective brainstorming\nrequires some hard-nosed advance work.\nThe cardinal rule of brainstorming is that you cannot do it success-\nfully in a vacuum. Before you go into that meeting, you have to\nknow something about the problem you\u2019ll be working on. Don\u2019t\njust stride into the meeting expecting to wow everyone with your\nbrilliance. As with all things McKinsey, there is a method to prepar-\ning for your brainstorming session, whether you are the leader (or,\nas some prefer, moderator or facilitator) or just a participant.\nIf you have followed the outline of Part Two sequentially\u2014that\nis, you\u2019ve completed your research\u2014then half your preparation is\ndone already. Now make sure that everyone on the team knows\nwhat you know. Put your research into what McKinsey-ites call a\n\u201cfact pack,\u201d a neatly organized summary of the key points and\ndata that you\u2019ve discovered, and circulate it to your team. If you\nare the leader, make sure that all your team members put their\nresearch into fact packs. Making a fact pack is easy. It doesn\u2019t\nrequire a detailed structure, just a little thought about what is\nimportant and how to show it. Once everyone on the team has\nread all the fact packs, you\u2019ll all have the same knowledge base\nwhen it comes time to generate ideas.\nOnce you have absorbed your team\u2019s fact base, what next?\nMcKinsey-ites fall into two camps on this subject. The first group\nsays, \u201cFamiliarize yourself with the outlines of the problem and"}
{"115": "hypothesis; otherwise, you waste too much time flailing around\nlooking for ideas.\u201d I come down firmly between these two asser-\ntions\u2014they\u2019re both right. If you can come up with a hypothesis,\nfine; if you\u2019re the team leader, you probably ought to have one. Just\ndon\u2019t march into the team room saying, \u201cThis is the answer.\u201d The\nright attitude is, \u201cI think this may be how things are. Let\u2019s attack\nthis hypothesis as a group.\u201d\nYou can also prepare by brainstorming on your own, ahead of\ntime. Rather than come up with a single hypothesis, get an idea of\nthe likely setof hypotheses that your team will come up with\u2014the\nsolutions that fit within the scope of your project. You can then\nquickly dismiss hypotheses that are unrealistic, while giving your\nteam free rein to work on ideas that are more plausible. This\napproach keeps your brainstorming grounded in reality, which, as\none former McKinsey director observed, is all that is out there.\nHowever you go about it, just make sure that, at the\nvery least, you know the facts. Remember the Boy Scout motto:\n\u201cBe prepared.\u201d\nIN A WHITE ROOM\nThe point of brainstorming is the generation of new ideas. So\nstart with tabula rasa\u2014a clean slate. When you get your team\ninto the room, leave your preconceptions at the door. Bring\nthe facts you know, but find new ways of looking at them.\nIn the last section, I suggested that you spend a few hours on your"}
{"116": "ing meeting without any preconceptions. If that seems contradic-\ntory to you, well brainstorming benefits from a few contradic-\ntions\u2014provided they help stimulate your thinking.\nAgain, brainstorming is about generating new ideas. If all the\nteam members come into the room saying the same old things and\nagreeing with one another, then you\u2019ve gained nothing and wasted\ntime. Even worse, if the team leader comes in and imposes her view\non everyone else, the team has missed an opportunity to achieve a\nsolution that\u2019s more creative and, possibly, better.\nBrainstorming requires the participation of everyone in the\nroom, from the most senior director to the most junior analyst\u2014\nand there\u2019s no guarantee on any given day that the former will have\nbetter ideas than the latter. No one should be afraid to speak his\nmind in the brainstorming room. So, along with your preconcep-\ntions, check your hierarchy and deference at the door.\nHere\u2019s an example of how not to run a brainstorming session.\nWhen Kristin Asleson was a new associate, the SEM on her\nengagement called the team members in for a brainstorming ses-\nsion. When they got there, the SEM said, \u201cJust be quiet and watch\nme work through the problem on the white board.\u201d They then sat\nthere for the next hour watching the SEM think to himself. It may\nhave been instructive, but it wasn\u2019t brainstorming, unless it was\nbrainstorming as theater.\nHere are a few more \u201crules of the road\u201d for successful\nbrainstorming.\nThere are no bad ideas. No one should ever hesitate to open\nher mouth during a brainstorming session for fear of getting zinged\nwith the words \u201cThat\u2019s a bad idea.\u201d If the idea was sincerely\nmeant, but you disagree with it, take a minute to explain why.\nDebating ideas is part of the brainstorming process. Who knows?"}
{"117": "directed at the problem at hand don\u2019t count\u2014for example, \u201cLet\u2019s\nforget about the problem and go play Frisbee\u201d (unless you think\nthe team might benefit from a quick game).\nThere are no dumb questions. Just as there are no bad ideas,\nany question should be taken on its merits. Never be afraid to ask\nwhy something is the way it is or is done the way it\u2019s done. Often\nthe answer is \u201cWell, gee, that\u2019s the way we\u2019ve always done it\u201d\u2014\nwhich is not a good reason to do much of anything.\nNever discount the benefits of working through seemingly\nobvious or simple questions. For example, when I was working\non an engagement for a money management company, at our first\nbrainstorming meeting, the brand-new associate on our team\nasked, \u201cHow much money is there in the world?\u201d Rather than just\nsaying \u201clots,\u201d we spent the next 45 minutes thinking through the\ndynamics of international money management and came out with\nsome useful insights.\nBe prepared to kill your babies. This rather shocking notion\noriginated among Hollywood screenwriters. It means that if your\nidea, no matter how good, is not part of the team\u2019s answer at the\nend of the session, dispense with it. Look upon your hypothesis as\njust one more datum to throw into the brainstorming mix. Offer\nit up to your teammates and let them knock it around. It may be\n\u201cright\u201d or it may be \u201cwrong,\u201d but the main thing is that it should\nhelp the team think through the problem at hand. Don\u2019t invest a\nlot of your ego in your hypothesis; don\u2019t come to the meeting pre-\npared to die in a ditch defending it.\nKnow when to say when. Brainstorming takes time, but if you\nstay at it too long, you\u2019ll hit the point of rapidly diminishing\nreturns. The consensus among former McKinsey-ites is that a team\ncan stand about two hours of brainstorming before the atmosphere"}
{"118": "owls, people become tired, cranky, and slow off the mark as the\nnight wears on. There are always exceptions, of course. Sometimes\nyou get on a roll and the adrenaline keeps you productive until well\npast midnight. Sometimes you gain insight by contemplating your\ncolleague\u2019s plate of leftover fried rice. In general, though, it\u2019s best\nto call a halt before the team starts coasting. There\u2019s always the\nnext day, or the weekend.\nIf you must hold an all-day session, you have to make\nallowances to keep the participants\u2019 energy level up. Let conver-\nsations go on tangents; allow people to make jokes and let off\nsteam, but rein them in after a little while to keep them focused.\nTake breaks every now and then\u2014and not just for lunch, dinner,\nand calls of nature. If you can take a half-hour walk somewhere,\ndo so. It\u2019s a great opportunity for people to gather their thoughts\nand stretch their legs.\nGet it down on paper. Unlike a regular meeting, where some-\none has the job of taking minutes, brainstorming doesn\u2019t lend itself\nto precise note taking. Ideas flit around the room like mayflies and\ncan die just as quickly. You cannot, under any circumstances, leave\nthe room and turn off the lights without a permanent record of\nthe outcome. Don\u2019t think that, in the rush of coming up with a bril-\nliant idea, you\u2019ll never forget it. Once the adrenaline wears off and\nfatigue sets in, you will.\nMcKinsey uses an excellent device to preserve the outcomes of\nbrainstorming sessions. Although practically every meeting room\nhas a white board and markers that wipe clean with an eraser or\npaper towel, some have white boards that can make paper copies\nof whatever is written on them. It\u2019s an excellent way to get the\nfinal copy of that brilliant idea or killer chart you sketched at\n2 a.m."}
{"119": "paper doesn\u2019t erase\u2014so be neat. At the end of the meeting, give\nsomeone the job of transcribing all the flipcharts onto regular\npaper and circulating copies to the team.\nSOME BRAINSTORMING EXERCISES\nThe key to successful brainstorming is good preparation\nand a proper frame of mind. Here are a few tricks that\nMcKinsey-ites use to get the maximum benefit from their\nbrainstorming sessions.\nWhile Kristin Asleson was at the Firm, she took part in an experi-\nmental training program on brainstorming, during which she\nlearned the following exercises. They can help you get your brain-\nstorming seminar off to a good start.\nThe Post-itTM exercise. Give everyone in the room a pad of\nsticky notes. The participants then write out any relevant ideas\nthey have, one idea per note, and hand them over to the leader,\nwho reads them aloud. This is a very good way to generate a lot\nof ideas quickly without getting bogged down in discussing each\none as it comes out.\nThe flipchart exercise. Put a number of flipcharts around the\nroom, each one labeled with a different category or issue. Each team\nmember then goes around the room writing ideas down on the\nappropriate flipchart. If you like, you can give each team member\na different colored marker, so you know whose ideas are whose.\nBellyaches up front. Kristin remembers a particularly effec-"}
{"120": "We invited all the relevant players into a large room to dis-\ncuss options for change at the client. We asked them to tell us\nup front everything they didn\u2019t like about the program we\nhad presented. Once they had vented, we asked them to\ncome up with things that were good about it, and ways that\nit could be implemented within their own business units\u2014\nthis required some \u201ctough love\u201d from our ED. The tech-\nnique worked in two ways: It yielded some excellent ideas\nthat we would not have otherwise come up with; and it\nhelped a previously skeptical, if not outright hostile, man-\nagement team buy into McKinsey\u2019s solution.\nOne more tip for handling a grumbler or rabble-rouser at a\nbrainstorming session: Have the leader or moderator stand behind\nhim, and even touch him on the shoulder occasionally. This lets the\ntroublemaker know that he is being watched. If he mutters an\naside, the moderator can ask him to speak up, rather like the\nteacher who tells the note-passing student, \u201cWhy don\u2019t you share\nit with the class?\u201d\nTry these exercises to jazz up your own brainstorming sessions.\nYou\u2019ll be impressed with the results."}
{"121": "This page intentionally left blank."}
{"122": "(cid:2)\n(cid:2)\nPART THREE\nTHE\nM KINSEY WAY\nC\nOF SELLING\nSOLUTIONS\n(cid:2) (cid:2)"}
{"123": "(cid:2)\n(cid:2)\nYou\u2019ve read Parts One and Two. You know how to\nthink about business problems and how to work effec-\ntively to come up with practical solutions to them. Now\nit\u2019s time to go forth and conquer the world, right? Not\nquite. The best solution, no matter how well researched,\nhow thoroughly analyzed, how glitteringly, flawlessly\nstructured, is worth exactly nothing if your clients don\u2019t\nbuy into it.\nTo get your clients to buy into your solution, you\nhave to sell it to them. That\u2019s what we\u2019ll cover in Part\nThree. You\u2019ll learn how to put together a presentation\nthat conveys your ideas to your audience. You\u2019ll dis-\ncover how to manage internal communications so\neveryone on your team can stay \u201con message.\u201d You\u2019ll\nfind out how to work with and manage your client\norganizations and the people who work in them\u2014the\ngood ones and the bad ones. And you\u2019ll learn how to\ntake your brilliant solution from paper to real life\u2014how\nto make change happen.\n(cid:2) (cid:2)"}
{"124": "10\nMAKING PRESENTATIONS\nABOUT PRESENTATIONS AT McKINSEY\nMcKinsey communicates with its clients through presenta-\ntions. They may be formal presentations: meetings held\naround boardroom tables with neatly bound blue books.\nThey may be informal presentations between a few managers\nat the client and a couple of McKinsey consultants with sev-\neral charts hastily stapled together into a deck. As junior\nmembers advance through the ranks at the Firm, they spend\na lot of time presenting ideas to other people.\nMcKinsey has become extremely good at communicating\nin this way. You can apply many of the Firm\u2019s techniques in\nyour own presentations. They will help you get your message\nacross\u2014which, after all, is the goal of the process."}
{"125": "BE STRUCTURED\nFor your presentation to succeed, it must take the audience\ndown the path of your logic in clear, easy to follow steps.\nWe went over the structure of the McKinsey problem-solving\nprocess at great length in Part One of this book (see \u201cFeel Free to\nBe MECE\u201d in Chapter 1). The rest of the world sees the McKin-\nsey structure most often through the medium of the Firm\u2019s presen-\ntations. They are where the rubber meets the road.\nA presentation reflects the thinking of the person or team that\nput it together. If your presentation is sloppy and muddled, your\naudience will assume that your thinking is also sloppy and mud-\ndled\u2014regardless of whether that is the case. So, whatever structure\nyou applied to your thought process, apply it to your presenta-\ntion. If you use the McKinsey structure, use it in your presentation.\nIf you prefer some other organizing principle, make sure your pre-\nsentation reflects it\u2014assuming, of course, that your thought\nprocess is structured and logical.\nLet me reiterate that you do not have to use the McKinsey struc-\nture if you are not comfortable with it\u2014if it is not the way you\nthink. A friend of mine at business school became an entrepreneur;\nlike many entrepreneurs, he was capable of brilliant insights and\nintuitive leaps, but his thinking was not particularly organized. He\nmade many successful presentations using the basic structure of\n\u201cTell \u2019em what you\u2019re going to tell \u2019em, tell \u2019em, tell \u2019em what you\ntold \u2019em.\u201d He followed a structure and it worked for him.\nUsually, if you adhere to a structure that makes a step-by-step"}
{"126": "the patience for this. One McKinsey EM faced the problem of a\nsenior manager at his client who, when handed a presentation doc-\nument, would invariably leaf through it from beginning to end and\nthen \u201ctune out\u201d for the rest of the meeting. But the EM found a\nsolution. For his team\u2019s final presentation, he handed the manager\na blue book with all the pages stapled together\u2014no more leafing.\nREMEMBER THAT THERE ARE DIMINISHING\nMARGINAL RETURNS TO EFFORT\nResist the temptation to tweak your presentation right up to\nthe last minute. Weigh the value of a change against a good\nnight\u2019s sleep for you and your team. Don\u2019t let the best be the\nenemy of the good.\nMcKinsey-ites are bound together by a set of common experiences:\ntraining programs, interviews, \u201call-nighters,\u201d and more. One of\nthe most common and most unnecessary experiences shared by\nalmost every associate in the Firm is the 4 a.m. vigil in the copier\nroom waiting for the presentation booklets to be put together for\ntomorrow\u2019s (although now it\u2019s today\u2019s) big progress review. I once\nspent a pleasant two hours early one morning removing one chart\nfrom 40 spiral-bound copies of a blue book and replacing it with\na new version, all because of one typo. Another associate and his\nEM worked through the night cutting and pasting new numbers\nonto a chart with razor blades and a glue stick (this was before\ncomputer graphics became widely available at the Firm)."}
{"127": "one wants to ride in an airplane where the engine mounting bolts\nfit almost correctly. However, when you are preparing a presenta-\ntion, even to the hardest-nosed CEO of the most powerful corpo-\nration, don\u2019t let the best be the enemy of the good. At some point,\nusually well before the actual presentation, nitpicking changes no\nlonger add value. Learn to recognize that point and draw the line\non changes well in advance of the meeting.\nThink about it this way: What matters more, that your team\ngets a good night\u2019s sleep before the presentation or that there is a\ntypo in the final document? Every document of any length will\nhave a few typos, no matter how hard you search and despite spell-\nchecking software (or, sometimes, because of it). On rare occa-\nsions, that typo may have to be corrected, but only rarely. Far\nbetter that you come to the presentation rested, not harried and\nbedraggled; giving a presentation is stressful enough as it is.\nDrawing the line on changes requires discipline. If yours is the\nfinal say on the presentation, you just have to discipline yourself.\nTell yourself and your team that you want the documents printed,\ncopied, bound, or transferred onto slides\u2014whatever it is that you\nneed\u2014at least 24 hours before the big moment. Spend the time\nbetween then and the presentation rehearsing, discussing possible\nquestions that may arise, or just taking a relaxed day at the office,\nif you can.\nIf, as is the case for any EM at McKinsey, you put together the\npresentation but someone senior to you (e.g., your ED) has the\nfinal say on the document, then you have to manage upward as\nstrongly as you can. Be firm in telling your boss that she must sign\noff on the document in time for you to meet your 24-hour dead-\nline. Some senior managers can\u2019t resist meddling up to the last\nmoment\u2014you have to resist for them."}
{"128": "PREWIRE EVERYTHING\nA good business presentation should contain nothing new for\nthe audience. Walk all the players at the client through your\nfindings before you gather them into one room.\nImagine it is the start of your final presentation. Your findings have\nbeen kept confidential to avoid any leaks into the market. You and\nyour team are meeting with the top executives of your company,\nwho are eager finally to hear your recommendations. Your boss is\nhere; your boss\u2019s boss is here; the heads of all your company\u2019s busi-\nness units are here; your CEO is sitting at the head of the table\nhanging on your every word.\nYou begin to speak. \u201cLadies and gentlemen,\u201d you say, \u201cafter\nweeks of exhaustive research, my team and I have reached the con-\nclusion that the future of our company requires us to increase our\ninvestment in widget production by 75 percent over the next two\nyears.\u201d As you reach for your first chart of backup analysis, a mur-\nmur is heard from the audience. The director of the gadgets divi-\nsion is incensed. Surely, he says, the future of the company lies with\ngadgets. The CFO protests that the company doesn\u2019t have that\nlevel of funds available. The president of the widgets subsidiary\nrushes to your defense. Your moment in the corporate sun dis-\nsolves into a shouting match. Clearly, not everyone likes surprises.\nTo avoid this disaster scenario, McKinsey consultants engage\nin \u201cprewiring.\u201d Before they hold a presentation or progress review,\na McKinsey team will take all the relevant players in the client\norganization through their findings in private. That way, there are"}
{"129": "various players through our findings beforehand. Otherwise, it was\njust too risky. In effect, the actual presentation became perfor-\nmance art.\u201d\nWhen prewiring, you must remember the cardinal rule of being\na successful consultant or corporate troubleshooter: Not only do\nyou have to come up with the \u201cright\u201d answer; you also have to\nsell that answer to your client. Sometimes, this just requires sales-\nmanship; other times, it takes compromise. Suppose you walk into\nthe office of Bob, the Director of the Gadgets Division, and tell him\nthat you think the answer is to invest more in widgets, at the\nexpense of gadgets. He is unlikely to be pleased, but when you are\nalone with him in his office, you are far more likely to be able to\ntake him through your analysis step by step.\nBy the end of the process, the gadgets director may be con-\nvinced (great, move on to the next person) or he may come up\nwith some fact you hadn\u2019t known about that alters your recom-\nmendation (which does happen, believe me), or he may refuse to\naccept your recommendation without some changes. In the last\ncase, you have to negotiate. If the compromise is small, make it\nand move on; if his demands are too great, you will have to fig-\nure out a way to bypass him. Of course, if he throws you out of his\noffice (unlikely, but possible), you have a problem on your hands\nthe size of which is in proportion to the director of gadgets\u2019 power\nin the organization.\nLet\u2019s go back to our scenario at the beginning of this section.\nThis time, though, imagine that you have prewired your presenta-\ntion with all the senior managers at the table, including the unre-\nlenting director of gadgets. \u201cLadies and gentlemen,\u201d you say,\n\u201cafter weeks of exhaustive research, my team and I have reached\nthe conclusion that the future of our company requires us to"}
{"130": "tor of gadgets says, \u201cI\u2019ve heard this before, and it\u2019s horse \u2014\u2014\u2014.\nWe have to increase our gadget production.\u201d The CFO raises an\neyebrow, but says nothing\u2014you\u2019ve already shown him how he can\nfund the additional investment. The SVP of the widget subsidiary,\nsecure in the knowledge that she will be the winner at the end of\ntoday\u2019s presentation, merely looks in the direction of the CEO. The\nCEO leans back in his chair, steeples his fingers, and tells the gad-\ngets director: \u201cNow come on, Bob, I think we\u2019re all pulling on the\nsame oar here. Let\u2019s get through the presentation, then we can dis-\ncuss it at the end.\u201d You already know what that outcome will be.\nIsn\u2019t it better to skip the surprise ending?"}
{"131": "This page intentionally left blank."}
{"132": "11\nDISPLAYING DATA\nWITH CHARTS\nABOUT CHARTS AT McKINSEY\nMcKinsey relies on charts, graphical representations of infor-\nmation, as a primary means of communicating with its\nclients. The Firm has devoted a lot of time and effort to dis-\ncover what works with charts and what does not. You can\nfind most of this wisdom in a book by Gene Zelazny, the\nFirm\u2019s guru of charts and presentations, entitled Say It With\nCharts.* It\u2019s an excellent resource and I don\u2019t intend to repeat\nits contents here.\nIn this chapter, I explain the overarching McKinsey phi-\nlosophy of charts, and why it will work for you. I also share\nwith you the one McKinsey chart that I have never seen used\noutside the Firm."}
{"133": "KEEP IT SIMPLE \u2014 ONE MESSAGE PER CHART\nThe more complex a chart becomes, the less effective it is at\nconveying information. Use charts as a means of getting your\nmessage across, not as an art project.\nWhen I started at the Firm, the first pieces of equipment I was\nissued were a box of mechanical pencils, an eraser, and a set of\nruled, plastic templates with cutouts for various shapes: circles,\nrectangles, triangles, arrows, and so forth. \u201cDon\u2019t lose those tem-\nplates,\u201d I was told. \u201cThey\u2019re expensive to replace and you\u2019ll need\nthem to draw your charts.\u201d This was in 1989, hardly the Stone\nAge, and for years I had been using computer graphics to draw\ncharts and graphs in my previous jobs and at business school. It\nstruck me as slightly primitive: evidence of a corporate culture that\nwas inflexible in the face of advancing technology.\nI was partly right, for McKinsey\u2019s culture is strong and thus\nslow to change, but I was also partly wrong; those templates\nserved a very important purpose: to keep our charts simple.\nComputer graphics make it too easy to get fancy. The Firm uses\ncharts as a means of expressing information in a readily under-\nstandable form. The simpler things are, the easier they are to\nunderstand. Therefore, McKinsey prints its charts in black and\nwhite; it avoids three-dimensional graphics unless absolutely nec-\nessary to convey the message; and it adheres to the cardinal rule\nof one message per chart. The first two strictures are visual. The\nmedium must not overpower the message; hence the ban on dis-\ntracting colors or deceptive 3-D perspectives. The rule of one"}
{"134": "The information in a chart may be highly complex and expres-\nsive of multiple points or ideas; the job of the chartist is to pick\nwhich point to make. McKinsey consultants do this with the\n\u201clead,\u201d the caption at the top of the chart. A good lead expresses\nthe point of the chart in one simple sentence (see Figure 11-1). The\nsalient information in the chart may be highlighted, with a differ-\nent shading, an exploded pie slice, or (as I\u2019ve done here) with an\narrow, among other methods. If a chart offers several insights,\ncopy it with a new lead and the relevant information highlighted\n(see Figure 11-2).\nAlso, look in the lower left corner of either chart. You\u2019ll see a\nsource attribution. McKinsey charts always include one. Why? So\nthat when people ask, \u201cWhere did you get this information?\u201d you\nACME Widgets ran an operating loss last year\nACME WIDGET CORPORATION INCOME STATEMENT 1998\n($US Millions)\n150 \u2013170\n10 \u20132 6\n\u201320 18\nNet Widget Gain on Taxes Net Income\nSales\nSale of\nOperating Earnings Interest Gadgets\nExpenses Before Income Subsidiary\nInterest (Net)\nand Taxes\nSource: Acme Widget Corporation annual report."}
{"135": "We would have posted a loss in 1998 without the sale of our\ngadgets divison\nACME WIDGET CORPORATION INCOME STATEMENT 1998\n($US Millions)\n150 \u2013170\n10 \u20132 6\n\u201320 18\nNet Widget Gain on Taxes Net Income\nSales\nSale of\nOperating Earnings Interest Gadgets\nExpenses Before Income Subsidiary\nInterest (Net)\nand Taxes\nSource: Acme Widget Corporation annual report.\nFigure 11-2. A new lead offers different insight into the same information.\ncan tell them. Also, if at some future time you (or anyone else)\nwant to review the data, you\u2019ll know where to look.\nOne final word about charts: Too many will bore your audi-\nence. Use the absolute minimum necessary to make your point, or\nyou may find that your audience hasn\u2019t absorbed the last 10 to 15\npages of your presentation.\nTime has marched on since 1989, technology has advanced,\nand McKinsey has learned to live with computer graphics. Asso-\nciates can now draw their charts in PowerPointTM, and occasionally\nyou\u2019ll see a McKinsey chart with a bit of color. But the Firm still\nadheres to the admirable principle of simplicity."}
{"136": "USE A WATERFALL CHART TO SHOW THE FLOW\nThe waterfall chart\u2014seldom seen outside McKinsey and not\ngenerally available in computer graphics packages\u2014is an\nexcellent way to illustrate quantitative flows.\nWhen you looked at each sample chart in the last section, you\nprobably wondered exactly what type of chart you were seeing. It\nlooked something like a column chart, but not like any you\u2019d find\nin the chart libraries of Excel\u2122, Freelance\u2122, or any of the other\npopular computer graphics packages. The chart may have looked\nstrange, but I\u2019ll bet you didn\u2019t have a lot of trouble understanding\nit. It\u2019s called a waterfall chart, and McKinsey-ites use it all the time,\nalthough it\u2019s rarely found anywhere else.\nWhen I asked former McKinsey-ites what lessons they learned\nabout charts, one thing they all mentioned was waterfall charts.\nThey loved them, and sometimes used them in their own work,\nbut seldom saw them elsewhere. In my modest ambition to make\nthe world a better place, I herewith offer the secret of the water-\nfall chart.\nThe waterfall chart is an excellent method of illustrating how\nyou get from number A to number B. The charts in Figures 11-1\nand 11-2 depict a simplified income statement, starting with sales\non the left and ending with net income on the right, and show the\nvarious items that lead from one to the other. The starting point\n(sales in the example) is always a column that begins at zero. Pos-\nitive items such as interest income are depicted as columns that\nstart at the high point of the preceding column and reach upward."}
{"137": "total is the distance from the top of the last item (or bottom if that\nitem is negative) to the zero line. Subtotals can be included along\nthe way in the same manner.\nWaterfalls can depict static data (balance sheets, income state-\nments) or active data (time series data, cashflows). You can mix\nnegative and positive items (e.g., we started with 6 accounts,\ngained 3 in the first quarter, then lost 2 in the next quarter, for a\ntotal of 7), or you can segregate them to show, say, where value is\ncreated and where it is destroyed (e.g., we make money in wid-\ngets, gadgets, and thingamajigs; we lose money in flummeries,\nfrankincense, and myrrh).\nWhatever data you use, the waterfall chart is a versatile way\nto convey a lot of information in a clear, concise manner. So go\nwith the flow."}
{"138": "12\nMANAGING INTERNAL\nCOMMUNICATIONS\nABOUT INTERNAL COMMUNICATIONS\nAT McKINSEY\nThe success of a team-based operation depends on open com-\nmunication, both from the top down and from the bottom\nup. McKinsey has the same methods of internal communica-\ntion as those available to any modern organization: voice\nmail, e-mail, memos, meetings, the water fountain, and so\nforth. It is fair to say that in the area of internal communica-\ntions the Firm as a whole has no new insights to offer. On the\nother hand, former McKinsey-ites, in their many combined\nyears of experience, have garnered a number of useful meth-\nods for managing internal communications that you can use."}
{"139": "KEEP THE INFORMATION FLOWING\nInformation is to your team what gasoline is to a car\u2019s engine.\nIf you choke off the flow, you\u2019ll stall.\nElsewhere in this book, I\u2019ve mentioned the famous Mushroom\nMethod of management: \u201cKeep them in the dark, cover them\nwith manure, and see what crops up.\u201d Most people don\u2019t realize\nthat the Mushroom Method can operate in both directions;\nit is possible to keep your boss in the dark too. No matter which\ndirection the manure is dumped, the Mushroom Method is unpro-\nductive. For a successful team operation, you have to keep the\ninformation flowing.\nMake sure your team is up to date with at least the broad out-\nlines of your project. This is especially true of large undertakings.\nBeing \u201cin the loop\u201d will help your teammates understand how\ntheir work is contributing to the final goal, how their efforts are\nworthwhile. Conversely, when people feel that they are working\nin a vacuum, that they are alienated from the greater enterprise,\ntheir morale is sure to suffer. Also, if you keep your teammates up\nto date, they\u2019ll return the favor. At the ground level, they may be\ncloser to events than you are. Good information flow can help you\nspot emerging problems (or opportunities) faster.\nAlways keep your boss up to date with your team\u2019s progress.\nDon\u2019t think that your boss will stay out of your way if you keep\nher in the dark. She\u2019ll be far more comfortable when she knows\nthat everything is under control. If things are not under control,\nthen you want your boss to know exactly what the problems are so"}
{"140": "message (whether in the form of voice mail, e-mail, or memo) and\nthe meeting. I\u2019ll suggest some tips for successful messages in the\nnext section. Right now, let\u2019s concentrate on meetings.\nMeetings form the glue that holds your team together. Team\nmeetings allow excellent information flow, in all directions, and\nprovide a certain amount of social bonding. They help remind\nthose present that they are part of a team. Suzanne Tosini, a former\nMcKinsey EM and now a senior manager at Freddie Mac, noted\nthat one of the keys to a successful meeting is making sure every-\none attends. To ensure that people do show up, make team meet-\nings a regular item on everybody\u2019s schedule. If you have nothing\nto discuss, then cancel the meeting (as far in advance as possible);\nyour teammates can always find a use for the extra 45 minutes.\nSuzanne\u2019s other two keys for a successful meeting are an\nagenda and a leader. Keep the number of items on your agenda to\nthe minimum needed to make sure everyone is up to date with\nimportant events, issues, and problems. If something can be put on\nhold for another time, it probably should. If you are the leader,\nmake sure you cover your agenda as briskly as possible: Frequent\nmeetings are good, unnecessarily long ones are not.\nOne other method of internal communication is in a class by\nitself: learning by walking around. Some of the most valuable con-\nversations in my experience resulted from random encounters\u2014in\nthe corridors, at the water cooler, on the way to lunch, at the Firm,\nor at the client. You can gain a lot just by wandering around and\ntalking to people, and they can learn a lot from you. Never under-\nestimate the value of the random fact.\nHowever you choose to communicate with your team, make\nsure that you do so frequently and openly. You will boost your\nteam\u2019s efficiency and morale, as well as your boss\u2019s peace of mind."}
{"141": "THE THREE KEYS TO AN EFFECTIVE MESSAGE\nA good business message has three attributes: brevity, thor-\noughness and structure. Include all three in every voice mail,\ne-mail or memo you send and you\u2019ll get your message across.\nA message, whether it\u2019s an e-mail, a voice mail, a memo, or a\nsticky note covered in cramped handwriting, is a presentation in\nminiature\u2014a means of conveying information to an audience. As\nsuch, an effective message shares the same properties as an effec-\ntive presentation: It is brief, covering onlythe points the audience\nneeds to know; it is thorough, covering all the points the audi-\nence needs to know; and it has a structure that conveys those\npoints clearly to its audience.\n1. Brevity. Brevity, or rather the lack of it, is much more of a\nproblem in spoken than written communications. Many business-\npeople can write concise memos, but how many can record a con-\ncise voice mail? To join that select group, think before you speak\n(or write). Whittle down your message to the three or four points\nthat the audience needs to know. If necessary, write these things\ndown on paper. Some McKinsey-ites write the entire message out\nlike a script before sending a voice mail to their ED or DCS. I think\nthat\u2019s going a bit too far\u2014just the bullet points will do.\n2. Thoroughness. Make sure your message contains everything\nyour audience needs to know. You are not trying to keep your audi-\nence in suspense. Don\u2019t just tell your boss, \u201cI\u2019m doing X, Y, and Z.\nCall me if you have any questions.\u201d Tell her not only what you are\ndoing, but what the issues are and what your thoughts are on them."}
{"142": "3. Structure. To be readily understood, a message must fol-\nlow a structure, and that structure must be readily apparent to the\naudience. Even if you\u2019re just writing a one-page e-mail or leaving\na 30-second voice message, a simple structure will help your mes-\nsage get through. It can be as basic as this:\nWe have three problems. In order of increasing impor-\ntance, they are:\n1. Our widgets are too expensive.\n2. Our sales force is incompetent.\n3. Our widget factory was just destroyed in a freak\nmeteorite impact.\nSometimes McKinsey-ites can take structuring their messages a\nbit too far. One EM in the New York office was reputed to put her\nshopping lists in Firm format. Another left affectionate messages on\nhis wife\u2019s answering machine\u2014following the McKinsey structure.\nAlthough you needn\u2019t follow the extreme examples of these\noverzealous McKinsey-ites, in your business communications you\nwould do well to remember the three keys to effective messaging.\nALWAYS LOOK OVER YOUR SHOULDER\nYou cannot be an effective consultant if you don\u2019t maintain\nconfidentiality. Know when you can talk and when you can\u2019t.\nBe just a bit paranoid.\nDuring my first week at McKinsey, I went through a short orien-"}
{"143": "mentioned that when he stayed over at his girlfriend\u2019s apartment,\nhe kept his briefcase locked. At the time, that struck me as a bit\nextreme. After all, if you can\u2019t trust your girlfriend, whom can you\ntrust? (OK, I was young and na\u00efve at the time.) It was only after I\nhad been at the Firm a bit longer that I realized quite how seriously\nMcKinsey takes confidentiality.\nMcKinsey\u2019s corporate culture continually reinforces confiden-\ntiality. We always kept it in the back of our minds. If we were on\na plane, we didn\u2019t take client information out of our briefcases and\nwork on it; we never knew who might be sitting next to us\u2014a\ncompetitor, a journalist, maybe even someone from your client. If\nwe needed those three hours to work, it was our tough luck.\nWe never mentioned our clients by name outside the office,\nand sometimes not even at the Firm. McKinsey often works for\nmore than one client in an industry, so some information had to\nbe kept even from fellow consultants. We often used code words\nwhen discussing our clients, though not always successfully. One\nEM from Germany recalled coming home to find a note from his\ngirlfriend (who worked for a competing consulting firm) saying\nthat the dinner for Code A (pronounced, in German, as \u201ccode\nah\u201d) would take place at a fancy Munich restaurant. In fact, the\nname of the client company was Coda; the caller had used the\nclient\u2019s real name, although luckily he had been misunderstood.\nThe EM was not pleased.\nYou may not need to maintain confidentiality while working\non your business problem\u2014then again, maybe you do. Ask your-\nself a few simple questions: What would happen if you were sitting\non an airplane and one of your competitors saw what you were\nworking on? How about someone from your company who wasn\u2019t\ninvolved in the project? How about your boss?"}
{"144": "desk and file cabinets before you leave for the night. Don\u2019t talk\nabout the specifics of your work outside your team. (You can tell\nyour significant other only if he or she doesn\u2019t pose a security risk.)\nDon\u2019t take out sensitive material in public\u2014sensitive means any-\nthing that a competitor or journalist might find interesting. Be\nmindful of what you say over the phone, and be extra careful when\nsending faxes, e-mail, and voice messages: They can very easily end\nup in the wrong hands."}
{"145": "This page intentionally left blank."}
{"146": "13\nWORKING WITH CLIENTS\nABOUT WORKING WITH CLIENT TEAMS\nIt goes without saying that without clients there would be no\nMcKinsey. They pay the (enormous) bills that keep the Firm\ngoing. It is not, therefore, surprising that McKinsey-ites are\nalways told to put the client first. Hamish McDermott\nremarked that there was one true hierarchy at McKinsey:\nclient, firm, you (in descending order).\nIn this chapter, we will cover the two different aspects of\nworking with clients the McKinsey way. We will start with\ntechniques to get the most out of a client team, the people\nfrom the client organization who work with McKinsey to\nreach a solution; we\u2019ll also look at ways to keep a client\nteam from doing more harm than good. We will then move\non to managing the client\u2014in McKinsey\u2019s case, the senior\npeople at the client organization who had called in the Firm\nto begin with. You will learn how to keep your clients\nengaged and supportive of your efforts and also how to\nmake sure your solution actually gets implemented rather\nthan gathering dust on a high shelf.\nFor some readers, the issue of client teams may seem\nremote. After all, if you are not a consultant, when will you\nactually have to deal with client teams? The answer is\nsooner than you might think. As a problem solver in a large"}
{"147": "on a joint venture with a team from an entirely different\norganization. In that case, you will, I hope, find the\ndiscussion of client teams as useful as the discussion of\nmanaging your client."}
{"148": "KEEP THE CLIENT TEAM ON YOUR SIDE\nWhen you\u2019re working with a client team, you and the team\nhave to work together or you won\u2019t work at all. Make sure\nthat members of the client team understand why their efforts\nare important to you and beneficial for them.\nThe first thing to do when working with a client team is get them\non your side. Make sure that they want to help you. At McKinsey,\nwe learned that the key to keeping the client teams on our side was\nto turn their goals into our goals. They have to remember that if\ntheir mission fails, the McKinsey mission fails and if the McKinsey\nmission fails, their mission fails.\nMembers of the client team must also realize that working with\nMcKinsey will be a positive experience for them. They must be made\nto understand that, at a minimum, they\u2019ll learn things that they\nwould never otherwise know and that will help them in their careers.\nThey\u2019ll also have a chance to make real change happen in their orga-\nnization\u2014a rare experience in most people\u2019s working lives.\nFor example, when I was working on a reorganization project\nfor a Wall Street broker, my team worked with a client team made\nup of people from the IT department. One particular member of\nthe client team, whom I\u2019ll call Morty, was a mainframe computer\nprogrammer, and looked the part. He stood about five and a half\nfeet tall, in his business shoes, wore thick glasses and a suit that\nnever quite fit; he lived with his parents in Brooklyn. Morty didn\u2019t\nreally want to be on the client team; he had far too much \u201creal\u201d\nwork to do."}
{"149": "traders\u2014the people at the front end of the business. He got to\nask them questions and find out what they thought his depart-\nment was supposed to be doing. Morty learned how to apply his\nown skills to solve problems he would not usually see in his day-\nto-day work. He also became noticeably more confident and out-\nspoken during meetings as the study progressed. Working with\nMcKinsey was an eye-opener for Morty, and he loved it (espe-\ncially since he didn\u2019t have to write the interview notes; that job\nwas left to me).\nI\u2019ll close this section with a final note on the subject, and an\napparent contradiction of something I wrote earlier (see \u201cA Little\nTeam Bonding Goes a Long Way\u201d in Chapter 5). Team-bonding\nactivities really add value when working with client teams. Since\nthe client team is not bound by the same shared experiences as the\nMcKinsey team, a little social interaction between the two can make\nworking together a lot easier. A trip to the ballpark or dinner at a\ngood restaurant (when people put away their \u201coffice faces\u201d) can help\nthe members of each team realize that the others are real people too.\nHOW TO DEAL WITH \u201cLIABILITY\u201d CLIENT\nTEAM MEMBERS\nYou may find that not everyone on the client team has the\nsame abilities or goals as you do. Get \u201cliability\u201d members off\nthe client team if you can; otherwise, work around them.\nThere are two kinds of \u201cliability\u201d members on a client team: the"}
{"150": "On an engagement for a large New York bank, my team\nworked with a client team staffed with senior managers from var-\nious departments within the client organization: lending, investing,\nback office, and so on. Our member from the back office was a\nman I\u2019ll call Hank.\nHank was, shall we say, a diamond in the rough. He stood 6'4\"\ntall and looked like a former football player who had let himself\ngo\u2014which, in fact, he was. His ties never matched his shirts and he\ninvariably had food stains on his suit coat. Also, Hank knew his\narea of the bank inside and out and was probably as smart as any\nmember of the McKinsey team.\nHank didn\u2019t want to work with McKinsey. He thought that the\nFirm peddled an expensive line of baloney to credulous clients and\nleft the employees to clean up afterward He didn\u2019t want to be on\nthe client team\u2014he had real work to do. Still, his boss had assigned\nhim to the team, so he showed up every day, and stubbornly\nrefused to contribute. In short, Hank was useless.\nHow do you handle a Hank, or someone who is just too dumb\nor incompetent to do the work required of him? As a first (and eas-\niest) tactic, you can try to trade the liability out of your team and\nget somebody better.\nTrading doesn\u2019t always work, however; there might be no one\nbetter available and you\u2019re stuck with your own Hank. In that\ncase, you have to deal with Hank. Work around him. Give him a\ndiscrete section of the work that he can do; make sure it is neither\ncritical to the project nor impossible for anyone else on the team to\ndo. You\u2019ll have to rely on the other members of the team pick up\nthe slack.\nFor all his faults, Hank was better than Carlos. A superslick\noperator (BA, Oxford; MBA, Harvard) from Argentina, Carlos"}
{"151": "los had the patronage of a board-level faction within the client\ncompany that did not want McKinsey there; these board members\nfelt they knew which direction McKinsey would recommend and\nthey didn\u2019t like it.\nCarlos subtly but actively prevented us from getting our job\ndone. He sent us down blind alleys; he bad-mouthed us to the\nboard behind our backs; he sabotaged us during presentations. We\nquickly realized that Carlos was not our friend.\nHandling a Carlos, or any hostile client team member, is trick-\nier than dealing with a Hank. Again, the best tactic is to trade the\nsaboteur out of your team, but that\u2019s usually not feasible. If you\nhave a Carlos on board, it\u2019s because someone powerful in the orga-\nnization wants him there. The next best solution is to work around\nspies and saboteurs. Make use of their talents where you can and\nkeep sensitive information out of their hands when possible. If you\nknow who is behind the spy, find out what the ring leader\u2019s agenda\nis\u2014maybe you can use that to your advantage when it comes time\nto sell your solution.\nIn our case, we had to leave Carlos to our ED, who had the\npolitical skill and muscle to handle him. Even then, Carlos\nremained a thorn in our side throughout the engagement.\nLiability clients needn\u2019t be a disaster. Sometimes, you can\neven polish the rough diamonds. In Hank\u2019s case, after several\nweeks of working together, we managed to bond him to the team\nand got him to understand and, at least partially, buy into the\nMcKinsey way of problem solving. In the end, he did contribute\nto our solution."}
{"152": "ENGAGE THE CLIENT IN THE PROCESS\nIf the client doesn\u2019t support you, your project will stall. Keep\nyour clients engaged by keeping them involved.\nTo succeed as a management consultant or a business trou-\nbleshooter you must keep your client\u2014be it your boss or the man-\nagement of an organization that has hired you from the\noutside\u2014engaged in the problem-solving process. Being engaged in\nthe process means supporting your efforts, providing resources as\nneeded, and caring about the outcome. With engagement thus\ndefined, it is hard to imagine how any project could succeed with-\nout an engaged client.\nThe first step in keeping your clients engaged is to understand\ntheir agenda. Clients will support you only if they think your\nefforts contribute to their interests. Remember that their interests\nmay change over time. Frequent contact and regular updates\u2014\neven if it\u2019s just by memo\u2014will help you keep in touch with your\nclients and keep your projects \u201ctop of mind\u201d for them. Get on a\nclient\u2019s calendar up front. Schedule progress meetings with tenta-\ntive topics; if you need to reschedule, do it later.\nEarly \u201cwins\u201d (see \u201cPluck the Low-Hanging Fruit\u201d in Chapter\n3) will generate enthusiasm for your project\u2014the bigger, the better.\nThey give your clients something to sink their teeth into and make\nthem feel included in the problem-solving process. The long-run\nreturns on your work will be much greater if your clients feel that\nthey were involved in reaching the solution and that they under-\nstood it, rather than being handed the solution neatly wrapped and"}
{"153": "outside consultant, you will never get credit for your best work.\nIf your solution is truly effective, the client organization will claim\nit for its own. Suzanne Tosini saw that firsthand as an associate\nat McKinsey:\nI developed a huge cashflow model that the client was going\nto use to evaluate real estate acquisitions. I had spent months\non this project; it had been a Herculean effort. The client\nteam members did some work on it, but essentially it was my\nmodel. When the time came to roll out the model, at a train-\ning program for the senior people in the acquisitions depart-\nment, the client team members got up and talked about the\nmodel that they developed. I was sitting in the back and I\nthought, \u201cHey, that\u2019s my model.\u201d But then I realized that it\nwas much better for them to think that it was theirs. It was\nnot McKinsey\u2019s model, it was not Suzanne\u2019s model\u2014it was\ntheir model.\nIn truth, that\u2019s not such a bad thing.\nGET BUY-IN THROUGHOUT THE\nORGANIZATION\nIf your solution is to have a lasting impact on your client, you\nhave to get support for it at all levels of the organization.\nIf you come up with a brilliant solution, structure it logically,\nand present it to your client with clarity and precision, then your"}
{"154": "tance for your solution from everyone in the organization that\nit affects.\nFor instance, suppose you tell your board of directors that they\ncan boost widget profitability by reorganizing the widget sales\nforce and streamlining the widget production process. Your argu-\nment is compelling; the board ratifies your suggestion; champagne\ncorks pop and cigars ignite. One slight hitch remains: What do the\nsales force and the production-line workers think about all this?\nIf they don\u2019t like your ideas, if they put up a fight, then your solu-\ntion will not be implemented. It will end up on the great remainder\nshelf of business, right next to the Betamax.\nTo avoid this dire fate, you must sell your solution to every\nlevel of the organization, from the board on down. After you\u2019ve\npresented to the board, present to middle-level managers. They will\nprobably have day-to-day responsibility for implementing, so let\nthem know what\u2019s going on. Don\u2019t neglect the people on the line,\neither. The changes you recommend may have the greatest effect on\nthem, so their buy-in is vital to a successful implementation.\nFinally, serial presentations give the junior members of your team\na good opportunity to hone their presentation skills.\nTailor your approach to your audience. Don\u2019t make the same\npresentation to, say, the fleet drivers as you would to the CEO.\nAt the same time, respect your audience. Explain what is being\ndone and why. Show people the entire picture. Let them know\nhow their jobs fit into the organization as a whole. They\u2019re not\nstupid; they\u2019ll understand. Treat them with respect (remember, a\nlot of the time they don\u2019t get any) and they will respond positively\nmost of the time."}
{"155": "BE RIGOROUS ABOUT IMPLEMENTATION\nMaking change happen takes a lot of work. Be rigorous and\nthorough. Make sure someone takes responsibility for getting\nthe job done.\nImplementing recommendations for change is a big subject. Whole\nbooks can be (and have been) written on it. I will limit myself here\nto explaining a few ground rules that McKinsey consultants have\nlearned for implementing change.\nTo implement major change, you must operate according to a plan.\nYour implementation plan should be specific about what will happen\nand when\u2014at the lowest possible level of detail. Don\u2019t just write:\nWe must reorganize the widget sales force.\nInstead, write:\nWe must reorganize the widget sales force.\n\u2022 Hold training sessions for all sales regions (Start: March\n1. Responsibility: Tom.)\n\u2022 Reallocate sales staff to new sales teams by customer type.\n(Start: March 15. Responsibility: Dick.)\n\u2022 Take new sales teams to call on top 20 customers. (Start:\nApril 1. Responsibility: Harriet.)\nOne former EM gave a no-holds-barred recipe for a successful\nimplementation plan:\nState what needs to be done, and when it needs to be done"}
{"156": "Enough said.\nMake specific people responsible for implementing the solu-\ntion. Be careful about whom you pick. Make sure people have the\nskills necessary to get the job done. Enforce your deadlines and\ndon\u2019t allow exceptions unless absolutely necessary.\nThe right point person can make implementation a very smooth\nprocess. If that person is not going to be you, make sure you pick\nsomeone who can \u201ckick butt and take names.\u201d At one McKinsey\nclient, an international bank, the managers chose a rather frighten-\ning fellow named Lothar to implement a major change program in\ntheir back-office processing. Lothar, who looked and sounded a bit\nlike Arnold Schwarzenegger, had a very simple technique for getting\nthe job done. Using the detailed McKinsey implementation plan,\nhe assigned specific tasks to members of his team. Every two weeks\nthe team would meet, and anybody who had not accomplished his\nor her tasks for the period had to explain the failure to the entire\ngroup. After the first meeting, when a few of the team members had\nundergone a grilling from Lothar, no one ever missed a deadline.\nWhen, after a few months, the McKinsey EM rang Lothar for\nan update, he replied, \u201cEveryone talks about how tough it is to\nimplement. Seems pretty easy to me.\u201d"}
{"157": "This page intentionally left blank."}
{"158": "(cid:2)\n(cid:2)\nPART FOUR\nSURVIVING\nAT\nM KINSEY\nC\n(cid:2) (cid:2)"}
{"159": "(cid:2)\n(cid:2)\nIn Part Four, you will learn a few tricks for surviving\nnot just at McKinsey, but in any high-pressure organi-\nzation. Whether you\u2019re trying to maintain your sanity\nwhile traveling for weeks at a time, trying to climb the\ngreasy pole to success in your organization, or just trying\nto have a life while working 100 hours per week, there\u2019s\nsomething in Part Four that will help. As a bonus, I\u2019ll\nalso shed a little light on the McKinsey recruiting\nprocess and even give a few tips for those of you who\nwould like to try to join the Firm yourselves.\nContrary to what you might imagine after reading\nthis far, there is more to life at McKinsey than work.\nThen again, there\u2019s not all that much more, which is\nwhy Part Four is so short.\n(cid:2) (cid:2)"}
{"160": "14\nFIND YOUR OWN MENTOR\nTake advantage of others\u2019 experience if you\ncan. Find someone senior in your organiza-\ntion to be your mentor."}
{"161": "As Tarzan once remarked, it\u2019s a jungle out there. To make your\nway through the corporate rain forest, it helps to have a guide,\nsomeone more experienced than you who can show you the hidden\ntracks and steer you clear of the quicksand. These days, the fash-\nionable word for such a guide is mentor.\nMcKinsey maintains a comprehensive system of mentoring for\nits client service staff. Every consultant, from analyst to director,*\nis assigned a mentor to monitor and guide his or her career\nthrough the Firm. On the face of it, this sounds like a wonderful\nidea; it certainly seemed that way to me when, as an MBA, I was\nconsidering whether to join McKinsey\u2019s New York office.\nAs with so many wonderful ideas, the execution left a lot to\nbe desired. I was assigned a mentor within my first week in the\noffice, a very nice thirty-something partner. He bought me lunch at\na trendy Italian eatery where supermodels popped in to nibble on\narugula leaves. We talked about working at the Firm and how best\nto climb its greasy pole to success; it was a pleasant and informa-\ntive 45 minutes. I saw him once after that. Within about six\nmonths, he transferred to Mexico to open a new office for the Firm\nsouth of the border.\nAfter that, I got lost in the shuffle for several months. Eventu-\nally I was reassigned to another mentor. Although he had a good\nreputation as a mentor, I was one of 9 or 10 \u201cmentees\u201d that he\nhad, and I got very little out of the relationship beyond the pro\nforma dissection of my performance reviews.\nSo, was I cast adrift in the Sea of McKinsey without a guide?\nHardly. I did what most McKinsey-ites do when they want to suc-\nceed in the Firm: I hitched my wagon to a star. I did most of my\nwork with one ED, the same ED who recruited me into the Firm."}
{"162": "We had a good relationship\u2014call it chemistry. When I needed\nadvice I couldn\u2019t get elsewhere, I went to him. He tried to get me\nassigned to his teams on studies where I had expertise. I was con-\nfident that, as long as I performed well for him, he would be in\nmy corner when it came to assignments, reviews, and promotions.\nMy experience was typical of most McKinsey-ites. How much\nyou benefited from your official mentor was pretty much a matter\nof luck. If you wanted guidance, you had to go out and get it.\nI believe that\u2019s a lesson that applies in almost any large orga-\nnization. Find someone senior to you whose abilities and opinion\nyou respect; seek out the mentor\u2019s advice. Many people like to give\nadvice and are happy to dispense it when asked. Of course, it helps\nif you get along well too. Work with the mentor, if possible, and\nlearn all you can. Don\u2019t go to the well too often, however; you\ndon\u2019t want to become a pest.\nWhatever setup your organization has, make sure you find\nyour own mentor. Having a guide you trust and respect will help\nyou make it through your own corporate jungle."}
{"163": "This page intentionally left blank."}
{"164": "15\nSURVIVING ON THE ROAD\nTraveling across the country (or the globe)\ncan take a lot out of you. Making travel an\nadventure will lighten your load. So will\nproper planning and a good attitude."}
{"165": "Although working at McKinsey offers many advantages\u2014good\npay, interesting work, high-caliber colleagues\u2014the working condi-\ntions can be grueling. On top of the long hours, which routinely\ninclude all-nighters, many McKinsey consultants spend most of\ntheir time on the road, away from home, family, and friends.\nSometimes business travel for the Firm can be fun: a week in\nLondon or Paris, and why not take the weekend skiing in the Alps?\nJust as often, however, working out of town only adds to the grind.\nThere is nothing quite so mind-numbing, as one former EM noted,\nas the \u201cIf it\u2019s Tuesday, this must be Davenport\u201d cross-country trips\na consultant has to make when visiting all of, say, a manufactur-\ning company\u2019s many plants across America. Even worse, you can\nfind yourself commuting 1000 miles every Monday morning (or\nSunday night) to some far-flung client, as happened to Hamish\nMcDermott, who spent six long, cold months in Detroit working\nfor one of the big car makers. That kind of travel takes a toll on\nyour health, your relationships, and your sanity.*\nMcKinsey-ites have developed a number of ways to cope with\nthe rigors of travel. They all agree on the importance of maintain-\ning a proper attitude. Abe Bleiberg says:\nTry to look on business travel as an adventure. Even if I\u2019m\nstuck in Flint, Michigan for three months over the winter,\nat least I can tell my grandchildren, \u201cI survived a winter in\nFlint.\u201d Not everyone can say that.\nJason Klein adds:\nAct like a tourist. Make the most of where you are. If you\u2019re\ndoing a project in Northern California and you\u2019re a golfer,\n*On a personal note, I was a rare exception in that I spent only one night away from home"}
{"166": "take an afternoon and play Pebble Beach. You can keep your\nnose to the grindstone for only so long.\nRemember that travel is an opportunity to do things outside\nyour normal realm of experience. Here\u2019s Abe Bleiberg again:\nTraveling as much as I did for McKinsey enabled me to meet\npeople whom I would never otherwise have met. For\ninstance, I once worked on a project where, in one meeting,\npeople sat around a table trying to market toilet tissue.\nNever in a million years would I have ever been involved in\nselling toilet paper! It\u2019s not something I\u2019d want to dedicate\nmy life to, but that\u2019s part of the fun of working at the Firm.\nAnother key to surviving on the road: proper planning. If pos-\nsible, schedule your time at the client to make sure you are at home\non Fridays or Mondays. Pack light; learn what you need to have\nwith you on the road, rather than what you think you need. If you\ncan help it, fly with hand luggage only; just don\u2019t assume the air-\nline will let you take that extra carry-on bag. If you\u2019re going to be\nin one place for a long time, find out if the hotel has a room where\nyou can store your extra bags when you leave for the weekend\u2014\nand make sure it\u2019s not the employee smoking room (as Adam Gold\nlearned the hard way!). Find a reliable cab company. If you\u2019re rent-\ning a car, make sure you have clear and accurate directions to your\ndestination. Otherwise, you might find yourself, as once happened\nto Hamish McDermott, coming off the interstate and onto the\nmeanest of Detroit\u2019s mean streets with no on-ramp in sight (that\u2019s\nthe sort of adventure you can do without).\nDon\u2019t let the travel and the work become all-consuming, espe-\ncially if you\u2019re out of town for a long time. Find a way to enter-"}
{"167": "have dinner with and catch a show or a ball game. At the very\nleast, when you get back to the hotel do something before you go\nto sleep\u2014whether it\u2019s working out, reading, or just watching tele-\nvision. Don\u2019t let being on the road become an uninterrupted cycle\nof working, eating and sleeping.\nFor one final survival tip I am indebted to Eric Hartz, now\npresident of Security First Network Bank. He says:\nTreat everyone with tremendous respect. Sometimes McKin-\nsey people can be demanding and impatient; then they fail to\nunderstand why they don\u2019t get what they want. Some of my\ncolleagues were amazed at how I would get upgraded, or\nwould get a bag on after the plane was full\u2014things like that.\nFlight attendants, concierges, assistants at clients\u2014these peo-\nple have more authority than you realize and want to help\nthose who show respect for them. It also keeps your stress\nlevel down\u2014it\u2019s easier to be friendly than frustrated\u2014so it\u2019s\na win/win.\nThat\u2019s possibly the best advice in this book."}
{"168": "16\nTAKE THESE THREE THINGS\nWITH YOU WHEREVER\nYOU GO\nNarrow your traveling needs down to the\nvery few things you must have with you\nwhen you leave. Here are a few (mostly\nserious) ideas."}
{"169": "Anybody who travels frequently, whether for business or for plea-\nsure, knows the three things you always take with you when trav-\neling abroad, the famous PTM: passport, tickets, money.\nWhenever I travel on business, I always make sure I have three\nadditional things with me: a copy of my itinerary, a list of the\nnames and numbers of everyone I\u2019m going to see, and a good\nbook. Since, as I\u2019ve said before, things at McKinsey usually come\nin threes, I asked the former McKinsey-ites I interviewed what\nthree things they always have with them when they travel.\nHere are some of the answers grouped by category (after all,\nthis is a McKinsey list), along with explanatory notes, where\nappropriate.\nClothing\n\u2022 An extra shirt or blouse\n\u2022 Spare ties for the men\n\u2022 Spare pair of comfortable flat shoes for the women\n\u2022 Casual clothes\n\u2022 Workout clothes (\u201cIt\u2019s easy to let your fitness slide when\nyou\u2019re on the road.\u201d)\n\u2022 A cashmere sweater for keeping warm and comfy on\novernight flights.\nTools\n\u2022 A writing pad\n\u2022 A pad of graph paper (for hand-drawing charts)\n\u2022 A copy of whatever you sent to the client\n\u2022 An HP 12C calculator (\u201cBetter than a Swiss Army knife,\nalthough not quite as impressive on a date.\u201d)\nPersonal Care Items"}
{"170": "\u2022 A mini-makeup kit for the women\n\u2022 Antacid tablets\n\u2022 A bottle of Tylenol\n\u2022 A big bottle of Tylenol\nThings to Keep You Organized and in Touch\n\u2022 A personal organizer\n\u2022 Credit cards (\u201cI keep them in a separate wallet.\u201d)\n\u2022 The OAGTM (or other airline time table)\n\u2022 A cell phone (\u201cIf I forget anything, I can just have it faxed.\u201d)\n\u2022 Directions to the client (so you don\u2019t end up in the wrong\npart of Detroit)\nDiversions\n\u2022 A good book\n\u2022 Press clippings to read on the plane\n\u2022 Books on tape, especially if your travel includes long\nstretches of driving\n\u2022 Video games on a laptop computer\nThe prize for the oddest answer has to go to a former McKin-\nsey-ite from the D\u00fcsseldorf office who listed Coca-Cola. (\u201cI trav-\neled quite a bit in Eastern Europe. I can now drink Coke warm,\ncold, or hot without blinking.\u201d) Perhaps that belongs under per-\nsonal care.\nIf these answers have a common theme, it\u2019s \u201cbe prepared.\u201d\nMake sure you\u2019re never caught short without something you really\nneed. That being the case, the prize for the best three items goes\nto a former associate in the Washington, DC office (who justifiably\nwishes to remain anonymous). Our hero spent much of his time\nconsulting in Brazil, where the weather, among other things, is\nunpredictable. This would-be Boy Scout\u2019s three indispensable"}
{"171": "This page intentionally left blank."}
{"172": "17\nA GOOD ASSISTANT IS A\nLIFELINE\nCall the position secretary, administrative\nassistant, or whatever. The person who\ntakes your messages; keeps your schedule;\ndoes your typing, duplicating, and filing;\nand performs a dozen other office tasks is\nan exceptionally valuable resource. Treat\nyour secretary well."}
{"173": "In McKinsey\u2019s New York office, the competition to hire good sec-\nretaries is as intense as that for top MBA graduates. Like any large\norganization, McKinsey would fall apart were it not for an efficient\ncadre of secretaries to handle the myriad administrative duties that\nthe consultants are unavailable, unwilling or\u2014frankly\u2014unable to\ndo. When consultants are on the road for much of the time, their\nsecretaries are the lifeline that ties them to the rest of the Firm.\nTo attract the best, the Firm provides a real career path for secre-\ntaries. New recruits usually start out working with four or five asso-\nciates. The good ones move on to work for SEMs; the best get claimed\nby partners and directors. Secretaries receive regular training, just like\nconsultants, and they even get their own \u201cretreat\u201d every year. But\nthere\u2019s more to the path than that. Many of the managers running the\nFirm\u2019s administrative and recruiting functions started out as secre-\ntaries; now they have positions of considerable power and responsi-\nbility. All this is designed to help McKinsey attract and retain the best\nsecretaries, just as it seeks to attract and retain the best consultants.\nA good secretary will perform numerous tasks that make a\nMcKinsey consultant\u2019s life easier. These range from the obvious, such\nas typing, filing, and duplicating, to the not-so-obvious: filling out\ntime sheets, paying credit card bills for consultants on long assign-\nments, and sending flowers to significant others after yet another\nmissed date. In fact, it is the less obvious tasks that really make a dif-\nference in a consultant\u2019s life. Most McKinsey-ites can do their own\ntyping, many handle their own filing, and anyone can run the copier\nin a pinch. But knowing that there is someone \u201cback home\u201d whom\nyou can trust to do those other, niggling little things that you would\nnormally do if you were not 500 miles from your apartment for the\nnext six months\u2014that\u2019s going to make your life easier!\nThe alternative is pretty ugly. I saw a number of associates"}
{"174": "days after they were taken; clients were upset by poor telephone\nmanners. One consultant, who was keeping two boyfriends in\nignorance of each other, had her cover blown when her secretary,\ninstead of saying she was in Houston all week, told boyfriend num-\nber 1 that she was at a lunch date with boyfriend number 2.\nAssociates at McKinsey have to take potluck with their secre-\ntaries. I was extremely lucky. Sandy, my secretary, was excellent\nfrom the start. Although I shared her with four other consultants,\nshe always came through for me. I always gave her top marks in her\nevaluations (this made me nervous, because I was afraid a partner\nwould poach her). I made a point of treating her well. This meant\nnot just giving her flowers on Secretary\u2019s Day and something nice at\nChristmas; it meant giving her the respect she deserved in her job\nand making her job as easy to perform as possible.\nI always tried to give my secretary clear instructions about what\nI wanted. I let her know where I was at all times during the day, so\nthat she could reach me with important news or let clients and other\nconsultants get in contact with me. Most important, I tried when-\never possible to give her a chance to show initiative and make her\nown decisions: in putting together presentations, in running my\nschedule, and in acting as my interface with other consultants. This\nmade ours a relationship from which we both benefited.\nThese days, of course, a lot of people do not have a full-time\nsecretary. Maybe they just have a temp who comes in for a few\nhours a week, or a junior team member who gets stuck with the\n\u201cgrunt work.\u201d The principle remains: Treat them well, be clear\nabout what you want, and give them room to grow. Sure, a temp\nwill never rise in the corporate ranks, but you will still get better\nwork out of him if you treat him with respect. The junior team\nmember, on the other hand, will benefit immensely from a bit of"}
{"175": "This page intentionally left blank."}
{"176": "18\nRECRUITING McKINSEY\nSTYLE: HOW TO DO IT\n(AND HOW TO GET THROUGH IT)\nMcKinsey looks for specific attributes in a\nrecruit. Here\u2019s how it finds them (and how\nyou can show the Firm you have them)."}
{"177": "One of McKinsey\u2019s goals, as listed in its mission statement, is \u201cto\nbuild a firm that is able to attract, develop, excite, motivate, and\nretain exceptional people.\u201d The first stage in reaching that objec-\ntive is recruiting the best possible candidates to join the Firm. As\nI\u2019ve written elsewhere, McKinsey tries to skim off the cream, the\nelite of the elite at the top business schools, as well as law schools\nand economics and finance graduate programs. The Firm also goes\nout of its way to recruit \u201cnontraditional\u201d candidates from outside\nthe realms of business academia: doctors, scientists, and politi-\ncians, among others.\nBecause the Firm takes recruiting so seriously, it commits seri-\nous resources to it\u2014probably more, proportionately, than any\nother business organization. Every top business school, for\ninstance, has its own team of McKinsey consultants assigned to it,\ncomplete with its own charge code for expenses. The expenses can\nadd up too\u2014sending four consultants from New York to Philadel-\nphia, putting them up for five days at the best hotel in town, and\ntaking out dozens of MBAs to fancy restaurants doesn\u2019t come\ncheap. Furthermore, the EM on the team makes recruiting a full-\ntime commitment; at McKinsey\u2019s hourly rate, that represents a very\nlarge opportunity cost!\nEven on a small scale, McKinsey doesn\u2019t pinch pennies. When\nKristin Asleson took a highly courted JD-MBA out to lunch in\nNew York, she took her to Le Cirque. Ivana Trump held court\nat a rear corner table. Walter Cronkite walked in. They nodded\nto each other. As Kristin recalls, \u201cWe both thought that was\npretty cool.\u201d\nWith all this heavy weaponry, the Firm hunts first and fore-\nmost for analytical ability. As one former recruiter told me:"}
{"178": "dence that they knew how to structure problems. I also\nlooked for business judgment, the sense that the person\nknew the implications of his solutions. That\u2019s why I always\nused cases.\nCases are the weapon of choice in a McKinsey interview. They\nrange from the prosaic\u2014stripped-down versions of actual\nMcKinsey cases\u2014to the whimsical or even weird. Examples:\n\u201cHow many gas stations are there in the United States?\u201d \u201cWhy are\nmanhole covers round?\u201d*\nIn a case interview, the interviewer wants to see how well the\ninterviewee can think about a problem, rather than how correctly\nshe answers it. As with most business problems, there is no one\ntrue answer. Rather, succeeding in a case interview requires break-\ning the problem into its component pieces, asking relevant ques-\ntions, and making reasonable assumptions when necessary.\nFor instance, when figuring out the number of gas stations in the\nUnited States, you might start by asking how many cars there are\nin the country. The interviewer might tell you the number, or might\nsay, \u201cI don\u2019t know. You tell me.\u201d Well, you say to yourself, the pop-\nulation of the US is about 275 million. If the average household size\n(including singles) is, you guess, 2.5 people, than your trusty calcu-\nlator** tells you that yields 110 million households. The reviewer\nnods in agreement. You recall hearing somewhere that the average\nhousehold has 1.8 cars (or was that children?), so the United States\nmust have 198 million cars. Now, if you can only figure out how\n*When I was joining the Firm, one of my interviewers posed this challenge: \u201cYou\u2019ve just\nbeen appointed special assistant to the Mayor of New York City. He wants to know how\nto make New York a better place. What do you tell him?\u201d Being a native Bostonian, I could\nhave said a lot of things (first of all, get rid of the Yankees and the Mets), but I concen-"}
{"179": "many gas stations it takes to serve 198 million cars, you\u2019ll have the\nproblem solved. What matters is not the numbers, but the method\nyou use to reach them. When I was asked this question in an inter-\nview, I was off by a factor of 3, as the interviewer later told me, but\nit didn\u2019t matter for the purpose of testing my analytical ability.\nThere\u2019s more to the successful would-be McKinsey-ite than just\nanalytical ability, however. McKinsey consultants work in teams,\nso personality counts too. As Abe Bleiberg put it:\nI assumed that most of the people who made it into the inter-\nview process were smart enough to work at the Firm. So I\ntried to answer the question: Did I really want to work with\nthis person? Quite often, I rejected superintelligent, nasty\npeople. One of the great joys for me was saying, \u201cHe\u2019s\nincredibly brilliant, and I wouldn\u2019t have him on my team for\na million bucks.\u201d\nBeyond a candidate\u2019s fit with the interviewer, there is also a\ncandidate\u2019s fit with the Firm. To discover that, the interviewer has\nto get beyond the r\u00e9sum\u00e9 and penetrate the polish. Given how slick\nmany candidates are, the process can be tough.\nFor instance, Hamish McDermott met with one would-be\nMcKinsey-ite from Harvard Business School. Hamish tried the typ-\nical opening interview gambit: \u201cSo, tell me a bit about yourself.\u201d\nHarvard Man proceeded to give a very structured, prepared, and\nlong spiel listing all his strengths, virtues, and life experiences.\nKnowing that he was hearing a script, Hamish interrupted him\nwith a question. \u201cHow would you characterize your analytical\nabilities?\u201d he asked. Not wishing to break the flow of his mono-\nlogue, Harvard Man replied, \u201cI\u2019ll get back to that question in 10\nminutes.\u201d As Hamish recalls, \u201cThat was not the response I was"}
{"180": "No doubt, many of you want to know how to get a job at\nMcKinsey. The answer is simple: Be of above average intelligence,\npossess a record of academic achievement at a good college and a\ntop business school, show evidence of achievement in all previous\njobs, and demonstrate extraordinary analytical ability. Simple to\nsay, but not simple to do.\nIf you manage to clear all those hurdles, the key to your joining\nthe Firm may be the case interview. I\u2019ve already talked about cases,\nbut I\u2019ll leave you with the best description of how to handle a case,\ncourtesy of Jason Klein:\nI always asked the same case. I wasn\u2019t looking for a particu-\nlar answer, but I wanted to see how people dealt with a com-\nplex problem in which a lot of information gets thrown at\nthem at once. Some people froze; others just dug deeper and\ndeeper. They were the people I recommended."}
{"181": "This page intentionally left blank."}
{"182": "19\nIF YOU WANT A LIFE, LAY\nDOWN SOME RULES\nWhen you work 80 hours or more per\nweek, after eating, sleeping, and (you hope)\npersonal hygiene, there\u2019s not much time left\nover for anything else. If you want a life,\nyou have to do a little advance work."}
{"183": "One especially bittersweet memory of my time at the Firm comes\nfrom a study I did for a Wall Street investment bank. My girlfriend\n(now my wife) worked as a portfolio strategist in the same build-\ning as my client, and she had a schedule just as punishing as mine.\nMany times during the five months of that study we shared a cab\nride home\u2014at 2 a.m.!\nWhen I asked former McKinsey-ites how they left room for a\nsocial life, many of them replied that they didn\u2019t. As one of them\ntold me, \u201cI didn\u2019t do a good job of it because I didn\u2019t make enough\nrules. I was too afraid of jeopardizing my career.\u201d The lesson he\nlearned (if only in hindsight) was that if you want a life when you\nwork crazy hours, then you have to lay down some rules.\nHours of discussion with former McKinsey-ites have yielded\nthree rules for a better life while at the Firm.\n\u2022 Make one day a week off-limits. Pick a day\u2014most people\ntake Saturday or Sunday\u2014and tell your boss (and yourself)\nthat you never work on that day unless it\u2019s an absolute\nemergency. Most bosses (at least in my experience) will\nrespect that most of the time. Make sure that you respect\nit too. Spend that day with your friends, your family, or\njust the Sunday papers. Keep your mind off work and\nrelax a bit.\n\u2022 Don\u2019t take work home. Keep work and home separate. If\nyou need to stay at the office for another hour, that\u2019s better\nthan coming home and ignoring your kids because you still\nhave work to do. Home should be a place where you can\nbe yourself.\n\u2022 Plan ahead. If you travel during the workweek this is the\nmost important rule. Don\u2019t come back from the airport on"}
{"184": "of mind, especially when you\u2019re single. If you want to do\nanything more than curl up with a good book, then you\nhave to arrange things in advance.\nRules offer the great advantage of letting everyone know what\nto expect\u2014your boss, your significant other, your kids, and you.\nOf course, it can sometimes be difficult to stick to even these very\nbasic rules. When your priorities are \u201cclient, Firm, you,\u201d some-\ntimes you have to let your life take a backseat to your career. That\nleads to my final rule:\n\u2022 When all else fails, have a doorman. Then, at least, you\u2019ll\ncome home to clean laundry."}
{"185": "This page intentionally left blank."}
{"186": "(cid:2)\n(cid:2)\nPART FIVE\nLIFE AFTER\nM KINSEY\nC\n(cid:2)"}
{"187": "(cid:2)\n(cid:2)\nAs one former McKinsey-ite told me, leaving McKin-\nsey is never a question of whether\u2014it\u2019s a question of\nwhen. We used to say that the half-life of a class of new\nassociates is about two years\u2014by the end of that time,\nhalf will have left the Firm. That was true in my time\nthere and still is today.\nThere is life after McKinsey, however. In fact, there\nmay be more life, since you are unlikely to work the\nsame hours at the same intensity in any other job. There\nis no doubt, however, that the vast majority of former\nMcKinsey-ites land on their feet. A quick scan through\nthe McKinsey Alumni Directory, which now contains\nsome 5000 names, reveals any number of CEOs, CFOs,\nsenior managers, professors, and politicians.\nAll those alumni carry with them memories of\nMcKinsey, of lessons learned, of goals reached or missed.\nThe Firm has left its mark on them as much as they left\ntheir marks on their clients. In these last few pages,\nI want to share some of those memories with you.\n(cid:2)"}
{"188": "20\nTHE MOST VALUABLE LESSON\nEveryone who enters McKinsey leaves it\nwith a slightly different impression.\nAlthough most former McKinsey-ites have\ndecidedly mixed views of the Firm, all would\nsay they learned important lessons there.\nIn writing this book, I had two goals. The first was to trans-\nmit some of the skills and techniques that make McKinsey\nand McKinsey-ites so successful at what they do. The second\ngoal was to convey to the outsider some idea of what it\u2019s like\nto work at McKinsey and with McKinsey people. By the time\nyou\u2019ve come this far in the book, I hope that I have succeeded\nin both."}
{"189": "As we reach the final pages, I wanted to share with you the\nanswers to a question that I asked of every former McKinsey-ite\nI communicated with: \u201cWhat was the most valuable lesson you\nlearned at the Firm?\u201d Some of the answers involve material\nyou\u2019ve already seen, at some length, in this book. Others describe\nlessons that cannot be taught\u2014only learned. Here, in their own\nwords, are the most valuable lessons learned by a baker\u2019s dozen\nof former McKinsey-ites:\nPreserve your integrity at all times. You will encounter\nany number of gray areas in business life\u2014always take\nthe high road. Do The Wall Street Journal test. If you are\ncomfortable with reading about your actions on the front\npage of the WSJ, then it\u2019s OK. If not, you are pushing the\nethical envelope\u2014don\u2019t.\n\u2014Eric Hartz, Atlanta/DC/Paris offices, 1986\u201395;\nnow president of Security First Network Bank, Atlanta\n(cid:2) (cid:2) (cid:2)\nConsulting is best thought of as a profession. Putting the\nclient first is the key to successful client service; to do so, you\nmust maintain your professional objectivity. It has allowed\nme to stand my ground when a client hasn\u2019t heard what he\nwants to hear, or to walk away when the client doesn\u2019t want\nto work with me, and has helped me focus on what truly\ndrives value for my clients.\n\u2014Jeff Sakaguchi, LA office, 1989\u201395;\nnow an associate partner at Andersen Consulting\n(cid:2) (cid:2) (cid:2)"}
{"190": "Focusing resources and eliminating hierarchy lead to supe-\nrior decision making. The Firm\u2019s clients had a hard time\ndoing either one when attempting major change.\n\u2014Former EM in the Cleveland office\n(cid:2) (cid:2) (cid:2)\nPersonally, the most valuable lesson I learned was humility.\nI joined McKinsey as a 24-year-old associate with a track\nrecord of uninterrupted success. For the first time, I was sur-\nrounded by people who were better prepared and more\nskilled than I was. Professionally, I learned to structure prob-\nlems so that they can be solved. The Firm taught me that\nevery problem has a solution; it may not be perfect, but it\nwill allow me to take actions that are directionally correct.\n\u2014Wesley Sand, Chicago office, 1993\u201396\n(cid:2) (cid:2) (cid:2)\nI can\u2019t point to any one thing. It has to do with problem solv-\ning\u2014the idea that any problem, no matter how daunting, can\nbe broken into its constituents and solved. The other thing is\nthat there is nothing new under the sun. Whatever you\u2019re\ndoing, someone else has done it before\u2014find that person.\n\u2014Suzanne Tosini, New York and Caracas offices, 1990\u201395;\nnow a senior manager at Freddie Mac\n(cid:2) (cid:2) (cid:2)\nThe one firm concept. No stars, just meritocracy. That culture\nis extremely strong within the Firm, and I think it can work in\nother organizations too. I\u2019m implementing it in mine.\n\u2014Gresham Brebach, former DCS in the New York office;\nnow president and CEO of Nextera Enterprises"}
{"191": "Execution and implementation are the key. A blue book is\njust a blue book, unless you do something with it. Getting\nthings done is the most important thing.\n\u2014Former EM in the New York office\n(cid:2) (cid:2) (cid:2)\nI learned to value strongly honesty and integrity in busi-\nness\u2014this is something McKinsey inculcates in its people\nand insists upon.\n\u2014Hamish McDermott, New York office, 1990\u201394;\nnow a senior manager in a Wall Street investment bank\n(cid:2) (cid:2) (cid:2)\nDon\u2019t fear end products. They go a long way. Generate end\nproducts.\n\u2014Former associate in the New York office\n(cid:2) (cid:2) (cid:2)\nWhen faced with an amorphous situation, apply structure\nto it.\n\u2014Kristin Asleson, New York office, 1990\u201393; now work-\ning in Silicon Valley\n(cid:2) (cid:2) (cid:2)\nI think anything I say would be too cynical.\n\u2014Former associate in the London office\n(cid:2) (cid:2) (cid:2)\nI would rather be surrounded by smart people than have a\nhuge budget. Smart people will get you there faster."}
{"192": "You may be asking yourself, with some justification, what my most\nvaluable lesson was. I\u2019ve had a lot of time to think about that one.\nHere it goes:\nAnything that gets in the way of efficient communication is\nanathema to a strong organization. Fuzzy thinking, obfusca-\ntory jargon, impenetrable hierarchy, and playing the \u201cyes-\nman\u201d get in the way of adding value for customers or clients.\nStructured thinking, clear language, a meritocracy with the\nobligation to dissent, and professional objectivity allow an\norganization and its people to reach their maximum poten-\ntial. Of course, McKinsey has its own word for this\u2014it\u2019s\ncalled \u201cprofessionalism.\u201d\n\u2014Ethan M. Rasiel, New York office, 1989\u201392"}
{"193": "This page intentionally left blank."}
{"194": "21\nMEMORIES OF McKINSEY\nThe Firm leaves its alumni with vivid mem-\nories. Here is a sampling.\nI asked former McKinsey-ites not only for their most valuable les-\nson, but also for the thing they remember most about the Firm.\nAlthough this selection shows that McKinsey alumni took away\nmany memories, the strongest ones had to do with the people that\nmake the Firm what it is:\nWhat stays with me is the rigorous standard of information\nand analysis, the proving and double-proving of every rec-\nommendation, combined with the high standard of commu-\nnication both to clients and within the Firm.\n\u2014Former associate in the Boston and New York offices\n(cid:2) (cid:2) (cid:2)\nThe thing I remember most is the very high performance\nstandards and the relentless drive for excellence that you see\namong the people there. It\u2019s not something you will neces-\nsarily find outside the Firm. There\u2019s an attitude within the\nFirm that says, \u201cIf there\u2019s a problem, give us the resources\nand we\u2019ll solve it. We\u2019ll just go out and do it.\u201d Outside the\nFirm, you often run into the attitude that says, \u201cIt can\u2019t be\ndone,\u201d and that\u2019s just not acceptable at McKinsey."}
{"195": "The thing that I remember most, that I enjoyed most, was\nthe team problem solving. I enjoyed the power of thinking in\na small group of very smart people.\n\u2014Abe Bleiberg, Washington, DC office, 1990\u201396; now a\nvice president at Goldman Sachs\n(cid:2) (cid:2) (cid:2)\nStructure, structure, structure. MECE, MECE, MECE.\nHypothesis-driven, hypothesis-driven, hypothesis-driven.\n\u2014Former associate in the D\u00fcsseldorf and\nSan Francisco offices\n(cid:2) (cid:2) (cid:2)\nI have incredibly fond memories of the people at the Firm.\nThere is a density of high-quality, smart, motivated people\nthat I haven\u2019t found anywhere else.\n\u2014Hamish McDermott\n(cid:2) (cid:2) (cid:2)\nThe caliber of the people McKinsey recruits and retains\nthroughout the organization\u2014and not just in the consult-\ning function.\n\u2014Former associate in the New York office\n(cid:2) (cid:2) (cid:2)\nThe people. On average, they are smart and fun to be with.\n\u2014Former EM in the New York office\n(cid:2) (cid:2) (cid:2)\nThe average mental capacity of the staff, be it the newest\nassociate or the most senior director, and the approachabil-"}
{"196": "The collegiate atmosphere. The thing I miss most about\nMcKinsey is the canteen, not so much because the food was\ngood, but because you could always take time out for an\ninteresting conversation.\n\u2014Former associate in the London office\n(cid:2) (cid:2) (cid:2)\nThe quality of the people. In the corporate world, the\naverage-caliber employee is far below McKinsey\u2019s least\nintelligent.\n\u2014Wesley Sand\n(cid:2) (cid:2) (cid:2)\nThe people, the wide range of people one came in contact\nwith, both at the Firm and at the clients. McKinsey consul-\ntants shared a dedication to client service and a concern for\nthe needs of the client.\n\u2014Suzanne Tosini\n(cid:2) (cid:2) (cid:2)\nOne of the advantages of being an author is that, at least within the\nconfines of your own work, you get to have the last word. Where\npossible, I have tried to illustrate the key points in this book with\nstories from my experiences and those of former McKinsey-ites.\nFor my last story, I\u2019m going to reach a bit farther back than that.\nOne day in ancient Israel, a Gentile came to the great rabbi\nShammai and asked of him, \u201cTeach me the Law while I stand on\none foot.\u201d Although he was a great scholar, Shammai, was not\nknown for his patience; he called the Gentile impudent and chased\nhim away. The Gentile then took his question to Shammai\u2019s great"}
{"197": "rabbi\u2019s study, Hillel told him, \u201cDo unto others as you would have\nothers do unto you. The rest derives from that. Go and learn.\u201d\nWhat does this story have to do with McKinsey and with\nyour career or business at the turn of the millenium? I\u2019m not Hil-\nlel by a long stretch and the McKinsey way is not by any means\nholy writ. Still, it contains an essential core. To wit: Fact-based,\nstructured thinking combined with professional integrity will get\nyou on the road to your business goals. The rest derives from\nthat. Go and learn."}
{"198": "(cid:2)\n(cid:2)\nINDEX\n(cid:2) (cid:2)"}
{"199": "This page intentionally left blank."}
{"200": "A\nBoss:\nActionable recommendations, communicating with, 120\n10\u201311 enhancing image of, 67\u201368\nAdministrative assistants, 153\u2013155 of interviewee, 83\nAgenda(s): Brainstorming, 93\u2013101\nand client engagement, 133 exercises for, 100\u2013101\nmeeting, 121 preparation for, 95\u201396\nAnnual reports, 74\u201375 \u201crules\u201d for, 96\u2013100\nAnxiety, interviewee, 86\u201388 Brevity (of messages), 122\nApproach, development of, 15\u201328 Business travel (seeTravel)\nwith difficult problems, 24\u201328 Buy-in, getting, 134\u2013135\nand identification of the\nproblem, 15\u201316\nC\nand initial hypothesis, 21\u201322\ntool kit for, 17\u201319 Case interviews, 159\u2013161\nand unique client characteristics, Chain of command, McKinsey,\n19\u201320, 22\u201324 65\u201366\nAssertiveness, 69 Charts, 113\u2013118\nAssistants, 153\u2013155 daily, 37\u201338\nAssociates, McKinsey, 5 simplicity in, 114\u2013116\nAT&T, 57 waterfall, 117\u2013118\nClient teams, 127\u2013137\nengagement of, 133\u2013134\nB\ngetting buy-in from, 134\u2013135\n\u201cBellyaches up front\u201d technique, goals of, 129\u2013130\n100\u2013101 \u201cliability\u201d members of, 130\u2013132\nBest practices, 75\u201376 thoroughness with, 136\u2013137\nBig picture, looking at the, 41\u201342 Client(s):\nBill Clinton approach, 64 \u201cprewiring,\u201d 109\u2013111\nBlue books, 34, 107, 172 as priority, 170"}
{"201": "Clothing, travel, 150 Engagement managers (EMs)\nColumbo tactic, 85\u201386 (Cont.):\nCommunication(s): and team morale, 63, 64\nimportance of efficient, 173 team selection by, 59\ninternal, 119\u2013125 Engagement(s):\nConfidentiality, 123\u2013125 first day of, 4\nCredibility, maintaining, 40\u201341 structuring, 53\u201354\nCredibility gap, 5 team, 133\u2013134\nCulture, McKinsey, 114, 171 Entertainment, 147\u2013148\nD F\nDaily charts, 37\u201338 Fact packs, 93\nData, researching, 71\u201376 Facts:\nDatastream, 72 arriving at solution from, 24\u201325\nDCS (director of client services), 53n importance of, 4\u20135\nDifficult interviews, 88\u201390 Failure, 41\nDifficult problems, solving, 24\u201328 Flipchart exercise, 100\nDirector of client services (DCS), Flipcharts, 99\u2013100\n53n Forces at Work, 18\nDun & Bradstreet, 72\nG\nE\nGrunt, McKinsey, 81\nEDs (seeEngagement directors) Gut instincts, 4, 5\n80/20 rule, 30\u201331\nElevator test, 34\u201335\nH\nEMs (seeEngagement managers)\nEnd products, 172 Hierarchy, 65\u201369\nEngagement directors (EDs), 42n aggressive strategy for managing,\nas bosses, 67 68\u201369\nbrainstorming by, 93 eliminating, 171\nchoosing of associates by, 60 and pleasing the boss, 67\u201368\nin McKinsey hierarchy, 65\u201366 High-pressure organizations,\nas mentors, 142\u2013143 surviving at (seeSurviving\nand presentation changes, 108 at McKinsey)\nstructuring of engagement by, Hillel, Rabbi, 177\u2013178\n53\u201354 \u201cHitting singles,\u201d 39\u201341\nteam selection by, 59 Home, separating work and, 164\nEngagement managers (EMs), 2n Honesty, 42, 172\nas bosses, 67\nbrainstorming by, 93\nI\nchoosing of associates by, 60"}
{"202": "IH (seeInitial hypothesis) Listening (during interviews),\nImplementation, 136\u2013137, 172 81\u201383, 83\u201384\nIn Search of Excellence(Peters and Low-hanging fruit, plucking the,\nWaterman), 51 36\u201337, 133\nInformal marketing, 50\u201352\nInitial hypothesis (IH), 8\u201313, 21\u201322\nM\ndefining, 9\u201310\ngenerating, 10\u201312 Marketing, informal, 50\u201352\ntesting, 13 The McKinsey Quarterly,51\nIntegrity, professional, 42, 172 MECE, 6\u20138\nInternal communications, 119\u2013125 Meetings, 121\nand confidentiality, 123\u2013125 Memories of McKinsey, 175\u2013178\nand flow of information, Mentor, finding a, 141\u2013143\n120\u2013121 Messages, 121\u2013123\nkeys to effective, 122\u2013123 \u201cMix,\u201d team, 59\u201360\nInternet, 72 Morale, team, 62\u201364\nInterviews, 77\u201392 Mushroom Method, 120\nanxiety of interviewees in, 86\u201388\ncase, 159\u2013161\nO\ndifficult, 88\u201390\nindirect approach to, 84\u201385 \u201cOff-limits\u201d day, 164\nlistening and guiding during, Ohmae, Kenichi, 51\n81\u201383 Open-ended questions, asking, 84\npreparing for, 79\u201380 Other Issues, 8\nthank-you notes as follow-up Outliers, 75\nto, 91\u201392\ntips for successful, 83\u201386\nP\nIssue list, 6\u20138\nIssue tree, 12 Paraphrasing, 84\nPartners (seeEngagement directors)\nPD (seePractice development)\nJ\nPDNet, 72\u201373, 93\nJapanese (language), 81n Perfectionism, 107\u2013108\nPerformance standards, 175\nK Personal care items (for travel),\n150\u2013151\nKey drivers, 10\u201311, 33\u201334\nPeters, Tom, 51\nPlucking the low-hanging fruit,\nL 36\u201337, 133\nLeaders, meeting, 121 Politics, working through, 28\n\u201cLeads,\u201d chart, 115 Post-it exercise, 100\nLeakage, 21 Practice development (PD),\nLessons of working at McKinsey, 68\u201369"}
{"203": "S\nPreparation: (Cont.):\nfor interviewing, 79\u201380 Sandbaggers, 90\nPresentations, 105\u2013111 Say It With Charts(Gene Zelazny),\nperfectionist approach to, 113\n107\u2013108 Secretaries, 153\u2013155\npreparing client for, 109\u2013111 \u201cSelling\u201d the study, 49\u201355, 104\u2013137\nstructure of, 106\u2013107 with charts, 113\u2013118\n\u201cPrewiring\u201d clients, 109\u2013111 and flow of communications,\nProblem solving, 2, 171 119\u2013125\nProcter & Gamble, 35 and informal marketing, 50\u201352\nProfessional integrity, 42, 172 with presentations, 105\u2013111\nProfessional objectivity, and promises to client, 53\u201355\nmaintaining, 170 and working with client teams,\nProfessionalism, 173 127\u2013137\nProfit and loss statement, SEMs (seeSenior engagement\nre-creating competitor\u2019s, managers)\n44\u201345 Senior engagement managers\nPromises to client, making, 53\u201355 (SEMs), 4n,97, 154\n\u201cPulling rank,\u201d 89 Silence, 82\u201383\nSimplicity:\nin charts, 114\u2013116\nQ\nand key drivers, 33\nQuestions: Social life, rules for maintaining,\nin brainstorming sessions, 98 163\u2013165\nopen-ended, 84 Square Law of Computation, 33\nStructure:\napplying, 172\nR\nof engagements, 53\u201354\nRandom encounters, 121 of messages, 123\nRecommendations, actionable, of presentations, 106\u2013107\n10\u201311 Surviving at McKinsey, 140\u2013165\nRecruiting, 157\u2013161 business travel, 145\u2013151\nallocating resources for, 158 and importance of a good\nand analytical ability of secretary, 153\u2013155\ncandidates, 158\u2013159 mentor, finding a, 141\u2013143\nand \u201cfit\u201d of candidates, 160 recruiting, 157\u2013161\nRedefining the problem, 27 rules for, 163\u2013165\nReinventing the wheel, 17\u201319,\n72\u201374\nT\nResearch, 71\u201376\n\u201csmart,\u201d 72\u201374 \u201cTag-teams,\u201d 83\ntips for conducting, 74\u201376 Teams, 13\nResources, focusing, 171 assessing morale of, 62\u201364"}
{"204": "Teams (Cont.): Travel (Cont.):\nengagement of, 133\u2013134 treating others with respect\ngetting buy-in from, 134\u2013135 during, 148\ngoals of, 129\u2013130 Tweaking, 28\n\u201cliability\u201d members of,\n130\u2013132\nU\nMcKinsey, 57\u201364\n\u201cmix\u201d of, 59\u201360 Unique client characteristics,\nthoroughness with, 136\u2013137 19\u201320, 22\u201324\nThank-you notes, 91\u201392 Unrealistic expectations, creating,\nThoroughness, 136\u2013137 40\nThree (as magic number), 3n\nTool kit, 17\u201319\nW\nTools, travel, 150\nTrade publications, 10 The Wall Street Journaltest, 170\nTravel, 145\u2013151 Waterfall chart, 117\u2013118\nentertainment during, 147\u2013148 Waterman, Robert H. Jr., 51\nitems for, 149\u2013151 White boards, 99\nmaintaining proper attitude for,\n146\u2013147\nZ\nplanning for, 147, 151,\n164\u2013165 Zelazny, Gene, 113"}
{"205": "This page intentionally left blank."}
{"206": "ABOUT THE AUTHOR\nEthan M. Rasiel joined McKinsey & Company\u2019s New York\noffice in 1989 and worked there until 1992. While at \u201cthe\nFirm,\u201d his clients included major companies in the finance,\ntelecommunications, computing, and consumer goods sectors. He\nhas also worked as an investment banker and an equity fund man-\nager. He has a bachelor\u2019s degree from Princeton and an MBA from\nWharton. He now lives with his wife and family in Chapel Hill,\nNorth Carolina."}
{"207": "This page intentionally left blank."}
